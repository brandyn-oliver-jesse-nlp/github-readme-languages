{"repo":{"0":"janishar\/mit-deep-learning-book-pdf","1":"Angel-ML\/angel","2":"Alluxio\/alluxio","3":"haifengl\/smile","4":"alibaba\/Alink","5":"ICT-BDA\/EasyML","6":"OryxProject\/oryx","7":"kermitt2\/grobid","8":"SeldonIO\/seldon-server","9":"MindorksOpenSource\/AndroidTensorFlowMachineLearningExample","10":"o19s\/elasticsearch-learning-to-rank","11":"datumbox\/datumbox-framework","12":"oracle\/tribuo","13":"mimno\/Mallet","14":"imageprocessor\/cv4j","15":"kaiwaehner\/kafka-streams-machine-learning-examples","16":"EdwardRaff\/JSAT","17":"amitshekhariitbhu\/Android-TensorFlow-Lite-Example","18":"apache\/submarine","19":"Waikato\/moa","20":"antlr\/codebuff","21":"pmerienne\/trident-ml","22":"etsy\/Conjecture","23":"hanuor\/onyx","24":"yuantiku\/ytk-learn","25":"linkedin\/dagli","26":"opendistro-for-elasticsearch\/k-NN","27":"pushkar\/ABAGAIL","28":"ShifuML\/shifu","29":"Waikato\/meka","30":"HongZhaoHua\/jstarcraft-ai","31":"cheng-li\/pyramid","32":"sea-boat\/TextAnalyzer","33":"afsalashyana\/FakeImageDetection","34":"RumbleDB\/rumble","35":"ogrisel\/pignlproc","36":"projectmatris\/antimalwareapp","37":"kermitt2\/entity-fishing","38":"SmartDataAnalytics\/DL-Learner","39":"yinlou\/mltk","40":"apache\/flink-ml","41":"Daniel-Liu-c0deb0t\/Java-Machine-Learning","42":"algorithmfoundry\/Foundry","43":"ClearTK\/cleartk","44":"uea-machine-learning\/tsml","45":"wen-fei\/choice","46":"goessl\/MachineLearning","47":"Nova41\/SnowLeopard","48":"amidst\/toolbox","49":"anbud\/MarkovComposer","50":"tensorflow\/tensorflow","51":"TheAlgorithms\/C-Plus-Plus","52":"PaddlePaddle\/Paddle","53":"microsoft\/LightGBM","54":"onnx\/onnx","55":"davisking\/dlib","56":"apple\/turicreate","57":"bulletphysics\/bullet3","58":"VowpalWabbit\/vowpal_wabbit","59":"tensorflow\/serving","60":"interpretml\/interpret","61":"amazon-archives\/amazon-dsstne","62":"flashlight\/flashlight","63":"mlpack\/mlpack","64":"aksnzhy\/xlearn","65":"ARM-software\/ComputeLibrary","66":"facebookresearch\/TensorComprehensions","67":"ermig1979\/Simd","68":"4paradigm\/OpenMLDB","69":"IBM\/fhe-toolkit-linux","70":"microsoft\/EdgeML","71":"ml4a\/ml4a-ofx","72":"dmlc\/mshadow","73":"novak-99\/MLPP","74":"Samsung\/veles","75":"turi-code\/SFrame","76":"node-tensorflow\/node-tensorflow","77":"microsoft\/Windows-Machine-Learning","78":"dmlc\/dmlc-core","79":"microsoft\/Multiverso","80":"jubatus\/jubatus","81":"nladuo\/captcha-break","82":"niessner\/Matterport","83":"neoml-lib\/neoml","84":"NVIDIA\/nvvl","85":"fastmachinelearning\/hls4ml","86":"StanfordSNR\/puffer","87":"bytefish\/opencv","88":"perone\/euclidesdb","89":"mldbai\/mldb","90":"PaddlePaddle\/Serving","91":"deepmind\/reverb","92":"dmlc\/rabit","93":"pennyliang\/MachineLearning-C---code","94":"memo\/ofxMSATensorFlow","95":"neo-ai\/neo-ai-dlr","96":"FidoProject\/Fido","97":"chanyn\/3Dpose_ssl","98":"NVIDIA\/tensorflow","99":"alibaba\/BladeDISC","100":"huggingface\/transformers","101":"josephmisiti\/awesome-machine-learning","102":"scikit-learn\/scikit-learn","103":"fighting41love\/funNLP","104":"eriklindernoren\/ML-From-Scratch","105":"RasaHQ\/rasa","106":"aleju\/imgaug","107":"gunthercox\/ChatterBot","108":"mlflow\/mlflow","109":"microsoft\/nni","110":"ddbourgin\/numpy-ml","111":"PySimpleGUI\/PySimpleGUI","112":"ml-tooling\/best-of-ml-python","113":"rushter\/MLAlgorithms","114":"EpistasisLab\/tpot","115":"ludwig-ai\/ludwig","116":"clips\/pattern","117":"gradio-app\/gradio","118":"instillai\/machine-learning-course","119":"mindsdb\/mindsdb","120":"lazyprogrammer\/machine_learning_examples","121":"pymc-devs\/pymc","122":"automl\/auto-sklearn","123":"doccano\/doccano","124":"Jack-Cherish\/Machine-Learning","125":"scikit-learn-contrib\/imbalanced-learn","126":"alan-turing-institute\/sktime","127":"humphd\/have-fun-with-machine-learning","128":"kootenpv\/whereami","129":"mdbloice\/Augmentor","130":"lawlite19\/MachineLearning_Python","131":"ujjwalkarn\/DataSciencePython","132":"wepe\/MachineLearning","133":"rasbt\/mlxtend","134":"wandb\/client","135":"DistrictDataLabs\/yellowbrick","136":"aladdinpersson\/Machine-Learning-Collection","137":"online-ml\/river","138":"cleanlab\/cleanlab","139":"arielf\/weight-loss","140":"feast-dev\/feast","141":"Avik-Jain\/100-Days-of-ML-Code-Chinese-Version","142":"TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials","143":"polyaxon\/polyaxon","144":"uber\/causalml","145":"Trusted-AI\/adversarial-robustness-toolbox","146":"nidhaloff\/igel","147":"hudson-and-thames\/mlfinlab","148":"kubeflow\/pipelines","149":"ml-tooling\/opyrator"},"language":{"0":"Java","1":"Java","2":"Java","3":"Java","4":"Java","5":"Java","6":"Java","7":"Java","8":"Java","9":"Java","10":"Java","11":"Java","12":"Java","13":"Java","14":"Java","15":"Java","16":"Java","17":"Java","18":"Java","19":"Java","20":"Java","21":"Java","22":"Java","23":"Java","24":"Java","25":"Java","26":"Java","27":"Java","28":"Java","29":"Java","30":"Java","31":"Java","32":"Java","33":"Java","34":"Java","35":"Java","36":"Java","37":"Java","38":"Java","39":"Java","40":"Java","41":"Java","42":"Java","43":"Java","44":"Java","45":"Java","46":"Java","47":"Java","48":"Java","49":"Java","50":"C++","51":"C++","52":"C++","53":"C++","54":"C++","55":"C++","56":"C++","57":"C++","58":"C++","59":"C++","60":"C++","61":"C++","62":"C++","63":"C++","64":"C++","65":"C++","66":"C++","67":"C++","68":"C++","69":"C++","70":"C++","71":"C++","72":"C++","73":"C++","74":"C++","75":"C++","76":"C++","77":"C++","78":"C++","79":"C++","80":"C++","81":"C++","82":"C++","83":"C++","84":"C++","85":"C++","86":"C++","87":"C++","88":"C++","89":"C++","90":"C++","91":"C++","92":"C++","93":"C++","94":"C++","95":"C++","96":"C++","97":"C++","98":"C++","99":"C++","100":"Python","101":"Python","102":"Python","103":"Python","104":"Python","105":"Python","106":"Python","107":"Python","108":"Python","109":"Python","110":"Python","111":"Python","112":"Python","113":"Python","114":"Python","115":"Python","116":"Python","117":"Python","118":"Python","119":"Python","120":"Python","121":"Python","122":"Python","123":"Python","124":"Python","125":"Python","126":"Python","127":"Python","128":"Python","129":"Python","130":"Python","131":"Python","132":"Python","133":"Python","134":"Python","135":"Python","136":"Python","137":"Python","138":"Python","139":"Python","140":"Python","141":"Python","142":"Python","143":"Python","144":"Python","145":"Python","146":"Python","147":"Python","148":"Python","149":"Python"},"readme_contents":{"0":"[![Download](https:\/\/img.shields.io\/badge\/download-bookmarked%20book-orange.svg)](https:\/\/github.com\/janishar\/mit-deep-learning-book-pdf\/blob\/master\/complete-book-pdf\/Ian%20Goodfellow%2C%20Yoshua%20Bengio%2C%20Aaron%20Courville%20-%20Deep%20Learning%20(2017%2C%20MIT).pdf)\n[![Download](https:\/\/img.shields.io\/badge\/download-book-brightgreen.svg)](https:\/\/github.com\/janishar\/mit-deep-learning-book-pdf\/raw\/master\/complete-book-pdf\/deeplearningbook.pdf)\n\n# MIT Deep Learning Book (beautiful and flawless PDF version)\nMIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n\n# If this repository helps you in anyway, show your love :heart: by putting a :star: on this project :v:\n\n# Deep Learning\nAn MIT Press book\nIan Goodfellow and Yoshua Bengio and Aaron Courville\n\nThis is the most comprehensive book available on the deep learning and available as free html book for reading at http:\/\/www.deeplearningbook.org\/\n\n**Comment on this book by Elon Musk**\n>Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.\" -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceX\n\n**This is not available as PDF download. So, I have taken the prints of the HTML content and binded into a flawless PDF version of the book, as suggested by the website itself**\n\n**http:\/\/www.deeplearningbook.org\/ says:**\n>What is the best way to print the HTML format?\n\n>Printing seems to work best printing directly from the browser, using Chrome. Other browsers do not work as well.\n\n**This repository contains**\n1. The pdf version of the book which is available in html at http:\/\/www.deeplearningbook.org\/\n2. The book is available in chapter wise PDFs as well as complete book in PDF.\n\n**Some useful links for this learning:**\n1. [Exercises](http:\/\/www.deeplearningbook.org\/exercises.html)\n2. [Lecture Slides](http:\/\/www.deeplearningbook.org\/lecture_slides.html)\n3. [External links](http:\/\/www.deeplearningbook.org\/external.html)\n\n*If you like this book then buy a copy of it and keep it with you forever. This will help you and also support the authors and the people involved in the effort of bringing this beautiful piece of work to public. Buy it from amazon, It is not expensive ($72). [Amazon](https:\/\/www.amazon.com\/Deep-Learning-Adaptive-Computation-Machine\/dp\/0262035618)*\n\n\n\n```\nAn MIT Press book\n\nIan Goodfellow, Yoshua Bengio and Aaron Courville\n\nThe Deep Learning textbook is a resource intended to help students and practitioners\nenter the field of machine learning in general and deep learning in particular. \nThe online version of the book is now complete and will remain available online for free. \n\nCiting the book\n\nTo cite this book, please use this bibtex entry:\n\n@book{Goodfellow-et-al-2016,\n    title={Deep Learning},\n    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},\n    publisher={MIT Press},\n    note={\\url{http:\/\/www.deeplearningbook.org}},\n    year={2016}\n}\n```\n\n\n","1":"![](assets\/angel_logo.png)\n\n[![license](http:\/\/img.shields.io\/badge\/license-Apache2.0-brightgreen.svg?style=flat)](https:\/\/github.com\/Angel-ML\/angel\/blob\/branch-3.2.0\/LICENSE.TXT)\n[![Release Version](https:\/\/img.shields.io\/badge\/release-3.1.0-red.svg)](https:\/\/github.com\/tencent\/angel\/releases)\n[![PRs Welcome](https:\/\/img.shields.io\/badge\/PRs-welcome-brightgreen.svg)](https:\/\/github.com\/tencent\/angel\/pulls)\n[![Download Code](https:\/\/img.shields.io\/badge\/download-zip-green.svg)](https:\/\/github.com\/Angel-ML\/angel\/archive\/refs\/heads\/branch-3.2.0.zip)\n\n[(ZH-CN Version)](.\/README_CN.md)\n\n**Angel** is a high-performance distributed machine learning and graph computing platform based on the philosophy of Parameter Server. It is tuned for performance with big data from Tencent and has a wide range of applicability and stability, demonstrating increasing advantage in handling higher dimension model. Angel is jointly developed by Tencent and Peking University, taking account of both high availability  in industry and innovation in academia.\n\nWith model-centered core design concept, **Angel** partitions parameters of complex models into multiple parameter-server nodes, and implements a variety of machine learning algorithms and graph algorithms using efficient model-updating interfaces and functions, as well as flexible consistency model for synchronization.\n\n**Angel** is developed with **Java** and **Scala**.  It supports running on **Yarn**. With **PS Service** abstraction, it supports **Spark on Angel**.  Graph computing and deep learning frameworks support is under development and will be released in the future.\n\nWe welcome everyone interested in machine learning or graph computing to contribute code, create issues or pull requests. Please refer to  [Angel Contribution Guide](https:\/\/github.com\/Tencent\/angel\/blob\/master\/CONTRIBUTING.md) for more detail.\n\n## Introduction to Angel\n\n* [Architecture](.\/docs\/overview\/architecture_en.md)\n* [Code Framework](.\/docs\/overview\/code_framework_en.md)\n* [Design](.\/docs\/overview\/design_philosophy_en.md)\n* [Spark on Angel](.\/docs\/overview\/spark_on_angel_en.md)\n  * [Machine Learning](.\/docs\/overview\/spark_on_angel_mllib.md)\n  * [Graph Computing](.\/docs\/overview\/angel_graph_sona.md)\n\n## Design\n\n- [Model Partitioner](.\/docs\/design\/model_partitioner_en.md)\n- [SyncController](.\/docs\/design\/sync_controller_en.md)\n- [psFunc](.\/docs\/design\/psfFunc_en.md)\n- [Core API](.\/docs\/apis\/core_api_en.md)\n\n\n## Quick Start\n* [Quick Start](.\/docs\/tutorials\/angel_ps_quick_start_en.md)\n* [Spark on Angel Quick Start](.\/docs\/tutorials\/spark_on_angel_quick_start_en.md)\n\n\n## Programming Guide\n\n* [Angel Programming Guide](.\/docs\/programmers_guide\/angel_programing_guide_en.md)\n* [Spark on Angel Programming Guide](.\/docs\/programmers_guide\/spark_on_angel_programing_guide_en.md)\n\n## Algorithm\n\n- [**Angel or Spark On Angel\uff1f**](.\/docs\/algo\/angel_or_spark_on_angel.md)\n- [**Algorithm Parameter Description**](.\/docs\/algo\/model_config_details.md)\n- **Angel**\n  - **Traditional Machine Learning Methods**\n    - [Logistic Regression(LR)](.\/docs\/algo\/lr_on_angel_en.md)\n    - [Support Vector Machine(SVM)](.\/docs\/algo\/svm_on_angel_en.md)\n    - [Factorization Machine(FM)](.\/docs\/algo\/fm_on_angel.md)\n    - [Linear Regression](.\/docs\/algo\/linear_on_angel_en.md)\n    - [Robust Regression](.\/docs\/algo\/robust_on_angel_en.md)\n    - [Softmax Regression](.\/docs\/algo\/softmax_on_angel_en.md)\n    - [KMeans](.\/docs\/algo\/kmeans_on_angel_en.md)\n    - [GBDT](.\/docs\/algo\/gbdt_on_angel_en.md)\n    - [LDA\\*](.\/docs\/algo\/lda_on_angel_en.md) ([WarpLDA](.\/docs\/algo\/warp_lda_on_angel.md))\n- **Spark on Angel**\n  - **Angel-Mllib**\n    - [FM](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/recommendation.md)\n    - [DeepFM](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/recommendation.md)\n    - [DeepAndWide](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/recommendation.md)\n    - [DCN](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/recommendation.md)\n    - [XDeepFM](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/recommendation.md)\n    - [AttentionFM](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/recommendation.md)\n    - [PNN](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/recommendation.md)\n    - [FTRL](.\/docs\/algo\/ftrl_lr_spark.md)\n    - [Logistic Regression(LR)](.\/docs\/algo\/sona\/lr_sona.md)\n    - [FTRLFM](.\/docs\/algo\/ftrl_fm_spark_en.md)\n    - [GBDT](.\/docs\/algo\/sona\/feature_gbdt_sona.md)\n  - **Angel-Graph**\n    - [PageRank](.\/docs\/algo\/sona\/pagerank_on_sona_en.md)\n    - [KCORE](.\/docs\/algo\/sona\/kcore_sona_en.md)\n    - [HIndex](.\/docs\/algo\/sona\/hindex_sona_en.md)\n    - [Closeness](.\/docs\/algo\/sona\/closeness_sona_en.md)\n    - [CommonFriends](.\/docs\/algo\/sona\/commonfriends_sona_en.md)\n    - [ConnectedComponents](.\/docs\/algo\/sona\/CC_sona_en.md)\n    - [TriangleCountingUndirected](.\/docs\/algo\/sona\/triangle_count_undirected_en.md)\n    - [Louvain](.\/docs\/algo\/sona\/louvain_sona_en.md)\n    - [LPA](.\/docs\/algo\/sona\/LPA_sona_en.md)\n    - [LINE](.\/docs\/algo\/sona\/line_sona_en.md)\n    - [Word2Vec](.\/docs\/algo\/sona\/word2vec_sona_en.md)\n    - [GraphSage](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/graph.md)\n    - [GCN](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/graph.md)\n    - [DGI](https:\/\/github.com\/Angel-ML\/PyTorch-On-Angel\/blob\/branch-0.2.0\/docs\/graph.md)\n\n## Deployment\n\n* [Compilation Guide](.\/docs\/deploy\/source_compile_en.md)\n* [Running on Local](.\/docs\/deploy\/local_run_en.md)\n* [Running on Yarn](.\/docs\/deploy\/run_on_yarn_en.md)\n* [Configuration Details](.\/docs\/deploy\/config_details_en.md)\n* [Resource Configuration Guide](.\/docs\/deploy\/resource_config_guide_en.md)\n\n## Community\n* Mailing list: angel-tsc@lists.deeplearningfoundation.org\n* Angel homepage in Linux FD: https:\/\/lists.deeplearningfoundation.org\/g\/angel-main\n* [Committers & Contributors](.\/COMMITTERS.md)\n* [Contributing to Angel](.\/CONTRIBUTING.md)\n* [Roadmap](https:\/\/github.com\/Angel-ML\/angel\/wiki\/Roadmap)\n\n## FAQ\n* [Angel FAQ](https:\/\/github.com\/Tencent\/angel\/wiki\/Angel%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98)\n\n## Papers\n  1. Lele Yu, Bin Cui, Ce Zhang, Yingxia Shao. [LDA*: A Robust and Large-scale Topic Modeling System](http:\/\/www.vldb.org\/pvldb\/vol10\/p1406-yu.pdf). VLDB, 2017\n  2. Jiawei Jiang, Bin Cui, Ce Zhang, Lele Yu. [Heterogeneity-aware Distributed Parameter Servers](http:\/\/net.pku.edu.cn\/~cuibin\/Papers\/2017%20sigmod.pdf). SIGMOD, 2017\n  3. Jie Jiang, Lele Yu, Jiawei Jiang, Yuhong Liu and Bin Cui. [Angel: a new large-scale machine learning system](http:\/\/net.pku.edu.cn\/~cuibin\/Papers\/2017NSRangel.pdf). National Science Review (NSR), 2017\n  4. Jie Jiang, Jiawei Jiang,  Bin Cui and Ce Zhang. [TencentBoost: A Gradient Boosting Tree System with Parameter Server](http:\/\/net.pku.edu.cn\/~cuibin\/Papers\/2017%20ICDE%20boost.pdf).\tICDE, 2017\n  5. Jiawei Jiang, Bin Cui, Ce Zhang and Fangcheng Fu. [DimBoost: Boosting Gradient Boosting Decision Tree to Higher Dimensions](https:\/\/dl.acm.org\/citation.cfm?id=3196892). SIGMOD, 2018.\n  6. Jiawei Jiang, Pin Xiao, Lele Yu, Xiaosen Li.[PSGraph: How Tencent trains extremely large-scale graphs with Spark?](https:\/\/conferences.computer.org\/icde\/2020\/pdfs\/ICDE2020-5acyuqhpJ6L9P042wmjY1p\/290300b549\/290300b549.pdf).ICDE, 2020.\n","2":"[![logo](docs\/resources\/alluxio_logo.png \"Alluxio\")](https:\/\/www.alluxio.io)\n\n[![Slack](https:\/\/img.shields.io\/badge\/slack-alluxio--community-blue.svg?logo=slack)](https:\/\/www.alluxio.io\/slack)\n[![Release](https:\/\/img.shields.io\/github\/release\/alluxio\/alluxio\/all.svg)](https:\/\/www.alluxio.io\/download)\n[![Docker Pulls](https:\/\/img.shields.io\/docker\/pulls\/alluxio\/alluxio.svg)](https:\/\/hub.docker.com\/r\/alluxio\/alluxio)\n[![Documentation](https:\/\/img.shields.io\/badge\/docs-reference-blue.svg)](https:\/\/www.alluxio.io\/docs)\n[![Twitter Follow](https:\/\/img.shields.io\/twitter\/follow\/alluxio.svg?label=Follow&style=social)](https:\/\/twitter.com\/intent\/follow?screen_name=alluxio)\n[![License](https:\/\/img.shields.io\/github\/license\/alluxio\/alluxio.svg)](https:\/\/github.com\/Alluxio\/alluxio\/blob\/master\/LICENSE)\n\n## What is Alluxio\n[Alluxio](https:\/\/www.alluxio.io) (formerly known as Tachyon)\nis a virtual distributed storage system. It bridges the gap between\ncomputation frameworks and storage systems, enabling computation applications to connect to\nnumerous storage systems through a common interface. Read more about\n[Alluxio Overview](https:\/\/docs.alluxio.io\/os\/user\/stable\/en\/Overview.html).\n\nThe Alluxio project originated from a research project called Tachyon at AMPLab, UC Berkeley,\nwhich was the data layer of the Berkeley Data Analytics Stack ([BDAS](https:\/\/amplab.cs.berkeley.edu\/bdas\/)).\nFor more details, please refer to Haoyuan Li's PhD dissertation\n[Alluxio: A Virtual Distributed File System](https:\/\/www2.eecs.berkeley.edu\/Pubs\/TechRpts\/2018\/EECS-2018-29.html).\n\n## Who Uses Alluxio\n\nAlluxio is used in production to manage Petabytes of data in many leading companies, with\nthe largest deployment exceeding 3,000 nodes. You can find more use cases at\n[Powered by Alluxio](https:\/\/www.alluxio.io\/powered-by-alluxio) or visit our first community conference ([Data Orchestration Summit](https:\/\/www.alluxio.io\/data-orchestration-summit-2019\/)) to learn from other community members!\n\n## Who Owns and Manages Alluxio Project\n\nAlluxio Open Source Foundation is the owner of Alluxio project.\nProject operation is done by Alluxio Project Management Committee (PMC).\nYou can checkout more details in its structure and how to join Alluxio PMC \n[here](https:\/\/github.com\/Alluxio\/alluxio\/wiki\/Alluxio-Project-Management-Committee-(PMC)).\n\n## Community and Events\nPlease use the following to reach members of the community:\n\n* [Alluxio Community Slack Channel](https:\/\/www.alluxio.io\/slack): post your questions here if you seek for help for general questions or issues using Alluxio.\n* [Special Interest Groups (SIG) for Alluxio users and developers](#contributing)\n* Community Events: [upcoming online office hours, meetups and webinars](https:\/\/www.alluxio.io\/events)\n* Meetup Groups: [Global Online Meetup](https:\/\/www.meetup.com\/Alluxio-Global-Online-Meetup\/), [Bay Area Meetup](http:\/\/www.meetup.com\/Alluxio),\n[New York Meetup](https:\/\/www.meetup.com\/Alluxio-Open-Source-New-York-Meetup),\n[Beijing Alluxio Meetup](https:\/\/www.meetup.com\/meetup-group-iLMBZGhS\/), [Austin Meetup](https:\/\/www.meetup.com\/Cloud-Data-Orchestration-Austin\/)\n* [Alluxio Twitter](https:\/\/twitter.com\/alluxio); [Alluxio Youtube Channel](https:\/\/www.youtube.com\/channel\/UCpibQsajhwqYPLYhke4RigA); [Alluxio Mailing List](https:\/\/groups.google.com\/forum\/?fromgroups#!forum\/alluxio-users)\n\n## Download Alluxio\n\n### Binary download\n\nPrebuilt binaries are available to download at https:\/\/www.alluxio.io\/download .\n\n### Docker\n\nDownload and start an Alluxio master and a worker. More details can be found in [documentation](https:\/\/docs.alluxio.io\/os\/user\/stable\/en\/deploy\/Running-Alluxio-On-Docker.html).\n\n```console\n# Create a network for connecting Alluxio containers\n$ docker network create alluxio_nw\n# Create a volume for storing ufs data\n$ docker volume create ufs\n# Launch the Alluxio master\n$ docker run -d --net=alluxio_nw \\\n    -p 19999:19999 \\\n    --name=alluxio-master \\\n    -v ufs:\/opt\/alluxio\/underFSStorage \\\n    alluxio\/alluxio master\n# Launch the Alluxio worker\n$ export ALLUXIO_WORKER_RAMDISK_SIZE=1G\n$ docker run -d --net=alluxio_nw \\\n    --shm-size=${ALLUXIO_WORKER_RAMDISK_SIZE} \\\n    --name=alluxio-worker \\\n    -v ufs:\/opt\/alluxio\/underFSStorage \\\n    -e ALLUXIO_JAVA_OPTS=\"-Dalluxio.worker.ramdisk.size=${ALLUXIO_WORKER_RAMDISK_SIZE} -Dalluxio.master.hostname=alluxio-master\" \\\n    alluxio\/alluxio worker\n```\n\n### MacOS Homebrew\n\n```console\n$ brew install alluxio\n```\n\n## Quick Start\n\nPlease follow the [Guide to Get Started](https:\/\/docs.alluxio.io\/os\/user\/stable\/en\/Getting-Started.html)\nto run a simple example with Alluxio.\n\n## Report a Bug\n\nTo report bugs, suggest improvements, or create new feature requests, please open a [Github Issue](https:\/\/github.com\/alluxio\/alluxio\/issues).\nIf you are not sure whether you run into bugs or simply have general questions with respect to Alluxio, post your questions on [Alluxio Slack channel](www.alluxio.io\/slack).\n\n## Depend on Alluxio\n\nAlluxio project provides several different client artifacts for external projects to depend on Alluxio client:\n\n- Artifact `alluxio-shaded-client` is recommended generally for a project to use Alluxio client.\n  The jar of this artifact is self-contained (including all dependencies in a shaded form to prevent dependency conflicts),\n  and thus larger than the following two artifacts.\n- Artifact `alluxio-core-client-fs` provides\n  [Alluxio Java file system API](https:\/\/docs.alluxio.io\/os\/user\/stable\/en\/api\/FS-API.html#alluxio-java-api))\n  to access all Alluxio-specific functionalities.\n  This artifact is included in `alluxio-shaded-client`.\n- Artifact `alluxio-core-client-hdfs` provides\n  [HDFS-Compatible file system API](https:\/\/docs.alluxio.io\/os\/user\/stable\/en\/api\/FS-API.html#hadoop-compatible-java-client).\n  This artifact is included in `alluxio-shaded-client`.\n\nHere are examples to declare the dependecies on  `alluxio-shaded-client` using Maven:\n\n  ```xml\n  <dependency>\n    <groupId>org.alluxio<\/groupId>\n    <artifactId>alluxio-shaded-client<\/artifactId>\n    <version>2.6.0<\/version>\n  <\/dependency>\n  ```\n\n## Contributing\n\nContributions via GitHub pull requests are gladly accepted from their original author. Along with\nany pull requests, please state that the contribution is your original work and that you license the\nwork to the project under the project's open source license. Whether or not you state this\nexplicitly, by submitting any copyrighted material via pull request, email, or other means you agree\nto license the material under the project's open source license and warrant that you have the legal\nauthority to do so.\nFor a more detailed step-by-step guide, please read\n[how to contribute to Alluxio](https:\/\/docs.alluxio.io\/os\/user\/stable\/en\/contributor\/Contributor-Getting-Started.html).\nFor new contributor, please take two [new contributor tasks](https:\/\/github.com\/Alluxio\/new-contributor-tasks).\n\nFor advanced feature requests and contributions, \nAlluxio core team is hosting regular online meetings with community users and developers to iterate the project in two special interest groups:\n\n* Alluxio and AI workloads: e.g., running Tensorflow, Pytorch on Alluxio through the POSIX API. Checkout the [meeting notes](https:\/\/docs.google.com\/spreadsheets\/d\/1OlprIiUkGjMuZJ_6cLTJYVJpTGpnTWkFhHzX16tYNDQ\/)\n* Alluxio and Presto workloads: e.g., running Presto on Alluxio, running Alluxio catalog service. Checkout the [meeting notes](https:\/\/docs.google.com\/spreadsheets\/d\/1V-fxqfG_oj3B1ZWSgbRWVuTHFvjL3pq6uXgAL-xvFQA\/)\n\nSubscribe our [public calendar](https:\/\/calendar.google.com\/calendar\/embed?src=alluxio.com_g9ec8agk27baqu2nu692ft1m3s%40group.calendar.google.com&ctz=America%2FLos_Angeles) to join us.\n\n## Useful Links\n\n- [Alluxio Website](https:\/\/www.alluxio.io\/)\n- [Downloads](https:\/\/www.alluxio.io\/download)\n- [Releases and Notes](https:\/\/www.alluxio.io\/download\/releases\/)\n- [Documentation](https:\/\/www.alluxio.io\/docs\/)\n","3":"# Smile\n\n[![Join the chat at https:\/\/gitter.im\/haifengl\/smile](https:\/\/badges.gitter.im\/haifengl\/smile.svg)](https:\/\/gitter.im\/haifengl\/smile?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Maven Central](https:\/\/maven-badges.herokuapp.com\/maven-central\/com.github.haifengl\/smile-core\/badge.svg)](https:\/\/maven-badges.herokuapp.com\/maven-central\/com.github.haifengl\/smile-core)\n\n[Smile (Statistical Machine Intelligence and Learning Engine)](https:\/\/haifengl.github.io\/)\nis a fast and comprehensive machine learning, NLP, linear algebra,\ngraph, interpolation, and visualization system in Java and Scala.\nWith advanced data structures and algorithms, Smile delivers\nstate-of-art performance. Smile is well documented and please\ncheck out the project [website](https:\/\/haifengl.github.io\/)\nfor programming guides and more information.\n\nSmile covers every aspect of machine learning, including classification,\nregression, clustering, association rule mining, feature selection,\nmanifold learning, multidimensional scaling, genetic algorithms,\nmissing value imputation, efficient nearest neighbor search, etc.\n\nSmile implements the following major machine learning algorithms:\n\n- **Classification:**\nSupport Vector Machines, Decision Trees, AdaBoost, Gradient Boosting,\nRandom Forest, Logistic Regression, Neural Networks, RBF Networks,\nMaximum Entropy Classifier, KNN, Na\u00efve Bayesian,\nFisher\/Linear\/Quadratic\/Regularized Discriminant Analysis.\n\n- **Regression:**\nSupport Vector Regression, Gaussian Process, Regression Trees,\nGradient Boosting, Random Forest, RBF Networks, OLS, LASSO, ElasticNet,\nRidge Regression.\n\n- **Feature Selection:**\nGenetic Algorithm based Feature Selection, Ensemble Learning based Feature\nSelection, TreeSHAP, Signal Noise ratio, Sum Squares ratio.\n\n- **Clustering:**\nBIRCH, CLARANS, DBSCAN, DENCLUE, Deterministic Annealing, K-Means,\nX-Means, G-Means, Neural Gas, Growing Neural Gas, Hierarchical\nClustering, Sequential Information Bottleneck, Self-Organizing Maps,\nSpectral Clustering, Minimum Entropy Clustering.\n\n- **Association Rule & Frequent Itemset Mining:**\nFP-growth mining algorithm.\n\n- **Manifold Learning:**\nIsoMap, LLE, Laplacian Eigenmap, t-SNE, UMAP, PCA, Kernel PCA,\nProbabilistic PCA, GHA, Random Projection, ICA.\n\n- **Multi-Dimensional Scaling:**\nClassical MDS, Isotonic MDS, Sammon Mapping.\n\n- **Nearest Neighbor Search:**\nBK-Tree, Cover Tree, KD-Tree, SimHash, LSH.\n\n- **Sequence Learning:**\nHidden Markov Model, Conditional Random Field.\n\n- **Natural Language Processing:**\nSentence Splitter and Tokenizer, Bigram Statistical Test, Phrase Extractor,\nKeyword Extractor, Stemmer, POS Tagging, Relevance Ranking\n\nYou can use the libraries through Maven central repository by adding the\nfollowing to your project pom.xml file.\n```\n    <dependency>\n      <groupId>com.github.haifengl<\/groupId>\n      <artifactId>smile-core<\/artifactId>\n      <version>2.6.0<\/version>\n    <\/dependency>\n```\n\nFor NLP, use the artifactId smile-nlp.\n\nFor Scala API, please use\n```\n    libraryDependencies += \"com.github.haifengl\" %% \"smile-scala\" % \"2.6.0\"\n```\n\nFor Kotlin API, add the below into the `dependencies` section\nof Gradle build script.\n```\n    implementation(\"com.github.haifengl:smile-kotlin:2.6.0\")\n```\n\nFor Clojure API, add the following dependency to your project or build file:\n```\n    [org.clojars.haifengl\/smile \"2.6.0\"]\n```\n\nSome algorithms rely on BLAS and LAPACK (e.g. manifold learning,\nsome clustering algorithms, Gaussian Process regression, MLP, etc).\nTo use these algorithms, you should include OpenBLAS for optimized matrix\ncomputation:\n```\n    libraryDependencies ++= Seq(\n      \"org.bytedeco\" % \"javacpp\"   % \"1.5.4\"        classifier \"macosx-x86_64\" classifier \"windows-x86_64\" classifier \"linux-x86_64\" classifier \"linux-arm64\" classifier \"linux-ppc64le\" classifier \"android-arm64\" classifier \"ios-arm64\",\n      \"org.bytedeco\" % \"openblas\"  % \"0.3.10-1.5.4\" classifier \"macosx-x86_64\" classifier \"windows-x86_64\" classifier \"linux-x86_64\" classifier \"linux-arm64\" classifier \"linux-ppc64le\" classifier \"android-arm64\" classifier \"ios-arm64\",\n      \"org.bytedeco\" % \"arpack-ng\" % \"3.7.0-1.5.4\"  classifier \"macosx-x86_64\" classifier \"windows-x86_64\" classifier \"linux-x86_64\" classifier \"linux-arm64\" classifier \"linux-ppc64le\"\n    )\n```\nIn this example, we include all supported 64-bit platforms and filter out\n32-bit platforms. The user should include only the needed platforms to save\nspaces.\n\nIf you prefer other BLAS implementations, you can use any library found on\nthe \"java.library.path\" or on the class path, by specifying it with the\n\"org.bytedeco.openblas.load\" system property. For example, to use the BLAS\nlibrary from the Accelerate framework on Mac OS X, we can pass options such\nas `-Djava.library.path=\/usr\/lib\/ -Dorg.bytedeco.openblas.load=blas`.\n\nFor a default installation of MKL that would be `-Dorg.bytedeco.openblas.load=mkl_rt`.\nOr you may simply include `smile-mkl` module in your project, which includes\nMKL binaries. With `smile-mkl` module in the class path, Smile will\nautomatically switch to MKL.\n```\n    libraryDependencies += \"com.github.haifengl\" %% \"smile-mkl\" % \"2.6.0\"\n```\n\n## Shell\nSmile comes with interactive shells for Java, Scala and Kotlin.\nDownload pre-packaged Smile from the\n[releases page](https:\/\/github.com\/haifengl\/smile\/releases).\nIn the home directory of Smile, type\n```\n    .\/bin\/smile\n```\nto enter the Scala shell. You can run any valid Scala expressions\nin the shell. In the simplest case, you can use it as a calculator.\nBesides, all high-level Smile operators are predefined in the shell.\nBy default, the shell uses up to 75% memory. If you need more memory\nto handle large data, use the option `-J-Xmx` or `-XX:MaxRAMPercentage`.\nFor example,\n```\n    .\/bin\/smile -J-Xmx30G\n```\nYou can also modify the configuration file `.\/conf\/smile.ini` for the\nmemory and other JVM settings.\n\nTo use Java's JShell, type\n```\n    .\/bin\/jshell.sh\n```\nwhich has Smile's jars in the classpath. Similarly, run\n```\n    .\/bin\/kotlin.sh\n```\nto enter Kotlin REPL.\n\n## Model Serialization\nMost models support the Java `Serializable` interface (all classifiers\ndo support `Serializable` interface) so that you can use them in Spark.\nFor reading\/writing the models in non-Java code, we suggest [XStream]\n(https:\/\/github.com\/x-stream\/xstream) to serialize the trained models.\nXStream is a simple library to serialize objects to XML and back again.\nXStream is easy to use and doesn't require mappings (actually requires\nno modifications to objects). [Protostuff](http:\/\/code.google.com\/p\/protostuff\/)\nis a nice alternative that supports forward-backward compatibility\n(schema evolution) and validation. Beyond XML, Protostuff supports many\nother formats such as JSON, YAML, protobuf, etc.\n\n## Visualization\nSmile provides a Swing-based data visualization library SmilePlot,\nwhich provides scatter plot, line plot, staircase plot, bar plot,\nbox plot, histogram, 3D histogram, dendrogram, heatmap, hexmap,\nQQ plot, contour plot, surface, and wireframe.\n\nTo use SmilePlot, add the following to dependencies\n```\n    <dependency>\n      <groupId>com.github.haifengl<\/groupId>\n      <artifactId>smile-plot<\/artifactId>\n      <version>2.6.0<\/version>\n    <\/dependency>\n```\n\nSmile also support data visualization in declarative approach.\nWith `smile.plot.vega package`, we can create a specification\nthat describes visualizations as mappings from data to properties\nof graphical marks (e.g., points or bars). The specification is\nbased on [Vega-Lite](https:\/\/vega.github.io\/vega-lite\/). The\nVega-Lite compiler automatically produces visualization components\nincluding axes, legends, and scales. It then determines properties\nof these components based on a set of carefully designed rules.\n\n## Gallery\n<table class=\"center\" width=\"100%\">\n    <tr>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-kpca.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-kpca-small.png\" alt=\"Kernel PCA\"><\/a>\n                <figcaption><h2>Kernel PCA<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-isomap.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-isomap-small.png\" alt=\"IsoMap\"><\/a>\n                <figcaption><h2>IsoMap<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n    <\/tr>\n    <tr>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-mds.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-mds-small.png\" alt=\"MDS\"><\/a>\n                <figcaption><h2>Multi-Dimensional Scaling<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-som.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-som-small.png\" alt=\"SOM\"><\/a>\n                <figcaption><h2>SOM<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n    <\/tr>\n    <tr>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-ann.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-ann-small.png\" alt=\"Neural Network\"><\/a>\n                <figcaption><h2>Neural Network<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-svm.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-svm-small.png\" alt=\"SVM\"><\/a>\n                <figcaption><h2>SVM<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n    <\/tr>\n    <tr>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-agglomerative-clustering.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-agglomerative-clustering-small.png\" alt=\"Agglomerative Clustering\"><\/a>\n                <figcaption><h2>Agglomerative Clustering<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-xmeans.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-xmeans-small.png\" alt=\"X-Means\"><\/a>\n                <figcaption><h2>X-Means<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n    <\/tr>\n    <tr>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-dbscan.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-dbscan-small.png\" alt=\"DBSCAN\"><\/a>\n                <figcaption><h2>DBSCAN<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-neural-gas.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-neural-gas-small.png\" alt=\"Neural Gas\"><\/a>\n                <figcaption><h2>Neural Gas<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n    <\/tr>\n    <tr>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-wavelet.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-wavelet-small.png\" alt=\"Wavelet\"><\/a>\n                <figcaption><h2>Wavelet<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n        <td width=\"50%\">\n            <figure>\n                <a href=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-mixture.png\"><img src=\"http:\/\/haifengl.github.io\/gallery\/smile-demo-mixture-small.png\" alt=\"Mixture\"><\/a>\n                <figcaption><h2>Exponential Family Mixture<\/h2><\/figcaption>\n            <\/figure>\n        <\/td>\n    <\/tr>\n<\/table>\n\n","4":"<font size=7>English| [\u7b80\u4f53\u4e2d\u6587](README.md)<\/font>\n\n# Alink\n\nAlink is the Machine Learning algorithm platform based on Flink, developed by the PAI team of Alibaba computing platform.\nWelcome everyone to join the Alink open source user group to communicate.\n \n \n<div align=center>\n<img src=\"https:\/\/img.alicdn.com\/tfs\/TB1kQU0sQY2gK0jSZFgXXc5OFXa-614-554.png\" height=\"25%\" width=\"25%\">\n<\/div>\n\n#### List of Algorithms\n\n<div align=center>\n<img src=\"https:\/\/img.alicdn.com\/tfs\/TB1AEOeoBr0gK0jSZFnXXbRRXXa-1320-1048.png\" height=\"60%\" width=\"60%\">\n<\/div>\n\n#### PyAlink\n\n<div align=center>\n<img src=\"https:\/\/img.alicdn.com\/tfs\/TB1TmKloAL0gK0jSZFxXXXWHVXa-2070-1380.png\" height=\"60%\" width=\"60%\">\n<\/div>\n\n# Quick start\n\n## PyAlink Manual\n\n### Preparation before use:\n---------\n\n\n#### About package names and versions:\n  - PyAlink provides different Python packages for Flink versions that Alink supports: \n  package `pyalink` always maintains Alink Python API against the latest Flink version, which is 1.13, \n  while `pyalink-flink-***` support old-version Flink, which are `pyalink-flink-1.12`, `pyalink-flink-1.11`, `pyalink-flink-1.10` and `pyalink-flink-1.9` for now. \n  - The version of python packages always follows Alink Java version, like `1.5.4`.\n  \n#### Installation steps:\n\n1. Make sure the version of python3 on your computer is 3.6, 3.7 or 3.8.\n2. Make sure Java 8 is installed on your computer.\n3. Use pip to install:\n  `pip install pyalink`, `pip install pyalink-flink-1.12`, `pip install pyalink-flink-1.11`, `pip install pyalink-flink-1.10` or `pip install pyalink-flink-1.9`.\n\n\n#### Potential issues:\n\n1. `pyalink` and\/or `pyalink-flink-***` can not be installed at the same time. Multiple versions are not allowed.\nIf `pyalink` or `pyalink-flink-***` was\/were installed, please use `pip uninstall pyalink` or `pip uninstall pyalink-flink-***` to remove them.\n\n2. If `pip install` is slow of failed, refer to [this article](https:\/\/segmentfault.com\/a\/1190000006111096) to change the pip source, or use the following download links:\n    - Flink 1.13\uff1a[Link](https:\/\/alink-release.oss-cn-beijing.aliyuncs.com\/v1.5.5\/pyalink-1.5.5-py3-none-any.whl) (MD5: 4d6ebc65eadebf68835c9834d30f2e17)\n    - Flink 1.12\uff1a[Link](https:\/\/alink-release.oss-cn-beijing.aliyuncs.com\/v1.5.5\/pyalink_flink_1.12-1.5.5-py3-none-any.whl) (MD5: b7ab1a4d99837af70fd0902b86b5806f)\n    - Flink 1.11\uff1a[Link](https:\/\/alink-release.oss-cn-beijing.aliyuncs.com\/v1.5.5\/pyalink_flink_1.11-1.5.5-py3-none-any.whl) (MD5: 2ff6b9836be67be484e8a7fe961d7307)\n    - Flink 1.10\uff1a[Link](https:\/\/alink-release.oss-cn-beijing.aliyuncs.com\/v1.5.5\/pyalink_flink_1.10-1.5.5-py3-none-any.whl) (MD5: 8a26eef0a664b0bc1a6a70215c7c8f05)\n    - Flink 1.9: [Link](https:\/\/alink-release.oss-cn-beijing.aliyuncs.com\/v1.5.5\/pyalink_flink_1.9-1.5.5-py3-none-any.whl) (MD5: a75ab7f0212cb1ae0886e7d38fc9e713)\n3. If multiple version of Python exist, you may need to use a special version of `pip`, like `pip3`;\nIf Anaconda is used, the command should be run in Anaconda prompt. \n\n\n#### Download file system and Catalog dependency jar files:\n\nAfter PyAlink installed, you can run ```download_pyalink_dep_jars``` to download dependency jars for file system and Hive.\n(If there is an error that could not find the command, you can run the python command ```python3 -c 'from pyalink.alink.download_pyalink_dep_jars import main;main()'``` directly.)\n\nAfter executed the command, you'll see a prompt asking you about the dependencies and their versions to be downloaded. \nThe following dependencies and their versions of jars are supported:\n\n- OSS\uff1a3.4.1\n- Hadoop\uff1a2.8.3\n- Hive\uff1a2.3.4\n- MySQL: 5.1.27\n- Derby: 10.6.1.0\n- SQLite: 3.19.3\n- S3-hadoop: 1.11.788\n- S3-presto: 1.11.788\n- odps: 0.36.4-public\n\nThese jars will be installed to the ```lib\/plugins``` folder of PyAlink. \nNote that these command require the access for the folder.\n\nYou can also add the argument ```-d``` when executing the command, i.e.  ```download_pyalink_dep_jars -d```.\nIt will install all dependency jars.\n\n### Start using: \n-------\nYou can start using PyAlink with Jupyter Notebook to provide a better experience.\n\nSteps for usage: \n\n1. Start Jupyter: ```jupyter notebook``` in terminal\n, and create Python 3 notebook.\n\n2. Import the pyalink package: ```from pyalink.alink import *```.\n\n3. Use this command to create a local runtime environment:\n\n   ```useLocalEnv(parallism, flinkHome=None, config=None)```.\n\n   Among them, the parameter  ```parallism```  indicates the degree of parallelism used for execution;```flinkHome``` is the full path of flink, and usually no need to set; ```config``` is the configuration parameter accepted by Flink. After running, the following output appears, indicating that the initialization of the running environment is successful.\n```\nJVM listening on ***\nPython listening on ***\n```\n4. Start writing PyAlink code, for example:\n```python\nsource = CsvSourceBatchOp()\\\n    .setSchemaStr(\"sepal_length double, sepal_width double, petal_length double, petal_width double, category string\")\\\n    .setFilePath(\"https:\/\/alink-release.oss-cn-beijing.aliyuncs.com\/data-files\/iris.csv\")\nres = source.select([\"sepal_length\", \"sepal_width\"])\ndf = res.collectToDataframe()\nprint(df)\n```\n\n### Write code: \n------\nIn PyAlink, the interface provided by the algorithm component is basically the same as the Java APIs, that is, an algorithm component is created through the default construction method, then the parameters are set through ```setXXX```, and other components are connected through ```link \/ linkTo \/ linkFrom```.\n\nHere, Jupyter Notebook's auto-completion mechanism can be used to provide writing convenience.\n\nFor batch jobs, you can trigger execution through methods such as ```print \/ collectToDataframe \/ collectToDataframes``` of batch components or ```BatchOperator.execute ()```; for streaming jobs, start the job with ```StreamOperator.execute ()```.\n\n### More usage: \n------\n - [Interchange between DataFrame and Operator](docs\/pyalink\/pyalink-dataframe.md)\n - [StreamOperator data preview](docs\/pyalink\/pyalink-stream-operator-preview.md)\n - [UDF\/UDTF\/SQL usage](docs\/pyalink\/pyalink-udf.md)\n - [Use with PyFlink](docs\/pyalink\/pyalink-pyflink.md)\n - [PyAlink Q&A](docs\/pyalink\/pyalink-qa.md)\n\n## Java API Manual\n\n### KMeans Example\n```java\nString URL = \"https:\/\/alink-release.oss-cn-beijing.aliyuncs.com\/data-files\/iris.csv\";\nString SCHEMA_STR = \"sepal_length double, sepal_width double, petal_length double, petal_width double, category string\";\n\nBatchOperator data = new CsvSourceBatchOp()\n        .setFilePath(URL)\n        .setSchemaStr(SCHEMA_STR);\n\nVectorAssembler va = new VectorAssembler()\n        .setSelectedCols(new String[]{\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"})\n        .setOutputCol(\"features\");\n\nKMeans kMeans = new KMeans().setVectorCol(\"features\").setK(3)\n        .setPredictionCol(\"prediction_result\")\n        .setPredictionDetailCol(\"prediction_detail\")\n        .setReservedCols(\"category\")\n        .setMaxIter(100);\n\nPipeline pipeline = new Pipeline().add(va).add(kMeans);\npipeline.fit(data).transform(data).print();\n```\n\n### With Flink-1.13\n```xml\n<dependency>\n    <groupId>com.alibaba.alink<\/groupId>\n    <artifactId>alink_core_flink-1.13_2.11<\/artifactId>\n    <version>1.5.5<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-streaming-scala_2.11<\/artifactId>\n    <version>1.13.0<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-table-planner_2.11<\/artifactId>\n    <version>1.13.0<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-clients_2.11<\/artifactId>\n    <version>1.13.0<\/version>\n<\/dependency>\n```\n\n### With Flink-1.12\n```xml\n<dependency>\n    <groupId>com.alibaba.alink<\/groupId>\n    <artifactId>alink_core_flink-1.12_2.11<\/artifactId>\n    <version>1.5.5<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-streaming-scala_2.11<\/artifactId>\n    <version>1.12.1<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-table-planner_2.11<\/artifactId>\n    <version>1.12.1<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-clients_2.11<\/artifactId>\n    <version>1.12.1<\/version>\n<\/dependency>\n```\n\n### With Flink-1.11\n```xml\n<dependency>\n    <groupId>com.alibaba.alink<\/groupId>\n    <artifactId>alink_core_flink-1.11_2.11<\/artifactId>\n    <version>1.5.5<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-streaming-scala_2.11<\/artifactId>\n    <version>1.11.0<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-table-planner_2.11<\/artifactId>\n    <version>1.11.0<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-clients_2.11<\/artifactId>\n    <version>1.11.0<\/version>\n<\/dependency>\n```\n\n### With Flink-1.10\n```xml\n<dependency>\n    <groupId>com.alibaba.alink<\/groupId>\n    <artifactId>alink_core_flink-1.10_2.11<\/artifactId>\n    <version>1.5.5<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-streaming-scala_2.11<\/artifactId>\n    <version>1.10.0<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-table-planner_2.11<\/artifactId>\n    <version>1.10.0<\/version>\n<\/dependency>\n```\n\n### With Flink-1.9\n\n```xml\n<dependency>\n    <groupId>com.alibaba.alink<\/groupId>\n    <artifactId>alink_core_flink-1.9_2.11<\/artifactId>\n    <version>1.5.5<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-streaming-scala_2.11<\/artifactId>\n    <version>1.9.0<\/version>\n<\/dependency>\n<dependency>\n    <groupId>org.apache.flink<\/groupId>\n    <artifactId>flink-table-planner_2.11<\/artifactId>\n    <version>1.9.0<\/version>\n<\/dependency>\n```\n\n\nGet started to run Alink Algorithm with a Flink Cluster\n--------\n\n1. Prepare a Flink Cluster:\n```shell\n  wget https:\/\/archive.apache.org\/dist\/flink\/flink-1.13.0\/flink-1.13.0-bin-scala_2.11.tgz\n  tar -xf flink-1.13.0-bin-scala_2.11.tgz && cd flink-1.13.0\n  .\/bin\/start-cluster.sh\n```\n\n2. Build Alink jar from the source:\n```shell\n  git clone https:\/\/github.com\/alibaba\/Alink.git\n  # add <scope>provided<\/scope> in pom.xml of alink_examples.\n  cd Alink && mvn -Dmaven.test.skip=true clean package shade:shade\n```\n\n3. Run Java examples:\n```shell\n  .\/bin\/flink run -p 1 -c com.alibaba.alink.ALSExample [path_to_Alink]\/examples\/target\/alink_examples-1.5-SNAPSHOT.jar\n  # .\/bin\/flink run -p 1 -c com.alibaba.alink.GBDTExample [path_to_Alink]\/examples\/target\/alink_examples-1.5-SNAPSHOT.jar\n  # .\/bin\/flink run -p 1 -c com.alibaba.alink.KMeansExample [path_to_Alink]\/examples\/target\/alink_examples-1.5-SNAPSHOT.jar\n```\n\nDeployment\n---------\n\n[Cluster](docs\/deploy\/cluster-deploy.en-US.md)\n","5":"# Easy Machine Learning\n\n## What is Easy Machine Learning\nMachine learning algorithms have become the key components in many big data applications. However, the full potential of machine learning is still far from been realized because using machine learning algorithms is hard,\nespecially on distributed platforms such as Hadoop and Spark. The key barriers come from not only the implementation of the algorithms themselves, but also the processing for applying them to real applications which often involve multiple steps \nand different algorithms. \n\nOur platform **Easy Machine Learning** presents a general-purpose dataflow-based system for easing the process of applying machine learning algorithms to real world tasks. In the system a learning task is formulated as a directed acyclic graph (DAG) in which each node represents an operation \n(e.g. a machine learning algorithm), and each edge represents the flow of the data from one node to its descendants. The task can be defined manually or be cloned from existing tasks\/templates. After submitting a task to the cloud, each node will be automatically scheduled to execute according to the DAG. \nGraphical user interface is implemented for making users to create, configure, submit, and monitor a task in a drag-and-drop manner. Advantages of the system include \n \n1. Lowing the barriers of defining and executing machine learning tasks;\n \n2. Sharing and re-using the implementations of the algorithms, the job DAGs, and the experimental results;\n\n3. Seamlessly integrating the stand-alone algorithms as well as the distributed algorithms in one task.\n\n\n\n\nThe system consists of three major components: \n\n* A distributed machine learning library which implements not only popular used machine learning algorithms, but also the algorithms for data pre\/post-processing, data format transformation, feature generation, performance evaluation etc. These algorithms are mainly implemented based on Spark.  \n* A GUI-based machine learning studio system which enable users to create, configure, submit, monitor, and sharing their machine learning process in a drag-and-drop manner. All of the algorithms in the machine learning library can be accessed and configured in the studio system. They are the key building blocks for constructing machine learning tasks. \n<div align=center>\n<img src=\".\/img\/LR_DAG.png\" width=\"400\" height=\"300\" alt=\"An example dataflow DAG\"\/>\n<\/div>\n\n* A cloud service for executing the tasks. We build the service based on the open source big data platform of Hadoop and Spark. In order to build an platform, we organised a cluster of server on ***Docker***. After receiving a task DAG from the GUI, each node will be automatically scheduled to run when all of its dependent data sources are ready. The algorithm corresponds to the node will scheduled to run on Linux, Spark, or Map-Reduce, according to their implementation.\n<div align=center>\n<img src=\".\/img\/Docker_structure.png\" width=\"90%  alt=\"Docker studio\"\/>\n<\/div>\n\n\n## How to involve in our project\n\nPull all project and prepare some necessary environments and a kind of development utilities. Follows the step in **[Quick-start.md](https:\/\/github.com\/ICT-BDA\/EasyML\/blob\/master\/QuickStart.md)**, and you can create our system in your computer.\n\n\n## How to use Easy Machine Learning Studio \nAfter you have ran Easy ML\uff0cYou can login via `http:\/\/localhost:18080\/EMLStudio.html`with our official account `bdaict@hotmail.com` and password `bdaict`. For the best user experience, it is recommended to use Chrome.\n<div align=center>\n<img src=\".\/img\/home_page.png\" width=\"90%  alt=\"Homepage\"\/>\n<\/div>\n\n* As shown in the following figure, the users can create a machine learning task (a dataflow DAG) with the algorithms and data sets listed in the left panel of the page. They can choose to click the algorithms and data sets listed in the **`Program`** and **`Data`** panels. They can also click the **`Job`** panel, select an existing task, clone it, and make necessary modifications. The users can configure the task information and parameter values of each node in the right panel. The nodes in the task could corresponds to either a stand-alone Linux program or a distributed program running on Spark or Hadoop Map-Reduce.\n<div align=center>\n<img src=\".\/img\/job_construct.png\" width=\"90%  alt=\"job_structure\"\/>\n<\/div>\n\n* The task is submitted to run on the cloud after clicking the **`submit`** button. The status of each node is indicated with different colors, as shown in the following figure.\n<div align=center>\n<img src=\".\/img\/job_submit.png\" width=\"90%  alt=\"job_structure\"\/>\n<\/div>\n\n* Users could right click on the **`green output port`** of finished executing node to preview the output data. One could check the stdout and stderr logs from the right click menu of each finished executing node as well.\nThe users may check the outputs of a node by right clicking the corresponding output ports. The standard output and standard error information printed during the execution can be checked through right clicking the corresponding nodes and selects the menu **`Show STDOUT`** and **`Show STDERR`**.\n<div align=center>\n<img src=\".\/img\/job_stdout.png\" width=\"90%  alt=\"job_stdout\"\/>\n<\/div>\n\n* A finished (either success or not) task can be further modified and resubmitted to run, as shown in the following figure. Our system will only schedule the influenced nodes to run. The outputs of uninfluenced nodes are directly reused to save the running time and system resources.\n<div align=center>\n<img src=\".\/img\/job_reuse_submit.png\" width=\"90%  alt=\"job_stdout\"\/>\n<\/div>\n\n* The users can upload their own algorithm packages and data sets for creating their own tasks or shared with other users. By clicking the **`upload program`** button, the popup window allows the users to specify the necessary information of the algorithm package, including the name, the category, the description, and the command line pattern string etc, as shown in the following figure. The most important thing is to write the command line pattern string with the predefined format. It defined the input ports, output ports, and parameter settings of a node. We developed a tool in the panel for helping users to write the command line string patterns. By clicking the **`upload data`** button, users can upload a data set in the similar way as that of uploading a algorithms package.\n<div align=center>\n<img src=\".\/img\/Upload_Program.png\" width=\"90%  alt=\"job_stdout\"\/>\n<\/div>\n\n## How to experience our system\nWe apply an online service for you to experience our system. You can register your own account or use our official account to login the system. The website of the system is as belows:\n\n* Outside ICT, you can visit: [http:\/\/159.226.40.104:18080\/dev\/](http:\/\/159.226.40.104:18080\/dev\/ \"http:\/\/159.226.40.104:18080\/dev\/\")\n* Inside ICT, you can visit: [http:\/\/10.60.0.50:18080\/dev\/](http:\/\/10.60.0.50:18080\/dev\/ \"http:\/\/10.60.0.50:18080\/dev\/\")\n\nIf you have any advice or problems when you expericen our system, welcome to contact us! You can leave us a message or give a email to `bdaict@hotmail.com`, thank you for your advice!\n\n## Papers and Presentations \n1. [EasyML: Ease the Process of Machine Learning with Data Flow.](http:\/\/www.bigdatalab.ac.cn\/~junxu\/publications\/SOSP2017-AISys-EasyML.pdf) SOSP AI System Workshop Shanghai Oct. 28, 2017 \n2. Tianyou Guo, Jun Xu, Xiaohui Yan, Jianpeng Hou, Ping Li, Zhaohui Li, Jiafeng Guo, and Xueqi Cheng. [Ease the Process of Machine Learning with Dataflow.](http:\/\/www.bigdatalab.ac.cn\/~junxu\/publications\/CIKM2016_BDADemo.pdf) Proceedings of the 25th ACM International Conference on Information and Knowledge Management (CIKM '16), Indianapolis, USA, pp. 2437-2440, 2016.\n\n## Acknowledgements\nThe following people contributed to the development of the EasyML project\uff1a\n\n* **Jun Xu**, School of Information, Renmin University of China. Homepage: [http:\/\/info.ruc.edu.cn\/academic_professor.php?teacher_id=169](http:\/\/info.ruc.edu.cn\/academic_professor.php?teacher_id=169)\n* **Xiaohui Yan**, Homepage: [http:\/\/xiaohuiyan.github.io\/](http:\/\/xiaohuiyan.github.io\/) \n* **Xinjie Chen**,  Institute of Computing Technolgy, Chinese Academy of Sciences\n* **Zhaohui Li**,  Institute of Computing Technolgy, Chinese Academy of Sciences\n* **Tianyou Guo**,  Sougou Inc\n* **Jianpeng Hou**,  Google China\n* **Ping Li**,  Tencent Wechat\n* **Jiashuo Cao**, Chengdu University of Information Technology\n* **Dong Huang**, University of Chinese Academy of Sciences\n* **Xueqi Cheng**, Institute of Computing Technolgy, Chinese Academy of Sciences. Homepage: [http:\/\/www.bigdatalab.ac.cn\/~cxq\/](http:\/\/www.bigdatalab.ac.cn\/~cxq\/)\n\n\n","6":"<img align=\"right\" src=\"http:\/\/oryx.io\/img\/OryxLogoMedium.png\" \/>\n\nOryx 2 is a realization of the lambda architecture built on [Apache Spark](http:\/\/spark.apache.org) \nand [Apache Kafka](http:\/\/kafka.apache.org), but with specialization for real-time large scale machine \nlearning. It is a framework for building applications, but also includes packaged, end-to-end \napplications for collaborative filtering, classification, regression and clustering.\n\nProceed to the [Oryx 2 site](http:\/\/oryx.io\/) for full documentation.\n\nJust looking to deploy a ready-made, end-to-end application for collaborative filtering, clustering or classification? Easy.\nProceed directly to:\n\n- Prepare your Hadoop cluster with [Cluster Setup](http:\/\/oryx.io\/docs\/admin.html)\n- Get a [Release](https:\/\/github.com\/OryxProject\/oryx\/releases)\n- Prepare a config file from the [Configuration Reference](http:\/\/oryx.io\/docs\/endusers.html#Configuration)\n- Run the binaries with [Running Oryx](http:\/\/oryx.io\/docs\/endusers.html#Running)\n- Learn about the REST API endpoints you can call in the [API Endpoint Reference](http:\/\/oryx.io\/docs\/endusers.html#API_Endpoint_Reference)\n\nDevelopers can consume Oryx 2 as a framework for building custom applications as well. \nFollowing the architecture overview below, proceed to \n[Making an Oryx App](http:\/\/oryx.io\/docs\/developer.html#Making_an_Oryx_App) \nto learn how to create a new application. You can review a [module diagram](https:\/\/sourcespy.com\/github\/oryx\/) \nas well to understand the project structure.\n\n<img src=\"http:\/\/oryx.io\/img\/Architecture.png\"\/>\n\n------\n\n[![Build Status](https:\/\/travis-ci.org\/OryxProject\/oryx.svg?branch=master)](https:\/\/travis-ci.org\/OryxProject\/oryx)\n[![Coverity](https:\/\/scan.coverity.com\/projects\/2697\/badge.svg)](https:\/\/scan.coverity.com\/projects\/2697)\n[![codecov.io](https:\/\/codecov.io\/github\/OryxProject\/oryx\/coverage.svg?branch=master)](https:\/\/codecov.io\/github\/OryxProject\/oryx?branch=master)\n","7":"# GROBID\n\n[![License](http:\/\/img.shields.io\/:license-apache-blue.svg)](http:\/\/www.apache.org\/licenses\/LICENSE-2.0.html)\n[![CircleCI](https:\/\/circleci.com\/gh\/kermitt2\/grobid.svg?style=svg)](https:\/\/circleci.com\/gh\/kermitt2\/grobid)\n[![Coverage Status](https:\/\/coveralls.io\/repos\/kermitt2\/grobid\/badge.svg)](https:\/\/coveralls.io\/r\/kermitt2\/grobid)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/grobid\/badge\/?version=latest)](https:\/\/readthedocs.org\/projects\/grobid\/?badge=latest)\n[![GitHub release](https:\/\/img.shields.io\/github\/release\/kermitt2\/grobid.svg)](https:\/\/github.com\/kermitt2\/grobid\/releases\/)\n[![Demo cloud.science-miner.com\/grobid](https:\/\/img.shields.io\/website-up-down-green-red\/https\/cloud.science-miner.com\/grobid.svg)](http:\/\/cloud.science-miner.com\/grobid)\n[![Docker Hub](https:\/\/img.shields.io\/docker\/pulls\/lfoppiano\/grobid.svg)](https:\/\/hub.docker.com\/r\/lfoppiano\/grobid\/ \"Docker Pulls\")\n[![Docker Hub](https:\/\/img.shields.io\/docker\/pulls\/grobid\/grobid.svg)](https:\/\/hub.docker.com\/r\/grobid\/grobid\/ \"Docker Pulls\")\n[![SWH](https:\/\/archive.softwareheritage.org\/badge\/origin\/https:\/\/github.com\/kermitt2\/grobid\/)](https:\/\/archive.softwareheritage.org\/browse\/origin\/?origin_url=https:\/\/github.com\/kermitt2\/grobid)\n\n## GROBID documentation\n\nVisit the [GROBID documentation](https:\/\/grobid.readthedocs.io) for more detailed information.\n\n## Summary\n\nGROBID (or Grobid, but not GroBid nor GroBiD) means GeneRation Of BIbliographic Data.\n\nGROBID is a machine learning library for extracting, parsing and re-structuring raw documents such as PDF into structured XML\/TEI encoded documents with a particular focus on technical and scientific publications. First developments started in 2008 as a hobby. In 2011 the tool has been made available in open source. Work on GROBID has been steady as a side project since the beginning and is expected to continue as such.\n\nThe following functionalities are available:\n\n- __Header extraction and parsing__ from article in PDF format. The extraction here covers the usual bibliographical information (e.g. title, abstract, authors, affiliations, keywords, etc.).\n- __References extraction and parsing__ from articles in PDF format, around .87 F1-score against on an independent PubMed Central set of 1943 PDF containing 90,125 references, and around .89 on a similar bioRxiv set. All the usual publication metadata are covered (including DOI, PMID, etc.).\n- __Citation contexts recognition and resolution__ of the full bibliographical references of the article. The accuracy of citation contexts resolution is above .78 f-score (which corresponds to both the correct identification of the citation callout and its correct association with a full bibliographical reference).\n- Parsing of __references in isolation__ (above .90 F1-score at instance-level, .95 F1-score at field level).\n- __Parsing of names__ (e.g. person title, forenames, middlename, etc.), in particular author names in header, and author names in references (two distinct models).\n- __Parsing of affiliation and address__ blocks.\n- __Parsing of dates__, ISO normalized day, month, year.\n- __Full text extraction and structuring__ from PDF articles, including a model for the overall document segmentation and models for the structuring of the text body (paragraph, section titles, reference callout, figure, table, etc.). \n- __Consolidation\/resolution of the extracted bibliographical references__ using the [biblio-glutton](https:\/\/github.com\/kermitt2\/biblio-glutton) service or the [CrossRef REST API](https:\/\/github.com\/CrossRef\/rest-api-doc). In both cases, DOI resolution performance is higher than 0.95 F1-score from PDF extraction.\n- __Extraction and parsing of patent and non-patent references in patent__ publications.\n- __PDF coordinates__ for extracted information, allowing to create \"augmented\" interactive PDF.\n\nIn a complete PDF processing, GROBID manages 55 final labels used to build relatively fine-grained structures, from traditional publication metadata (title, author first\/last\/middlenames, affiliation types, detailed address, journal, volume, issue, pages, doi, pmid, etc.) to full text structures (section title, paragraph, reference markers, head\/foot notes, figure captions, etc.).\n\nGROBID includes a comprehensive web service API, batch processing, a JAVA API, a Docker image, a generic evaluation framework (precision, recall, etc., n-fold cross-evaluation) and the semi-automatic generation of training data.\n\nGROBID can be considered as production ready. Deployments in production includes ResearchGate, Internet Archive Scholar, HAL Research Archive, INIST-CNRS, CERN (Invenio), scite.ai, Academia.edu and many more. The tool is designed for speed and high scalability in order to address the full scientific literature corpus.\n\nGROBID should run properly \"out of the box\" on Linux (64 bits) and macOS. We cannot ensure currently support for Windows as we did before (help welcome!).\n\nGROBID uses optionnally Deep Learning models relying on the [DeLFT](https:\/\/github.com\/kermitt2\/delft) library, a task-agnostic Deep Learning framework for sequence labelling and text classification, via [JEP](https:\/\/github.com\/ninia\/jep). GROBID can run with feature engineered CRF (default), Deep Learning architectures (with or without layout feature channels) or any mixtures of CRF and DL to balance scalability and accuracy. These models use joint text and visual\/layout information provided by [pdfalto](https:\/\/github.com\/kermitt2\/pdfalto).\n\n## Demo\n\nFor testing purposes, a public GROBID demo server is available at the following address: [https:\/\/cloud.science-miner.com\/grobid](https:\/\/cloud.science-miner.com\/grobid)\n\nThe Web services are documented [here](https:\/\/grobid.readthedocs.io\/en\/latest\/Grobid-service\/).\n\n_Warning_: Some quota and query limitation apply to the demo server! Please be courteous and do not overload the demo server. \n\n## Clients\n\nFor facilitating the usage GROBID service at scale, we provide clients written in Python, Java, node.js using the [web services](https:\/\/grobid.readthedocs.io\/en\/latest\/Grobid-service\/) for parallel batch processing:\n\n- <a href=\"https:\/\/github.com\/kermitt2\/grobid-client-python\" target=\"_blank\">Python GROBID client<\/a> (the most complete one in term of supported services and options)\n- <a href=\"https:\/\/github.com\/kermitt2\/grobid-client-java\" target=\"_blank\">Java GROBID client<\/a>\n- <a href=\"https:\/\/github.com\/kermitt2\/grobid-client-node\" target=\"_blank\">Node.js GROBID client<\/a>\n\nAll these clients will take advantage of the multi-threading for scaling large set of PDF processing. As a consequence, they will be much more efficient than the [batch command lines](https:\/\/grobid.readthedocs.io\/en\/latest\/Grobid-batch\/) (which use only one thread) and should be prefered. \n\nWe have been able recently to run the complete fulltext processing at around 10.6 PDF per second (around 915,000 PDF per day, around 20M pages per day) with the node.js client listed above during one week on one 16 CPU machine (16 threads, 32GB RAM, no SDD, articles from mainstream publishers), see [here](https:\/\/github.com\/kermitt2\/grobid\/issues\/443#issuecomment-505208132) (11.3M PDF were processed in 6 days by 2 servers without interruption).\n\nIn addition, a Java example project is available to illustrate how to use GROBID as a Java library: [https:\/\/github.com\/kermitt2\/grobid-example](https:\/\/github.com\/kermitt2\/grobid-example). The example project is using GROBID Java API for extracting header metadata and citations from a PDF and output the results in BibTeX format.  \n\nFinally, the following python utilities can be used to create structured full text corpora of scientific articles. The tool simply takes a list of strong identifiers like DOI or PMID, performing the identification of online Open Access PDF, full text harvesting, metadata agreegation and Grobid processing in one workflow at scale: [article-dataset-builder](https:\/\/github.com\/kermitt2\/article-dataset-builder)\n\n## How GROBID works \n\nVisit the [documentation page describing the system](https:\/\/grobid.readthedocs.io\/en\/latest\/Principles\/). To summarize, the key design principles of GROBID are:\n\n- GROBID uses a [cascade of sequence labeling models](https:\/\/grobid.readthedocs.io\/en\/latest\/Principles\/#document-parsing-as-a-cascade-of-sequence-labeling-models) to parse a document. \n\n- The different models [do not work on text, but on **Layout Tokens**](https:\/\/grobid.readthedocs.io\/en\/latest\/Principles\/#layout-tokens-not-text) to exploit various visual\/layout information avalable for every tokens.\n\n- GROBID does not use training data derived from existing publisher XML documents, but [small, high quality sets](https:\/\/grobid.readthedocs.io\/en\/latest\/Principles\/#training-data-qualitat-statt-quantitat) of manually labeled training data. \n\n- Technical choices and [default settings](https:\/\/grobid.readthedocs.io\/en\/latest\/Principles\/#balancing-accuracy-and-scalability) are driven by the ability to process PDF quickly, with commodity hardware and with good parallelization and scalabilty capacities.\n\nDetailed end-to-end [benchmarking](https:\/\/grobid.readthedocs.io\/en\/latest\/Benchmarking\/) are available [GROBID documentation](https:\/\/grobid.readthedocs.org) and continuously updated.\n\n## GROBID Modules\n\nA series of additional modules have been developed for performing __structure aware__ text mining directly on scholar PDF, reusing GROBID's PDF processing and sequence labelling weaponery:\n\n- [software-mention](https:\/\/github.com\/Impactstory\/software-mentions): recognition of software mentions and attributes in scientific literature\n- [grobid-quantities](https:\/\/github.com\/kermitt2\/grobid-quantities): recognition and normalization of physical quantities\/measurements\n- [grobid-superconductors](https:\/\/github.com\/lfoppiano\/grobid-superconductors): recognition of superconductor material and properties in scientific literature\n- [entity-fishing](https:\/\/github.com\/kermitt2\/entity-fishing), a tool for extracting Wikidata entities from text and document, can also use Grobid to pre-process scientific articles in PDF, leading to more precise and relevant entity extraction and the capacity to annotate the PDF with interative layout. \n- [dataseer-ml](https:\/\/github.com\/dataseer\/dataseer-ml): identification of sections and sentences introducing a dataset in a scientific article, and classification of the type of this dataset.  \n- [grobid-ner](https:\/\/github.com\/kermitt2\/grobid-ner): named entity recognition\n- [grobid-astro](https:\/\/github.com\/kermitt2\/grobid-astro): recognition of astronomical entities in scientific papers\n- [grobid-bio](https:\/\/github.com\/kermitt2\/grobid-bio): a bio-entity tagger using BioNLP\/NLPBA 2004 dataset\n- [grobid-dictionaries](https:\/\/github.com\/MedKhem\/grobid-dictionaries): structuring dictionaries in raw PDF format\n\n## Release and changes\n\nSee the [Changelog](CHANGELOG.md).\n\n## License\n\nGROBID is distributed under [Apache 2.0 license](http:\/\/www.apache.org\/licenses\/LICENSE-2.0). \n\nThe documentation is distributed under [CC-0](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/) license and the annotated data under [CC-BY](https:\/\/creativecommons.org\/licenses\/by\/4.0\/) license.\n\nIf you contribute to GROBID, you agree to share your contribution following these licenses. \n\nMain author and contact: Patrice Lopez (<patrice.lopez@science-miner.com>)\n\n## Sponsors\n\nej-technologies provided us a free open-source license for its Java Profiler. Click the JProfiler logo below to learn more.\n\n[![JProfiler](doc\/img\/jprofiler_medium.png)](http:\/\/www.ej-technologies.com\/products\/jprofiler\/overview.html)\n\n## How to cite\n\nIf you want to cite this work, please refer to the present GitHub project, together with the [Software Heritage](https:\/\/www.softwareheritage.org\/) project-level permanent identifier. For example, with BibTeX:\n\n```bibtex\n@misc{GROBID,\n    title = {GROBID},\n    howpublished = {\\url{https:\/\/github.com\/kermitt2\/grobid}},\n    publisher = {GitHub},\n    year = {2008--2022},\n    archivePrefix = {swh},\n    eprint = {1:dir:dab86b296e3c3216e2241968f0d63b68e8209d3c}\n}\n```\n\nSee the [GROBID documentation](https:\/\/grobid.readthedocs.org\/en\/latest\/References) for more related resources. \n","8":"\n# **Update January 2018**\n\n * [Seldon Core open sourced](https:\/\/github.com\/SeldonIO\/seldon-core). \n    * Seldon Core focuses purely on deploying a wide range of ML models on Kubernetes, allowing complex runtime serving graphs to be managed in production. Seldon Core is a progression of the goals of the Seldon-Server project but also a more restricted focus to solving the final step in a machine learning project which is serving models in production. Please have a look at the [project page](https:\/\/github.com\/SeldonIO\/seldon-core) which includes extensive documentation to investigate further.\n\n\n\n# Seldon Server : * * Archived * *\n\n**This project is not actively maintained anymore please see** [Seldon Core](https:\/\/github.com\/SeldonIO\/seldon-core).\n\nSeldon Server is a machine learning platform that helps your data science team deploy models into production.\n\nIt provides an open-source data science stack that runs within a [Kubernetes](http:\/\/kubernetes.io\/) Cluster. You can use Seldon to deploy machine learning and deep learning models into production on-premise or in the cloud (e.g. [GCP](http:\/\/docs.seldon.io\/kubernetes-google-cloud.html), AWS, Azure).\n\nSeldon supports models built with TensorFlow, Keras, Vowpal Wabbit, XGBoost, Gensim and any other model-building tool  \u2014 it even supports models built with commercial tools and services where the model is exportable.\n\nIt includes an API with two key endpoints:\n\n1.  **[Predict](http:\/\/docs.seldon.io\/prediction-guide.html)** - Build and deploy supervised machine learning models created in any machine learning library or framework at scale using containers and [microservices](http:\/\/docs.seldon.io\/api-microservices.html).\n2.  **[Recommend](http:\/\/docs.seldon.io\/content-recommendation-guide.html)** - High-performance user activity and content based recommendation engine with various algorithms ready to run out of the box. \n\nOther features include:\n\n- Complex dynamic [algorithm configuration and combination](http:\/\/docs.seldon.io\/advanced-recommender-config.html) with no downtime: run A\/B and Multivariate tests, cascade algorithms and create ensembles.\n- Command Line Interface ([CLI](http:\/\/docs.seldon.io\/seldon-cli.html)) for configuring and managing Seldon Server.\n- Secure OAuth 2.0 REST and [gRPC](http:\/\/docs.seldon.io\/grpc.html) APIs to streamline integration with your data and application.\n- Grafana dashboard for [real-time analytics](http:\/\/docs.seldon.io\/analytics.html) built with Kafka Streams, Fluentd and InfluxDB.\n\nSeldon is used by some of the world's most innovative organisations \u2014 it's the perfect machine learning deployment platform for start-ups and can scale to meet the demands of large enterprises.\n\n## Get Started\n\nIt takes a few minutes to install Seldon on a Kubernetes cluster. Visit our [install guide](http:\/\/docs.seldon.io\/install.html) and read our [tech docs](http:\/\/docs.seldon.io).\n\n## Community & Support\n\n* Join the [Seldon Users Group](https:\/\/groups.google.com\/forum\/#!forum\/seldon-users).\n* [Register for our newsletter](http:\/\/eepurl.com\/6X6n1) to be the first to receive updates about our products and events.\n* Visit [our website](https:\/\/www.seldon.io\/), follow [@seldon_io](https:\/\/twitter.com\/seldon_io) on Twitter and like [our Facebook page](https:\/\/www.facebook.com\/seldonhq\/).\n* If you're in London, meet us at [TensorFlow London](https:\/\/www.meetup.com\/TensorFlow-London\/) - a community of over 1200 data scientists that we co-organise.\n* We also offer [commercial support plans and managed services](https:\/\/www.seldon.io\/enterprise\/).\n\n## License\nSeldon is available under [Apache Licence, Version 2.0](https:\/\/github.com\/SeldonIO\/seldon-server\/blob\/master\/README.md)\n","9":"# Android TensorFlow Machine Learning Example\n[![Mindorks](https:\/\/img.shields.io\/badge\/mindorks-opensource-blue.svg)](https:\/\/mindorks.com\/open-source-projects)\n[![Mindorks Community](https:\/\/img.shields.io\/badge\/join-community-blue.svg)](https:\/\/mindorks.com\/join-community)\n[![Open Source Love](https:\/\/badges.frapsoft.com\/os\/v1\/open-source.svg?v=102)](https:\/\/opensource.org\/licenses\/Apache-2.0)\n[![License](https:\/\/img.shields.io\/badge\/license-Apache%202.0-blue.svg)](https:\/\/github.com\/amitshekhariitbhu\/AndroidTensorFlowMachineLearningExample\/blob\/master\/LICENSE)\n\n<img src=https:\/\/raw.githubusercontent.com\/MindorksOpenSource\/AndroidTensorFlowMachineLearningExample\/master\/assets\/ml_android.png >\n\n##  About Android TensorFlow Machine Learning Example\n* This is an example project for integrating [TensorFlow](https:\/\/github.com\/tensorflow\/tensorflow) into Android application\n* How to build TensorFlow project to use with Android project.\n* How to build TensorFlow library(.so file and jar file) to use with Android Application.\n* This project include an example for object detection for an image taken from camera using TensorFlow library.\n\n# [Read this article. It describes everything about building TensorFlow for Android.](https:\/\/blog.mindorks.com\/android-tensorflow-machine-learning-example-ff0e9b2654cc)\n\n# [Check the Android TensorFlow Lite Machine Learning Example.](https:\/\/github.com\/amitshekhariitbhu\/Android-TensorFlow-Lite-Example)\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/MindorksOpenSource\/AndroidTensorFlowMachineLearningExample\/master\/assets\/keyboard_example.png\" width=\"250\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/MindorksOpenSource\/AndroidTensorFlowMachineLearningExample\/master\/assets\/pen_example.png\" width=\"250\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/MindorksOpenSource\/AndroidTensorFlowMachineLearningExample\/master\/assets\/wallet_example.png\" width=\"250\">\n<\/p>\n<img src=https:\/\/raw.githubusercontent.com\/MindorksOpenSource\/AndroidTensorFlowMachineLearningExample\/master\/assets\/sample_combined.png >\n<br>\n<br>\n\n### Find this project useful ? :heart:\n* Support it by clicking the :star: button on the upper right of this page. :v:\n\n### Credits\n* The classifier example has been taken from Google TensorFlow example.\n\n[Check out Mindorks awesome open source projects here](https:\/\/mindorks.com\/open-source-projects)\n\n### License\n```\n   Copyright (C) 2017 MINDORKS NEXTGEN PRIVATE LIMITED\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n### Contributing to Android TensorFlow Machine Learning Example\nJust make pull request. You are in!\n","10":"[![Build Status](https:\/\/travis-ci.com\/o19s\/elasticsearch-learning-to-rank.svg?branch=master)](https:\/\/travis-ci.com\/o19s\/elasticsearch-learning-to-rank)\n\nThe Elasticsearch Learning to Rank plugin uses machine learning to improve search relevance ranking. It's powering search at places like Wikimedia Foundation and Snagajob!\n\n# What this plugin does...\n\nThis plugin:\n\n- Allows you to store features (Elasticsearch query templates) in Elasticsearch\n- Logs features scores (relevance scores) to create a training set for offline model development\n- Stores linear, xgboost, or ranklib ranking models in Elasticsearch that use features you've stored\n- Ranks search results using a stored model\n\n## Where's the docs?\n\nWe recommend taking time to [read the docs](http:\/\/elasticsearch-learning-to-rank.readthedocs.io). There's quite a bit of detailed information about learning to rank basics and how this plugin can ease learning to rank development. \n\nYou can also participate in regular [trainings](http:\/\/opensourceconnections.com\/events\/training) on Elasticsearch Learning to Rank, which support the free work done on this plugin.\n\n## I want to jump in!\n\nThe demo lives in another repo now, [Hello LTR](https:\/\/github.com\/o19s\/hello-ltr) and it has both ES and Solr example. Follow the directions for Elasticsearch in the README to set up the environment and start with the [notebooks\/elasticsearch\/tmdb\/hello-ltr.ipynb](https:\/\/github.com\/o19s\/hello-ltr\/blob\/master\/notebooks\/elasticsearch\/tmdb\/hello-ltr%20(ES).ipynb). Have fun!\n\n# Installing\n\nSee the full list of [prebuilt versions](https:\/\/github.com\/o19s\/elasticsearch-learning-to-rank\/releases) and select the version that matches your Elasticsearch version. If you don't see a version available, see the link below for building or file a request via [issues](https:\/\/github.com\/o19s\/elasticsearch-learning-to-rank\/issues).\n\nTo install, you'd run a command like this but replacing with the appropriate prebuilt version zip:\n\n`.\/bin\/elasticsearch-plugin install https:\/\/github.com\/o19s\/elasticsearch-learning-to-rank\/releases\/download\/v1.5.4-es7.11.2\/ltr-plugin-v1.5.4-es7.11.2.zip`\n\n(It's expected you'll confirm some security exceptions, you can pass `-b` to `elasticsearch-plugin` to automatically install)\n\nIf you already are running Elasticsearch, don't forget to restart!\n\n# Known issues\nAs any other piece of software, this plugin is not exempt from issues. Please read the [known issues](KNOWN_ISSUES.md) to learn about the current issues that we are aware of. This file might include workarounds to mitigate them when possible.\n\n# Build and Deploy Locally\n\nNotes if you want to dig into the code or build for a version there's no build for, please feel free to run the build and installation process yourself:\n\n```\n.\/gradlew clean check\n.\/bin\/elasticsearch-plugin install file:\/\/\/path\/to\/elasticsearch-learning-to-rank\/build\/distributions\/ltr-<LTR-VER>-es<ES-VER>.zip\n```\n\n# How to Contribute\n\nFor more information on helping us out (we need your help!), developing with the plugin, creating docs, etc please read [CONTRIBUTING.md](\/CONTRIBUTING.md).\n\n## Elastic Release Support\nWe do our best to officially support `*.*.1` releases of Elasticsearch.  If you have a need for \"dot-oh\" compatibility or a version we don't support please consider submitting a PR.\n\n \n# Who built this?\n- [Initially developed](http:\/\/opensourceconnections.com\/blog\/2017\/02\/14\/elasticsearch-learning-to-rank\/) at [OpenSource Connections](http:\/\/opensourceconnections.com).\n- Significant contributions by [Wikimedia Foundation](https:\/\/wikimediafoundation.org\/wiki\/Home), [Snagajob Engineering](https:\/\/engineering.snagajob.com\/), [Bonsai](https:\/\/bonsai.io\/), and [Yelp Engineering](https:\/\/engineeringblog.yelp.com\/)\n- Thanks to [Jettro Coenradie](https:\/\/amsterdam.luminis.eu\/author\/jettro\/) for porting to ES 6.1\n\n## Other Acknowledgments & Stuff To Read\n- Bloomberg's [Learning to Rank work for Solr](https:\/\/issues.apache.org\/jira\/browse\/SOLR-8542)\n- Our Berlin Buzzwords Talk, [We built an Elasticsearch Learning to Rank plugin. Then came the hard part](https:\/\/berlinbuzzwords.de\/17\/session\/we-built-elasticsearch-learning-rank-plugin-then-came-hard-part)\n- Blog article on [How is Search Different from Other Machine Learning Problems](http:\/\/opensourceconnections.com\/blog\/2017\/08\/03\/search-as-machine-learning-prob\/)\n- Also check out our other relevance\/search thingies: book [Relevant Search](http:\/\/manning.com\/books\/relevant-search), projects [Elyzer](http:\/\/github.com\/o19s\/elyzer), [Splainer](http:\/\/splainer.io), and [Quepid](http:\/\/quepid.com)\n","11":"Datumbox Machine Learning Framework\n===================================\n[![Build Status](https:\/\/api.travis-ci.org\/datumbox\/datumbox-framework.svg)](https:\/\/travis-ci.org\/datumbox\/datumbox-framework) [![Windows Build status](https:\/\/ci.appveyor.com\/api\/projects\/status\/2aqkak8kmt8ooj4i?svg=true)](https:\/\/ci.appveyor.com\/project\/datumbox\/datumbox-framework) [![Maven Central](https:\/\/maven-badges.herokuapp.com\/maven-central\/com.datumbox\/datumbox-framework-lib\/badge.svg)](https:\/\/maven-badges.herokuapp.com\/maven-central\/com.datumbox\/datumbox-framework-lib) [![License](https:\/\/img.shields.io\/:license-apache-brightgreen.svg)](.\/LICENSE)\n\n[![Datumbox](http:\/\/www.datumbox.com\/img\/logo.png)](http:\/\/www.datumbox.com\/)\n\nThe Datumbox Machine Learning Framework is an open-source framework written in Java which allows the rapid development Machine Learning and Statistical applications. The main focus of the framework is to include a large number of machine learning algorithms & statistical methods and to be able to handle large sized datasets. \n\nCopyright & License\n-------------------\n\nCopyright (C) 2013-2020 [Vasilis Vryniotis](http:\/\/blog.datumbox.com\/author\/bbriniotis\/). \n\nThe code is licensed under the [Apache License, Version 2.0](.\/LICENSE).\n\nInstallation & Versioning\n-------------------------\n\nDatumbox Framework is available on [Maven Central Repository](http:\/\/search.maven.org\/#search%7Cga%7C1%7Cg%3A%22com.datumbox%22%20AND%20a%3A%22datumbox-framework-lib%22). \n\nThe latest stable version of the framework is 0.8.2 (Build 20200805). To use it, add the following snippet in your pom.xml:\n```\n    <dependency>\n        <groupId>com.datumbox<\/groupId>\n        <artifactId>datumbox-framework-lib<\/artifactId>\n        <version>0.8.2<\/version>\n    <\/dependency>\n```\n\nThe latest snapshot version of the framework is 0.8.3-SNAPSHOT (Build 20201014). To test it, update your pom.xml as follows:\n```\n    <repository>\n       <id>sonatype-snapshots<\/id>\n       <name>sonatype snapshots repo<\/name>\n       <url>https:\/\/oss.sonatype.org\/content\/repositories\/snapshots<\/url>\n    <\/repository>\n\n    <dependency>\n        <groupId>com.datumbox<\/groupId>\n        <artifactId>datumbox-framework-lib<\/artifactId>\n        <version>0.8.3-SNAPSHOT<\/version>\n    <\/dependency>\n```\n\nThe [develop branch](https:\/\/github.com\/datumbox\/datumbox-framework\/tree\/develop) is the development branch (default github branch), while the [master branch](https:\/\/github.com\/datumbox\/datumbox-framework\/tree\/master) contains the latest stable version of the framework. All the stable releases are marked with [tags](https:\/\/github.com\/datumbox\/datumbox-framework\/releases).\n\nThe releases of the framework follow the [Semantic Versioning](http:\/\/semver.org\/) approach. For detailed information about the various releases check out the [Changelog](.\/CHANGELOG.md).\n\nDocumentation and Code Examples\n-------------------------------\n\nAll the public methods and classes of the Framework are documented with Javadoc comments. Moreover for every model there is a JUnit Test which clearly shows how to train and use the models. Finally for more examples on how to use the framework checkout the [Code Examples](https:\/\/github.com\/datumbox\/datumbox-framework-examples\/) or the [official Blog](http:\/\/blog.datumbox.com\/).\n\nPre-trained Models\n------------------\n\nDatumbox comes with a large number of pre-trained models which allow you to perform Sentiment Analysis (Document & Twitter), Subjectivity Analysis, Topic Classification, Spam Detection, Adult Content Detection, Language Detection, Commercial Detection, Educational Detection and Gender Detection. To get the binary models check out the [Datumbox Zoo](https:\/\/github.com\/datumbox\/datumbox-framework-zoo\/).\n\nWhich methods\/algorithms are supported?\n---------------------------------------\n\nThe Framework currently supports performing multiple Parametric & non-parametric Statistical tests, calculating descriptive statistics on censored & uncensored data, performing ANOVA, Cluster Analysis, Dimension Reduction, Regression Analysis, Timeseries Analysis, Sampling and calculation of probabilities from the most common discrete and continues Distributions. In addition it provides several implemented algorithms including Max Entropy, Naive Bayes, SVM, Bootstrap Aggregating, Adaboost, Kmeans, Hierarchical Clustering, Dirichlet Process Mixture Models, Softmax Regression, Ordinal Regression, Linear Regression, Stepwise Regression, PCA and several other techniques that can be used for feature selection, ensemble learning, linear programming solving and recommender systems.\n\nBug Reports\n-----------\n\nDespite the fact that parts of the Framework have been used in commercial applications, not all classes are equally used\/tested. Currently the framework is in Alpha version, so you should expect some changes on the public APIs on future versions. If you spot a bug please [submit it as an Issue](https:\/\/github.com\/datumbox\/datumbox-framework\/issues) on the official Github repository. \n\nContributing\n------------\n\nThe Framework can be improved in many ways and as a result any contribution is welcome. By far the most important feature missing from the Framework is the ability to use it from command line or from other languages such as Python. Other important enhancements include improving the documentation, the test coverage and the examples, improving the architecture of the framework and supporting more Machine Learning and Statistical Models. If you make any useful changes on the code, please consider contributing them by sending a pull request.\n\nAcknowledgements\n----------------\n\nMany thanks to [Eleftherios Bampaletakis](http:\/\/gr.linkedin.com\/pub\/eleftherios-bampaletakis\/39\/875\/551) for his invaluable input on improving the architecture of the Framework. Also many thanks to ej-technologies GmbH for providing a license for their [Java Profiler](http:\/\/www.ej-technologies.com\/products\/jprofiler\/overview.html) and to JetBrains for providing a license for their [Java IDE](https:\/\/www.jetbrains.com\/idea\/).\n\nUseful Links\n------------\n\n- [Code Examples](https:\/\/github.com\/datumbox\/datumbox-framework-examples\/)\n- [Datumbox Zoo: Pre-trained models](https:\/\/github.com\/datumbox\/datumbox-framework-zoo\/)\n- [Datumbox.com](http:\/\/www.datumbox.com\/)\n- [Machine Learning Blog](http:\/\/blog.datumbox.com\/)\n\n","12":"<p align=\"center\"><img width=\"50%\" alt=\"Tribuo Logo\" src=\"docs\/img\/Tribuo_Logo_Colour.png\" \/><\/p>\n\n# Tribuo - A Java prediction library (v4.2)\n\n[Tribuo](https:\/\/tribuo.org) is a machine learning library in Java that\nprovides multi-class classification, regression, clustering, anomaly detection\nand multi-label classification. Tribuo provides implementations of popular ML\nalgorithms and also wraps other libraries to provide a unified interface.\nTribuo contains all the code necessary to load, featurise and transform data.\nAdditionally, it includes the evaluation classes for all supported prediction\ntypes. Development is led by [Oracle Labs](https:\/\/labs.oracle.com)' Machine\nLearning Research Group;  we welcome community contributions.\n\nAll trainers are configurable using the\n[OLCUT](https:\/\/github.com\/oracle\/olcut) configuration system. This allows a\nuser to define a trainer in an xml file and repeatably build models.  Example\nconfigurations for each of the supplied Trainers can be found in the config\nfolder of each package. These configuration files can also be written in json\nor edn by using the appropriate OLCUT configuration dependency. Models and\ndatasets are serializable using Java serialization. \n\nAll models and evaluations include a serializable provenance object which\nrecords the creation time of the model or evaluation, the identity of the data\nand any transformations applied to it, as well as the hyperparameters of the\ntrainer. In the case of evaluations, this provenance information also includes\nthe specific model used. Provenance information can be extracted as JSON, or\nserialised directly using Java serialisation. For production deployments,\nprovenance information can be redacted and replaced with a hash to provide\nmodel tracking through an external system.  Many Tribuo models can be exported\nin ONNX format for deployment in other languages, platforms or cloud services.\n\nTribuo runs on Java 8+, and we test on LTS versions of Java along with the\nlatest release.  Tribuo itself is a pure Java library and is supported on all\nJava platforms;  however, some of our interfaces require native code and are\nthus supported only where there is native library support. We test on x86\\_64\narchitectures on Windows 10, macOS and Linux (RHEL\/OL\/CentOS 7+), as these are\nsupported platforms for the native libraries with which we interface. If you're\ninterested in another platform and wish to use one of the native library\ninterfaces (ONNX Runtime, TensorFlow, and XGBoost), we recommend reaching out\nto the developers of those libraries. Note the reproducibility package\nrequires Java 17, and as such is not part of the `tribuo-all` Maven Central\ndeployment.\n\n## Documentation\n\n* [Library Architecture](docs\/Architecture.md)\n* [Package Overview](docs\/PackageOverview.md)\n* Javadoc [4.2](https:\/\/tribuo.org\/learn\/4.2\/javadoc), [4.1](https:\/\/tribuo.org\/learn\/4.1\/javadoc\/), [4.0](https:\/\/tribuo.org\/learn\/4.0\/javadoc\/)\n* [Helper Programs](docs\/HelperPrograms.md)\n* [Developer Documentation](docs\/Internals.md)\n* [Roadmap](docs\/Roadmap.md)\n* [Frequently Asked Questions](docs\/FAQs.md)\n\n## Tutorials\n\nTutorial notebooks, including examples of Classification, Clustering,\nRegression, Anomaly Detection, TensorFlow, document classification, columnar\ndata loading, working with externally trained models, and the configuration\nsystem, can be found in the [tutorials](tutorials). These use the\n[IJava](https:\/\/github.com\/SpencerPark\/IJava) Jupyter notebook kernel, and work\nwith Java 10+, except the reproducibility tutotiral which requires Java 17.  To\nconvert the tutorials' code back to Java 8, in most cases simply replace the\n`var` keyword with the appropriate types.\n\n## Algorithms\n\n### General predictors\n\nTribuo includes implementations of several algorithms suitable for a wide range \nof prediction tasks:\n\n|Algorithm|Implementation|Notes|\n|---|---|---|\n|Bagging|Tribuo|Can use any Tribuo trainer as the base learner|\n|Random Forest|Tribuo|For both classification and regression|\n|Extra Trees|Tribuo|For both classification and regression|\n|K-NN|Tribuo|Includes options for several parallel backends, as well as a single threaded backend|\n|Neural Networks|TensorFlow|Train a neural network in TensorFlow via the Tribuo wrapper. Models can be deployed using the ONNX interface or the TF interface|\n\nThe ensembles and K-NN use a combination function to produce their output.\nThese combiners are prediction task specific, but the ensemble & K-NN \nimplementations are task agnostic. We provide voting and averaging combiners\nfor multi-class classification, multi-label classification and regression tasks.\n\n### Classification\n\nTribuo has implementations or interfaces for:\n\n|Algorithm|Implementation|Notes|\n|---|---|---|\n|Linear models|Tribuo|Uses SGD and allows any gradient optimizer|\n|Factorization Machines|Tribuo|Uses SGD and allows any gradient optimizer|\n|CART|Tribuo||\n|SVM-SGD|Tribuo|An implementation of the Pegasos algorithm|\n|Adaboost.SAMME|Tribuo|Can use any Tribuo classification trainer as the base learner|\n|Multinomial Naive Bayes|Tribuo||\n|Regularised Linear Models|LibLinear||\n|SVM|LibSVM or LibLinear|LibLinear only supports linear SVMs|\n|Gradient Boosted Decision Trees|XGBoost||\n\nTribuo also supplies a linear chain CRF for sequence classification tasks. This\nCRF is trained via SGD using any of Tribuo's gradient optimizers.\n\nTo explain classifier predictions there is an implementation of the LIME\nalgorithm. Tribuo's implementation allows the mixing of text and tabular data,\nalong with the use of any sparse model as an explainer (e.g., regression trees,\nlasso etc), however it does not support images.\n\n### Regression\n\nTribuo's regression algorithms are multidimensional by default. Single \ndimensional implementations are wrapped in order to produce multidimensional\noutput.\n\n|Algorithm|Implementation|Notes|\n|---|---|---|\n|Linear models|Tribuo|Uses SGD and allows any gradient optimizer|\n|Factorization Machines|Tribuo|Uses SGD and allows any gradient optimizer|\n|CART|Tribuo||\n|Lasso|Tribuo|Using the LARS algorithm|\n|Elastic Net|Tribuo|Using the co-ordinate descent algorithm|\n|Regularised Linear Models|LibLinear||\n|SVM|LibSVM or LibLinear|LibLinear only supports linear SVMs|\n|Gradient Boosted Decision Trees|XGBoost||\n\n### Clustering\n\nTribuo includes infrastructure for clustering and also supplies two \nclustering algorithm implementations. We expect to implement additional\nalgorithms over time.\n\n|Algorithm|Implementation|Notes|\n|---|---|---|\n|HDBSCAN\\*|Tribuo|A density-based algorithm which discovers clusters and outliers|\n|K-Means|Tribuo|Includes both sequential and parallel backends, and the K-Means++ initialisation algorithm|\n\n### Anomaly Detection\n\nTribuo offers infrastructure for anomaly detection tasks. \nWe expect to add new implementations over time.\n\n|Algorithm|Implementation|Notes|\n|---|---|---|\n|One-class SVM|LibSVM||\n|One-class linear SVM|LibLinear||\n\n### Multi-label classification\n\nTribuo offers infrastructure for multi-label classification, along\nwith a wrapper which converts any of Tribuo's multi-class classification\nalgorithms into a multi-label classification algorithm. We expect to add \nmore multi-label specific implementations over time.\n\n|Algorithm|Implementation|Notes|\n|---|---|---|\n|Independent wrapper|Tribuo|Converts a multi-class classification algorithm into a multi-label one by producing a separate classifier for each label|\n|Classifier Chains|Tribuo|Provides classifier chains and randomized classifier chain ensembles using any of Tribuo's multi-class classification algorithms|\n|Linear models|Tribuo|Uses SGD and allows any gradient optimizer|\n|Factorization Machines|Tribuo|Uses SGD and allows any gradient optimizer|\n\n### Interfaces\n\nIn addition to our own implementations of Machine Learning algorithms, Tribuo\nalso provides a common interface to popular ML tools on the JVM. If you're\ninterested in contributing a new interface, open a GitHub Issue, and we can\ndiscuss how it would fit into Tribuo.\n\nCurrently we have interfaces to:\n\n* [LibLinear](https:\/\/github.com\/bwaldvogel\/liblinear-java) - via the LibLinear-java port of the original [LibLinear](https:\/\/www.csie.ntu.edu.tw\/~cjlin\/liblinear\/) (v2.43).\n* [LibSVM](https:\/\/www.csie.ntu.edu.tw\/~cjlin\/libsvm\/) - using the pure Java transformed version of the C++ implementation (v3.25).\n* [ONNX Runtime](https:\/\/onnxruntime.ai) - via the Java API contributed by our group (v1.9.0).\n* [TensorFlow](https:\/\/tensorflow.org) - Using [TensorFlow Java](https:\/\/github.com\/tensorflow\/java) v0.4.1 (based on TensorFlow v2.7.1). This allows the training and deployment of TensorFlow models entirely in Java.\n* [XGBoost](https:\/\/xgboost.ai) - via the built in XGBoost4J API (v1.5.0).\n\n## Binaries\n\nBinaries are available on Maven Central, using groupId `org.tribuo`. To pull\nall of Tribuo, including the bindings for TensorFlow, ONNX Runtime and XGBoost\n(which are native libraries), use:\n\nMaven:\n```xml\n<dependency>\n    <groupId>org.tribuo<\/groupId>\n    <artifactId>tribuo-all<\/artifactId>\n    <version>4.2.1<\/version>\n    <type>pom<\/type>\n<\/dependency>\n```\nor from Gradle:\n```groovy\nimplementation (\"org.tribuo:tribuo-all:4.2.1@pom\") {\n    transitive = true \/\/ for build.gradle (i.e., Groovy)\n    \/\/ isTransitive = true \/\/ for build.gradle.kts (i.e., Kotlin)\n}\n```\n\nThe `tribuo-all` dependency is a pom which depends on all the Tribuo\nsubprojects except for the reproducibility project which requires Java 17.\n\nMost of Tribuo is pure Java and thus cross-platform, however some of the\ninterfaces link to libraries which use native code. Those interfaces\n(TensorFlow, ONNX Runtime and XGBoost) only run on supported platforms for the\nrespective published binaries, and Tribuo has no control over which binaries\nare supplied. If you need support for a specific platform, reach out to the\nmaintainers of those projects. As of the 4.1 release these native packages all\nprovide x86\\_64 binaries for Windows, macOS and Linux. It is also possible to\ncompile each package for macOS ARM64 (i.e., Apple Silicon), though there are no\nbinaries available on Maven Central for that platform. When developing on an\nARM platform you can select the `arm` profile in Tribuo's pom.xml to disable\nthe native library tests.\n\nIndividual jars are published for each Tribuo module. It is preferable to\ndepend only on the modules necessary for the specific project. This prevents\nyour code from unnecessarily pulling in large dependencies like TensorFlow.\n\n## Compiling from source\n\nTribuo uses [Apache Maven](https:\/\/maven.apache.org\/) v3.5 or higher to build.\nTribuo is compatible with Java 8+, and we test on LTS versions of Java along\nwith the latest release. To build, simply run `mvn clean package`. All Tribuo's\ndependencies should be available on Maven Central. Please file an issue for\nbuild-related issues if you're having trouble (though do check if you're\nmissing proxy settings for Maven first, as that's a common cause of build\nfailures, and out of our control).\n\n## Repository Layout\n\nDevelopment happens on the `main` branch, which has the version number of the\nnext Tribuo release with \"-SNAPSHOT\" appended to it. Tribuo major and minor\nreleases will be tagged on the `main` branch, and then have a branch named\n`vA.B.X-release-branch` (for release `vA.B.0`) branched from the tagged release\ncommit for any point releases (i.e., `vA.B.1`, `vA.B.2` etc) following from\nthat major\/minor release. Those point releases are tagged on the specific\nrelease branch e.g., `v4.0.2` is tagged on the `v4.0.X-release-branch`.\n\n## Contributing\n\nWe welcome contributions! See our [contribution guidelines](.\/CONTRIBUTING.md).\n\nWe have a discussion mailing list\n[tribuo-devel@oss.oracle.com](mailto:tribuo-devel@oss.oracle.com), archived\n[here](https:\/\/oss.oracle.com\/pipermail\/tribuo-devel\/). We're investigating\ndifferent options for real time chat, check back in the future. For bug\nreports, feature requests or other issues, please file a [Github\nIssue](https:\/\/github.com\/oracle\/tribuo\/issues).\n\nSecurity issues should follow our [reporting guidelines](.\/SECURITY.md).\n\n## License\n\nTribuo is licensed under the [Apache 2.0 License](.\/LICENSE.txt).\n\n## Release Notes:\n\n- [v4.2.1](https:\/\/github.com\/oracle\/tribuo\/blob\/main\/docs\/release-notes\/tribuo-v4-2-1-release-notes.md) - Bug fixes for KMeans' multithreading, nondeterministic iteration orders affecting ONNX export and K-Means initialization, and upgraded TF-Java to 0.4.1.\n- [v4.2.0](https:\/\/github.com\/oracle\/tribuo\/blob\/main\/docs\/release-notes\/tribuo-v4-2-release-notes.md) - Added factorization machines, classifier chains, HDBSCAN. Added ONNX export and OCI Data Science integration. Added reproducibility framework. Various other small fixes and improvements, including the regression fixes from v4.1.1. Filled out the remaining javadoc, added 4 new tutorials (onnx export, multi-label classification, reproducibility, hdbscan), expanded existing tutorials.\n- [v4.1.1](https:\/\/github.com\/oracle\/tribuo\/blob\/main\/docs\/release-notes\/tribuo-v4-1-1-release-notes.md) - Bug fixes for multi-output regression, multi-label evaluation, KMeans & KNN with SecurityManager, and update TF-Java 0.4.0.\n- [v4.1.0](https:\/\/github.com\/oracle\/tribuo\/blob\/main\/docs\/release-notes\/tribuo-v4-1-release-notes.md) - Added TensorFlow training support, a BERT feature extractor, ExtraTrees, K-Means++, many linear model & CRF performance improvements, new tutorials on TF and document classification. Many bug fixes & documentation improvements.\n- [v4.0.2](https:\/\/github.com\/oracle\/tribuo\/blob\/main\/docs\/release-notes\/tribuo-v4-0-2-release-notes.md) - Many bug fixes (CSVDataSource, JsonDataSource, RowProcessor, LibSVMTrainer, Evaluations, Regressor serialization). Improved javadoc and documentation. Added two new tutorials (columnar data and external models).\n- [v4.0.1](https:\/\/github.com\/oracle\/tribuo\/blob\/main\/docs\/release-notes\/tribuo-v4-0-1-release-notes.md) - Bugfix for CSVReader to cope with blank lines, added IDXDataSource to allow loading of native MNIST format data.\n- [v4.0.0](https:\/\/github.com\/oracle\/tribuo\/blob\/main\/docs\/release-notes\/tribuo-v4-0-release-notes.md) - Initial public release.\n- v3 - Added provenance system, the external model support and onnx integrations.\n- v2 - Expanded beyond a classification system, to support regression, clustering and multi-label classification.\n- v1 - Initial internal release. This release only supported multi-class classification.\n","13":"[![Build Status](https:\/\/travis-ci.com\/MNCC\/Mallet.svg?branch=master)](https:\/\/travis-ci.com\/MNCC\/Mallet)\n[![codecov](https:\/\/codecov.io\/gh\/MNCC\/Mallet\/branch\/master\/graph\/badge.svg)](https:\/\/codecov.io\/gh\/MNCC\/Mallet)\n\nMallet\n======\n\nWebsite: https:\/\/mimno.github.io\/Mallet\/\n\nMALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\n\nMALLET includes sophisticated tools for document classification: efficient routines for converting text to \"features\", a wide variety of algorithms (including Na\u00efve Bayes, Maximum Entropy, and Decision Trees), and code for evaluating classifier performance using several commonly used metrics.\n\nIn addition to classification, MALLET includes tools for sequence tagging for applications such as named-entity extraction from text. Algorithms include Hidden Markov Models, Maximum Entropy Markov Models, and Conditional Random Fields. These methods are implemented in an extensible system for finite state transducers.\n\nTopic models are useful for analyzing large collections of unlabeled text. The MALLET topic modeling toolkit contains efficient, sampling-based implementations of Latent Dirichlet Allocation, Pachinko Allocation, and Hierarchical LDA.\n\nMany of the algorithms in MALLET depend on numerical optimization. MALLET includes an efficient implementation of Limited Memory BFGS, among many other optimization methods.\n\nIn addition to sophisticated Machine Learning applications, MALLET includes routines for transforming text documents into numerical representations that can then be processed efficiently. This process is implemented through a flexible system of \"pipes\", which handle distinct tasks such as tokenizing strings, removing stopwords, and converting sequences into count vectors.\n\nAn add-on package to MALLET, called GRMM, contains support for inference in general graphical models, and training of CRFs with arbitrary graphical structure.\n\n## Installation\n\nTo build a Mallet 2.0 development release, you must have the Apache ant build tool installed. From the command prompt, first change to the mallet directory, and then type\n`ant`\n\nIf `ant` finishes with `\"BUILD SUCCESSFUL\"`, Mallet is now ready to use.\n\nIf you would like to deploy Mallet as part of a larger application, it is helpful to create a single \".jar\" file that contains all of the compiled code. Once you have compiled the individual Mallet class files, use the command:\n`ant jar`\n\nThis process will create a file \"mallet.jar\" in the \"dist\" directory within Mallet.\n\n## Usage\n\nOnce you have installed Mallet you can use it using the following command:\n```\nbin\/mallet [command] --option value --option value ...\n```\nType `bin\/mallet` to get a list of commands, and use the option `--help` with any command to get a description of valid options.\n\nFor details about the commands please visit the API documentation and website at: https:\/\/mimno.github.io\/Mallet\/\n\n\n## List of Algorithms:\n\n* Topic Modelling\n  * LDA\n  * Parallel LDA\n  * DMR LDA\n  * Hierarchical LDA\n  * Labeled LDA\n  * Polylingual Topic Model\n  * Hierarchical Pachinko Allocation Model (PAM)\n  * Weighted Topic Model\n  * LDA with integrated phrase discovery\n  * Word Embeddings (word2vec) using skip-gram with negative sampling\n* Classification\n  * AdaBoost\n  * Bagging\n  * Winnow\n  * C45 Decision Tree\n  * Ensemble Trainer\n  * Maximum Entropy Classifier (Multinomial Logistic Regression)\n  * Naive Bayes\n  * Rank Maximum Entropy Classifier\n  * Posterior Regularization Auxiliary Model\n* Clustering\n  * Greedy Agglomerative\n  * Hill Climbing\n  * K-Means\n  * K-Best\n* Sequence Prediction Models\n  * Conditional Random Fields\n  * Maximum Entropy Markov Models\n  * Hidden Markov Models\n  * Semi-Supervised Sequence Prediction Models\n* Linear Regression\n\n\n\n","14":"# cv4j\n\n[![License](https:\/\/img.shields.io\/badge\/license-Apache%202-lightgrey.svg)](https:\/\/www.apache.org\/licenses\/LICENSE-2.0.html)\n[![Build Status](https:\/\/travis-ci.org\/imageprocessor\/cv4j.svg?branch=master)](https:\/\/travis-ci.org\/imageprocessor\/cv4j)\n[![](https:\/\/jitpack.io\/v\/imageprocessor\/cv4j.svg)](https:\/\/jitpack.io\/#imageprocessor\/cv4j)\n\nThe target is to set up a high quality and real-time image process and machine learning library which is implemented in pure java. The framework can run application on java desktop and android platform.\n\n![](logo.png)\n\n#  The latest version\n\nModule|cv4j|rxcv4j\n---|:-------------:|:-------------:\nThe latest version|[![](https:\/\/jitpack.io\/v\/imageprocessor\/cv4j.svg)](https:\/\/jitpack.io\/#imageprocessor\/cv4j)|[![](https:\/\/jitpack.io\/v\/imageprocessor\/cv4j.svg)](https:\/\/jitpack.io\/#imageprocessor\/cv4j)\n\n# Download and use\n## 1.Download cv4j separately\n\n```groovy\nimplementation 'com.github.imageprocessor.cv4j:cv4j:0.1.2'\n```\n\n## 2.Download rxcv4j\n\nrxcv4j is packaged with RxJava2.x, if you download this module, you don\u2019t need to download cv4j.\n\n```groovy\nimplementation 'com.github.imageprocessor.cv4j:rxcv4j:0.1.2'\n```\n\n\nFunctions currently implemented:\n![](cv4j.png)\n\n#  Common filters\n\n| Filter name        | effect          | Remarks          |\n| ------------- |:-------------:| :-------------:|\n|BoxBlurFilter|\u76d2\u5b50\u6a21\u7cca\uff0c\u652f\u6301\u6c34\u5e73\u4e0e\u5782\u76f4\u7ecf\u5411\u6a21\u7cca\uff0c\u6a21\u7cca\u534a\u5f84\u901a\u5e38\u4e3a\u5947\u65701\u30013\u30015\u30017\u30019\u300111\u300115|Fast fuzzy algorithm|\n|CarveFilter|\u6d6e\u96d5\u6548\u679c, \u652f\u6301\u4e24\u79cd\u6d6e\u96d5\u7c7b\u578b\uff0c\u6839\u636eboolean\u53c2\u6570\u51b3\u5b9a\u4f7f\u7528\u54ea\u4e00\u79cd|Pixel difference calculation\n|ColorFilter|\u989c\u8272\u5339\u914d\uff0c\u652f\u630112\u79cd\u989c\u8272\u98ce\u683c\u8f6c\u6362(\u76f8\u5f53\u4e8e12\u4e2a\u6ee4\u955c)\uff0c \u57fa\u4e8e\u67e5\u627e\u8868\u5b9e\u73b0\uff0c\u901f\u5ea6\u6781\u5feb|LUT\u52a0\u901f\u8ba1\u7b97|\n|ConBriFilter|\u8c03\u6574\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u4e0e\u4eae\u5ea6\uff0c\u9ed8\u8ba4\u662f\u63d0\u5347\u5bf9\u6bd4\uff0c\u964d\u4f4e\u4eae\u5ea6\uff0c\u9ed8\u8ba4\u503c\u5206\u522b\u4e3a1.2\uff0c 0.7|\u5bf9\u6bd4\u5ea6\u4e0e\u4eae\u5ea6\u8c03\u6574|\n|EmbossFilter|\u8f67\u82b1\u6548\u679c\uff0c\u57fa\u4e8e\u8ddf\u6d6e\u96d5\u6548\u679c\u7c7b\u4f3c\uff0c\u4f46\u662f\u66f4\u52a0\u7075\u6d3b\uff0c\u53ef\u4ee5\u6267\u884c\u66f4\u591a\u50cf\u7d20\u503c\u6821\u6b63|\u8f67\u82b1\u6548\u679c|\n|ExposureFilter|\u66dd\u5149\u6548\u679c\uff0c\u57fa\u4e8e\u56fe\u50cf\u50cf\u7d20\u503c\u53d6\u53cd\uff0c\u6709\u4e00\u79cd\u7c7b\u4f3c\u5149\u5b66\u76f8\u673a\u66dd\u5149\u7167\u7247\u7684\u6548\u679c|\u5149\u7206\u6548\u679c\n|FastEPFilter|\u8fb9\u7f18\u4fdd\u7559\u6ee4\u6ce2-\u57fa\u4e8e\u79ef\u5206\u56fe\u7684\u8fb9\u7f18\u4fdd\u7559\u6ee4\u6ce2\uff0c\u662f\u4e00\u79cd\u5f88\u597d\u7684\u566a\u58f0\u6291\u5236\u7b97\u6cd5\uff0c\u5e38\u7528\u6765\u5b9e\u73b0\u4eba\u8138\u7f8e\u5316\u7684\u5173\u952e\u6b65\u9aa4|\u8fb9\u7f18\u4fdd\u7559|\n|FloSteDitheringFilter|\u6296\u52a8\u6548\u679c-\u57fa\u4e8e\u9519\u8bef\u6269\u5c55\u7684\u6296\u52a8\u7b97\u6cd5\uff0c\u5b9e\u73b0\u50cf\u7d20\u70b9\u79bb\u6563\u586b\u5145\u4e0e\u4e8c\u503c\u5316\u663e\u793a|\u6296\u52a8\u6548\u679c|\n|GammaFilter|\u4f3d\u9a6c\u6821\u6b63\uff0c\u57fa\u4e8e\u5149\u5b66\u76f8\u673a\u62cd\u7167\u4e4b\u540e\uff0c\u7ecf\u5e38\u9700\u8981\u505a\u6b64\u64cd\u4f5c|\u4f3d\u9a6c\u6821\u6b63|\n|GaussianBlurFilter|\u9ad8\u65af\u6a21\u7cca\uff0c\u57fa\u4e8e\u9ad8\u65af\u516c\u5f0f\u7684\u56fe\u50cf\u6a21\u7cca\uff0c\u6bd4\u76d2\u5b50\u6a21\u7cca\u6709\u66f4\u597d\u7684\u6548\u679c\uff0c\u4f46\u662f\u901f\u5ea6\u8f83\u6162|\u9ad8\u65af\u6a21\u7cca|\n|GaussianNoiseFilter|\u9ad8\u65af\u566a\u58f0-\u4ea7\u751f\u9ad8\u65af\u968f\u673a\u566a\u58f0\uff0c\u5728\u56fe\u50cf\u4e2d\u4f7f\u7528\u9ad8\u65af\u968f\u673a\u566a\u58f0|\u9ad8\u65af\u566a\u58f0|\n|GlowFilter|\u5149\u7ebf\u6548\u679c\uff0c\u5728\u56fe\u50cf\u4e2d\u6a21\u4eff\u81ea\u7136\u5149\u7ebf\u7167\u5c04|\u5149\u7ebf\u6548\u679c|\n|GradientFilter|\u68af\u5ea6\u6548\u679c\uff0c\u57fa\u4e8e\u56fe\u50cf\u4e00\u9636\u5bfc\u6570\u7b97\u5b50\u5b9e\u73b0\u7684\u68af\u5ea6\u6548\u679c\uff0c\u5f88\u597d\u7684\u663e\u793a\u4e86\u56fe\u50cf\u4e3b\u8981\u8f6e\u5ed3\u4e0e\u8fb9\u754c|\u68af\u5ea6\u6548\u679c|\n|MeansBinaryFilter|\u9ed1\u767d\u6548\u679c\uff0c \u57fa\u4e8e\u56fe\u50cf\u5747\u503c\u5b9e\u73b0\u56fe\u50cf\u4e8c\u503c\u5316\u6548\u679c|\u9ed1\u767d\u6548\u679c|\n|MosaicFilter|\u9a6c\u8d5b\u514b\u6548\u679c\uff0c\u57fa\u4e8e\u79ef\u5206\u56fe\u5b9e\u73b0\u7684\u56fe\u50cf\u9a6c\u8d5b\u514b\u6548\u679c\uff0c\u901f\u5ea6\u5feb\u6548\u679c\u597d\uff0c\u652f\u6301\u77e9\u5f62\u9a6c\u8d5b\u514b|\u9a6c\u8d5b\u514b\u6548\u679c|\n|MotionFilter|\u79fb\u52a8\u6548\u679c\uff0c\u57fa\u4e8e\u6a21\u7cca\u4e0e\u51e0\u4f55\u5f62\u53d8\u5b9e\u73b0\u7684\u56fe\u50cf\u6a21\u7cca\u79fb\u52a8\u6548\u679c|\u79fb\u52a8\u6548\u679c|\n|NatureFilter|\u81ea\u7136\u7cfb\u6ee4\u955c,\u6839\u636e\u53c2\u6570\u9009\u62e9\uff0c\u603b\u8ba1\u6709\u516b\u79cd\u81ea\u7136\u98ce\u683c\u7684\u989c\u8272\u6ee4\u955c(\u76f8\u5f53\u4e8e8\u4e2a\u6ee4\u955c)|\u81ea\u7136\u7cfb\u6548\u679c|\n|OilPaintFilter|\u6cb9\u753b\u6548\u679c\uff0c\u57fa\u4e8e\u8f6e\u5ed3\u4e0e\u989c\u8272\u5206\u5e03\u5b9e\u73b0\u7684\u6cb9\u753b\u6548\u679c\uff0c\u903c\u771f\u6a21\u62df\u79cb\u65e5\u98ce\u60c5|\u6cb9\u753b\u6548\u679c|\n|SepiaToneFilter|\u6000\u65e7\u98ce\u683c\uff0c \u7ecf\u5178\u7684\u56fe\u50cf\u6000\u65e7\u98ce\u683c\u6ee4\u955c\uff0c\u4e00\u79d2\u751f\u6210|\u6000\u65e7\u98ce\u683c|\n|SinCityFilter|\u90aa\u6076\u4e4b\u57ce\uff0c\u57fa\u4e8e\u50cf\u7d20\u503c\u5c40\u90e8\u53d8\u5316\u7684\u6ee4\u955c\uff0c\u53ef\u4ee5\u6839\u636e\u8f93\u5165\u53c2\u6570\u8c03\u6574\u9608\u503c\u8303\u56f4\u4e0e\u5bf9\u6bd4\u989c\u8272\u503c|\u90aa\u6076\u4e4b\u57ce|\n|SpotlightFilter|\u63a2\u7167\u706f\u6548\u679c\uff0c\u57fa\u4e8e\u4e2d\u5fc3\u50cf\u7d20\u8ddd\u79bb\u800c\u8c03\u6574\u56fe\u50cf\u4eae\u5ea6\u7684\uff0c\u4ea7\u751f\u4eae\u5ea6\u6269\u5c55\u6548\u679c|\u63a2\u7167\u706f\u6ee4\u955c|\n|StrokeAreaFilter|\u94c5\u7b14\u753b\u6548\u679c\uff0c \u6a21\u4eff\u94c5\u7b14\u6a21\u7cca\u5b9e\u73b0\uff0c \u53ef\u4ee5\u5c06\u8f93\u5165\u56fe\u50cf\u53d8\u4e86\u5199\u751f\u94c5\u7b14\u753b\uff0c\u7ec6\u8282\u591a\u5c11\u6839\u636e\u53c2\u6570\u51b3\u5b9a|\u94c5\u7b14\u753b\u6548\u679c|\n|VignetteFilter|\u8fb9\u6846\u6548\u679c\uff0c\u7ed9\u56fe\u50cf\u52a0\u4e0a\u7acb\u4f53\u8fb9\u6846\uff0c\u9ed8\u8ba4\u8fb9\u6846\u4e3a\u9ed1\u8272\uff0c\u53ef\u4ee5\u901a\u8fc7\u8f93\u5165\u53c2\u6570\u8c03\u6574\u8fb9\u6846\u5927\u5c0f\u4e0e\u8fb9\u6846\u989c\u8272|\u7acb\u4f53\u8fb9\u6846|\n|WaterFilter|\u6c34\u6ce2\u7eb9\u6548\u679c-\u6a21\u62df\u6c34\u6ce2\u7eb9\u6269\u5c55\u6548\u679c\uff0c\u5728\u8f93\u5165\u56fe\u50cf\u4e2d|\n|WhiteImageFilter|\u589e\u767d\u4eae\u5ea6-\u901a\u8fc7\u53c2\u6570\u53ef\u4ee5\u8c03\u6574\u56fe\u50cf\u4eae\u5ea6\uff0c\u6709\u7167\u7247\u589e\u767d\u6548\u679c|\u589e\u767d\u6548\u679c|\n\n# \u7a7a\u95f4\u5377\u79ef\u6ee4\u955c\n\n| filter       | \u540d\u79f0        | \u4f5c\u7528        |\n|:-------------|:-------------| :-------------|\n|ConvolutionHVFilter |\u5377\u79ef|\u6a21\u7cca\u6216\u8005\u964d\u566a|\n|MinMaxFilter|\u6700\u5927\u6700\u5c0f\u503c\u6ee4\u6ce2|\u53bb\u566a\u58f0|\n|SAPNoiseFilter |\u6912\u76d0\u566a\u58f0|\u589e\u52a0\u566a\u58f0|\n|SharpFilter |\u9510\u5316|\u589e\u5f3a|\n|MedimaFilter |\u4e2d\u503c\u6ee4\u6ce2|\u53bb\u566a\u58f0|\n|LaplasFilter |\u62c9\u666e\u62c9\u65af|\u63d0\u53d6\u8fb9\u7f18|\n|FindEdgeFilter |\u5bfb\u627e\u8fb9\u7f18|\u68af\u5ea6\u63d0\u53d6|\n|SobelFilter |\u68af\u5ea6|\u83b7\u53d6x\u3001y\u65b9\u5411\u7684\u68af\u5ea6\u63d0\u53d6|\n|VarianceFilter |\u65b9\u5dee\u6ee4\u6ce2|\u9ad8\u901a\u6ee4\u6ce2|\n|MaerOperatorFilter |\u9a6c\u5c14\u64cd\u4f5c|\u9ad8\u901a\u6ee4\u6ce2|\n|USMFilter |USM|\u589e\u5f3a|\n\n# \u8054\u7cfb\u65b9\u5f0f\ncv4j QQ\u4ea4\u6d41\u7fa4\uff1a492962708\n\ncv4j slack\u534f\u540c\u7ba1\u7406\uff1a[Join the Slack team for cv4j](https:\/\/cv4j.slack.com\/join\/shared_invite\/MTg5MDE1NDk2NDA1LTE0OTU4NzM2MjAtMjI3YTg0YzkyMA)\n\ncv4j\u7684\u7cfb\u5217\u6587\u7ae0\uff1ahttp:\/\/www.jianshu.com\/nb\/10401400\n\n# ChangeLog\n[\u7248\u672c\u66f4\u65b0\u8bb0\u5f55](CHANGELOG.md)\n\n# Contributing\n[Pull requests](https:\/\/help.github.com\/categories\/collaborating-with-issues-and-pull-requests\/) are welcome; see the [contributor guidelines](CONTRIBUTING.md) for details.\n","15":"# Machine Learning + Kafka Streams Examples\n\nThis project contains **examples which demonstrate how to deploy analytic models to mission-critical, scalable production leveraging [Apache Kafka](https:\/\/kafka.apache.org\/) and its [Streams API](https:\/\/docs.confluent.io\/current\/streams\/index.html).**\nExamples will include analytic models built with TensorFlow, Keras, H2O, Python, DeepLearning4J and other technologies.\n\n![Kafka Open Source Ecosystem for a Scalable Mission Critical Machine Learning Infrastructure](http:\/\/www.kai-waehner.de\/blog\/wp-content\/uploads\/2017\/10\/Apache_Kafka_Ecosystem_Kafka_Streams_Machine_Learning.png \"Kafka Open Source Ecosystem for a Scalable Mission Critical Machine Learning Infrastructure\")\n\n## Material (Blogs Posts, Slides, Videos)\n\nHere is some material about this topic if you want to read and listen to the theory instead of just doing hands-on:\n\n- Blog Post: [How to Build and Deploy Scalable Machine Learning in Production with Apache Kafka](https:\/\/www.confluent.io\/blog\/build-deploy-scalable-machine-learning-production-apache-kafka\/)\n- Slide Deck: [Apache Kafka + Machine Learning => Intelligent Real Time Applications](https:\/\/www.slideshare.net\/KaiWaehner\/apache-kafka-streams-machine-learning-deep-learning)\n- Slide Deck: [Deep Learning at Extreme Scale (in the Cloud) \u2028with the Apache Kafka Open Source Ecosystem](https:\/\/www.slideshare.net\/KaiWaehner\/deep-learning-at-extreme-scale-in-the-cloud-with-the-apache-kafka-open-source-ecosystem)\n- Video Recording: [Deep Learning in Mission Critical and Scalable Real Time Applications with Open Source Frameworks](https:\/\/vimeo.com\/jaxtv\/review\/256406763\/7fbf4213be)\n- Blog Post: [Using Apache Kafka to Drive Cutting-Edge Machine Learning - Hybrid ML Architectures, AutoML, and more...](https:\/\/www.confluent.io\/blog\/using-apache-kafka-drive-cutting-edge-machine-learning)\n- Blog Post: [Machine Learning with Python, Jupyter, KSQL and TensorFlow](https:\/\/www.confluent.io\/blog\/machine-learning-with-python-jupyter-ksql-tensorflow)\n- Blog Post: [Streaming Machine Learning with Tiered Storage and Without a Data Lake](https:\/\/www.confluent.io\/blog\/streaming-machine-learning-with-tiered-storage\/)\n\n## Use Cases and Technologies\n\n##### The following examples are already available including unit tests:\n\n* Deployment of a H2O GBM model to a Kafka Streams application for prediction of flight delays\n* Deployment of a H2O Deep Learning model to a Kafka Streams application for prediction of flight delays\n* Deployment of a pre-built TensorFlow CNN model for image recognition\n* Deployment of a DL4J model to predict the species of Iris flowers\n* Deployment of a Keras model (trained with TensorFlow backend) using the Import Model API from DeepLearning4J\n\n**More sophisticated use cases around Kafka Streams and other technologies will be added over time in this or related Github project. Some ideas**:\n\n* Image Recognition with H2O and TensorFlow (to show the difference of using H2O instead of using just low level TensorFlow APIs)\n* Anomaly Detection with Autoencoders leveraging DeepLearning4J.\n* Cross Selling and Customer Churn Detection using classical Machine Learning algorithms but also Deep Learning\n* Stateful Stream Processing to combine different model execution steps into a more powerful workflow instead of \"just\" inferencing single events (a good example might be a streaming process with sliding or session windows).\n* Keras to build different models with Python, TensorFlow, Theano and other Deep Learning frameworks under the hood + Kafka Streams as generic Machine Learning infrastructure to deploy, execute and monitor these different models.\n\n##### Some other Github projects exist already with more ML + Kafka content:\n\nThe most exciting and powerful example first:\n[Streaming Machine Learning at Scale from 100000 IoT Devices with HiveMQ, Apache Kafka and TensorFLow](https:\/\/github.com\/kaiwaehner\/hivemq-mqtt-tensorflow-kafka-realtime-iot-machine-learning-training-inference)\n\nHere some more demos:\n\n- Deep Learning UDF for KSQL: [Streaming Anomaly Detection of MQTT IoT Sensor Data using an Autoencoder](https:\/\/github.com\/kaiwaehner\/ksql-udf-deep-learning-mqtt-iot)\n- End-to-End ML Integration Demo: [Continuous Health Checks with Anomaly Detection using KSQL, Kafka Connect, Deep Learning and Elasticsearch](https:\/\/github.com\/kaiwaehner\/ksql-fork-with-deep-learning-function)\n- TensorFlow Serving + gRPC + Kafka Streams on Github => Stream Processing and RPC \/ Request-Response concepts combined: [Model inference with Apache Kafka, Kafka Streams and a TensorFlow model deployed on a TensorFlow Serving model server](https:\/\/github.com\/kaiwaehner\/tensorflow-serving-java-grpc-kafka-streams)\n- Solving the impedance mismatch between Data Scientist and Production Engineer: [Python, Jupyter, TensorFlow, Keras, Apache Kafka, KSQL](https:\/\/github.com\/kaiwaehner\/python-jupyter-apache-kafka-ksql-tensorflow-keras)\n\n## Requirements, Installation and Usage\nThe code is developed and tested on Mac and Linux operating systems. As Kafka does not support and work well on Windows, this is not tested at all.\n\nJava 8 and Maven 3 are required. Maven will download all required dependencies.\n\nJust download the project and run\n\n                mvn clean package\n\nYou can do this in main directory or each module separately.\n\nApache Kafka 2.5 is currently used. The code is also compatible with Kafka and Kafka Streams 1.1 and 2.x.\n\n**Please make sure to run the Maven build without any changes first.** If it works without errors, you can change library versions, Java version, etc. and see if it still works or if you need to adjust code. \n\nEvery examples includes an implementation and an unit test. The examples are very simple and lightweight. No further configuration is needed to build and run it. Though, for this reason, the generated models are also included (and increase the download size of the project).\n\nThe unit tests use some Kafka helper classes like EmbeddedSingleNodeKafkaCluster in package **com.github.megachucky.kafka.streams.machinelearning.test.utils** so that you can run it without any other configuration or Kafka setup. \nIf you want to run an implementation of a main class in package **com.github.megachucky.kafka.streams.machinelearning**, you need to start a Kafka cluster (with at least one Zookeeper and one Kafka broker running) and also create the required topics. So check out the unit tests first.\n\n\n### Example 1 - Gradient Boosting with H2O.ai for Prediction of Flight Delays\n\nDetailed info in [h2o-gbm](h2o-gbm\/readme.md)\n\n### Example 2 - Convolutional Neural Network (CNN) with TensorFlow for Image Recognition\n\nDetailed info in [tensorflow-image-recognition](tensorflow-image-recognition\/readme.md)\n\n### Example 3 - Iris Prediction using a Neural Network with DeepLearning4J (DL4J)\n\nDetailed info in [dl4j-deeplearning-iris](dl4j-deeplearning-iris\/readme.md)\n\n### Example 4 - Python + Keras + TensorFlow + DeepLearning4j\n\nDetailed info in [tensorflow-kerasm](tensorflow-keras\/readme.md)\n\n\n\n\n\n","16":"# Java Statistical Analysis Tool\n\n<a href='https:\/\/travis-ci.org\/EdwardRaff\/JSAT\/builds'><img src='https:\/\/travis-ci.org\/EdwardRaff\/JSAT.svg?branch=master'><\/a>\n\n\nJSAT is a library for quickly getting started with Machine Learning problems. It is developed in my free time, and made available for use under the GPL 3. Part of the library is for self education, as such - all code is self contained. JSAT has no external dependencies, and is pure Java. I also aim to make the library suitably fast for small to medium size problems. As such, much of the code supports parallel execution.\n\nThe current master branch of JSAT is going through a larger refactoring as JSAT moves to Java 8. This may cause some examples to break if used against the head version, but they should be fixible with minimal changes.\n\n## Get JSAT\n\nTher current release of JSAT is version 0.0.9, and supports Java 6. The current master branch is now Java 8+. \n\nYou can download JSAT from maven central, add the below to your pom file\n\n```xml\n<dependencies>\n  <dependency>\n    <groupId>com.edwardraff<\/groupId>\n    <artifactId>JSAT<\/artifactId>\n    <version>0.0.9<\/version>\n  <\/dependency>\n<\/dependencies>\n```\n\nIf you want to use the bleeding edge, but don't want to bother building yourself, I recommend you look at [jitpack.io](https:\/\/jitpack.io\/#EdwardRaff\/JSAT). It can build a POM repo for you for any specific commit version. Click on \"Commits\" in the link and then click \"get it\" for the commit version you want. \n\nIf you want to read the javadoc's online, you can find them hosted [on my website here](http:\/\/www.edwardraff.com\/jsat_docs\/JSAT-0.0.8-javadoc\/). \n\n## Why use JSAT? \n\nFor research and specialized needs, JSAT has one of the largest collections of algorithms available in any framework. See an incomplete list [here](https:\/\/github.com\/EdwardRaff\/JSAT\/wiki\/Algorithms). \n\nAdditional, there are unfortunately not as many ML tools for Java as there are for other languages. Compared to Weka, JSAT is [usually faster](http:\/\/jsatml.blogspot.com\/2015\/03\/jsat-vs-weka-on-mnist.html). \n\nIf you want to use JSAT and the GPL is not something that will work for you, let me know and we can discuss the issue.\n\nSee the [wiki](https:\/\/github.com\/EdwardRaff\/JSAT\/wiki) for more information as well as some examples on how to use JSAT. \n\n## Note\n\nUpdates to JSAT may be slowed as I begin a PhD program in Computer Science. The project isn\u2019t abandoned! I just have limited free time, and will be balancing my PhD work with a full time job. If you discover more hours in the day, please let me know! Development will be further slowed due to some health issues. I'll continue to try and be prompt on any bug reports and emails, but new features will be a bit slower. Please use the github issues first for contact. \n\n## Citations\n\nIf you use JSAT and find it helpful, citations are appreciated! Please cite the [JSAT paper](http:\/\/www.jmlr.org\/papers\/v18\/16-131.html) published at JMLR. If you're feeling a little lazy, the bibtex is below:\n\n```\n@article{JMLR:v18:16-131,\nauthor = {Raff, Edward},\njournal = {Journal of Machine Learning Research},\nnumber = {23},\npages = {1--5},\ntitle = {JSAT: Java Statistical Analysis Tool, a Library for Machine Learning},\nurl = {http:\/\/jmlr.org\/papers\/v18\/16-131.html},\nvolume = {18},\nyear = {2017}\n}\n```\n","17":"# Android TensorFlow Lite Machine Learning Example\n\n[![Mindorks](https:\/\/img.shields.io\/badge\/mindorks-opensource-blue.svg)](https:\/\/mindorks.com\/open-source-projects)\n[![Mindorks Community](https:\/\/img.shields.io\/badge\/join-community-blue.svg)](https:\/\/mindorks.com\/join-community)\n[![Open Source Love](https:\/\/badges.frapsoft.com\/os\/v1\/open-source.svg?v=102)](https:\/\/opensource.org\/licenses\/Apache-2.0)\n[![License](https:\/\/img.shields.io\/badge\/license-Apache%202.0-blue.svg)](https:\/\/github.com\/amitshekhariitbhu\/Android-TensorFlow-Lite-Example\/blob\/master\/LICENSE)\n\n##  About Android TensorFlow Lite Machine Learning Example\n* This is an example project for integrating [TensorFlow Lite](https:\/\/www.tensorflow.org\/mobile\/tflite\/) into Android application\n* This project include an example for object detection for an image taken from camera using TensorFlow Lite library.\n\n# [Read this article. It describes everything about TensorFlow Lite for Android.](https:\/\/afteracademy.com\/blog\/android-tensorflow-lite-machine-learning-example)\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/amitshekhariitbhu\/Android-TensorFlow-Lite-Example\/master\/assets\/keyboard_example.png\" width=\"250\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/amitshekhariitbhu\/Android-TensorFlow-Lite-Example\/master\/assets\/pen_example.png\" width=\"250\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/amitshekhariitbhu\/Android-TensorFlow-Lite-Example\/master\/assets\/wallet_example.png\" width=\"250\">\n<\/p>\n<img src=https:\/\/raw.githubusercontent.com\/amitshekhariitbhu\/Android-TensorFlow-Lite-Example\/master\/assets\/sample_combined.png >\n<br>\n<br>\n\n### Find this project useful ? :heart:\n* Support it by clicking the :star: button on the upper right of this page. :v:\n\n### Credits\n* The classifier example has been taken from Google TensorFlow example.\n\n[Check out Mindorks awesome open source projects here](https:\/\/mindorks.com\/open-source-projects)\n\n### License\n```\n   Copyright (C) 2018 MINDORKS NEXTGEN PRIVATE LIMITED\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n### Contributing to Android TensorFlow Lite Machine Learning Example\nJust make pull request. You are in!\n","18":"<!---\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<div align=\"center\">\n\n![Colored_logo_with_text](website\/static\/img\/icons\/color_logo_with_text.png)\n\n![Submarine workflow](https:\/\/github.com\/apache\/submarine\/actions\/workflows\/master.yml\/badge.svg?branch=master) ![python-sdk workflow](https:\/\/github.com\/apache\/submarine\/actions\/workflows\/python.yml\/badge.svg?branch=master) [![License](https:\/\/img.shields.io\/badge\/license-Apache%202-4EB1BA.svg)](https:\/\/www.apache.org\/licenses\/LICENSE-2.0.html) [![PyPI version](https:\/\/badge.fury.io\/py\/apache-submarine.svg)](https:\/\/badge.fury.io\/py\/apache-submarine)\n\n<\/div>\n\n# What is Apache Submarine?\n\n**Apache Submarine** (Submarine for short) is an **End-to-End Machine Learning Platform** to allow data scientists to create end-to-end machine learning workflows. On **Submarine**, data scientists can finish each stage in the ML model lifecycle, including data exploration, data pipeline creation, model training, serving, and monitoring.\n\n## Why Submarine?\n\nSome open-source and commercial projects are trying to build an end-to-end ML platform. What's the vision of Submarine?\n\n### Problems\n\n1. Many platforms lack easy-to-use user interfaces (API, SDK, and IDE, etc.)\n2. In the same company, data scientists in different teams usually spend much time on developments of existing feature sets and models.\n3. Data scientists put emphasis on domain-specific tasks (e.g. Click-Through-Rate), but they need to implement their models from scratch with SDKs provided by existing platforms.\n4. Many platforms lack a unified workbench to manage each component in the ML lifecycle.\n\n_Theodore Levitt_ once said:\n\n```\n\u201cPeople don\u2019t want to buy a quarter-inch drill. They want a quarter-inch hole.\u201d\n```\n\n### Goals of Submarine\n\n#### Model Training (Experiment)\n\n- Run\/Track distributed training `experiment` on prem or cloud via easy-to-use UI\/API\/SDK.\n- Easy for data scientists to manage versions of `experiment` and dependencies of `environment`.\n- Support popular machine learning frameworks, including **TensorFlow**, **PyTorch**, **Horovod**, and **MXNet**\n- Provide pre-defined **template** for data scientists to implement domain-specific tasks easily (e.g. using DeepFM template to build a CTR prediction model)\n- Support many compute resources (e.g. CPU and GPU, etc.)\n- Support **Kubernetes** and **YARN**\n- Pipeline is also on the backlog, we will look into pipeline for training in the future.\n\n#### Notebook Service\n\n- Submarine aims to provide a notebook service (e.g. Jupyter notebook) which allows users to manage notebook instances running on the cluster.\n\n#### Model Management (Serving\/versioning\/monitoring, etc.)\n\n- Model management for model-serving\/versioning\/monitoring is on the roadmap.\n\n## Easy-to-use User Interface\n\nAs mentioned above, Submarine attempts to provide **Data-Scientist-friendly** UI to make data scientists have a good user experience. Here're some examples.\n\n### Example: Submit a distributed Tensorflow experiment via Submarine Python SDK\n\n#### Run a Tensorflow Mnist experiment\n\n```python\n\n# New a submarine client of the submarine server\nsubmarine_client = submarine.ExperimentClient(host='http:\/\/localhost:8080')\n\n# The experiment's environment, could be Docker image or Conda environment based\nenvironment = EnvironmentSpec(image='apache\/submarine:tf-dist-mnist-test-1.0')\n\n# Specify the experiment's name, framework it's using, namespace it will run in,\n# the entry point. It can also accept environment variables. etc.\n# For PyTorch job, the framework should be 'Pytorch'.\nexperiment_meta = ExperimentMeta(name='mnist-dist',\n                                 namespace='default',\n                                 framework='Tensorflow',\n                                 cmd='python \/var\/tf_dist_mnist\/dist_mnist.py --train_steps=100')\n# 1 PS task of 2 cpu, 1GB\nps_spec = ExperimentTaskSpec(resources='cpu=2,memory=1024M',\n                             replicas=1)\n# 1 Worker task\nworker_spec = ExperimentTaskSpec(resources='cpu=2,memory=1024M',\n                                 replicas=1)\n\n# Wrap up the meta, environment and task specs into an experiment.\n# For PyTorch job, the specs would be \"Master\" and \"Worker\".\nexperiment_spec = ExperimentSpec(meta=experiment_meta,\n                                 environment=environment,\n                                 spec={'Ps':ps_spec, 'Worker': worker_spec})\n\n# Submit the experiment to submarine server\nexperiment = submarine_client.create_experiment(experiment_spec=experiment_spec)\n\n# Get the experiment ID\nid = experiment['experimentId']\n\n```\n\n#### Query a specific experiment\n\n```python\nsubmarine_client.get_experiment(id)\n```\n\n#### Wait for finish\n\n```python\nsubmarine_client.wait_for_finish(id)\n```\n\n#### Get the experiment's log\n\n```python\nsubmarine_client.get_log(id)\n```\n\n#### Get all running experiment\n\n```python\nsubmarine_client.list_experiments(status='running')\n```\n\nFor a quick-start, see [Submarine On K8s](https:\/\/submarine.apache.org\/docs\/gettingStarted\/quickstart)\n\n### Example: Submit a pre-defined experiment template job\n\n### Example: Submit an experiment via Submarine UI\n\n(Available on 0.5.0, see Roadmap)\n\n## Architecture, Design and requirements\n\nIf you want to know more about Submarine's architecture, components, requirements and design doc, they can be found on [Architecture-and-requirement](https:\/\/submarine.apache.org\/docs\/designDocs\/architecture-and-requirements)\n\nDetailed design documentation, implementation notes can be found at: [Implementation notes](https:\/\/submarine.apache.org\/docs\/designDocs\/implementation-notes)\n\n## Apache Submarine Community\n\nRead the [Apache Submarine Community Guide](https:\/\/submarine.apache.org\/docs\/community\/README)\n\nHow to contribute [Contributing Guide](https:\/\/submarine.apache.org\/docs\/community\/contributing)\n\nLogin Submarine slack channel: [https:\/\/join.slack.com\/t\/asf-submarine\/shared_invite](https:\/\/join.slack.com\/t\/asf-submarine\/shared_invite\/zt-18614cyqs-UhspdUOneiyg~ZPiVomDqw)\n\nIssue Tracking: https:\/\/issues.apache.org\/jira\/projects\/SUBMARINE\n\n## User Document\n\nSee [User Guide Home Page](https:\/\/submarine.apache.org\/docs\/)\n\n## Developer Document\n\nSee [Developer Guide Home Page](https:\/\/submarine.apache.org\/docs\/devDocs\/Development\/)\n\n## Roadmap\n\nWhat to know more about what's coming for Submarine? Please check the roadmap out: https:\/\/cwiki.apache.org\/confluence\/display\/SUBMARINE\/Roadmap\n\n## Resources\n\n[Apache submarine: a unified machine learning platform made simple](https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3517207.3526984) at EuroMLSys '22\n\n## License\n\nThe Apache Submarine project is licensed under the Apache 2.0 License. See the [LICENSE](.\/LICENSE) file for details.\n","19":"# MOA (Massive Online Analysis)\n[![Build Status](https:\/\/travis-ci.org\/Waikato\/moa.svg?branch=master)](https:\/\/travis-ci.org\/Waikato\/moa)\n[![Maven Central](https:\/\/img.shields.io\/maven-central\/v\/nz.ac.waikato.cms.moa\/moa-pom.svg)](https:\/\/mvnrepository.com\/artifact\/nz.ac.waikato.cms)\n[![DockerHub](https:\/\/img.shields.io\/badge\/docker-available-blue.svg?logo=docker)](https:\/\/hub.docker.com\/r\/waikato\/moa)\n[![License: GPL v3](https:\/\/img.shields.io\/badge\/License-GPLv3-blue.svg)](https:\/\/www.gnu.org\/licenses\/gpl-3.0)\n\n![MOA][logo]\n\n[logo]: http:\/\/moa.cms.waikato.ac.nz\/wp-content\/uploads\/2014\/11\/LogoMOA.jpg \"Logo MOA\"\n\nMOA is the most popular open source framework for data stream mining, with a very active growing community ([blog](http:\/\/moa.cms.waikato.ac.nz\/blog\/)). It includes a collection of machine learning algorithms (classification, regression, clustering, outlier detection, concept drift detection and recommender systems) and tools for evaluation. Related to the WEKA project, MOA is also written in Java, while scaling to more demanding problems.\n\nhttp:\/\/moa.cms.waikato.ac.nz\/\n\n## Using MOA\n\n* [Getting Started](http:\/\/moa.cms.waikato.ac.nz\/getting-started\/)\n* [Documentation](http:\/\/moa.cms.waikato.ac.nz\/documentation\/)\n* [About MOA](http:\/\/moa.cms.waikato.ac.nz\/details\/)\n\nMOA performs BIG DATA stream mining in real time, and large scale machine learning. MOA can be extended with new mining algorithms, and new stream generators or evaluation measures. The goal is to provide a benchmark suite for the stream mining community. \n\n## Mailing lists\n* MOA users: http:\/\/groups.google.com\/group\/moa-users\n* MOA developers: http:\/\/groups.google.com\/group\/moa-development\n\n## Citing MOA\nIf you want to refer to MOA in a publication, please cite the following JMLR paper: \n\n> Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer (2010);\n> MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604 \n\n\n","20":"# CodeBuff smart formatter\n\nBy Terence Parr (primary developer), Fangzhou (Morgan) Zhang (help with initial development), Jurgen Vinju (co-author of academic paper, help with empirical results and algorithm discussions).\n\n[kaby76](https:\/\/github.com\/kaby76) has done a [C# port](https:\/\/github.com\/kaby76\/cs-codebuff).\n\n## Abstract\n\nCode formatting is not particularly exciting but many researchers would consider it either unsolved or not well-solved.  The two well-established solutions are:\n\n1.  Build a custom program that formats code for specific a language with ad hoc techniques, typically subject to parameters such as \"*always put a space between operators*\".\n2.  Define a set of formal rules that map input patterns to layout instructions such as \"*line these expressions up vertically*\".\n\nEither techniques are painful and finicky.  \n\nThis repository is a step towards what we hope will be a universal code formatter that uses machine learning to look for patterns in a corpus and to format code using those patterns.  \n\nIt requires Java 8. See `pom.xml` for dependencies (e.g., ANTLR 4.x, ...).\n\n*Whoa!* It appears to work.  Academic paper, [Towards a Universal Code Formatter through Machine Learning](http:\/\/arxiv.org\/abs\/1606.08866) accepted to SLE2016.  Sample output is in the paper or next section. Video from [Terence's presentation](https:\/\/www.youtube.com\/watch?v=Mni2HVGGUdo).\n\n## Sample output\n\nAll input is completed squeezed of whitespace\/newlines so only the output really matters when examining CodeBuff output. You can check out the [output](https:\/\/github.com\/antlr\/codebuff\/tree\/master\/output) dir for leave-one-out formatting of the various [corpora](https:\/\/github.com\/antlr\/codebuff\/tree\/master\/corpus). But, here are some sample formatting results.\n\n### SQL\n\n```sql\nSELECT *\nFROM DMartLogging\nWHERE DATEPART(day, ErrorDateTime) = DATEPART(day, GetDate())\n      AND DATEPART(month, ErrorDateTime) = DATEPART(month, GetDate())\n      AND DATEPART(year, ErrorDateTime) = DATEPART(year, GetDate())\nORDER BY ErrorDateTime\n    DESC\n```\n\n```sql\nSELECT\n    CASE WHEN SSISInstanceID IS NULL\n        THEN 'Total'\n    ELSE SSISInstanceID END SSISInstanceID\n    , SUM(OldStatus4) AS OldStatus4\n    , SUM(Status0) AS Status0\n    , SUM(Status1) AS Status1\n    , SUM(Status2) AS Status2\n    , SUM(Status3) AS Status3\n    , SUM(Status4) AS Status4\n    , SUM(OldStatus4 + Status0 + Status1 + Status2 + Status3 + Status4) AS InstanceTotal\nFROM\n    (\n        SELECT\n            CONVERT(VARCHAR, SSISInstanceID)             AS SSISInstanceID\n            , COUNT(CASE WHEN Status = 4 AND\n                              CONVERT(DATE, LoadReportDBEndDate) <\n                              CONVERT(DATE, GETDATE())\n                        THEN Status\n                    ELSE NULL END)             AS OldStatus4\n            , COUNT(CASE WHEN Status = 0\n                        THEN Status\n                    ELSE NULL END)             AS Status0\n            , COUNT(CASE WHEN Status = 1\n                        THEN Status\n                    ELSE NULL END)             AS Status1\n            , COUNT(CASE WHEN Status = 2\n                        THEN Status\n                    ELSE NULL END)             AS Status2\n            , COUNT(CASE WHEN Status = 3\n                        THEN Status\n                    ELSE NULL END)             AS Status3\n--, COUNT ( CASE WHEN Status = 4 THEN Status ELSE NULL END ) AS Status4\n            , COUNT(CASE WHEN Status = 4 AND\n                              DATEPART(DAY, LoadReportDBEndDate) = DATEPART(DAY, GETDATE())\n                        THEN Status\n                    ELSE NULL END)             AS Status4\n        FROM dbo.ClientConnection\n        GROUP BY SSISInstanceID\n    ) AS StatusMatrix\nGROUP BY SSISInstanceID\n```\n\n### Java\n\n```java\npublic class Interpreter {\n    ...\n    public static final Set<String> predefinedAnonSubtemplateAttributes = new HashSet<String>() {\n                                                                              {\n                                                                                  add(\"i\");\n                                                                                  add(\"i0\");\n                                                                              }\n                                                                          };\n...\n    public int exec(STWriter out, InstanceScope scope) {\n        final ST self = scope.st;\n        if ( trace ) System.out.println(\"exec(\"+self.getName()+\")\");\n        try {\n            setDefaultArguments(out, scope);\n            return _exec(out, scope);\n        }\n        catch (Exception e) {\n            StringWriter sw = new StringWriter();\n            PrintWriter pw = new PrintWriter(sw);\n            e.printStackTrace(pw);\n            pw.flush();\n            errMgr.runTimeError(this,\n                                scope,\n                                ErrorType.INTERNAL_ERROR,\n                                \"internal error: \"+sw.toString());\n            return 0;\n        }\n    }\n...\n    protected int _exec(STWriter out, InstanceScope scope) {\n        final ST self = scope.st;\n        int start = out.index(); \/\/ track char we're about to write\n        int prevOpcode = 0;\n        int n = 0; \/\/ how many char we write out\n        int nargs;\n        int nameIndex;\n        int addr;\n        String name;\n        Object o, left, right;\n        ST st;\n        Object[] options;\n        byte[] code = self.impl.instrs;        \/\/ which code block are we executing\n        int ip = 0;\n        while ( ip<self.impl.codeSize ) {\n            if ( trace|| debug ) trace(scope, ip);\n            short opcode = code[ip];\n            \/\/count[opcode]++;\n            scope.ip = ip;\n            ip++; \/\/jump to next instruction or first byte of operand\n            switch ( opcode ) {\n                case Bytecode.INSTR_LOAD_STR:\n                    \/\/ just testing...\n                    load_str(self, ip);\n                    ip += Bytecode.OPND_SIZE_IN_BYTES;\n                    break;\n                case Bytecode.INSTR_LOAD_ATTR:\n                    nameIndex = getShort(code, ip);\n                    ip += Bytecode.OPND_SIZE_IN_BYTES;\n                    name = self.impl.strings[nameIndex];\n                    try {\n                        o = getAttribute(scope, name);\n                        if ( o== ST.EMPTY_ATTR ) o = null;\n                        }\n                    catch (STNoSuchAttributeException nsae) {\n                        errMgr.runTimeError(this, scope, ErrorType.NO_SUCH_ATTRIBUTE, name);\n                        o = null;\n                    }\n                    operands[++sp] = o;\n                    break;\n...\n```\n\n### ANTLR\n\n```\nreferenceType : classOrInterfaceType | typeVariable | arrayType ;\n\nclassOrInterfaceType\n    :   (   classType_lfno_classOrInterfaceType\n        |   interfaceType_lfno_classOrInterfaceType\n        )\n        (   classType_lf_classOrInterfaceType\n        |   interfaceType_lf_classOrInterfaceType\n        )*\n    ;\n```\n\n```\nclassModifier\n    :   annotation\n    |   'public'\n    |   'protected'\n    |   'private'\n    |   'abstract'\n    |   'static'\n    |   'final'\n    |   'strictfp'\n    ;\n```\n\n```\ntypeSpecifier\n    :   (   'void'\n        |   'char'\n        |   'short'\n        |   'int'\n        |   'long'\n        |   'float'\n        |   'double'\n        |   'signed'\n        |   'unsigned'\n        |   '_Bool'\n        |   '_Complex'\n        |   '__m128'\n        |   '__m128d'\n        |   '__m128i'\n        )\n    |   '__extension__' '(' ('__m128' | '__m128d' | '__m128i') ')'\n    |   atomicTypeSpecifier\n    |   structOrUnionSpecifier\n    |   enumSpecifier\n    |   typedefName\n    |   '__typeof__' '(' constantExpression ')' \/\/ GCC extension\n    ;\n```\n\n## Build complete jar\n\nTo make a complete jar with all of the dependencies, do this from the repo main directory:\n\n```bash\n$ mvn clean compile install\n```\n\nThis will leave you with artifact `target\/codebuff-1.4.19.jar` or whatever the version number is and put the jar into the usual maven local cache.\n\n## Formatting files\n\nTo use the formatter, you need to use class `org.antlr.codebuff.Tool`.  Commandline usage:\n\n* `-g` *grammar-name*. The grammar must be run through ANTLR and be compiled (and in the `CLASSPATH`). For example, for `Java8.g4`, use `-g Java8`, not the filename. For separated grammar files, like `ANTLRv4Parser.g4` and `ANTLRv4Lexer.g4`, use `-g ANTLRv4`. If the grammar is in a package, use fully-qualified like `-g org.antlr.codebuff.ANTLRv4`.\n* `-rule` *start-rule*. Start rule of the grammar where parsing of a full file starts, such as `compilationUnit` in `Java.g4`.\n* `-corpus` *root-dir-of-samples*\n* [`-files` *file-extension]*. E.g., use `java`, `g4`, `c`, ...\n* [`-indent` *num-spaces]*.  This defaults to 4 spaces indentation.\n* [`-comment` *line-comment-name*]. As a failsafe, CodeBuff allows you to specify the token name for single-line comments, such as `LINE_COMMENT`, within the grammar so that it can ensure there is a line break after a single line,.\n* [`-o` *output-file*]. Filename with optional path to where output should go.\n* *file-to-format*. Filename (with optional path) must be last.\n\nOutput goes to standard out unless you use `-o`.\n \n```bash\n$ java -jar target\/codebuff-1.4.19.jar  \\\n       -g org.antlr.codebuff.ANTLRv4 \\\n       -rule grammarSpec \\\n       -corpus corpus\/antlr4\/training \\\n       -files g4 \\\n       -indent 4 \\\n       -comment LINE_COMMENT \\\n       T.g4\n```\n\n```bash\n$ java -jar target\/codebuff-1.4.19.jar \\\n       -g org.antlr.codebuff.Java \\\n       -rule compilationUnit \\\n       -corpus corpus\/java\/training\/stringtemplate4 \\\n       -files java \\\n       -comment LINE_COMMENT \\\n       T.java\n```\n\nThese examples work for the grammars specified because they are already inside the complete jar. For parsers compiled outside of the jar, you might need to do something like:\n\n```bash\njava java -cp target\/codebuff-1.4.19.jar:$CLASSPATH \\\n       org.antlr.codebuff.Tool  \\\n       -g org.antlr.codebuff.ANTLRv4 \\\n       -rule grammarSpec -corpus corpus\/antlr4\/training \\\n       -files g4 -indent 4 -comment LINE_COMMENT T.g4\n```\n\n### Grammar requirements\n\nAll whitespace should go to the parser on a hidden channel. For example, here is a rule that does that:\n\n```\nWS  :\t[ \\t\\r\\n\\f]+ -> channel(HIDDEN)\t;\n```\n\nComments should also:\n\n```\nBLOCK_COMMENT\n\t:\t'\/*' .*? ('*\/' | EOF)  -> channel(HIDDEN)\n\t;\n\nLINE_COMMENT\n\t:\t'\/\/' ~[\\r\\n]*  -> channel(HIDDEN)\n\t;\n```\n\nYou can have line comments match newlines if you want.\n\n## Speed tests\n\nThe paper cites some speed tests for training and formatting time for\n\n* [guava corpus](https:\/\/github.com\/antlr\/codebuff\/tree\/master\/corpus\/java\/training\/guava) and [java grammar](https:\/\/github.com\/antlr\/codebuff\/blob\/master\/grammars\/org\/antlr\/codebuff\/Java.g4)\n* [guava corpus](https:\/\/github.com\/antlr\/codebuff\/tree\/master\/corpus\/java\/training\/guava) and [java8 grammar](https:\/\/github.com\/antlr\/codebuff\/blob\/master\/grammars\/org\/antlr\/codebuff\/Java8.g4)\n* [antlr corpus](https:\/\/github.com\/antlr\/codebuff\/tree\/master\/corpus\/antlr4\/training) and [antlr parser grammar](https:\/\/github.com\/antlr\/codebuff\/blob\/master\/grammars\/org\/antlr\/codebuff\/ANTLRv4Parser.g4), [antlr lexer grammar](https:\/\/github.com\/antlr\/codebuff\/blob\/master\/grammars\/org\/antlr\/codebuff\/ANTLRv4Lexer.g4)\n\nFirst, here is my machine configuration:\n\n<img src=images\/imac.png width=250>\n\nMemory speed seems to make a big difference given how much we have to trawl through memory---The tests shown below were done with 1867 MHz DDR3 RAM.  We set an initial 4G RAM, 1M stack size.  First build everything:\n\n```bash\n$ mvn clean compile install\n```\n\nThen you can run the speed tests as shown in following subsections.\n\n#### ANTLR corpus\n\n```bash\n$ java -Xmx4G -Xss1M -cp target\/codebuff-1.4.19.jar org.antlr.codebuff.validation.Speed -antlr corpus\/antlr4\/training\/Java8.g4\nLoaded 12 files in 172ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 353ms formatting = 340ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 188ms formatting = 161ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 145ms formatting = 153ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 130ms formatting = 129ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 123ms formatting = 113ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 114ms formatting = 116ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 93ms formatting = 90ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 80ms formatting = 90ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 73ms formatting = 88ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 72ms formatting = 71ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 71ms formatting = 69ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 71ms formatting = 73ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 76ms formatting = 63ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 70ms formatting = 70ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 70ms formatting = 69ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 73ms formatting = 70ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 70ms formatting = 68ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 71ms formatting = 66ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 70ms formatting = 70ms\nantlr training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/antlr4\/training\/Java8.g4 = 73ms formatting = 72ms\nmedian of [5:19] training 72ms\nmedian of [5:19] formatting 70ms\n```\n\n#### Guava corpus, Java grammar\n\n```bash\n$ java -Xms4G -Xss1M -cp target\/codebuff-1.4.19.jar org.antlr.codebuff.validation.Speed -java_guava corpus\/java\/training\/guava\/cache\/LocalCache.java\nLoaded 511 files in 1949ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1984ms formatting = 2669ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1747ms formatting = 3166ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1784ms formatting = 2811ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1507ms formatting = 1742ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1499ms formatting = 2832ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1582ms formatting = 2663ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1499ms formatting = 2807ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1561ms formatting = 2815ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1521ms formatting = 2136ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1545ms formatting = 2811ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1501ms formatting = 2800ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1506ms formatting = 2581ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1494ms formatting = 2838ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1494ms formatting = 2789ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1497ms formatting = 2621ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1501ms formatting = 2714ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1506ms formatting = 2816ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1512ms formatting = 2733ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1515ms formatting = 2587ms\njava_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1508ms formatting = 2430ms\nmedian of [5:19] training 1506ms\nmedian of [5:19] formatting 2733ms\n```\n\n#### Guava corpus, Java8 grammar\n\nLoad time here is very slow (2.5min) because the Java8 grammar is meant to reflect the language spec. It has not been optimized for performance. Once the corpus is loaded, training and formatting times are about the same as for Java grammar.\n\n```bash\n$ java -Xms4G -Xss1M -cp target\/codebuff-1.4.19.jar \\\n       org.antlr.codebuff.validation.Speed \\\n       -java8_guava corpus\/java\/training\/guava\/cache\/LocalCache.java\nLoaded 511 files in 159947ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 2238ms formatting = 23312ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1913ms formatting = 2368ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1855ms formatting = 2277ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1856ms formatting = 2267ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1868ms formatting = 2348ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1890ms formatting = 2263ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1866ms formatting = 2328ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1855ms formatting = 2247ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1856ms formatting = 2243ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1871ms formatting = 2204ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1863ms formatting = 2244ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1850ms formatting = 2212ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1861ms formatting = 2215ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1877ms formatting = 2257ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1843ms formatting = 2249ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1842ms formatting = 2205ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1869ms formatting = 2343ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1864ms formatting = 2225ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1851ms formatting = 2260ms\njava8_guava training of \/Users\/parrt\/antlr\/code\/codebuff\/corpus\/java\/training\/guava\/cache\/LocalCache.java = 1871ms formatting = 2200ms\nmedian of [5:19] training 1863ms\nmedian of [5:19] formatting 2244ms\n```\n\n## Generating graphs from paper\n\nIn the *Towards a Universal Code Formatter Through Machine Learning* paper, we have three graphs to support our conclusions. This sections shows how to reproduce them. (Note that these jobs take many minutes to run; maybe up to 30 minutes for one of them on a fast box.)\n\nThe Java code generates python code that uses matplotlib. The result of running the python is a PDF of the graph (that also pops up in a window).\n\n### Box plot with median error rates\n\nTo generate:\n\n<img src=\"images\/leave_one_out.png\" width=\"400\">\n\ndo this:\n\n```bash\n$ mvn clean compile install\n$ java -Xms8G -Xss1M -cp target\/codebuff-1.4.19.jar org.antlr.codebuff.validation.LeaveOneOutValidator\n...\nwrote python code to python\/src\/leave_one_out.py\n$ cd python\/src\n$ python leave_one_out.py &\n```\n\n### Plot showing effect of corpus size on error rate\n\nTo generate:\n\n<img src=\"images\/subset_validator.png\" width=\"400\">\n\ndo this:\n\n```bash\n$ mvn clean compile install\n$ java -Xms8G -Xss1M -cp target\/codebuff-1.4.19.jar org.antlr.codebuff.validation.SubsetValidator\n...\nwrote python code to python\/src\/subset_validator.py\n$ cd python\/src\n$ python subset_validator.py &\n```\n\n### Plot showing effect of varying model parameter k\n\nTo generate:\n\n<img src=\"images\/vary_k.png\" width=\"400\">\n\ndo this:\n\n```bash\n$ mvn clean compile install\n$ java -Xms8G -Xss1M -cp target\/codebuff-1.4.19.jar org.antlr.codebuff.validation.TestK\n...\nwrote python code to python\/src\/vary_k.py\n$ cd python\/src\n$ python vary_k.py &\n```\n","21":"Trident-ML is a realtime online machine learning library. It allows you to build real time predictive features using scalable online algorithms.\nThis library is built on top of [Storm](https:\/\/github.com\/nathanmarz\/storm), a distributed stream processing framework which runs on a cluster of machines and supports horizontal scaling.\nThe packaged algorithms are designed to fit into limited memory and processing time but they don't work in a distributed way.\n\nTrident-ML currently supports : \n* Linear classification (Perceptron, Passive-Aggressive, Winnow, AROW)\n* Linear regression (Perceptron, Passive-Aggressive)\n* Clustering (KMeans)\n* Feature scaling (standardization, normalization)\n* Text feature extraction\n* Stream statistics (mean, variance)\n* Pre-Trained Twitter sentiment classifier\n\n# API Overview\n\nTrident-ML is based on [Trident](https:\/\/github.com\/nathanmarz\/storm\/wiki\/Trident-tutorial), a high-level abstraction for doing realtime computing.\nIf you're familiar with high level batch processing tools like Pig or Cascading, the concepts of Trident will be very familiar.\n\nIt's recommended to read the [Storm and Trident documentation](https:\/\/github.com\/nathanmarz\/storm\/wiki\/Documentation).\n\n## Create instances\n\nTrident-ML process unbounded streams of data implemented by an infinite collection of [Instance](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/core\/Instance.java) or [TextInstance](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/core\/TextInstance.java).\nCreating instances is the first step to build a prediction tools.\nTrident-ML offers [Trident functions](https:\/\/github.com\/nathanmarz\/storm\/wiki\/Trident-API-Overview#functions) to convert Trident tuples to instances :\n\n* Use [InstanceCreator](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/preprocessing\/InstanceCreator.java) to create [Instance](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/core\/Instance.java)\n\n```java\nTridentTopology toppology = new TridentTopology();\n\ntoppology\n  \/\/ Emit tuples with 2 random features (named x0 and x1) and an associated boolean label (named label)\n  .newStream(\"randomFeatures\", new RandomFeaturesSpout())\n  \n  \/\/ Transform trident tuple to instance\n  .each(new Fields(\"label\", \"x0\", \"x1\"), new InstanceCreator<Boolean>(), new Fields(\"instance\"));\n```\n\n* Use [TextInstanceCreator](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/preprocessing\/TextInstanceCreator.java) to create [TextInstance](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/core\/TextInstance.java)\n\n```java\nTridentTopology toppology = new TridentTopology();\n\ntoppology\n  \/\/ Emit tuples containing text and associated label (topic)\n  .newStream(\"reuters\", new ReutersBatchSpout())\n\n  \/\/ Convert trident tuple to text instance\n  .each(new Fields(\"label\", \"text\"), new TextInstanceCreator<Integer>(), new Fields(\"instance\"));\n```\n\n## Supervised classification\nTrident-ML includes differents algorithms to do supervised classification : \n* [PerceptronClassifier](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/classification\/PerceptronClassifier.java)\nimplements a binary classifier based on an averaged kernel-based perceptron.\n* [WinnowClassifier](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/classification\/WinnowClassifier.java)\nimplements [Winnow algorithm](http:\/\/link.springer.com\/content\/pdf\/10.1007%2FBF00116827.pdf).\nIt scales well to high-dimensional data and performs better than a perceptron when many dimensions are irrelevant. \n* [BWinnowClassifier](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/classification\/BWinnowClassifier.java)\n is an implementation of the [Balanced Winnow algorithm](http:\/\/link.springer.com\/content\/pdf\/10.1007%2FBF00116827.pdf) \nan extension of the original Winnow algorithm.\n* [AROWClassifier](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/classification\/AROWClassifier.java)\nis an simple and efficient implementation of [Adaptive Regularization of Weights](http:\/\/books.nips.cc\/papers\/files\/nips22\/NIPS2009_0611.pdf).\nIt combines several useful properties : large margin training, confidence weighting, and the capacity to handle non-separable data.\n* [PAClassifier](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/classification\/PAClassifier.java)\nimplements the [Passive-Aggressive binary classifier](http:\/\/eprints.pascal-network.org\/archive\/00002147\/01\/CrammerDeKeShSi06.pdf)\na margin based learning algorithm.\n* [MultiClassPAClassifier](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/classification\/MultiClassPAClassifier.java)\na variant of the Passive-Aggressive performing one-vs-all multiclass classification.\n\nTheses classifiers learn from a datastream of labeled [Instance](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/core\/Instance.java)\nusing a [ClassifierUpdater](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/classification\/ClassifierUpdater.java).\nAnother datastream of unlabeled instance can be classified with a [ClassifyQuery](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/classification\/ClassifyQuery.java).\n\nThe following example learn a NAND function and classify instances comming from a DRPC stream.\n\n```java\nTridentTopology toppology = new TridentTopology();\n\n\/\/ Create perceptron state from labeled instances stream\nTridentState perceptronModel = toppology\n  \/\/ Emit tuple with a labeled instance of enhanced NAND features\n  \/\/ i.e. : {label=true, features=[1.0 0.0 1.0]} or {label=false, features=[1.0 1.0 1.0]}  \n  .newStream(\"nandsamples\", new NANDSpout())\n\t\t\t\t\n  \/\/ Update perceptron\n  .partitionPersist(new MemoryMapState.Factory(), new Fields(\"instance\"), new ClassifierUpdater<Boolean>(\"perceptron\", new PerceptronClassifier()));\n\n\/\/ Classify instance from a DRPC stream\ntoppology.newDRPCStream(\"predict\", localDRPC)\n  \/\/ Transform DRPC ARGS to unlabeled instance\n  .each(new Fields(\"args\"), new DRPCArgsToInstance(), new Fields(\"instance\"))\n\n  \/\/ Classify instance using perceptron state\n  .stateQuery(perceptronModel, new Fields(\"instance\"), new ClassifyQuery<Boolean>(\"perceptron\"), new Fields(\"prediction\"));\n```\t\t\t\t\n\nTrident-ML provides the [KLDClassifier](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/nlp\/KLDClassifier.java)\nwhich implements a [text classifier using the Kullback-Leibler Distance](http:\/\/lvk.cs.msu.su\/~bruzz\/articles\/classification\/Using%20Kullback-Leibler%20Distance%20for%20Text%20Categorization.pdf).\n\nHere's the code to build a news classifier using Reuters dataset :\n\n```java\nTridentTopology toppology = new TridentTopology();\n\n\/\/ Create KLD classifier state from labeled instances stream\nTridentState classifierState = toppology\n  \/\/ Emit tuples containing text and associated label (topic)\n  .newStream(\"reuters\", new ReutersBatchSpout())\n\n  \/\/ Convert trident tuple to text instance\n  .each(new Fields(\"label\", \"text\"), new TextInstanceCreator<Integer>(), new Fields(\"instance\"))\n  \n  \/\/ Update classifier\n  .partitionPersist(new MemoryMapState.Factory(), new Fields(\"instance\"), new TextClassifierUpdater(\"newsClassifier\", new KLDClassifier(9)));\n\n\/\/ Classification stream\ntoppology.newDRPCStream(\"classify\", localDRPC)\n\n  \/\/ Convert DRPC args to text instance\n  .each(new Fields(\"args\"), new TextInstanceCreator<Integer>(false), new Fields(\"instance\"))\n\n  \/\/ Query classifier with text instance\n  .stateQuery(classifierState, new Fields(\"instance\"), new ClassifyTextQuery(\"newsClassifier\"), new Fields(\"prediction\"));\n```\n\n## Unsupervised classification\n[KMeans](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/clustering\/KMeans.java)\nis an implementation of the well known [k-means algorithm](http:\/\/en.wikipedia.org\/wiki\/K-means_clustering)\nwhich partitions instances into clusters.\n\nUse a [ClusterUpdater](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/clustering\/ClusterUpdater.java)\nor a [ClusterQuery](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/clustering\/ClusterQuery.java)\nto respectively udpate clusters or query the clusterer :\n\n```java\nTridentTopology toppology = new TridentTopology();\n\n\/\/ Training stream\nTridentState kmeansState = toppology\n  \/\/ Emit tuples with a instance containing an integer as label and 3 double features named (x0, x1 and x2)\n  .newStream(\"samples\", new RandomFeaturesForClusteringSpout())\n\n  \/\/ Convert trident tuple to instance\n  .each(new Fields(\"label\", \"x0\", \"x1\", \"x2\"), new InstanceCreator<Integer>(), new Fields(\"instance\"))\n\n  \/\/ Update a 3 classes kmeans\n  .partitionPersist(new MemoryMapState.Factory(), new Fields(\"instance\"), new ClusterUpdater(\"kmeans\", new KMeans(3)));\n\n\/\/ Cluster stream\ntoppology.newDRPCStream(\"predict\", localDRPC)\n  \/\/ Convert DRPC args to instance\n  .each(new Fields(\"args\"), new DRPCArgsToInstance(), new Fields(\"instance\"))\n\n  \/\/ Query kmeans to classify instance\n  .stateQuery(kmeansState, new Fields(\"instance\"), new ClusterQuery(\"kmeans\"), new Fields(\"prediction\"));\n```\n\n## Stream statistics\nStream statistics such as mean, standard deviation and count can be easily computed using Trident-ML.\nTheses statistics are stored in a [StreamStatistics](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/stats\/StreamStatistics.java) object.\nStatistics update and query are performed respectively using a [StreamStatisticsUpdater](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/stats\/StreamStatisticsUpdater.java) and a [StreamStatisticsQuery](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/stats\/StreamStatisticsQuery.java) :\n\n```java\nTridentTopology toppology = new TridentTopology();\n\n\/\/ Update stream statistics\nTridentState streamStatisticsState = toppology\n  \/\/ emit tuples with random features\n  .newStream(\"randomFeatures\", new RandomFeaturesSpout())\n\n  \/\/ Transform trident tuple to instance\n  .each(new Fields(\"x0\", \"x1\"), new InstanceCreator(), new Fields(\"instance\"))\n\n  \/\/ Update stream statistics\n  .partitionPersist(new MemoryMapState.Factory(), new Fields(\"instance\"), new StreamStatisticsUpdater(\"randomFeaturesStream\", StreamStatistics.fixed()));\n\n\/\/ Query stream statistics (with DRPC)\ntoppology.newDRPCStream(\"queryStats\", localDRPC)\n  \/\/ Query stream statistics\n  .stateQuery(streamStatisticsState, new StreamStatisticsQuery(\"randomFeaturesStream\"), new Fields(\"streamStats\"));\n\n```\nNote that Trident-ML can suppport concept drift in a sliding window manner.\nUse StreamStatistics#adaptive(maxSize) instead of StreamStatistics#fixed() to construct StreamStatistics implementation with a maxSize length window.\n\n\n## Preprocessing data\nData preprocessing is an important step in the data mining process. \nTrident-ML provides Trident functions to transform raw features into a representation that is more suitable for machine learning algorithms.\n\n* [Normalizer](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/preprocessing\/Normalizer.java) scales individual instances to have unit norm. \n\n```java\nTridentTopology toppology = new TridentTopology();\n\ntoppology\n  \/\/ Emit tuples with 2 random features (named x0 and x1) and an associated boolean label (named label)\n  .newStream(\"randomFeatures\", new RandomFeaturesSpout())\n\n  \/\/ Convert trident tuple to instance\n  .each(new Fields(\"label\", \"x0\", \"x1\"), new InstanceCreator<Boolean>(), new Fields(\"instance\"))\n\t  \n  \/\/ Scales features to unit norm\n  .each(new Fields(\"instance\"), new Normalizer(), new Fields(\"scaledInstance\"));\n```\n\n* [StandardScaler](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/preprocessing\/StandardScaler.java) transform raw features to standard normally distributed data (Gaussian with zero mean and unit variance). It uses [Stream Statistics](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/stats\/StreamStatistics.java) to remove mean and scale to variance.\n\n```java\nTridentTopology toppology = new TridentTopology();\n\ntoppology\n  \/\/ Emit tuples with 2 random features (named x0 and x1) and an associated boolean label (named label)\n  .newStream(\"randomFeatures\", new RandomFeaturesSpout())\n\n  \/\/ Convert trident tuple to instance\n  .each(new Fields(\"label\", \"x0\", \"x1\"), new InstanceCreator<Boolean>(), new Fields(\"instance\"))\n\t\t  \n  \/\/ Update stream statistics\n  .partitionPersist(new MemoryMapState.Factory(), new Fields(\"instance\"), new StreamStatisticsUpdater(\"streamStats\", new StreamStatistics()), new Fields(\"instance\", \"streamStats\")).newValuesStream()\n\n  \/\/ Standardize stream using original stream statistics\n  .each(new Fields(\"instance\", \"streamStats\"), new StandardScaler(), new Fields(\"scaledInstance\"));\n```\n\n## Pre-trained classifier\n\nTrident-ML includes a pre-trained [twitter sentiment classifier](https:\/\/github.com\/pmerienne\/trident-ml\/blob\/master\/src\/main\/java\/com\/github\/pmerienne\/trident\/ml\/nlp\/TwitterSentimentClassifier.java).\nIt was built on a subset of the [Twitter Sentiment Corpus by Niek Sanders](http:\/\/www.sananalytics.com\/lab\/twitter-sentiment\/) with a multi class PA classifier and classifies raw tweets as positive (true) or negative (false).\nThis classifier is implemented as a trident function and can be easily used in a trident topology : \n\n```java\nTridentTopology toppology = new TridentTopology();\n\n\/\/ Classification stream\ntoppology.newDRPCStream(\"classify\", localDRPC)\n  \/\/ Query classifier with text instance\n  .each(new Fields(\"args\"), new TwitterSentimentClassifier(), new Fields(\"sentiment\"));\n```\n\n# Maven integration : \n\nTrident-Ml is hosted on Clojars (a Maven repository). \nTo include Trident-ML in your project , add the following to your pom.xml: : \n ```xml\n <repositories>\n\t<repository>\n\t\t<id>clojars.org<\/id>\n\t\t<url>http:\/\/clojars.org\/repo<\/url>\n\t<\/repository>\n<\/repositories>\n\n<dependency>\n\t<groupId>com.github.pmerienne<\/groupId>\n\t<artifactId>trident-ml<\/artifactId>\n\t<version>0.0.4<\/version>\n<\/dependency>\n ```\n\n# Does trident-ml support distributed learning?\nStorm allows trident-ml to process batches of tuples in a distributed way (batches will be computed among several nodes). This means that trident-ml can scale horizontally with workload.\n\nHowever Storm prevents state updates to append simultaneously and the model learning is done in a state update. That's why, the learning step can't be distributed. Thankfully this lack of parallelization isn't a real bottle neck because the incremental algorithms are very fast (and simple!).\n\nDistributed algorithms will not be implemented in trident-ml, the whole design prevents this. \n\nSo you can't achieve distributed learning however but you can still partition the streams to pre-process\/enrich your data in a distributed manner.\n\n# Copyright and license\n\nCopyright 2013-2015 Pierre Merienne\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n[http:\/\/www.apache.org\/licenses\/LICENSE-2.0](http:\/\/www.apache.org\/licenses\/LICENSE-2.0)\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n","22":"# Conjecture [![Build Status](https:\/\/travis-ci.org\/etsy\/Conjecture.svg?branch=master)](https:\/\/travis-ci.org\/etsy\/Conjecture)\n\nConjecture is a framework for building machine learning models in Hadoop using the Scalding DSL.\nThe goal of this project is to enable the development of statistical models as viable components\nin a wide range of product settings. Applications include classification and categorization,\nrecommender systems, ranking, filtering, and regression (predicting real-valued numbers).\nConjecture has been designed with a primary emphasis on flexibility and can handle a wide variety of inputs.\nIntegration with Hadoop and scalding enable seamless handling of extremely large data volumes,\nand integration with established ETL processes. Predicted labels can either be consumed directly\nby the web stack using the dataset loader, or models can be deployed and consumed by live web code.\nCurrently, binary classification (assigning one of two possible labels to input data points)\nis the most mature component of the Conjecture package.\n\n# Tutorial\nThere are a few stages involved in training a machine learning model using Conjecture.\n\n## Create Training Data\nWe represent the training data as \"feature vectors\" which are just mappings of feature names to real values.\nIn this case we represent them as a java map of strings to doubles\n(although we have a class StringKeyedVector which provides convenience methods for feature vector construction).\nWe also need the true label of each instance, which we represent as 0 and 1\n(the mapping of these binary labels to e.g., \"male\" and \"female\" is up to the user).\nWe construct BinaryLabeledInstances, which are just wrappers for a feature vector and a label.\n\n    val bl = new BinaryLabeledInstance(0.0)\n    bl.addTerm(\"bias\", 1.0)\n    bl.addTerm(\"some_feature\", 0.5)\n\n## Training a Classifier\nClassifiers are essentially trained by presenting the labeled instances to them.  There are several kinds \nof linear classifiers we implement, among them:\n\n* Logistic regression,\n* Perceptron,\n* MIRA (a large margin perceptron model),\n* Passive aggressive.\n\nThese models all have several options, such as learning rate, regularization parameters and so on.  We supply\nreasonable defaults for these parameters although they can be changed readily.  To train a linear model\nsimply call the update function with the labeled instance:\n\n    val p = new LogisticRegression()\n    p.update(bl)\n\nIn order to make this procedure tractable for large datasets, we provided scalding wrappers for the training.\nThese operate by training several small models on mappers, then aggregating them into a final complete model\non the reducers.  This wrapper is called like so:\n\n    new BinaryModelTrainer(args)\n      .train(instances, 'instance, 'model)\n      .write(SequenceFile(\"model\"))\n      .map('model -> 'model){ x : UpdateableBinaryModel => new com.google.gson.Gson.toJson(x) }\n      .write(Tsv(\"model_json\"))\n\nThis code segment will train a model using a pipe called instances which has a field called instance which contains\nthe BinaryLabeledInstance objects.  It produces a pipe with a single field containing the completed model, which can\nthen be written to disk.\n\nThis class uses the command line args object from scalding, in order to let you set some options on the command line.\nSome useful options are:\n\n| Argument                            | Possible values                               | Default            | Meaning                                          |\n|-------------------------------------|-----------------------------------------------|--------------------|--------------------------------------------------|\n| --model                             | mira, logistic_regression, passive_aggressive | passive_aggressive | The type of model to use.                        |\n| --iters                             | 1, 2, 3...                                    | 1                  | The number of iterations of training to perform. |\n| --zero_class_prob, --one_class_prob | [0, 1]                                        | 1                  |                                                  |\n\nTo see all the command line options, see the BinaryModelTrainer class.\n\n## Evaluating a Classifier\nIt is important to get a sense of the performance you can expect out of your classifier on unseen data.\nIn order to do this we recommend to use cross validation.\nIn essence, your input set of instances is split up into testing and training portions (multiple different ways),\nthen a classifier is trained on each training portion, and evaluated (against the true labels which are present)\nusing the testing portion.\nThis is all wrapped up in a class called BinaryCrossValidator, it is used like so:\n\n    new BinaryCrossValidator(args, 5)\n      .crossValidate(instances, 'instance)\n      .write(Tsv(\"model_xval\"))\n\nThis class also takes the command line arguments, which it passes to a model trainer for each fold.\nThis allows the specification of options to the cross validated models on the command line.\nThe output contains statistics about the performance of the model as well as the confusion matrices\nfor each fold.\n\nA script is included which cross validates a logistic regression model on the iris dataset.\n\n\n\n","23":"[![Android Arsenal](https:\/\/img.shields.io\/badge\/Android%20Arsenal-Onyx-red.svg?style=plastic)](http:\/\/android-arsenal.com\/details\/1\/4089)    \n[![API](https:\/\/img.shields.io\/badge\/API-7%2B-yellow.svg?style=plastic)](https:\/\/android-arsenal.com\/api?level=7)\n[![AppVeyor](https:\/\/img.shields.io\/appveyor\/ci\/gruntjs\/grunt.svg?maxAge=2592000)]()\n\n# onyx\n----      \nOnyx is a library for android that can be used by developers to understand what type of content they are enabling inside their apps.\nAn example can be to limit adult content in apps specifically made for children. Through Onyx you can get the characteristics of an image and then determine if you want to block it or allow it.\nThe possibilities are endless, there can be a zillion use case scenarios. Onyx is proud to be powered by world's best visual recognition technology - Clarifai.      \nNote - The gathering of information about the images is done through the implementation of technologies like Artificial intelligence, machine learning, and deep learning.\n\n----\n# Code is under maintainence.\n### For now the default branch is set to develop. Some features may not work as expected. If you wish to contribute then use the code from the master branch.\n\n![](https:\/\/s8.postimg.org\/pla6wqs5h\/onyx.png)\n------    \n# Download    \n### Using Gradle: under dependencies section:   \n  \n    compile 'com.hanuor.onyx:onyx:1.1.4'  \n\n### or Using Maven:\n    <dependency>\n    <groupId>com.hanuor.onyx<\/groupId>\n    <artifactId>onyx<\/artifactId>\n    <version>1.1.4<\/version>\n    <type>pom<\/type>\n    <\/dependency>    \n\n### or Using Ivy:         \n     <dependency org='com.hanuor.onyx' name='onyx' rev='1.1.4'>\n     <artifact name='$AID' ext='pom'><\/artifact>\n     <\/dependency>\t    \n\t \n\n------\n# Documentation\n\n### Getting tags for an Image\nPretty simple -  \nUse **.getTagsfromApi() method.** Example is given below      \n\n\n     Onyx.with(Context context).fromURL(String url).getTagsfromApi(new OnTaskCompletion() {\n                    @Override\n                    public void onComplete(ArrayList<String> response) {\n                        \/\/get an arraylist of tags here\n                        \/\/do whatever you want here\n                            \n                      }\n                });\n            }\n        });\n\n\t\t\n### Getting tags as well as their probability of occurring\nUse the **.getTagsandProbability() method.** Example is given below       \n\n      Onyx.with(Context context).fromURL(String url).getTagsandProbability(new OnTaskCompletion() {\n                    @Override\n                    public void onComplete(ArrayList<String> response) {\n                       \/\/results will be in the form of array-list\n\t\t\t\t\t   \/\/for eg. - [Mammal-0.9972132, Wolf-0.9962321, Snow-0.993212]\n\t\t\t\t\t   \/\/Do whatever you want to do here\n                    }\n                });\n\t\t\t\t\n\t\t\t\t\n### Getting tags from a video (Whaaaaa......a!)\n**Convert video into a stream of byte[].** and see the example below.    \n\n     \n\t Onyx.with(Context context).fromVideoArray(byte[] videoArray).getTagsfromApi(new OnTaskCompletion() {\n\t\t\t\t\t@Override\n                    public void onComplete(ArrayList<String> response) {\n                       \/\/results will be in the form of array-list\n\t\t\t\t\t   \/\/Do whatever you want to do here\n                    }\n                });       \n\t\t\t\t\n### Getting tags and their probability of occurring from a video     \n**Convert video into a stream of byte[].** and see the example below.    \n     \n\t Onyx.with(Context context).fromVideoArray(byte[] videoArray).getTagsandProbability(new OnTaskCompletion() {\n\t\t\t\t\t@Override\n                    public void onComplete(ArrayList<String> response) {\n                       \/\/results will be in the form of array-list\n\t\t\t\t\t   \/\/for eg. - [Mammal-0.9972132, Wolf-0.9962321, Snow-0.993212]\n\t\t\t\t\t  \n\t\t\t\t\t   \/\/Do whatever you want to do here\n                    }\n                });  \n\t\t\t\t\n\t\t\t\t         \n------\n### Compatibility\n\n**Minimum Android SDK**: Onyx requires a minimum API level of **7**.    \n\n---------\n### Special thanks to:       \n\nClarifai         \n----------      \n \n **Please do notify us if you're using our library in your app. We'd be more than happy to list your app here!**    \n-----------     \n###How it looks like?     \n[![Screenshot](anim2.gif)](https:\/\/cl.ly\/1z1j0847331d)      \n![](https:\/\/s8.postimg.org\/i6vw11yph\/playicon.png)\n[Onyx demo app](https:\/\/play.google.com\/store\/apps\/details?id=com.hanuor.onyx_sample)\n\n    \n\n\n\n\n---------\n\n### License\nCopyright 2016 Hanuor.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n","24":"**Ytk-learn** is a distributed machine learning library which implements most of popular machine learning algorithms. It runs on single, multiple machines and major distributed environments(hadoop, spark)\uff0cand supports major operating systems(Linux, Windows, Mac OS)\uff0cthe communication of distributed environments is implemented based on [ytk-mp4j](https:\/\/github.com\/yuantiku\/ytk-mp4j) which is pure java, mpi-like message passing interface.\n\n### Features\n\n- Supports most of operating systems: Linux, Mac OS, Windows\n- Supports various platforms: single machine, common cluster, hadoop, spark \n- Supports local file system and hdfs file system\n- Provides uniform file system interface and can be applied to other file systems easily.\n- Provides user friendly codes for online prediction.\n- Without complex installation, only needs Java SE Runtime Environment 8 installation.\n\nFor more details, refer to [features](docs\/features.md)\n\n### Documents\n\n- [Running Guide](docs\/running_guide.md)\n- [Demo](demo)\n- [Model Introduction](docs\/models.md)\n- [Data Format](docs\/data_format.md)\n- [Evaluation Metrics](docs\/evaluation_metrics.md)\n- [Performance Guide](docs\/performance_guide.md)\n- [Online Prediction Guide](docs\/online.md)\n\n### Experiments\n\nWe compare our GBDT with [XGBoost](https:\/\/github.com\/dmlc\/xgboost) and [LightGBM](https:\/\/github.com\/Microsoft\/LightGBM), see [gbdt experiments](docs\/gbdt_experiments.md) for more details.\n\n### Environment Requirements\n\nTo run or develop ytk-learn\uff0cjust install [JRE 8](http:\/\/www.oracle.com\/technetwork\/java\/javase\/downloads\/jre8-downloads-2133155.html) or [JDK 8](http:\/\/www.oracle.com\/technetwork\/java\/javase\/downloads\/jdk8-downloads-2133151.html) and set [JAVA_HOME](https:\/\/docs.oracle.com\/cd\/E19182-01\/820-7851\/inst_cli_jdk_javahome_t\/).\n","25":"# Dagli\n[![Maven badge](https:\/\/maven-badges.herokuapp.com\/maven-central\/com.linkedin.dagli\/core\/badge.svg)](https:\/\/search.maven.org\/search?q=g:com.linkedin.dagli)\n[![javadoc](https:\/\/javadoc.io\/badge2\/com.linkedin.dagli\/all\/javadoc.svg)](https:\/\/javadoc.io\/doc\/com.linkedin.dagli\/all)\n\nDagli is a machine learning framework that makes it easy to write bug-resistant, readable, efficient, maintainable and \ntrivially deployable models in [Java 9+](documentation\/java.md) (and other JVM languages).\n\nHere's an introductory example of a text classifier implemented as a pipeline that uses the active leaves of a \nGradient Boosted Decision Tree model (XGBoost) as well as a high-dimensional set of ngrams as features in a logistic \nregression classifier:\n\n    Placeholder<String> text = new Placeholder<>();\n    Placeholder<LabelType> label = new Placeholder<>(); \n    Tokens tokens = new Tokens().withInput(text);\n    \n    NgramVector unigramFeatures = new NgramVector().withMaxSize(1).withInput(tokens);\n    Producer<Vector> leafFeatures = new XGBoostClassification<>()\n        .withFeaturesInput(unigramFeatures)\n        .withLabelInput(label)\n        .asLeafFeatures();\n\n    NgramVector ngramFeatures = new NgramVector().withMaxSize(3).withInput(tokens);\n    LiblinearClassification<LabelType> prediction = new LiblinearClassification<LabelType>()\n        .withFeaturesInput().fromVectors(ngramFeatures, leafFeatures)\n        .withLabelInput(label);\n\n    DAG2x1.Prepared<String, LabelType, DiscreteDistribution<LabelType>> trainedModel = \n        DAG.withPlaceholders(text, label).withOutput(prediction).prepare(textList, labelList);\n    \n    LabelType predictedLabel = trainedModel.apply(\"Some text for which to predict a label\", null);\n    \/\/ trainedModel now can be serialized and later loaded on a server, in a CLI app, in a Hive UDF...\n\nThis code is fairly minimal; Dagli also provides mechanisms to more elegantly encapsulate example data \n([@Structs](documentation\/structs.md)), read in data (e.g. from delimiter-separated value or Avro files), evaluate model \nperformance, and much more.  You can find demonstrations of these among the \n[many code examples provided with Dagli](documentation\/examples.md).    \n\n# Maven Coordinates\nDagli is [split into a number of modules](documentation\/modules.md) that are published to \n[Maven Central](https:\/\/search.maven.org\/search?q=g:com.linkedin.dagli); just add dependencies on those you need in your \nproject.  For example, the dependencies for our above introductory example might look like this in Gradle:\n\n    implementation 'com.linkedin.dagli:common:15.0.0-beta9'            \/\/ commonly used transformers: bucketization, model selection, ngram featurization, etc.\n    implementation 'com.linkedin.dagli:text-tokenization:15.0.0-beta9' \/\/ the text tokenization transformer (\"Tokens\")\n    implementation 'com.linkedin.dagli:liblinear:15.0.0-beta9'         \/\/ the Dagli Liblinear classification model\n    implementation 'com.linkedin.dagli:xgboost:15.0.0-beta9'           \/\/ the Dagli XGBoost classification and regression models\n    \nIf you're in a hurry, you can instead add a dependency on `all`:\n\n    implementation 'com.linkedin.dagli:all:15.0.0-beta9'  \/\/ not recommended for production due to classpath bloat \n\nTo train neural networks, you'll also need to add a\n[dependency for either CPU- or GPU-backed linear algebra](examples\/neural-network\/build.gradle):\n\n    implementation \"org.nd4j:nd4j-native-platform:1.0.0-beta7\" \/\/ CPU-only computation\n    \/\/ implementation \"org.nd4j:nd4j-cuda-10.2-platform:1.0.0-beta7\" \/\/ alternatively, we can use CUDA 10.2 (GPU)\n    \/\/ implementation \"org.deeplearning4j:deeplearning4j-cuda-10.2:1.0.0-beta7\" \/\/ along with cuDNN 7.6 (optional)\n    \n    \n# Benefits\n- Write your machine learning pipeline as a directed acyclic graph (DAG) **once** for both training and inference.  No \nneed to specify a pipeline for training and a separate pipeline for inference.  You define it, train it, and predict \nwith a single pipeline definition.\n- Bug-resiliency: easy-to-read ML pipeline definitions, ubiquitous static typing, and most things in Dagli are \n**immutable**.\n- Portability: works on your server, in a Hadoop mapper, a CLI program, in your IDE, etc. on any platform\n- Deployability: an entire pipeline is serialized and deserialized as a single object\n- Abstraction: creating new transformations and models is straightforward and these can be reused in any Dagli pipeline\n- Speed: highly parallel multithreaded execution, graph (pipeline) optimizations, minibatching\n- Inventory: many, many useful pipeline components ready to use, right out of the box.  Neural networks, logistic \nregression, gradient boosted decision trees, FastText, cross-validation, cross-training, feature selection, data \nreaders, evaluation, feature transformations...\n- Java: easily use from any JVM language with the support of your IDE's code completion, type hints, inline \ndocumentation, etc.\n\n# Overview\nAs might be surmised from the name, \n[Dagli represents machine learning pipelines as directed acyclic graphs](documentation\/dag.md) (DAGs).\n\n- The \"roots\" of the graph \n    - `Placeholder`s (which represent the training and inference example data)\n    - `Generator`s (which automatically generate a value for each example, such as a `Constant`, `ExampleIndex`, \n    `RandomDouble`, etc.)\n- Transformers, the \"child nodes\" of the graph\n    - Data transformations (e.g. `Tokens`, `BucketIndex`, `Rank`, `Index`, etc.)\n    - Learned models (e.g. `XGBoostRegression`, `LiblinearClassifier`, `NeuralNetwork`, etc.)\n\nTransformers may be *preparable* or *prepared*.  Dagli uses the word \"preparation\" rather than \"training\" because many \n`PreparableTransformer`s are not statistical models; e.g. `BucketIndex` examines all the preparation examples to find \nthe optimal bucket boundaries with the most even distribution of values amongst the buckets.\n\nWhen a DAG is prepared with training\/preparation data, the `PreparableTransformer`s (like `BucketIndex` or \n`XGBoostRegression`) become `PreparedTransformer`s (like `BucketIndex.Prepared` or `XGBoostRegression.Prepared`) which \nare then subsequently used to actually transform the input values (both during DAG preparation so the results may be fed\nto downstream transformers and later, during inference in the prepared DAG).\n\nOf course, many transformers are already \"prepared\" and don't require preparation; a prepared DAG containing no \npreparable transformers may be created directly (e.g. `DAG.Prepared.withPlaceholders(...).withOutputs(...)`) and used to\ntransform data without any preparation\/training step. \n\nDAGs are encapsulated by a `DAG` class corresponding to their input and output arities, e.g. `DAG2x1<String, Integer, \nDouble>` is a pipeline that accepts examples with a `String` and `Integer` feature and outputs a `Double` result.\nGenerally, it's better design to provide all the example data together as a single [@Struct](documentation\/structs.md) \nor other type rather than as multiple inputs.  DAGs are also themselves transformers and can thus be embedded within \nother, larger DAGs.\n\n# Examples\nProbably the easiest way to get a feel for how Dagli models are written and used is from the \n[numerous code examples](documentation\/examples.md).  The example code is more verbose than would be seen in practice, \nbut--combined with explanatory comments for almost every step--these can be an excellent pedagogic tool.\n\n# Finding the Right Transformer\nDagli includes a large and growing library of transformers.  The [examples](documentation\/examples.md) illustrate the\nuse of a number of transformers, and the [Javadoc](https:\/\/javadoc.io\/doc\/com.linkedin.dagli\/all) is searchable.  You\nmay also want to check the [module summary](documentation\/modules.md) for a broader overview of what is available.\n\n# Adding New Transformers\nIf an existing transformer doesn't do what you want, you can often wrap an existing function\/method with a \n`FunctionResultX` transformer (where `X` is the function's arity, e.g. 1 or 4).  Otherwise, it's \n[easy to create your own transformers](documentation\/transformers.md).  \n\n# Documentation\n- [Overview of Dagli Examples](documentation\/examples.md)\n- [Overview of Dagli Modules](documentation\/modules.md)\n- [How Dagli Represents ML Pipelines as DAGs](documentation\/dag.md)\n- [Usage and Creation of Transformers](documentation\/transformers.md)\n- [@Structs: Autogenerated, immutable convenience classes for storing fields](documentation\/structs.md)\n- [Using Avro Data with Dagli](documentation\/avro.md)\n\n# Alternative ML Solutions\n\nDagli lets Java (and JVM) developers easily define readable, reusable, bug-resistant models and train them efficiently on \nmodern multicore, GPU-equipped machines.\n\nOf course, there is no \"one size fits all\" ML framework.  Dagli provides a layer-oriented API for defining novel neural\nnetworks, but for unusual architectures or cutting-edge research, TensorFlow, PyTorch, DeepLearning4J and others may be \nbetter options (Dagli supports the integration of arbitrary DeepLearning4J architectures into the model pipeline \nout-of-the-box, and, for example, pre-trained TensorFlow models can also be incorporated with a custom wrapper.)\n\nSimilarly, while Dagli models have been trained with *billions* of examples, extremely large scale training across \nmultiple machines may be better served by platforms such as Hadoop, Spark, and Kubeflow.  Hadoop\/Hive\/Spark\/Presto\/etc. \nare of course commonly used to pull data to train and evaluate Dagli models, but it is also very feasible to, e.g. create\ncustom UDFs that train, evaluate or apply Dagli models.  \n\n[Further discussion comparing extant pipelined and joint modeling with Dagli](documentation\/comparison.md).\n\n\n# Version History\n- `15.0.0-beta9`: *10\/4\/21*:\n    - `BinaryConfusionMatrix` now calculates F1-scores as 0 (rather than NaN) when precision and recall are both 0\n    - Fixed corner case where neural networks with multiple logically equivalent layers were improperly considered \n      invalid.\n    - Fixed vector sequence input bug in DL4J neural networks\n- `15.0.0-beta8`: *8\/21\/21*: Added default constructors to Dagli's implementation of DL4J vertices where needed to \n    ensure their serializability \n- `15.0.0-beta7`: *4\/12\/21*: Loosened erroneously-strict generic constraint on argument to \n   `NNClassification::withMultilabelLabelsInput(...)` \n- `15.0.0-beta6`: *1\/26\/21*: Added workaround for \n    [DL4J bug](https:\/\/community.konduit.ai\/t\/bertiterator-produces-npe-while-training-on-gpu\/580) that caused a null \n    pointer exception when using CUDA (GPU) to train neural networks.  Thanks to @cyberbeat for reporting this.\n- `15.0.0-beta5`: *11\/15\/20*: [aggregated Javadoc](https:\/\/javadoc.io\/doc\/com.linkedin.dagli\/all) now available\n- `15.0.0-beta4`: *11\/11\/20*: `xgboost` now bundles in [support for Windows](xgboost\/README.md)\n- `15.0.0-beta3`: *11\/9\/20*: Input Configurators and `MermaidVisualization`\n    - This is a major version increment and may not be compatible with models from 14.*\n    - [Input configurators](documentation\/transformers.md#input-configurators) for more convenient, readable \n      configuration of transformer inputs; e.g., \n      `new LiblinearClassification<LabelType>().withFeaturesInput().fromNumbers(numberInput1, numberInput2...)...`\n    - New graph visualizer for rendering Dagli graphs as Mermaid markup\n    - [Full list of improvements](documentation\/v15.0.0-beta3.md)\n- `14.0.0-beta2` *9\/27\/20*: update dependency metadata to prevent the annotation processors' dependencies from \n  transitively leaking into the client's classpath  \n- `14.0.0-beta1`: initial public release\n\n## Versioning Policy\nDagli's current public release is designated as \"beta\" due to extensive changes relative to previous \n(LinkedIn-internal) releases and the greater diversity of applications entailed by a public release. \n\nWhile in beta, releases with potentially breaking API or serialization changes will be accompanied by a major version \nincrement (e.g. `14.0.0-beta2` to `15.0.0-beta3`).  After the beta period concludes, subsequent revisions will be backward\ncompatible to allow large projects to depend on multiple versions of Dagli without dependency shading.\n\n# License\n[Licensed under the BSD 2-Clause license](LICENSE).\n\nCopyright 2020 LinkedIn Corporation.  All Rights Reserved.\n","26":"[![Testing Workflow](https:\/\/github.com\/opendistro-for-elasticsearch\/k-NN\/workflows\/Testing%20Workflow\/badge.svg)](https:\/\/github.com\/opendistro-for-elasticsearch\/k-NN\/actions)\n[![codecov](https:\/\/codecov.io\/gh\/opendistro-for-elasticsearch\/k-NN\/branch\/master\/graph\/badge.svg)](https:\/\/codecov.io\/gh\/opendistro-for-elasticsearch\/k-NN)\n[![Documentation](https:\/\/img.shields.io\/badge\/doc-reference-blue)](https:\/\/opendistro.github.io\/for-elasticsearch-docs\/docs\/knn\/)\n[![Chat](https:\/\/img.shields.io\/badge\/chat-on%20forums-blue)](https:\/\/discuss.opendistrocommunity.dev\/c\/k-NN\/)\n![PRs welcome!](https:\/\/img.shields.io\/badge\/PRs-welcome!-success)\n\n# Open Distro for Elasticsearch KNN\n\nOpen Distro for Elasticsearch enables you to run nearest neighbor search on billions of documents across thousands of dimensions with the same ease as running any regular Elasticsearch query. You can use aggregations and filter clauses to further refine your similarity search operations. K-NN similarity search powers use cases such as product recommendations, fraud detection, image and video search, related document search, and more.\n\n## Documentation\n\nThe README provides information for development of the k-NN plugin. To learn more about plugin usage, please see our [documentation](https:\/\/opendistro.github.io\/for-elasticsearch-docs\/docs\/knn). Do not hesitate to [create an issue](https:\/\/github.com\/opendistro-for-elasticsearch\/k-NN\/issues\/new) if something is missing from the documentation!\n\n## Setup\n\n1. Check out the package from version control.\n2. Launch Intellij IDEA, choose **Import Project**, and select the `settings.gradle` file in the root of this package.\n3. To build from the command line, set `JAVA_HOME` to point to a JDK 14 before running `.\/gradlew build`.\n\n## Build\n\nThe package uses the [Gradle](https:\/\/docs.gradle.org\/6.6.1\/userguide\/userguide.html) build system.\n\n1. Checkout this package from version control.\n2. To build from command line set `JAVA_HOME` to point to a JDK >=14\n3. Run `.\/gradlew build`\n\n## JNI Library\n\nThe plugin relies on a JNI library to perform approximate k-NN search. For plugin installations from archive(.zip), it is necessary to ensure ```.so``` file for Linux and ```.jnilib``` file for Mac OS are present in the Java library path. This can be possible by copying .so\/.jnilib to either $ES_HOME or by adding manually ```-Djava.library.path=<path_to_lib_files>``` in ```jvm.options``` file\n\nTo build the JNI Library, follow these steps:\n\n```\ncd jni\ncmake .\nmake\n```\n\nThe library will be placed in the `jni\/release` directory.\n\nTo build an RPM or DEB of the JNI library, follow these steps:\n\n```\ncd jni\ncmake .\nmake package\n```\n\nThe artifacts will be placed in the `jni\/packages` directory.\n\n## JNI Library Artifacts\n\nWe build and distribute binary library artifacts with Opendistro for Elasticsearch. We build the library binary, RPM and DEB in [this GitHub action](https:\/\/github.com\/opendistro-for-elasticsearch\/k-NN\/blob\/main\/.github\/workflows\/CD.yml). We use Centos 7 with g++ 4.8.5 to build the DEB, RPM and ZIP. Additionally, in order to provide as much general compatibility as possible, we compile the library without optimized instruction sets enabled. For users that want to get the most out of the library, they should follow [this section](#jni-library) and build the library from source in their production environment, so that if their environment has optimized instruction sets, they take advantage of them.\n\n## Running Multi-node Cluster Locally\n\nIt can be useful to test and debug on a multi-node cluster. In order to launch a 3 node cluster with the KNN plugin installed, run the following command:\n\n```\n.\/gradlew run -PnumNodes=3\n```\n\nIn order to run the integration tests with a 3 node cluster, run this command:\n\n```\n.\/gradlew :integTest -PnumNodes=3\n```\n\n### Debugging\n\nSometimes it is useful to attach a debugger to either the Elasticsearch cluster or the integration test runner to see what's going on. For running unit tests, hit **Debug** from the IDE's gutter to debug the tests. For the Elasticsearch cluster, first, make sure that the debugger is listening on port `5005`. Then, to debug the cluster code, run:\n\n```\n.\/gradlew :integTest -Dcluster.debug=1 # to start a cluster with debugger and run integ tests\n```\n\nOR\n\n```\n.\/gradlew run --debug-jvm # to just start a cluster that can be debugged\n```\n\nThe Elasticsearch server JVM will connect to a debugger attached to `localhost:5005` before starting. If there are multiple nodes, the servers will connect to debuggers listening on ports `5005, 5006, ...`\n\nTo debug code running in an integration test (which exercises the server from a separate JVM), first, setup a remote debugger listening on port `8000`, and then run:\n\n```\n.\/gradlew :integTest -Dtest.debug=1\n```\n\nThe test runner JVM will connect to a debugger attached to `localhost:8000` before running the tests.\n\nAdditionally, it is possible to attach one debugger to the cluster JVM and another debugger to the test runner. First, make sure one debugger is listening on port `5005` and the other is listening on port `8000`. Then, run:\n```\n.\/gradlew :integTest -Dtest.debug=1 -Dcluster.debug=1\n```\n\n## Contributions\n\nWe appreciate and encourage contributions from the community. If you experience a bug or have a feature request, please create an issue for it. If you decide to make a contribution, please fill out the Pull Request template with as much detail as possible. Also, when creating a title for your Pull Request, please do not include a prefix such as `Bug Fix:`. Instead, please use the corresponding tag to label the purpose of the Pull Request.\n\n## REQUEST FOR COMMENT (RFC)\n\nWe'd like to get your comments! Please read the plugin RFC [document](https:\/\/github.com\/opendistro-for-elasticsearch\/k-NN\/blob\/development\/RFC.md) and raise an issue to add your comments and questions.\n\n## Credits and Acknowledgments\n\nThis project uses the Apache 2.0-licensed [Non-Metric Space Library](https:\/\/github.com\/nmslib\/nmslib\/). Thank you to Bilegsaikhan Naidan, Leonid Boytsov, Yury Malkov, David Novak and all those who have contributed to that project!\n\n## Code of Conduct\n\nThis project has adopted an [Open Source Code of Conduct](https:\/\/opendistro.github.io\/for-elasticsearch\/codeofconduct.html).\n\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS\/Amazon Security via our [vulnerability reporting page](http:\/\/aws.amazon.com\/security\/vulnerability-reporting\/). Please do **not** create a public GitHub issue.\n\n\n## Licensing\n\nSee the [LICENSE](.\/LICENSE.txt) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\n\n## Copyright\n\nCopyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n","27":"ABAGAIL\n=======\n\n[![Build Status](https:\/\/travis-ci.org\/pushkar\/ABAGAIL.svg?branch=master)](https:\/\/travis-ci.org\/pushkar\/ABAGAIL)\n\nThe library contains a number of interconnected Java packages that implement machine learning and artificial intelligence algorithms. These are artificial intelligence algorithms implemented for the kind of people that like to implement algorithms themselves.\n\nUsage\n------\n*For discrete optimization problems see java examples [\/src\/opt\/test](https:\/\/github.com\/pushkar\/ABAGAIL\/tree\/master\/src\/opt\/test) or jython versions [\/jython](https:\/\/github.com\/pushkar\/ABAGAIL\/tree\/master\/jython)   \n*For jython | csv | python and grid search examples see [\/jython](https:\/\/github.com\/pushkar\/ABAGAIL\/blob\/master\/jython)   \n*Also see [Wiki](https:\/\/github.com\/pushkar\/ABAGAIL\/wiki), [FAQ](https:\/\/github.com\/pushkar\/ABAGAIL\/blob\/master\/faq.md) \n\nHere is a simple example of how to import data and build a neural network using the iris data set (taken from [IrisTest.java](https:\/\/github.com\/pushkar\/ABAGAIL\/blob\/master\/src\/opt\/test\/IrisTest.java)).  Train and test error will be exported in csv format to the current working directory.   \n```\n\/\/import data\nDataSetReader dsr = new CSVDataSetReader((new File(\"src\/opt\/test\/iris.txt\")).getAbsolutePath());\nDataSet ds = dsr.read();\n\n\/\/split last attribute for label\nLabelSplitFilter lsf = new LabelSplitFilter();\nlsf.filter(ds);\n\n\/\/encode label as one-hot array and get outputLayerSize\nDiscreteToBinaryFilter dbf = new DiscreteToBinaryFilter();\ndbf.filter(ds.getLabelDataSet());\noutputLayerSize=dbf.getNewAttributeCount();\n\n\/\/test-train split\nint percentTrain=75;\nRandomOrderFilter randomOrderFilter = new RandomOrderFilter();\nrandomOrderFilter.filter(ds);\nTestTrainSplitFilter testTrainSplit = new TestTrainSplitFilter(percentTrain);\ntestTrainSplit.filter(ds);\ntrain=testTrainSplit.getTrainingSet();\ntest=testTrainSplit.getTestingSet();\n\n\/\/standardize data\nStandardMeanAndVariance smv = new StandardMeanAndVariance();\nsmv.fit(train);\nsmv.transform(train);\nsmv.transform(test);\n\n\/\/create backprop network using builder\nBackPropagationNetwork network = new BackpropNetworkBuilder()\n  .withLayers(new int[] {25,10,outputLayerSize})\n  .withDataSet(train, test)\n  .withIterations(5000)\n  .train();\n  \n\/\/create opt network using builder\nFeedForwardNetwork optNetwork = new OptNetworkBuilder()\n  .withLayers(new int[] {25,10,outputLayerSize})\n  .withDataSet(train, test)\n  .withSA(100000, .975)\n  .withIterations(1000)\n  .train();\n\n```\n\n\nContributing\n------------\n\n1. Fork it.\n2. Create a branch (`git checkout -b my_branch`)\n3. Commit your changes (`git commit -am \"Awesome feature\"`)\n4. Push to the branch (`git push origin my_branch`)\n5. Open a [Pull Request][1]\n6. Enjoy a refreshing Diet Coke and wait \n\nFeatures\n========\n\n### Hidden Markov Models\n\n* Baum-Welch reestimation algorithm, scaled forward-backward algorithm, Viterbi algorithm\n* Support for Input-Output Hidden Markov Models\n* Write your own output or transition probability distribution or use the provided distributions, including neural network based conditional probability distributions\n* Neural Networks\n\n### Feed-forward backpropagation neural networks of arbitrary topology\n* Configurable error functions with sum of squares, weighted sum of squares\n* Multiple activation functions with logistic sigmoid, linear, tanh, and soft max\n* Choose your weight update rule with standard update rule, standard update rule with momentum, Quickprop, RPROP\n* Online and batch training\n* Support Vector Machines\n\n### Fast training with the sequential minimal optimization algorithm\n* Support for linear, polynomial, tanh, radial basis function kernels\n* Decision Trees\n\n### Information gain or GINI index split criteria\n* Binary or all attribute value splitting\n* Chi-square signifigance test pruning with configurable confidence levels\n* Boosted decision stumps with AdaBoost\n* K Nearest Neighbors\n\n### Fast kd-tree implementation for instance based algorithms of all kinds\n* KNN Classifier with weighted or non-weighted classification, customizable distance function\n* Linear Algebra Algorithms\n\n### Basic matrix and vector math, a variety of matrix decompositions based on the standard algorithms\n* Solve square systems, upper triangular systems, lower triangular systems, least squares\n* Singular Value Decomposition, QR Decomposition, LU Decomposition, Schur Decomposition, Symmetric Eigenvalue Decomposition, Cholesky Factorization\n* Make your own matrix decomposition with the easy to use Householder Reflection and Givens Rotation classes\n* Optimization Algorithms\n\n### Randomized hill climbing, simulated annealing, genetic algorithms, and discrete dependency tree MIMIC\n* Make your own crossover functions, mutation functions, neighbor functions, probability distributions, or use the provided ones.\n* Optimize the weights of neural networks and solve travelling salesman problems\n* Graph Algorithms\n\n### Kruskals MST and DFS\n* Clustering Algorithms\n\n### EM with gaussian mixtures, K-means\n* Data Preprocessing\n\n### PCA, ICA, LDA, Randomized Projections\n* Convert from continuous to discrete, discrete to binary\n* Reinforcement Learning\n\n### Value and policy iteration for Markov decision processes\n\n[1]: https:\/\/help.github.com\/articles\/using-pull-requests\n","28":"[<img src=\"images\/logo\/shifu.png\" alt=\"Shifu\" align=\"left\">](http:\/\/shifu.ml)<div align=\"right\"><div>[![Build Status](https:\/\/travis-ci.org\/ShifuML\/shifu.svg)](https:\/\/travis-ci.org\/ShifuML\/shifu?branch=develop)<\/div><div>[![Maven Central](https:\/\/maven-badges.herokuapp.com\/maven-central\/ml.shifu\/shifu\/badge.svg)](https:\/\/maven-badges.herokuapp.com\/maven-central\/ml.shifu\/shifu)<\/div><\/div>\n\n#\n\n## Download\n\nPlease [download](https:\/\/github.com\/ShifuML\/shifu\/wiki\/shifu-0.12.0-hdp-yarn.tar.gz) latest shifu [here](https:\/\/github.com\/ShifuML\/shifu\/wiki\/shifu-0.12.0-hdp-yarn.tar.gz).\n\n## Getting Started\nAfter shifu downloading, build your first model with Shifu [tutorial](https:\/\/github.com\/ShifuML\/shifu\/wiki\/Tutorial---Build-Your-First-ML-Model). More details about shifu can be found in our [wiki pages](https:\/\/github.com\/ShifuML\/shifu\/wiki).\n\n## What is Shifu?\nShifu is an open-source, end-to-end machine learning and data mining framework built on top of Hadoop. Shifu is designed for data scientists, simplifying the life-cycle of building machine learning models. While originally built for fraud modeling, Shifu is generalized for many other modeling domains.\n\nOne of Shifu's pros is an end-to-end modeling pipeline in machine learning. With only configurations settings, a whole machine pipeline can be built and model can be much more easy to develop and push to production. The pipeline defined in Shifu is in below:\n\n![Shifu Pipeline](https:\/\/raw.githubusercontent.com\/wiki\/ShifuML\/shifu\/images\/new-shifu-pipeline.png)\n\nShifu provides a simple command-line interface for each step of the model building process, including\n\n* Statistic calculation & variable selection to determine the most predictive variables in your data\n* [Variable normalization](https:\/\/github.com\/ShifuML\/shifu\/wiki\/Variable%20Transform%20in%20Shifu)\n* [Distributed variable selection based on sensitivity analysis](https:\/\/github.com\/ShifuML\/shifu\/wiki\/Variable%20Selection%20in%20Shifu)\n* [Distributed neural network model training](https:\/\/github.com\/ShifuML\/shifu\/wiki\/Distributed%20Neural%20Network%20Training%20in%20Shifu)\n* [Distributed tree ensemble model training](https:\/\/github.com\/ShifuML\/shifu\/wiki\/Distributed%20Tree%20Ensemble%20Model%20Training%20in%20Shifu)\n* Post training analysis & model evaluation\n* [Distributed Tensorflow on Shifu](https:\/\/github.com\/ShifuML\/shifu\/wiki\/Distributed-Tensorflow-Support-On-Shifu)\n\nShifu\u2019s fast Hadoop-based, distributed neural network \/ logistic regression \/ gradient boosted trees training can reduce model training time from days to hours on TB data sets. Shifu integrates with Pig workflows on Hadoop, and Shifu-trained models can be integrated into production code with a simple Java API. Shifu leverages Pig, Akka, Encog and other open source projects.\n\n[Guagua](https:\/\/github.com\/ShifuML\/guagua), an in-memory iterative computing framework on Hadoop YARN is developed as sub-project of Shifu to accelerate training progress.\n\nMore details about shifu can be found in our [wiki pages](https:\/\/github.com\/ShifuML\/shifu\/wiki)\n\n## Conference\n\n* [QCON Shanghai 2015](http:\/\/2015.qconshanghai.com\/presentation\/2827) [Slides](http:\/\/www.slideshare.net\/pengshanzhang\/large-scale-machine-learning-at-pay-pal-risk)\n\n* [BDTC Beijing 2016](http:\/\/bdtc2016.hadooper.cn\/dct\/page\/70107)\n\n* [Strata Beijing 2017](https:\/\/strata.oreilly.com.cn\/strata-cn\/public\/schedule\/detail\/59593?locale=en)\n\n## Contributors\n\n - Zhanghao Hu (zhanhu@paypal.com)\n - Grahame Jastrebski (gjastrebski@paypal.com)\n - Lavar Li (lulli@paypal.com)\n - Mark Liu (yliu15@paypal.com)\n - David Zhang (pengzhang@paypal.com)\n - Xin Zhong (xinzhong@paypal.com)\n - Simon Zhang (jzhang13@paypal.com)\n - Sharma Nitin (nsharma1@paypal.com)\n - Wayne Zhu (wzhu1@paypal.com)\n - Devin Wu (haifwu@paypal.com)\n - Fred Bai (webai@paypal.com)\n\n## Google Group\n\nPlease join [Shifu group](https:\/\/groups.google.com\/forum\/#!forum\/shifuml) if questions, bugs or anything else.\n\n## Copyright and License\n\nCopyright 2012-2019, PayPal Software Foundation under the [Apache License](LICENSE.txt).\n","29":"# Meka\n\nThe MEKA project provides an open source implementation of methods for multi-label learning and evaluation.\n\nhttp:\/\/waikato.github.io\/meka\/\n\n## Documentation\n\nSee http:\/\/waikato.github.io\/meka\/documentation\/ for sources of documentation regarding MEKA.\n\nIn particular, \n\n* See the `Tutorial.pdf` for detailed information on obtaining, using and extending MEKA.\n* For a list of included methods and command line examples for them, see: http:\/\/meka.sourceforge.net\/methods.html\n* For examples on how to use MEKA in your Java code: https:\/\/github.com\/Waikato\/meka\/tree\/master\/src\/main\/java\/mekaexamples\n\nIf you have a specific question, search the Github issues and, if you can't find\na solution, create an issue and label it as *question*:\n\nhttps:\/\/github.com\/Waikato\/meka\/issues\n\n## Bugs, and Future Enhancements\n\nA list of current Issues in Meka (known bugs, planned improvements, feature wishlist) \ncan be found at https:\/\/github.com\/Waikato\/meka\/issues\n\nThe Meka developers never have enough time to implement everything that should be in Meka. \nIf you would like to contribute something new to Meka, or help with any of the existing issues, \nplease get in touch with the developers. \n\n","30":"# JStarCraft AI\r\n\r\n****\r\n\r\n[![License](https:\/\/img.shields.io\/badge\/license-Apache%202-4EB1BA.svg)](https:\/\/www.apache.org\/licenses\/LICENSE-2.0.html)\r\n[![Total lines](https:\/\/tokei.rs\/b1\/github\/HongZhaoHua\/jstarcraft-ai?category=lines)](https:\/\/tokei.rs\/b1\/github\/HongZhaoHua\/jstarcraft-ai?category=lines)\r\n[![Codacy Badge](https:\/\/api.codacy.com\/project\/badge\/Grade\/8e39a24e1be740c58b83fb81763ba317)](https:\/\/www.codacy.com\/project\/HongZhaoHua\/jstarcraft-ai\/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=HongZhaoHua\/jstarcraft-ai&amp;utm_campaign=Badge_Grade_Dashboard)\r\n\r\n\u5e0c\u671b\u8def\u8fc7\u7684\u540c\u5b66,\u987a\u624b\u7ed9JStarCraft\u6846\u67b6\u70b9\u4e2aStar,\u7b97\u662f\u5bf9\u4f5c\u8005\u7684\u4e00\u79cd\u9f13\u52b1\u5427!\r\n\r\n****\r\n\r\n**JStarCraft AI\u662f\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6.\u9075\u5faaApache 2.0\u534f\u8bae.**\r\n\r\n\u5728\u5b66\u672f\u754c,\u7edd\u5927\u591a\u6570\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u662fPython.\r\n\r\n\u5728\u5de5\u4e1a\u754c,\u7edd\u5927\u591a\u6570\u5f00\u53d1\u4eba\u5458\u4f7f\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u662fJava.\r\n\r\nJStarCraft AI\u662f\u4e00\u4e2a\u57fa\u4e8eJava\u8bed\u8a00\u7684\u673a\u5668\u5b66\u4e60\u5de5\u5177\u5305,\u7531\u4e00\u7cfb\u5217\u7684\u6570\u636e\u7ed3\u6784,\u7b97\u6cd5\u548c\u6a21\u578b\u7ec4\u6210.\r\n\r\n\u76ee\u6807\u662f\u4f5c\u4e3a\u5728\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u4ece\u4e8b\u673a\u5668\u5b66\u4e60\u7814\u53d1\u7684\u76f8\u5173\u4eba\u5458\u4e4b\u95f4\u7684\u6865\u6881.\u666e\u53ca\u673a\u5668\u5b66\u4e60\u5728Java\u9886\u57df\u7684\u5e94\u7528.\r\n\r\n|\u4f5c\u8005|\u6d2a\u948a\u6866|\r\n|---|---\r\n|E-mail|110399057@qq.com, jstarcraft@gmail.com\r\n\r\n****\r\n\r\n## JStarCraft AI\u67b6\u6784\r\n\r\nJStarCraft AI\u6846\u67b6\u5404\u4e2a\u6a21\u5757\u4e4b\u95f4\u7684\u5173\u7cfb:\r\n![ai](https:\/\/github.com\/HongZhaoHua\/jstarcraft-tutorial\/blob\/master\/ai\/JStarCraft%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A1%86%E6%9E%B6%E7%BB%84%E4%BB%B6%E5%9B%BE.png \"JStarCraft AI\u67b6\u6784\")\r\n\r\n****\r\n\r\n## JStarCraft AI\u7279\u6027\r\n* [1.\u6570\u636e(data)](https:\/\/github.com\/HongZhaoHua\/jstarcraft-ai\/wiki\/%E6%95%B0%E6%8D%AE)\r\n    * \u5c5e\u6027\u4e0e\u7279\u5f81\r\n        * \u8fde\u7eed\r\n        * \u79bb\u6563\r\n    * \u6a21\u5757\u4e0e\u5b9e\u4f8b\r\n    * \u9009\u62e9,\u6392\u5e8f\u4e0e\u5207\u5272\r\n* 2.\u73af\u5883(environment)\r\n    * \u4e32\u884c\u8ba1\u7b97\r\n    * \u5e76\u884c\u8ba1\u7b97\r\n    * CPU\u8ba1\u7b97\r\n    * GPU\u8ba1\u7b97\r\n* 3.\u6570\u5b66(math)\r\n    * \u7b97\u6cd5(algorithm)\r\n        * \u5fae\u79ef\u5206(calculus)\r\n        * \u76f8\u5173\u6027(correlation)\r\n            * \u8ddd\u79bb(distance)\r\n            * \u76f8\u4f3c\u5ea6(similarity)\r\n        * \u5206\u89e3(decomposition)\r\n        * \u6838\u6280\u5de7(kernel)\r\n        * \u6982\u7387(probability)\r\n    * [\u6570\u636e\u7ed3\u6784(structure)](https:\/\/github.com\/HongZhaoHua\/jstarcraft-ai\/wiki\/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84)\r\n        * \u6807\u91cf\r\n        * \u5411\u91cf\r\n        * \u77e9\u9635\r\n        * \u5f20\u91cf\r\n        * \u5355\u5143\r\n        * \u8868\u5355\r\n* 4.\u8c03\u5236\u89e3\u8c03(modem)\r\n* [5.\u6a21\u578b(model)](https:\/\/github.com\/HongZhaoHua\/jstarcraft-ai\/wiki\/%E6%A8%A1%E5%9E%8B)\r\n    * \u7ebf\u6027\u6a21\u578b(linear)\r\n    * \u8fd1\u90bb\u6a21\u578b(nearest neighbor)\r\n    * \u77e9\u9635\u5206\u89e3\u6a21\u578b(matrix factorization)\r\n    * \u795e\u7ecf\u7f51\u7edc\u6a21\u578b(neutral network)\r\n        * \u8ba1\u7b97\u56fe\r\n            * \u8282\u70b9\r\n            * \u5c42\r\n        * \u6b63\u5411\u4f20\u64ad\u4e0e\u53cd\u5411\u4f20\u64ad\r\n        * \u6fc0\u6d3b\u51fd\u6570\r\n        * \u68af\u5ea6\u66f4\u65b0\r\n    * \u6982\u7387\u56fe\u6a21\u578b(probabilistic graphical)\r\n    * \u89c4\u5219\u6a21\u578b(rule)\r\n    * \u652f\u6301\u5411\u91cf\u673a\u6a21\u578b(support vector machine)\r\n    * \u6811\u6a21\u578b(tree)\r\n* 6.\u4f18\u5316(optimization)\r\n    * \u68af\u5ea6\u4e0b\u964d\u6cd5(gradient descent)\r\n        * \u6279\u91cf\u68af\u5ea6\u4e0b\u964d(batch gradient descent)\r\n        * \u968f\u673a\u68af\u5ea6\u4e0b\u964d(stochastic gradient descent)\r\n    * \u725b\u987f\u6cd5\u548c\u62df\u725b\u987f\u6cd5(newton method\/quasi newton method)\r\n    * \u5171\u8f6d\u68af\u5ea6\u6cd5(conjugate gradient)\r\n    * [\u8bd5\u63a2\u6cd5(heuristic)](https:\/\/github.com\/HongZhaoHua\/jstarcraft-ai\/wiki\/%E8%AF%95%E6%8E%A2%E6%B3%95)\r\n        * \u6a21\u62df\u9000\u706b\u7b97\u6cd5(simulate anneal)\r\n        * \u9057\u4f20\u7b97\u6cd5(genetic)\r\n        * \u8681\u7fa4\u7b97\u6cd5(ant colony)\r\n        * \u7c92\u5b50\u7fa4\u7b97\u6cd5(particle swarm)\r\n* [7.\u6709\u76d1\u7763\u5b66\u4e60(supervised)](https:\/\/github.com\/HongZhaoHua\/jstarcraft-ai\/wiki\/%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0)\r\n    * \u5206\u7c7b\r\n    * \u56de\u5f52\r\n* [8.\u65e0\u76d1\u7763\u5b66\u4e60(unsupervised)](https:\/\/github.com\/HongZhaoHua\/jstarcraft-ai\/wiki\/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0)\r\n    * \u805a\u7c7b\r\n    * \u5173\u8054\r\n* [9.\u4e30\u5bcc\u7684\u8bc4\u4f30\u6307\u6807](#\u8bc4\u4f30\u6307\u6807)\r\n    * [\u6392\u5e8f\u6307\u6807](#\u6392\u5e8f\u6307\u6807)\r\n    * [\u8bc4\u5206\u6307\u6807](#\u8bc4\u5206\u6307\u6807)\r\n\r\n****\r\n\r\n## JStarCraft AI\u6559\u7a0b\r\n\r\n* 1.\u8bbe\u7f6e\u4f9d\u8d56\r\n    * [Maven\u4f9d\u8d56](#Maven\u4f9d\u8d56)\r\n    * [Gradle\u4f9d\u8d56](#Gradle\u4f9d\u8d56)\r\n* 2.\u914d\u7f6e\u73af\u5883\r\n    * [\u8bbe\u7f6eCPU\u73af\u5883](#\u8bbe\u7f6eCPU\u73af\u5883)\r\n    * [\u8bbe\u7f6eGPU\u73af\u5883](#\u8bbe\u7f6eGPU\u73af\u5883)\r\n    * [\u4f7f\u7528\u73af\u5883\u4e0a\u4e0b\u6587](#\u4f7f\u7528\u73af\u5883\u4e0a\u4e0b\u6587)\r\n* 3.\u4f7f\u7528\u6570\u636e\r\n    * [\u6570\u636e\u8868\u793a](#\u6570\u636e\u8868\u793a)\r\n    * [\u6570\u636e\u8f6c\u6362](#\u6570\u636e\u8f6c\u6362)\r\n        * ARFF\r\n        * CSV\r\n        * JSON\r\n        * Parquet\r\n        * HQL\r\n        * SQL\r\n    * [\u6570\u636e\u5904\u7406](#\u6570\u636e\u5904\u7406)\r\n        * \u9009\u62e9\r\n        * \u6392\u5e8f\r\n        * \u5207\u5272\r\n\r\n#### Maven\u4f9d\u8d56\r\n\r\n```maven\r\n<dependency>\r\n    <groupId>com.jstarcraft<\/groupId>\r\n    <artifactId>ai<\/artifactId>\r\n    <version>1.0<\/version>\r\n<\/dependency>\r\n```\r\n\r\n#### Gradle\u4f9d\u8d56\r\n\r\n```gradle\r\ncompile group: 'com.jstarcraft', name: 'ai', version: '1.0'\r\n```\r\n\r\n#### \u8bbe\u7f6eCPU\u73af\u5883\r\n\r\n```maven\r\n<dependency>\r\n    <groupId>org.nd4j<\/groupId>\r\n    <artifactId>nd4j-native-platform<\/artifactId>\r\n    <version>1.0.0-beta3<\/version>\r\n<\/dependency>\r\n```\r\n\r\n#### \u8bbe\u7f6eGPU\u73af\u5883\r\n\r\n* CUDA 9.0\r\n\r\n```maven\r\n<dependency>\r\n    <groupId>org.nd4j<\/groupId>\r\n    <artifactId>nd4j-cuda-9.0-platform<\/artifactId>\r\n    <version>1.0.0-beta3<\/version>\r\n<\/dependency>\r\n```\r\n\r\n* CUDA 9.1\r\n\r\n```maven\r\n<dependency>\r\n    <groupId>org.nd4j<\/groupId>\r\n    <artifactId>nd4j-cuda-9.1-platform<\/artifactId>\r\n    <version>1.0.0-beta3<\/version>\r\n<\/dependency>\r\n```\r\n\r\n* CUDA 9.2\r\n\r\n```maven\r\n<dependency>\r\n    <groupId>org.nd4j<\/groupId>\r\n    <artifactId>nd4j-cuda-9.2-platform<\/artifactId>\r\n    <version>1.0.0-beta3<\/version>\r\n<\/dependency>\r\n```\r\n\r\n* CUDA 10.0\r\n\r\n```maven\r\n<dependency>\r\n    <groupId>org.nd4j<\/groupId>\r\n    <artifactId>nd4j-cuda-10.0-platform<\/artifactId>\r\n    <version>1.0.0-beta3<\/version>\r\n<\/dependency>\r\n```\r\n\r\n* CUDA 10.1\r\n\r\n```maven\r\n<dependency>\r\n    <groupId>org.nd4j<\/groupId>\r\n    <artifactId>nd4j-cuda-10.1-platform<\/artifactId>\r\n    <version>1.0.0-beta3<\/version>\r\n<\/dependency>\r\n```\r\n\r\n#### \u4f7f\u7528\u73af\u5883\u4e0a\u4e0b\u6587\r\n\r\n```java\r\n\/\/ \u83b7\u53d6\u9ed8\u8ba4\u73af\u5883\u4e0a\u4e0b\u6587\r\nEnvironmentContext context = EnvironmentContext.getContext();\r\n\/\/ \u5728\u73af\u5883\u4e0a\u4e0b\u6587\u4e2d\u6267\u884c\u4efb\u52a1\r\nFuture<?> task = context.doTask(() - > {\r\n    int dimension = 10;\r\n    MathMatrix leftMatrix = getRandomMatrix(dimension);\r\n    MathMatrix rightMatrix = getRandomMatrix(dimension);\r\n    MathMatrix dataMatrix = getZeroMatrix(dimension);\r\n    dataMatrix.dotProduct(leftMatrix, false, rightMatrix, true, MathCalculator.PARALLEL);\r\n});\r\n```\r\n\r\n#### \u6570\u636e\u8868\u793a\r\n\r\n* \u672a\u5904\u7406\u7684\u5f62\u5f0f(\u8f6c\u6362\u524d)\r\n\r\n| \u7528\u6237(User) | \u65e7\u624b\u673a\u7c7b\u578b(Item) | \u65b0\u624b\u673a\u7c7b\u578b(Item) | \u8bc4\u5206(Score) |\r\n| :----: | :----: | :----: | :----: |\r\n| Google Fan | Android | Android | 3 |\r\n| Google Fan | Android | IOS | 1 |\r\n| Google Fan | IOS | Android | 5 |\r\n| Apple Fan | IOS | IOS | 3 |\r\n| Apple Fan | Android | IOS | 5 |\r\n| Apple Fan | IOS | Android | 1 |\r\n\r\n* \u5df2\u5904\u7406\u7684\u5f62\u5f0f(\u8f6c\u6362\u540e)\r\n\r\n| \u5b9a\u6027(User) | \u5b9a\u6027(Item) | \u5b9a\u6027(Item) | \u5b9a\u91cf(Score) |\r\n| :----: | :----: | :----: | :----: |\r\n| 0 | 0 | 0 | 3 |\r\n| 0 | 0 | 1 | 1 |\r\n| 0 | 1 | 0 | 5 |\r\n| 1 | 1 | 1 | 3 |\r\n| 1 | 0 | 1 | 5 |\r\n| 1 | 1 | 0 | 1 |\r\n\r\n#### \u6570\u636e\u8f6c\u6362\r\n\r\n**\u6570\u636e\u8f6c\u6362\u5668**(DataConverter)\u8d1f\u8d23\u5404\u79cd\u5404\u6837\u7684\u683c\u5f0f\u8f6c\u6362\u4e3aJStarCraft AI\u6846\u67b6\u80fd\u591f\u5904\u7406\u7684**\u6570\u636e\u6a21\u5757**(DataModule).\r\n\r\nJStarCraft AI\u6846\u67b6\u5404\u4e2a\u8f6c\u6362\u5668\u4e0e\u5176\u5b83\u7cfb\u7edf\u4e4b\u95f4\u7684\u5173\u7cfb: \r\n\r\n![converter](https:\/\/github.com\/HongZhaoHua\/jstarcraft-tutorial\/blob\/master\/ai\/%E8%BD%AC%E6%8D%A2%E5%99%A8%E7%B1%BB%E5%9B%BE.png \"\u8f6c\u6362\u5668\")\r\n\r\n* \u5b9a\u4e49\u6570\u636e\u5c5e\u6027\r\n\r\n```java\r\n\/\/ \u5b9a\u6027\u5c5e\u6027\r\nMap<String, Class<?>> qualityDifinitions = new HashMap<>();\r\nqualityDifinitions.put(\"user\", String.class);\r\nqualityDifinitions.put(\"item\", String.class);\r\n\r\n\/\/ \u5b9a\u91cf\u5c5e\u6027\r\nMap<String, Class<?>> quantityDifinitions = new HashMap<>();\r\nquantityDifinitions.put(\"score\", float.class);\r\nDataSpace space = new DataSpace(qualityDifinitions, quantityDifinitions);\r\n```\r\n\r\n* \u5b9a\u4e49\u6570\u636e\u6a21\u5757\r\n\r\n```java\r\nTreeMap<Integer, String> configuration = new TreeMap<>();\r\nconfiguration.put(1, \"user\");\r\nconfiguration.put(3, \"item\");\r\nconfiguration.put(4, \"score\");\r\nDataModule module = space.makeDenseModule(\"module\", configuration, 1000);\r\n```\r\n\r\n**JStarCraft AI\u6846\u67b6\u517c\u5bb9\u7684\u683c\u5f0f**\r\n\r\n* ARFF\r\n\r\n```java\r\n\/\/ ARFF\u8f6c\u6362\u5668\r\nArffConverter converter = new ArffConverter(space.getQualityAttributes(), space.getQuantityAttributes());\r\n\r\n\/\/ \u83b7\u53d6\u6d41\r\nFile file = new File(this.getClass().getResource(\"module.arff\").toURI());\r\nInputStream stream = new FileInputStream(file);\r\n\r\n\/\/ \u8f6c\u6362\u6570\u636e\r\nint count = converter.convert(module, stream, null, null, null);\r\n```\r\n\r\n* CSV\r\n\r\n```java\r\n\/\/ CSV\u8f6c\u6362\u5668\r\nCsvConverter converter = new CsvConverter(',', space.getQualityAttributes(), space.getQuantityAttributes());\r\n\r\n\/\/ \u83b7\u53d6\u6d41\r\nFile file = new File(this.getClass().getResource(\"module.csv\").toURI());\r\nInputStream stream = new FileInputStream(file);\r\n\r\n\/\/ \u8f6c\u6362\u6570\u636e\r\nint count = converter.convert(module, stream, null, null, null);\r\n```\r\n\r\n* JSON\r\n\r\n```java\r\n\/\/ JSON\u8f6c\u6362\u5668\r\nJsonConverter converter = new JsonConverter(space.getQualityAttributes(), space.getQuantityAttributes());\r\n\r\n\/\/ \u83b7\u53d6\u6d41\r\nFile file = new File(this.getClass().getResource(\"module.json\").toURI());\r\nInputStream stream = new FileInputStream(file);\r\n\r\n\/\/ \u8f6c\u6362\u6570\u636e\r\nint count = converter.convert(module, stream, null, null, null);\r\n```\r\n\r\n* HQL\r\n\r\n```java\r\n\/\/ HQL\u8f6c\u6362\u5668\r\nQueryConverter converter = new QueryConverter(space.getQualityAttributes(), space.getQuantityAttributes());\r\n\r\n\/\/ \u83b7\u53d6\u6e38\u6807\r\nString selectDataHql = \"select data.user, data.leftItem, data.rightItem, data.score from MockData data\";\r\nSession session = sessionFactory.openSession();\r\nQuery query = session.createQuery(selectDataHql);\r\nScrollableResults iterator = query.scroll();\r\n\r\n\/\/ \u8f6c\u6362\u6570\u636e\r\nint count = converter.convert(module, iterator, null, null, null);\r\nsession.close();\r\n```\r\n\r\n* SQL\r\n\r\n```java\r\n\/\/ SQL\u8f6c\u6362\u5668\r\nQueryConverter converter = new QueryConverter(space.getQualityAttributes(), space.getQuantityAttributes());\r\n\r\n\/\/ \u83b7\u53d6\u6e38\u6807\r\nString selectDataSql = \"select user, leftItem, rightItem, score from MockData\";\r\nSession session = sessionFactory.openSession();\r\nQuery query = session.createQuery(selectDataSql);\r\nScrollableResults iterator = query.scroll();\r\n\r\n\/\/ \u8f6c\u6362\u6570\u636e\r\nint count = converter.convert(module, iterator, null, null, null);\r\nsession.close();\r\n```\r\n\r\n#### \u6570\u636e\u5904\u7406\r\n\r\n* \u9009\u62e9\r\n\r\n```java\r\n\r\n```\r\n\r\n* \u6392\u5e8f\r\n\r\n```java\r\n\r\n```\r\n\r\n* \u5207\u5272\r\n\r\n```java\r\n\r\n```\r\n\r\n****\r\n\r\n## \u8bc4\u4f30\u6307\u6807\r\n\r\n#### \u6392\u5e8f\u6307\u6807\r\n- AUC\r\n- Diversity\r\n- MAP\r\n- MRR\r\n- NDCG\r\n- Novelty\r\n- Precision\r\n- Recall\r\n\r\n#### \u8bc4\u5206\u6307\u6807\r\n- MAE\r\n- MPE\r\n- MSE\/RMSE\r\n","31":"# Pyramid \n\n## A Java Machine Learning Library\n\nPyramid is a Java machine learning library which implements many state-of-the-art machine learning algorithms, including\n\n* Binary and Multi-class classification algorithms:\n    * Logistic Regression with L1 regularization (Lasso), L2 regularization (Ridge) and L1+L2 regularization (Elastic-net)\n    * Variational Bayesian Logistic Regression\n    * [Gradient Boosted Trees](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/GB-Classifier)\n    * Naive Bayes\n    * Error-Correcting Output Codes (ECOC)\n    * Support Vector Machines (SVM)\n* Multi-label classification algorithms:\n    * Binary Relevance\n    * Power Set\n    * Probabilistic Classifier Chain (PCC)\n    * [Conditional Random Field (CRF)](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/CRF)\n    * [Multi-label Gradient Boosted Trees](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/CBM)\n    * [Conditional Bernoulli Mixture (CBM)](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/CBM)\n    * [BR-rerank](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/BR-rerank)\n* Regression algorithms:\n    * Linear Regression with L1 regularization (Lasso), L2 regularization (Ridge) and L1+L2 regularization (Elastic-net)\n    * Variational Bayesian Linear Regression\n    * Regression Tree\n    * [Gradient Boosted Trees](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/GB-Regressor)\n* Learning to rank algorithms:\n    * LambdaMART\n* Clustering: \n    * K Means\n    * Gaussian Mixture\n    * [Bernoulli Mixture](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/Bernoulli-Mixtures)\n\n_At the moment, not all algorithms are released. We are actively working on tidying up the source files and adding documentations. We will release a few algorithms at a time when they are ready and hope to have all algorthms released soon!_\n## **Requirements**\nIf you just want to use pyramid as a command line tool (which is very simple), all you need is [Java 8](http:\/\/www.oracle.com\/technetwork\/java\/javase\/downloads\/jdk8-downloads-2133151.html).\n\nIf you are also a Java developer and wish to call Pyramid Java APIs, you will also need [Maven](https:\/\/maven.apache.org\/).\n\n## **Setup**\nPyramid doesn't require any installation effort. All you need is downloading the latest [pre-compiled package](https:\/\/github.com\/cheng-li\/pyramid\/releases) (with a name like pyramid-x.x.x.zip) and decompressing it. Now you can move into the created folder and type \n\n`.\/pyramid config\/welcome.properties`\n\nYou will see a welcome message and that means everything is working perfectly.\n\nWindows users please see the [notes](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/Notes-for-Windows-Users).\n## **Command Line Usage**\nAll algorithms\/functions implemented in Pyramid can be run though a simple command, with the following syntax:\n\n`.\/pyramid <properties_file>`\n\nExample: \n\n`.\/pyramid config\/welcome.properties`\n\nor\n\n`.\/pyramid config\/cbm.properties`\n\n`pyramid` is a launcher script and `<properties_file>` is a file specifying the name of the algorithm and all necessary parameters, such as the input data, output folder, and learning algorithm hyper parameters. The `<properties_file>` can be specified by either an absolute or a relative path.\n\nTo run different algorithms, you just need to invoke the program with different properties files. The list of available algorithms and their corresponding properties file templates can be found in the [Wiki](https:\/\/github.com\/cheng-li\/pyramid\/wiki#a-java-machine-learning-library).\n \n\n## **Building from Source**\n_If you are a Java developer who prefer working with the source code or want to contribute to the Pyramid package:_\n\nPyramid uses [Maven](https:\/\/maven.apache.org\/) for its build system.\n\nTo compile and package the project from the source code, simply run the `mvn clean package -DskipTests` command in the cloned directory. The compressed package will be created under the core\/target\/releases directory.\n\n## Feedback\nWe welcome your feedback on the package. To ask questions, request new features or report bugs, please contact Cheng Li  via chengli.email@gmail.com.\n\nAnswers to some commonly asked questions can be found in [FAQ](https:\/\/github.com\/cheng-li\/pyramid\/wiki\/FAQ).\n","32":"# TextAnalyzer\r\n\r\nA text analyzer which is based on machine learning, statistics and dictionaries that can analyze text.\r\n\r\nSo far, it supports hot word extracting, text classification, part of speech tagging, named entity recognition, chinese word segment, extracting address, synonym, text clustering, word2vec model, edit distance, chinese word segment, sentence similarity,word sentiment tendency, name recognition, idiom recognition, placename recognition, organization recognition, traditional chinese recognition, pinyin transform.\r\n\r\n# Features\r\n\r\n***extracting hot words from text.***\r\n1. to gather statistics via frequence.\r\n2. to gather statistics via by tf-idf algorithm\r\n3. to gather statistics via a score factor additionally.\r\n\r\n***extracting address from text.***\r\n\r\n***synonym can be recognized***\r\n\r\n***SVM Classificator***\r\n\r\nThis analyzer supports to classify text by svm. it involves vectoring the text. We can train the samples and then make a classification by the model.\r\n\r\nFor convenience,the model,tfidf and vector will be stored.\r\n\r\n***kmeans clustering && xmeans clustering***\r\n\r\nThis analyzer supports to clustering text by kmeans and xmeans.\r\n\r\n***vsm clustering***\r\n\r\nThis analyzer supports to clustering text by vsm.\r\n\r\n***part of speech tagging***\r\n\r\nIt's implemented by HMM model and decoder by viterbi algorithm.\r\n\r\n***google word2vec model***\r\n\r\nThis analyzer supports to use word2vec model.\r\n\r\n***chinese word segment***\r\n\r\nThis analyzer supports to do chinese word segment.\r\n\r\n***edit distance***\r\n\r\nThis analyzer supports calculating edit distance on char level or word level.\r\n\r\n***sentence similarity***\r\n\r\nThis analyzer supports calculating similarity between two sentences.\r\n\r\n\r\n# How To Use\r\n\r\n***just simple like this***\r\n\r\n## Extracting Hot Words\r\n\r\n1. indexing a document and get a docId.\r\n\r\n```\r\nlong docId = TextIndexer.index(text);\r\n```\r\n\r\n2. extracting by docId.\r\n\r\n```\r\n HotWordExtractor extractor = new HotWordExtractor();\r\n List<Result> list = extractor.extract(0, 20, false);\r\n if (list != null) for (Result s : list)\r\n    System.out.println(s.getTerm() + \" : \" + s.getFrequency() + \" : \" + s.getScore());\r\n```\r\n\r\na result contains term,frequency and score.\r\n\r\n```\r\n\u5931\u4e1a\u8bc1 : 1 : 0.31436604\r\n\u6237\u53e3 : 1 : 0.30099702\r\n\u5355\u4f4d : 1 : 0.29152703\r\n\u63d0\u53d6 : 1 : 0.27927202\r\n\u9886\u53d6 : 1 : 0.27581802\r\n\u804c\u5de5 : 1 : 0.27381304\r\n\u52b3\u52a8 : 1 : 0.27370203\r\n\u5173\u7cfb : 1 : 0.27080503\r\n\u672c\u5e02 : 1 : 0.27080503\r\n\u7ec8\u6b62 : 1 : 0.27080503\r\n```\r\n\r\n## Extracting Address\r\n\r\n```\r\nString str =\"xxxx\";\r\nAddressExtractor extractor = new AddressExtractor();\r\nList<String> list = extractor.extract(str);\r\n```\r\n\r\n## SVM Classificator\r\n\r\n1. training the samples.\r\n\r\n```\r\nSVMTrainer trainer = new SVMTrainer();\r\ntrainer.train();\r\n```\r\n\r\n2. predicting text classification.\r\n\r\n```\r\ndouble[] data = trainer.getWordVector(text);\r\ntrainer.predict(data);\r\n```\r\n\r\n## Kmeans Clustering && Xmeans Clustering\r\n\r\n```\r\nList<String> list = DataReader.readContent(KMeansCluster.DATA_FILE);\r\nint[] labels = new KMeansCluster().learn(list);\r\n```\r\n\r\n## VSM Clustering\r\n\r\n```\r\nList<String> list = DataReader.readContent(VSMCluster.DATA_FILE);\r\nList<String> labels = new VSMCluster().learn(list);\r\n```\r\n\r\n## Part Of Speech Tagging\r\n```\r\nHMMModel model = new HMMModel();\r\nmodel.train();\r\nViterbiDecoder decoder = new ViterbiDecoder(model);\r\ndecoder.decode(words);\r\n```\r\n\r\n## Define Your Own Named Entity\r\n\r\nMITIE is an information extractor library comes up with MIT NLP term , which github is https:\/\/github.com\/mit-nlp\/MITIE .\r\n\r\n***train total\\_word\\_feature\\_extractor***\r\n\r\nPrepare your word set, you can put them into a txt file in the directory of 'data'.\r\n\r\nAnd then do things below:\r\n\r\n```\r\ngit clone https:\/\/github.com\/mit-nlp\/MITIE.git\r\ncd tools\r\ncd wordrep\r\nmkdir build\r\ncd build\r\ncmake ..\r\ncmake --build . --config Release\r\nwordrep -e data\r\n```\r\n\r\nFinally you get the total\\_word\\_feature\\_extractor model.\r\n\r\n\r\n***train ner\\_model***\r\n\r\nWe can use Java\\C++\\Python to train the ner model, anyway we must use the total\\_word\\_feature\\_extractor model to train it.\r\n\r\nif Java,\r\n\r\n```\r\nNerTrainer nerTrainer = new NerTrainer(\"model\/mitie_model\/total_word_feature_extractor.dat\");\r\n```\r\n\r\n\r\nif C++,\r\n\r\n```\r\nner_trainer trainer(\"model\/mitie_model\/total_word_feature_extractor.dat\");\r\n```\r\n\r\nif Python,\r\n\r\n```\r\ntrainer = ner_trainer(\"model\/mitie_model\/total_word_feature_extractor.dat\")\r\n```\r\n\r\n\r\n***build shared library***\r\n\r\nDo commands below:\r\n\r\n```\r\ncd mitielib\r\nD:\\MITIE\\mitielib>mkdir build\r\nD:\\MITIE\\mitielib>cd build\r\nD:\\MITIE\\mitielib\\build>cmake ..\r\nD:\\MITIE\\mitielib\\build>cmake --build . --config Release --target install\r\n```\r\n\r\nThen we get these below:\r\n\r\n```\r\n-- Install configuration: \"Release\"\r\n-- Installing: D:\/MITIE\/mitielib\/java\/..\/javamitie.dll\r\n-- Installing: D:\/MITIE\/mitielib\/java\/..\/javamitie.jar\r\n-- Up-to-date: D:\/MITIE\/mitielib\/java\/..\/msvcp140.dll\r\n-- Up-to-date: D:\/MITIE\/mitielib\/java\/..\/vcruntime140.dll\r\n-- Up-to-date: D:\/MITIE\/mitielib\/java\/..\/concrt140.dll\r\n```\r\n\r\n\r\n## Word2vec\r\nwe must set the word2vec's path system parameter when startup,just like this `-Dword2vec.path=D:\\Google_word2vec_zhwiki1710_300d.bin`.\r\n\r\nusing google model.\r\n\r\n```\r\nWord2Vec vec = Word2Vec.getInstance(true);\r\nSystem.out.println(\"\u72d7|\u732b: \" + vec.wordSimilarity(\"\u72d7\", \"\u732b\"));\r\n```\r\n\r\nusing java model\r\n\r\n```\r\nWord2Vec vec = Word2Vec.getInstance(false);\r\nSystem.out.println(\"\u72d7|\u732b: \" + vec.wordSimilarity(\"\u72d7\", \"\u732b\"));\r\n```\r\n\r\n\r\n## Segment&Search\r\n```\r\nDictSegment segment = new DictSegment();\r\nSystem.out.println(segment.seg(\"\u6211\u662f\u4e2d\u56fd\u4eba\"));\r\nSystem.out.println(segment.Search(\"\u6211\u5728\u5e7f\u5dde\u5e02\"));\r\n```\r\n\r\n## Edit Distance\r\nchar level,\r\n\r\n```\r\nCharEditDistance cdd = new CharEditDistance();\r\ncdd.getEditDistance(\"what\", \"where\");\r\ncdd.getEditDistance(\"\u6211\u4eec\u662f\u4e2d\u56fd\u4eba\", \"\u4ed6\u4eec\u662f\u65e5\u672c\u4eba\u5416\uff0c\u56db\u8d35\u5b50\");\r\ncdd.getEditDistance(\"\u662f\u6211\", \"\u6211\u662f\");\r\n```\r\n\r\nword level,\r\n\r\n```\r\nList list1 = new ArrayList<String>();\r\nlist1.add(new EditBlock(\"\u8ba1\u7b97\u673a\",\"\"));\r\nlist1.add(new EditBlock(\"\u591a\u5c11\",\"\"));\r\nlist1.add(new EditBlock(\"\u94b1\",\"\"));\r\nList list2 = new ArrayList<String>();\r\nlist2.add(new EditBlock(\"\u7535\u8111\",\"\"));\r\nlist2.add(new EditBlock(\"\u591a\u5c11\",\"\"));\r\nlist2.add(new EditBlock(\"\u94b1\",\"\"));\r\ned.getEditDistance(list1, list2);\r\n```\r\n\r\n## Sentence Similarity\r\n\r\n```\r\nString s1 = \"\u6211\u4eec\u662f\u4e2d\u56fd\u4eba\";\r\nString s2 = \"\u4ed6\u4eec\u662f\u65e5\u672c\u4eba\uff0c\u56db\u8d35\u5b50\";\r\nSentenceSimilarity ss = new SentenceSimilarity();\r\nSystem.out.println(ss.getSimilarity(s1, s2));\r\ns1 = \"\u6211\u4eec\u662f\u4e2d\u56fd\u4eba\";\r\ns2 = \"\u6211\u4eec\u662f\u4e2d\u56fd\u4eba\";\r\nSystem.out.println(ss.getSimilarity(s1, s2));\r\n```\r\n\r\n## Get Synonym via Cilin Dictionary\r\n\r\n```\r\nCilinDictionary dict = CilinDictionary.getInstance();\r\nSet<String> code = dict.getCilinCoding(\"\u4eba\u7c7b\");\r\nSystem.out.println(dict.getCilinWords(code.iterator().next()));\r\n[\u5168\u4eba\u7c7b, \u751f\u4eba, \u4eba\u7c7b]\r\n```\r\n\r\n## Words' Similarity by Cilin\r\n```\r\nString s1 = \"\u4e2d\u56fd\u4eba\";\r\nString s2 = \"\u708e\u9ec4\u5b50\u5b59\";\r\nCilinSimilarity cs = new CilinSimilarity();\r\nSystem.out.println(cs.getSimilarity(s1, s2));\r\ns1 = \"\u6c7d\u8f66\";\r\ns2 = \"\u6469\u6258\";\r\nSystem.out.println(cs.getSimilarity(s1, s2));\r\n```\r\n\r\n## Get Hownet Glossary\r\n```\r\nHownetGlossary glossary = HownetGlossary.getInstance();\r\nCollection<Term> coll = glossary.getTerms(\"\u4eba\u7c7b\");\r\nfor (Term t : coll)\r\n  System.out.println(t);\r\n```\r\n\r\n## Get Hownet Sememe\r\n```\r\nHownetSememe sememe = HownetSememe.getInstance();\r\nCollection<String> coll = sememe.getDefine(\"\u7528\u5177\");\r\nfor (String t : coll)\r\n  System.out.println(t);\r\n```\r\n\r\n## Hownet Words Similarity\r\n```\r\nHownetSimilarity hownetSimilarity = new HownetSimilarity();\r\nSystem.out.println(\"hownet similarity : \" + hownetSimilarity.getSimilarity(\"\u4e2d\u56fd\", \"\u7f8e\u56fd\"));\r\n```\r\n\r\n## Get Pinyin \r\n```\r\nSystem.out.println(PinyinUtil.getInstance().getPinyin(\"\u54c8\u54c8\"));\r\nSystem.out.println(PinyinUtil.getInstance().getPinyin(\"\u4e2d\"));\r\nSystem.out.println(PinyinUtil.getInstance().getPinyin(\"\u4e2d\u56fd\"));\r\n```\r\n\r\n## Pinyin Similarity\r\n```\r\nString s1 = \"\u4eca\u5929\";\r\nString s2 = \"\u660e\u5929\";\r\nPinyinSimilarity cs = new PinyinSimilarity();\r\nSystem.out.println(cs.getSimilarity(s1, s2));\r\n```\r\n\r\n## Information Extractor\r\n### usage\r\nWe have provided Python and Java APIs for extractor,choose one of them.\r\n\r\n### python\r\ndo a predict by this below,\r\n```\r\npython crf_ner.py predict \"\u6d4b\u8bd5\u6587\u672c\" \"..\/model\/crf.model\"\r\n```\r\n\r\n### java \r\n\r\n```\r\nList list = JCYExtractor.getIDs(text);\r\n\r\nlist = JCYExtractor.getNames(text);\r\n\r\nJCYExtractor.getAddrs(text);\r\n```\r\n\r\n### train a model \r\n1. To collect corpus.\r\n2. Tagging corpus,we support those labels below,\r\n\r\n```\r\n# IB : ID beginning\r\n# IE : ID ending\r\n# IM : ID middle\r\n# U : unlabeled\r\n# PB : person beginning\r\n# PE : person ending\r\n# PM : person middle\r\n# BB : birthday beginning\r\n# BM : birthday middle\r\n# BE : birthday ending\r\n# LB : location beginning\r\n# LM : location middle\r\n# LE : location endings\r\n```\r\n\r\nfor example,\r\n\r\n```\r\n\u88ab\tU\r\n\u4e0d\tU\r\n\u8d77\tU\r\n\u8bc9\tU\r\n\u4eba\tU\r\n\u4f0d\tPB\r\n\u67d0\tPM\r\n\u67d0\tPE\r\n\uff0c\tU\r\n```\r\n\r\n3. Put all samples to the directory of `data\/jcy_data\/train`.\r\n4. Call `train` function in the `crf_ner.py` script\uff0cthe model will produce in the directory of `model` which name is `crf.model`.\r\n\r\n\r\n## Word Tendency\r\n\r\n```\r\nWordSentimentTendency tendency = new WordSentimentTendency();\r\nSystem.out.println(tendency.getTendency(\"\u9ad8\u5174\"));\r\nSystem.out.println(tendency.getTendency(\"\u4f24\u5fc3\"));\r\n```\r\n\r\n## Chinese&English Name Recognition\r\n\r\n```\r\nNameDict.get().searchName(\"\u6c6a\u5efa\u662f\u534e\u5927\u57fa\u56e0\u8463\u4e8b\u957f\");\r\nNameDict.get().searchEnglishName(\"Tom and Jim are my friends\");\r\n```\r\n\r\n## Idiom Recognition\r\n\r\n```\r\nIdiomDict.get().searchIdiom(\"\u4ece\u524d\u6709\u4e2a\u4eba\u963f\u8c00\u5949\u627f\");\r\n```\r\n\r\n## Placename Recognition\r\n\r\n```\r\nPlacenameDict.get().searchPlacename(\"\u6211\u4f4f\u5728\u5929\u6cb3\u5317\u8def\uff0c\u4e0d\u5728\u5e7f\u5dde\u5927\u9053\u4e2d\uff0c\u5728\u5929\u6cb3\u533a\");\r\n```\r\n\r\n## Organization Recognition\r\n\r\n```\r\nOrganizationDict.get().searchOrganization(\"\u53bb\u963f\u91cc\u5df4\u5df4\u627e\u670b\u53cb\");\r\n```\r\n\r\n## Traditional Chinese Recognition\r\n\r\n```\r\nList<Integer> list = TraditionalDict.get().prefixSearch(\"1\u96bb\u5927\u72d7\");\r\nfor(int i:list)\r\n\tSystem.out.println(TraditionalDict.get().getStringByIndex(i));\r\n```\r\n\r\n\r\n## Pinyin Transform \r\n\r\n```\r\nPinyinDict.get().getStringByIndex(PinyinDict.get().exactlySearch(\"\u4e00\u5fc3\u4e00\u610f\"));\r\n```","33":"# Fake Image Detection Using Machine Learning\n\nThe objective of this project is to identify fake images(Fake images are the images that are digitally altered images). The problem with existing fake image detection system is that they can be used detect only specific tampering methods like splicing, coloring etc. We approached the problem using machine learning and neural network to detect almost all kinds of tampering on images.\n\nUsing latest image editing softwares, it is possible to make alterations on image which are too difficult for human eye to detect. Even with a complex neural network, it is not possible to determine whether an image is fake or not without identifying a common factor across almost all fake images. So, instead of giving direct raw pixels to the neural network, we gave error level analysed image.\n\nThis project provides two level analysis for the image. At first level, it checks the image metadata. Image metadata is not that much reliable since it can be altered using simple programs. But most of the images we come across will have non-altered metadata which helps to identify the alterations. For example, if an image is edited with Adobe Photoshop, the metadata will contain even the version of the Adobe Photoshop used.\n\nIn the second level, the image is converted into error level analysed format and will be resized to 100px x 100px image. Then these 10,000 pixels with RGB values (30,000 inputs) is given in to the input layer of Multilayer perceptron network. Output layer contain two neurons. One for fake image and one for real image. Depending upon the value of these neuron outputs along with metadata analyser output, we determine whether the image is fake or not and how much chance is there for the given image to be tampered.\n\n### Feature Engineering\n  1. Dr. Neal Krawetz proposed a method called [Error Level Analysis(ELA)](http:\/\/www.hackerfactor.com\/papers\/bh-usa-07-krawetz-wp.pdf) that exploits the lossy compression of JPEG images. When an image is altered, the compression ratio of the specific portion changes with respect to other parts. A well trained neural network can detect the anomaly by and determine whether the image is fake or not.\n  2. The second parameter considered is metadata of the image. A parallel module is added to the program which checks the metadata to determine the signature of various image editing programs. Since it is costly to execute a neural network, the metadata inspection will considerably increase the performance by detecting tampering at an early stage.\n\n### Neural network structure\n| Layer | Neurons |\n| ------------- | ------------- |\n| Input Layer  | 30,000 |\n| Hidden Layer 1  | 5000 - Sigmoid |\n| Hidden Layer 2  | 1000 - Sigmoid |\n| Hidden Layer 3  | 100 - Sigmoid |\n| Output Layer  | 2 |\n\n\n### Watch on YouTube\n[![Watch a video](https:\/\/img.youtube.com\/vi\/MVIN9HrS8UY\/0.jpg)](https:\/\/www.youtube.com\/watch?v=MVIN9HrS8UY)\n\n### Tools Used\n\n#### [Neuroph Studio](http:\/\/neuroph.sourceforge.net\/)\n Neuroph studio is an open source Java neural network framework that helps to easily build and use neural networks. It also provides a direct interface for loading images\n#### [Metadata-extractor](https:\/\/github.com\/drewnoakes\/metadata-extractor)\n Metadata-extractor is an open source java library used to extract metadata from images.\n#### [JavaFX](http:\/\/docs.oracle.com\/javase\/8\/javase-clienttechnologies.htm)\n JavaFX is used to implement modern user interface for the application.\n\n### Flow Chart : Detection\n<img src=http:\/\/i.imgur.com\/TKX7uV6.png>\n\n### Flow Chart : Training\n<img src=http:\/\/i.imgur.com\/wUoo1kb.png>\n\n### Project Status \n- [x] Implement Metadata Procesing Module\n- [x] Design a User Interface\n- [x] Implement Image Feature Extractor\n- [x] Design Neural Network using Neuroph Studio\n- [x] Implement Neural Network interface with JavaFX\n- [x] Connect Neural Network \n- [x] Integrate Neural Network to Master \n- [x] Train Network\n- [x] Test Network\n- [x] Read feedback from user and learn instantly\n- [x] Add network training interface\n- [x] Add module to apply error level analysis on a set of images\n- [x] Improve look and feel\n- [x] Train with more data\n- [x] Add batch testing module\n- [x] Detach User Inteface from core\n- [x] Implement Command Line Interface\n- [ ] Reach success rate of 90%\n\nJournal link : https:\/\/www.academia.edu\/37977449\/Fake_Image_Detection_Using_Machine_Learning\n\n\n### Screenshots\n<img src=https:\/\/i.imgur.com\/vzfdecs.png>\n<img src=https:\/\/i.imgur.com\/T3TVsuj.png>\n<img src=https:\/\/i.imgur.com\/0mzmfFp.png>\n<img src=https:\/\/i.imgur.com\/z8DzhGD.png>\n<img src=https:\/\/i.imgur.com\/mvc9tp0.png>\n<img src=https:\/\/i.imgur.com\/yHQ5JGx.png>\n","34":"# RumbleDB\n\nWith RumbleDB, you can query with ease a lot of different nested, heterogeneous data formats like JSON, CSV, Parquet, Avro, LibSVM, text, etc.\n\nRumbleDB exposes a query language rather than a DataFrame API, for more flexibility, more productivity but also because a lot of data simply will not fit in DataFrames.\n\nYou can query it in place from any local file systems or data lakes (Azure blob storage, Amazon S3, HDFS, etc).\n\nYou can prepare, clean up, validate your data and put it right into your machine learning pipelines with RumbleDB ML.\n\nGetting started: you will find a Jupyter notebook that introduces the JSONiq language on top of RumbleDB [here](https:\/\/colab.research.google.com\/github\/RumbleDB\/rumble\/blob\/master\/RumbleSandbox.ipynb). You can also run it locally if you prefer.\n\nThe documentation also contains an introduction specific to RumbleDB and how you can read input datasets, but we have not converted it to Jupyter notebooks yet (this will follow).\n\n[The documentation of the latest official release is available here.](http:\/\/rumble.readthedocs.io\/en\/latest\/)\n\n[The documentation of the current master (for the adventurous and curious) is available here.](http:\/\/sparksoniq.readthedocs.io\/en\/latest\/)\n\n","35":"# pignlproc\n\nApache Pig utilities to build training corpora for machine learning \/\nNLP out of public Wikipedia and DBpedia dumps.\n\n## Project status\n\nThis project is alpha \/ experimental code. Features are implemented when needed.\n\nSome preliminary results are available in this blog post:\n\n  * [Mining Wikipedia with Hadoop and Pig for Natural Language Processing](http:\/\/www.nuxeo.com\/blog\/mining-wikipedia-with-hadoop-and-pig-for-natural-language-processing\/)\n\n## Building from source\n\nInstall maven (tested with 2.2.1) and java jdk 6, then:\n\n    $ mvn assembly:assembly\n\nThis should download the dependencies, build a jar in the target\/\nsubfolder and run the tests.\n\n## Usage\n\nThe following introduces some sample scripts to demo the User Defined\nFunctions provided by pignlproc for some practical Wikipedia mining tasks.\n\nThose examples demo how to use pig on your local machine on sample\nfiles. In production (with complete dumps) you might want to startup a\nreal Hadoop cluster, upload the dumps into HDFS, adjust the above paths\nto match your setup and remove the '-x local' command line parameter to\ntell pig to use your Hadoop cluster.\n\nThe [pignlproc wiki](https:\/\/github.com\/ogrisel\/pignlproc\/wiki) provides\ncomprehensive documentation on where to download the dumps from and how\nto setup a Hadoop cluster on EC2 using [Apache Whirr](\nhttp:\/\/incubator.apache.org\/whirr).\n\n### Extracting links from a raw Wikipedia XML dump\n\nYou can take example on the extract-links.pig example script:\n\n    $ pig -x local \\\n      -p PIGNLPROC_JAR=target\/pignlproc-0.1.0-SNAPSHOT.jar \\\n      -p LANG=fr \\\n      -p INPUT=src\/test\/resources\/frwiki-20101103-pages-articles-sample.xml \\\n      -p OUTPUT=\/tmp\/output \\\n      examples\/extract_links.pig\n\n### Building a NER training \/ evaluation corpus from Wikipedia and DBpedia\n\nThe goal of those samples scripts is to extract a pre-formatted corpus\nsuitable for the training of sequence labeling algorithms such as MaxEnt\nor CRF models with [OpenNLP](http:\/\/incubator.apache.org\/opennlp),\n[Mallet](http:\/\/mallet.cs.umass.edu\/) or\n[crfsuite](http:\/\/www.chokkan.org\/software\/crfsuite\/).\n\nTo achieve this you can run time following scripts (splitted into somewhat\nindependant parts that store intermediate results to avoid recomputing\neverything from scratch when you can the source files or some parameters.\n\nThe first script parses a wikipedia dump and extract occurrences of\nsentences with outgoing links along with some ordering and positioning\ninformation:\n\n    $ pig -x local \\\n      -p PIGNLPROC_JAR=target\/pignlproc-0.1.0-SNAPSHOT.jar \\\n      -p LANG=en \\\n      -p INPUT=src\/test\/resources\/enwiki-20090902-pages-articles-sample.xml \\\n      -p OUTPUT=workspace \\\n      examples\/ner-corpus\/01_extract_sentences_with_links.pig\n\nThe parser has been measured to run at a processing of 1MB\/s on in local\nmode on a MacBook Pro of 2009.\n\nThe second script parses dbpedia dumps assumed to be in the folder\n\/home\/ogrisel\/data\/dbpedia:\n\n    $ pig -x local \\\n      -p PIGNLPROC_JAR=target\/pignlproc-0.1.0-SNAPSHOT.jar \\\n      -p LANG=en \\\n      -p INPUT=\/home\/ogrisel\/data\/dbpedia \\\n      -p OUTPUT=workspace \\\n      examples\/ner-corpus\/02_dbpedia_article_types.pig\n\nThis step should complete in a couple of minutes in local mode.\n\nThis script could be adapted \/ replaced to use other typed entities\nknowledge bases linked to Wikipedia with downloadable dumps in NT\nor TSV formats; for instance: [freebase](http:\/\/freebase.com) or\n[Uberblic](http:\/\/uberblic.org).\n\nThe third script merges the partial results of the first two scripts and\norder back the results by grouping the sentences of the same article\ntogether to be able to build annotated sentences suitable for OpenNLP\nfor instance:\n\n    $ pig -x local \\\n      -p PIGNLPROC_JAR=target\/pignlproc-0.1.0-SNAPSHOT.jar \\\n      -p INPUT=workspace \\\n      -p OUTPUT=workspace \\\n      -p LANG=en \\\n      -p TYPE_URI=http:\/\/dbpedia.org\/ontology\/Person \\\n      -p TYPE_NAME=person \\\n      examples\/ner-corpus\/03bis_filter_join_by_type_and_convert.pig\n\n    $ head -3 workspace\/opennlp_person\/part-r-00000\n    The Table Talk of <START:person> Martin Luther <END> contains the story of a 12-year-old boy who may have been severely autistic .\n    The New Latin word autismus ( English translation autism ) was coined by the Swiss psychiatrist <START:person> Eugen Bleuler <END> in 1910 as he was defining symptoms of schizophrenia .\n    Noted autistic <START:person> Temple Grandin <END> described her inability to understand the social communication of neurotypicals , or people with normal neural development , as leaving her feeling \"like an anthropologist on Mars \" .\n\n\n### Building a document classification corpus\n\nTODO: Explain howto extract bag of words or ngrams and document frequency\nfeatures suitable for document classification using a SGD model from\n[Mahout](http:\/\/mahout.apache.org) for instance.\n\n\n## License\n\nCopyright 2010 Nuxeo and contributors:\n\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n  http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n\n","36":"[![License Info](https:\/\/img.shields.io\/badge\/license-GNU_GPLv3-blue.svg?style=flat-square)](https:\/\/github.com\/projectmatris\/antimalwareapp) [![F-Droid](https:\/\/img.shields.io\/f-droid\/v\/tech.projectmatris.antimalwareapp.svg)](https:\/\/f-droid.org\/packages\/tech.projectmatris.antimalwareapp) [![build](https:\/\/github.com\/projectmatris\/antimalwareapp\/actions\/workflows\/android.yml\/badge.svg?branch=development)](https:\/\/github.com\/projectmatris\/antimalwareapp\/actions\/workflows\/android.yml) [![Chat - Matrix](https:\/\/img.shields.io\/badge\/chat-Matrix-blue.svg)](https:\/\/matrix.to\/#\/#LibreAV:matrix.org) [![Crowdin](https:\/\/badges.crowdin.net\/libreav\/localized.svg)](https:\/\/crowdin.com\/project\/libreav)\n\n![LibreAV Banner v1.0](https:\/\/res.cloudinary.com\/dixyd9fa6\/image\/upload\/v1594366724\/githubbanner_oyc3ly.png)\n\nA free and open source anti-malware for android using machine learning.\n\n[<img src=\"https:\/\/fdroid.gitlab.io\/artwork\/badge\/get-it-on.png\" height=\"75\" \/>](https:\/\/f-droid.org\/packages\/tech.projectmatris.antimalwareapp)\n\n> #### \u26a0\ufe0f\u26a0\ufe0f PLEASE BE AWARE THAT LIBREAV IS STILL IN ITS EARLY STAGES OF DEVELOPMENT AND MAY CONTAIN A LOT OF FALSE POSITIVES IN PREDICTION. THIS PROJECT IS DEVELOPED FOR EDUCATIONAL PURPOSES ONLY AND WE MAKE NO GUARANTEE THAT THE OUTPUT PRODUCED BY LIBREAV IS ABSOLUTELY CORRECT. SO PLEASE DO NOT JUDGE OTHER APPLICATIONS USING THE OUTPUT OF LIBREAV ALONE.\u26a0\ufe0f\u26a0\ufe0f\n## About\nLibreAV is an attempt to detect malwares on android devices by utilizing machine learning approach.\n\n## Features\n\n- Real time scanning\n- On device inference\n- Lightweight\n- 100% free and no ads\n\n## How it works?\n\nLibreAV uses permissions and intent-filters to detect malicious apps. While scanning, it loads the machine learning model and extracts permissions and intents from the installed applications on the user's device. These extracted features are then fed to the machine learning model in the form of a vector. The machine learning model returns a prediction score between 0 and 1 that denote the degree of maliciousness of the scanned application. We use this score to classify the scanned app into one of the following categories:\n1. Goodware: The prediction score is less than 0.5\n2. Risky: Prediction score between 0.5 and 0.75\n3. Malware: Prediction score is greater than 0.75\n4. Unknown: If LibreAV is unable to extract permissions and intents from an app, then that app is labelled as 'Unknown'\n\nYou can check the code for building machine learning model [here](https:\/\/github.com\/projectmatris\/antimalwareapp_ml)\n\n## Contributing to LibreAV\n\nCheck out our contribution guidelines [here](https:\/\/github.com\/projectmatris\/antimalwareapp\/blob\/development\/CONTRIBUTING.md)\n\n* **Translation**: If you want to help translate LibreAV into your language head over to the [Crowdin project](https:\/\/crowdin.com\/project\/libreav).\n* **Discussion**: Matrix channel [#LibreAV:matrix.org](https:\/\/matrix.to\/#\/#LibreAV:matrix.org). You can also use [Github Discussions](https:\/\/github.com\/projectmatris\/antimalwareapp\/discussions) to ask questions, share ideas and engage with other community members.\n* **Report False Positives**: If you  think LibreAV detected false positives, please report it [here](https:\/\/github.com\/projectmatris\/antimalwareapp\/issues\/4).\n\n## Open Source License\n\nUnless explicitly stated otherwise all files in this repository are licensed under the [GNU General Public License v3.0](https:\/\/www.gnu.org\/licenses\/gpl-3.0-standalone.html). All projects **must** properly attribute [The Original Source](https:\/\/github.com\/projectmatris\/antimalwareapp).\n\n    LibreAV - Anti-malware for Android using machine learning\n    Copyright (C) 2020 Project Matris\n\n    This program is free software: you can redistribute it and\/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https:\/\/www.gnu.org\/licenses\/>.\n\nAn unmodified copy of the above license text must be included in all forks.\n\n## External Credits\n\n1. A huge thanks to [goorax](https:\/\/www.kaggle.com\/goorax) for sharing the dataset.\n2. [VirusShare.com](https:\/\/virusshare.com\/) for sharing access to malware samples.\n3. [Mike Penz](https:\/\/github.com\/mikepenz) for [AboutLibraries](https:\/\/github.com\/mikepenz\/AboutLibraries) \n\n> Android is a trademark of Google LLC.\n\n","37":"[![License](http:\/\/img.shields.io\/:license-apache-blue.svg)](http:\/\/www.apache.org\/licenses\/LICENSE-2.0.html)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/nerd\/badge\/?version=latest)](https:\/\/readthedocs.org\/projects\/nerd\/?badge=latest)\n[![SWH](https:\/\/archive.softwareheritage.org\/badge\/origin\/https:\/\/github.com\/kermitt2\/entity-fishing\/)](https:\/\/archive.softwareheritage.org\/browse\/origin\/?origin_url=https:\/\/github.com\/kermitt2\/entity-fishing)\n\n# entity-fishing\n\n*entity-fishing* performs the following tasks:\n\n* entity recognition and disambiguation against Wikidata in a raw text or partially-annotated text segment,\n![entity-fishing](doc\/images\/screen11.png)\n\n* entity recognition and disambiguation against Wikidata at document level, in particular for a PDF with layout positioning and structure-aware annotations,\n![entity-fishing](doc\/images\/screen10.png)\n\n* search query disambiguation (the _short text_ mode) - below disambiguation of the search query \"concrete pump sensor\" in the service test console,\n![Search query disambiguation](doc\/images\/screen8.png)\n\n* weighted term vector disambiguation (a term being a phrase),\n![Search query disambiguation](doc\/images\/screen5.png)\n\n* interactive disambiguation in text editing mode (experimental).  \n![Editor with real time disambiguation](doc\/images\/screen6.png)\n\n# Documentation\n\n[Presentation of entity-fishing at WikiDataCon 2017](https:\/\/grobid.s3.amazonaws.com\/presentations\/29-10-2017.pdf) for some design, implementation descriptions, and some evaluations.\n\nThe documentation of *entity-fishing* is available [here](http:\/\/nerd.readthedocs.io).\n\n# Demo\n\nFor testing purposes, a public entity-fishing demo server is available at the following address: [https:\/\/cloud.science-miner.com\/nerd](https:\/\/cloud.science-miner.com\/nerd)\n\nThe query DSL and Web services are documented [here](https:\/\/nerd.readthedocs.io\/en\/latest\/restAPI.html).\n\n_Warning_: Some quota and query limitation apply to the demo server! Please be courteous and do not overload the demo server. \n\n# Benchmarks\n\n![entity-fishing](doc\/images\/scores.png)\n\nEvaluations above correspond to the \"overall unnormalized accuracy\" scenario in [BLINK](https:\/\/github.com\/facebookresearch\/BLINK#benchmarking-blink). entity-fishing performs at 0.765 F-score, as compared to 0.8027 for BLINK, a fine-tuned BERT architectures. *entity-fishing* surpasses BLINK for the dataset AQUAINT, 0.891 vs. 0.8588, and MSNBC, 0.867 vs. 0.8509, despite being considerably faster and lighter than BLINK (see below).\n\nSee the [evaluation documentation](https:\/\/nerd.readthedocs.io\/en\/latest\/evaluation.html) and [Presentation of entity-fishing at WikiDataCon 2017](https:\/\/grobid.s3.amazonaws.com\/presentations\/29-10-2017.pdf) for more details. \n\n# Some use cases\n\nSome example of *entity-fishing* usages:\n\n* Tanti Kristanti from [Inria Paris](https:\/\/www.inria.fr) used off the shelf version of *entity-fishing* in the [CLEF HIPE 2020 competition shared task](http:\/\/ceur-ws.org\/Vol-2696\/paper_266.pdf), ranking first at the Entity Linking task for English and second best for French, in F1-score.\n\n* [Kairntech](https:\/\/kairntech.com) has integrated *entity-fishing* on their commercial platform [Sherpa](https:\/\/aclanthology.org\/2020.iwltp-1.9.pdf) to support analysis and enrichment of textual content, since 2020. \n\n* [SEALK](https:\/\/sealk.co), which is commercializing a M&A industry recommendation system, scaled *entity-fishing* to more than 1 million fulltext news documents in 2020. \n\n* *entity-fishing* has been deployed in the DARIAH-EU and Huma-Num infrastructure in the context of the [OPERAS HIRMEOS EU project](https:\/\/www.hirmeos.eu) in 2018.\n\nIf you are using *entity-fishing* and found it useful, we are happy to mention you in this section ! \n\n# Current version\n\n*entity-fishing* is a **work-in-progress** side project! Latest release version is `0.0.5`. \n\nThis version supports English, French, German, Italian, Spanish, Arabic, Mandarin, Russian and Japanese, with an in-house Named Entity Recognizer for English and French. For this version, the available knowledge base includes around 96 million entities from Wikidata - but you can create your own fresh knowledge base with the [GRISP](https:\/\/github.com\/kermitt2\/grisp) utility. \n\n**Runtime**: on local machine (Intel Haswel i7-4790K CPU 4.00GHz - 8 cores - 16GB - SSD)\n\n* 800 pubmed abstracts (172 787 tokens) processed in 126s with 1 client (1371 tokens\/s) \n\n* 4800 pubmed abstracts (1 036 722 tokens) processed in 216s with 6 concurrent clients (4800 tokens\/s) \n\n* 136 PDF (3443 pages, 1 422 943 tokens) processed in 1284s with 1 client (2.6 pages\/s, 1108.2 tokens\/s)\n\n* 816 PDF (20658 pages, 8 537 658 tokens) processed in 2094s with 6 concurrent clients (9.86 pages\/s, 4077 tokens\/s)\n\n**Accuracy**: f-score for disambiguation only between 76.5 and 89.1 on standard datasets (ACE2004, AIDA-CONLL-testb, AQUAINT, MSNBC) - to be improved in the next versions.\n\nThe knowledge base contains more than 1.5 billion objects, not far from 15 millions word and entity embeddings, however *entity-fishing* will work with 3-4 GB RAM memory after a 15 second start-up for the server - but please use SSD! \n\n## How to cite\n\nIf you want to cite this work, please refer to the present GitHub project, together with the [Software Heritage](https:\/\/www.softwareheritage.org\/) project-level permanent identifier. For example, with BibTeX:\n\n```bibtex\n@misc{entity-fishing,\n    title = {entity-fishing},\n    howpublished = {\\url{https:\/\/github.com\/kermitt2\/entity-fishing}},\n    publisher = {GitHub},\n    year = {2016--2022},\n    archivePrefix = {swh},\n    eprint = {1:dir:cb0ba3379413db12b0018b7c3af8d0d2d864139c}\n}\n```\n\n## License and contact\n\nDistributed under [Apache 2.0 license](http:\/\/www.apache.org\/licenses\/LICENSE-2.0). The dependencies used in the project are either themselves also distributed under Apache 2.0 license or distributed under a compatible license. \n\nMain author and contact: Patrice Lopez (<patrice.lopez@science-miner.com>)\n\n*entity-fishing* has been created, developed and is maintained by [SCIENCE-MINER](http:\/\/science-miner.com\/entity-disambiguation\/) (since 2015, first Open Source public version in 2016), with contributions of [Inria](http:\/\/inria.fr) Paris (2017-2018). \n","38":"Instructions:\n=============\n\nInstruction of first installation of Fuzzy DL\n\n 1.   Download from http:\/\/gaia.isti.cnr.it\/~straccia\/software\/fuzzyDL\/fuzzyDL.html\n\n 2.   Unzip the folder\n\n 3.   the \"FuzzyDL\" folder is the main folder\n\n 4.   Locating the SolverLinux dynamic libraries at |\/usr\/lib|, |lib| or |\/usr\/local\/lib| directories, \n      4.1. But you can add the path of the binary file to the library path by modifying the variable |LD_LIBRARY_PATH|. \n      For instance, if the libraries are in the same directory as the binary file, you can write in a terminal: export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH\n\n 5.   Must be ready for the execution as follow\n     5.a. java -jar FuzzyDL.jar filename\n     5.b. To read in the file. Some parameters are taken from the CONFIG file\n     5.c. To select the semantics, change the 'solver' parameter in the CONFIG file z (for Zadeh)    \/ l (for Luaksiewicz)\/ c (for Classical)\n\n 6.   Alternatively, leave the dynamic libraries in solverLinux and define:\n      export    LD_LIBRARY_PATH=$HOME\/Linux\/FuzzyDL\/solverLinux\n\n----------------------------------------------------------------------------\n\nInstruction of first installation of Gurobi Optimizer\n\n1. To Register and download from \nhttp:\/\/pages.gurobi.com\/DownloadRegistration.html?\/download\/gurobi-optimizer\n\nNote: Maybe it only works with an older release like 5.0.2. Still needs to be verified.\n\n2. Choose a destination directory. Actually,  \/opt is used for installation. \n\n3. To copy the Gurobi distribution to that directory and extract the contents. \n    Extraction will create a sub-directory gurobi560\/linux64 . Also, The <installdir> would be \/opt\/gurobi560\/linux64.\n \n4. In order to allow executable files to be found when needed, you have to modify a few environment variables:\n    \u2022 GUROBI_HOME should point to the <installdir>.\n    \u2022 PATH should be extended to include <installdir>\/bin.\n    \u2022 LD_LIBRARY_PATH should be extended to include <installdir>\/lib.\n \n5. In the case of using bash shell, need to add the following lines to the .bashrc files as follow:\n    \u2022 export GUROBI_HOME=\"\/opt\/gurobi560\/linux64\"\n    \u2022 export PATH=\"${PATH}:${GUROBI_HOME}\/bin\"\n    \u2022 export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:${GUROBI_HOME}\/lib\"\n\n6. If LD_LIBRARY_PATH is not already set, you would use the following instead:\n   export LD_LIBRARY_PATH=\"${GUROBI_HOME}\/lib\" (for Eclipse projects, this path has to be added to the environment variables).\n \n7. Ready to proceed to Obtain and Install the Gurobi License.\n7.1.  see the Licenses page: http:\/\/www.gurobi.com\/de\/download\/licenses\/current\n\n7.2.  Free Academic tab -> Accept Agreement -> Request License -> clicking on the License ID\n7.3.  Run the grbgetkey command like: grbgetkey 253e22f3-...\n      \n7.4.  In order to save the license key, recommended to accept the default location (hitting Enter)\n \n8. When you run the Gurobi Optimizer, it will look for the gurobi.lic key file in \/opt\/gurobi and \/opt\/gurobi560.\n\n9. If you choose to put the license key file in a non-default location, you should add a line like the following to you .bashrc file\n   (For setting the environment variable)\n\n    export GRB_LICENSE_FILE=\/usr\/home\/jones\/gurobi.lic\n\n\n\n","39":"# Machine Learning Tool Kit\n\nMLTK is a collection of various supervised machine learning algorithms, which is designed for directly training models and further development. For questions or suggestions with the code, please email <a href=\"mailto:machinelearningtoolkit@gmail.com\">machinelearningtoolkit@gmail.com<\/a>. \n\nSee [wiki](https:\/\/github.com\/yinlou\/mltk\/wiki) for full documentation, examples and other information.\n","40":"Flink ML is a library which provides machine learning (ML) APIs and\ninfrastructures that simplify the building of ML pipelines. Users can implement\nML algorithms with the standard ML APIs and further use these infrastructures to\nbuild ML pipelines for both training and inference jobs.\n\nFlink ML is developed under the umbrella of [Apache\nFlink](https:\/\/flink.apache.org\/).\n\n## <a name=\"build\"><\/a>Building the Project\n\nRun the `mvn clean package` command.\n\nThen you will find a JAR file that contains your application, plus any libraries\nthat you may have added as dependencies to the application:\n`target\/<artifact-id>-<version>.jar`.\n\n## <a name=\"benchmark\"><\/a>Benchmark\n\nFlink ML provides functionalities to benchmark its machine learning algorithms.\nFor detailed information, please check the [Benchmark\nGetting Started](.\/flink-ml-benchmark\/README.md).\n\n## <a name=\"documentation\"><\/a>Documentation\n\nThe documentation of Flink ML is located on the website:\nhttps:\/\/nightlies.apache.org\/flink\/flink-ml-docs-master\/ or in the docs\/\ndirectory of the source code.\n\n## <a name=\"contributing\"><\/a>Contributing\n\nYou can learn more about how to contribute in the [Apache Flink\nwebsite](https:\/\/flink.apache.org\/contributing\/how-to-contribute.html). For code\ncontributions, please read carefully the [Contributing\nCode](https:\/\/flink.apache.org\/contributing\/contribute-code.html) section for an\noverview of ongoing community work.\n\n## <a name=\"license\"><\/a>License\n\nThe code in this repository is licensed under the [Apache Software License\n2](LICENSE).\n","41":"# Java Machine Learning Library\nSimple machine learning (neural network) library for Java. The library is mainly for educational purposes, and it is way too slow to be used on actual projects.\n\nThe Korean translation of this README.md is [here](README_ko.md), if you prefer to read it in Korean.\n\n(**NOTE: PROBABLY OUTDATED. Just compile from source.**) If you want to download the compiled `.jar` file and include it to your own project, click [here](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/raw\/master\/JavaMachineLearning.jar).\n\nThis library recently got an overhaul that fixed many bugs and uses vectorized operations with a built-in tensor class, among many other features. The source code was also organized and comments were added.\n\n## Features\n- Feed-forward layers\n  - Fully connected\n  - Convolutional (2D convolution on 3D inputs with 4D weights)\n  - Max\/Average Pooling\n  - Dropout\n  - Activation\n  - Flatten (Conv\/Pooling -> FC)\n  - Scaling\n- Recurrent layer\n  - GRU Cells\n- Adam, Adagrad, momentum, NAG, Nesterov, SGD, RMSProp and AdaDelta optimizers\n- Mini-batch gradient descent\n  - Average gradients for each weight throughout each batch\n- Sigmoid, tanh, relu, hard sigmoid, and softmax activation functions\n- L1, L2, and elastic net regularization\n- Squared loss, binary cross entropy, and multi-class cross entropy\n  - Squared loss for regression\n  - Binary cross entropy + sigmoid activation for binary classification\n  - Multi-class cross entropy + softmax activation for general classification\n- Internally uses \"tensors\", which are multidimensional arrays\/matrices\n- Simple graphing class for graphing classification boundaries, points, lines, line plots, etc.\n- MNIST dataset loader\n- Save\/load weights to\/from files\n- Drawing GUI for MNIST\n- A bunch of testing classes and graphing examples\n- Image preprocessing\n\n## Tutorial\nThe API provided by this library is quite elegant (in my opinion) and very high level. A whole network can be created by initializing a `SequentialNN` class. That class provides the tools to add layers and build a complete network. When initializing that class, you need to specify the shape of the input as the parameter.\n\nUsing the `add` method in `SequentialNN`, you can add layers to the sequential model. These layers will be evaluated in the order they are added during forward propagation. To forward propagate, use the predict function and provide input(s) as tensors. Tensors are multidimensional arrays that are represented in a flat, column major order format internally. However, it provides a few constructors that accept (regular) row major arrays. To train a model, call the `train` method with inputs and expected target outputs. This method has many parameters that can be changed, such as the loss function, optimizer, regularizer, etc. A callback function can even be provided for every epoch of training.\n\nWith the addition of a `RecurrentLayer` class, inputs and outputs can span many time steps. For example, when using fully connected layer after a recurrent layer, the fully connected layer is applied to the outputs for every single time step. Another addition is a flexible `predict` function that allows a custom number of time steps to be evaluated. Recurrent layers can also be stateful throughout multiple training examples or predictions.\n\n### Vanilla Neural Networks\n\nHere is a piece of code that shows how easy it is to run a simple linear regression using a neural network:\n```java\n\/\/ neural network with 1 input and 1 output, no activation function\nSequentialNN nn = new SequentialNN(1);\nnn.add(new FCLayer(1));\n\n\/\/ y = 5x + 3\nTensor[] x = {\n\tt(0),\n\tt(1),\n\tt(2),\n\tt(3),\n\tt(4)\n};\n\nTensor[] y = {\n\tt(3 + 0 + 1),\n\tt(3 + 5 - 1),\n\tt(3 + 10 + 1),\n\tt(3 + 15 - 1),\n\tt(3 + 20 + 1)\n};\n\nnn.train(x,\n\ty,\n\t100, \/\/ number of epochs\n\t1, \/\/ batch size\n\tLoss.squared,\n\tnew SGDOptimizer(0.01),\n\tnull, \/\/ no regularizer\n\tfalse, \/\/do not shuffle data\n\ttrue); \/\/ verbose\n\n\/\/ try the network on new data\nSystem.out.println(nn.predict(t(5)));\n```\nYou can find the full source file [here](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/blob\/master\/src\/tests\/LinearGraph.java). Note that the `t` method is just a convenience method to create 1D tensors. The full code will produce a window with the points and the line formed by the weight\/bias graphed:\n![linear regression graph](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/blob\/master\/nn_linear_regression.png)\n\nOn a slightly different set of data (y = 5x instead of y = 5x + 3, no noise, and no bias), the loss\/error with respect to the weight can be graphed:\n![error wrt weight graph](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/blob\/master\/error_graph_squared.png)\nThe green dots represent weights that the training algorithm \"visited\" throughout training. The quadratic shape of the graph is due to the squared loss function. Note that it converges to the minimum, where the loss is the lowest, and that minimum is centered on x = 5, which is the slope of the linear function that we want to learn.\n\nThe following piece of code is for training a 3 layer neural network for the MNIST handwritten digit classification.\n```java\n\/\/ create a model with 784 input neurons, 300 hidden neurons, and 10 output neurons\n\/\/ use RELU for the hidden layer and softmax for the output layer\nSequentialNN nn = new SequentialNN(784);\nnn.add(new FCLayer(300));\nnn.add(new ActivationLayer(Activation.relu));\nnn.add(new FCLayer(10)); \/\/ 10 categories of numbers\nnn.add(new ActivationLayer(Activation.softmax));\n\n\/\/ load the training data\nTensor[] x = MNISTUtils.loadDataSetImages(\"train-images-idx3-ubyte\", Integer.MAX_VALUE);\nTensor[] y = MNISTUtils.loadDataSetLabels(\"train-labels-idx1-ubyte\", Integer.MAX_VALUE);\n\nlong start = System.currentTimeMillis();\n\nnn.train(Utils.flattenAll(x),\n\ty,\n\t100, \/\/ number of epochs\n\t100, \/\/ batch size\n\tLoss.softmaxCrossEntropy,\n\tnew MomentumOptimizer(0.5, true),\n\tnew L2Regularizer(0.0001),\n\ttrue, \/\/ shuffle the data after every epoch\n\tfalse);\n\nSystem.out.println(\"Training time: \" + Utils.formatElapsedTime(System.currentTimeMillis() - start));\n\n\/\/ save the learned weights\nnn.saveToFile(\"mnist_weights_fc.nn\");\n\n\/\/ predict on previously unseen testing data\nTensor[] testX = MNISTUtils.loadDataSetImages(\"t10k-images-idx3-ubyte\", Integer.MAX_VALUE);\nTensor[] testY = MNISTUtils.loadDataSetLabels(\"t10k-labels-idx1-ubyte\", Integer.MAX_VALUE);\nTensor[] testResult = nn.predict(Utils.flattenAll(testX));\n\n\/\/ prints the percent of images classified correctly\nSystem.out.println(\"Classification accuracy: \" + Utils.format(Utils.classificationAccuracy(testResult, testY)));\n```\nThe full code can be found [here](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/blob\/master\/src\/tests\/TrainMNISTFullyConnected.java).\n\n### Convolutional Neural Networks\n\nThe training code that uses convolutional layers for the same digit classification task can be found [here](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/blob\/master\/src\/tests\/TrainMNISTConv.java). However, the code is very slow, so a simpler test to see if the model can directly memorize some digits was conducted. The code is available [here](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/blob\/master\/src\/tests\/TrainMNISTConvMemorize.java). The architecture is very similar to the previous convolutional network:\n```java\nSequentialNN nn = new SequentialNN(28, 28, 1);\n\nnn.add(new ConvLayer(5, 32, PaddingType.SAME));\nnn.add(new ActivationLayer(Activation.relu));\nnn.add(new MaxPoolingLayer(2, 2));\n\nnn.add(new ConvLayer(5, 64, PaddingType.SAME));\nnn.add(new ActivationLayer(Activation.relu));\nnn.add(new MaxPoolingLayer(2, 2));\n\nnn.add(new FlattenLayer());\n\nnn.add(new FCLayer(1024));\nnn.add(new ActivationLayer(Activation.relu));\n\nnn.add(new DropoutLayer(0.3));\n\nnn.add(new FCLayer(10));\nnn.add(new ActivationLayer(Activation.softmax));\n```\nTraining this network takes around 20 minutes and it can memorize the input image's classes perfectly.\n\n### Recurrent Neural Networks\n\nCreating a recurrent neural network is also very simple. Currently, only GRU cells are supported, and I used that to learn and generate some Shakespeare and Alice's Adventures in Wonderland text.\n\nHere are the hyperparameters used:\n```java\nint epochs = 500;\nint batchSize = 10;\nint winSize = 20;\nint winStep = 20; \/\/ winSize = winStep so substrings are not repeated\nint genIter = 5000; \/\/ how many characters to generate\ndouble temperature = 0.1; \/\/ lower = less randomness\n```\nAnd here is the code that builds the 2 layer recurrent neural network model:\n```java\n\/\/ for each time step, the input is a one hot vector describing the current character\n\/\/ for each time step, the output is a one hot vector describing the next character\n\/\/ the recurrent layers are stateful, which means that the next state relies on the previous states\nSequentialNN nn = new SequentialNN(winSize, alphabet.length());\nnn.add(new RecurrentLayer(winSize, new GRUCell(), true));\nnn.add(new DropoutLayer(0.3));\nnn.add(new RecurrentLayer(winSize, new GRUCell(), true));\n\/\/ the same fully connected layer is applied for every single time step\nnn.add(new FCLayer(alphabet.length()));\n\/\/ scales the values by the temperature before softmax\nnn.add(new ScalingLayer(1 \/ temperature, false));\nnn.add(new ActivationLayer(Activation.softmax));\n```\nGo [here](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/blob\/master\/src\/tests\/GRUTest.java) if you want the full code for training the model and generating text.\n\nHere is the output from running on Shakespear's Sonnet #130:\n```\n[cxx]x\n\n  my mistress' eyes are nothing like the sun\n  coral is far more red, than her lips red\n  if snow be white, why then her breasts are dun\n  if hairs be wires, black wires grow on her head.\n  i have seen roses damask'd, red and white,\n  but no such roses see i in her cheeks\n  and in some perfumes is there more delight\n  than in the breath that from my mistress reeks.\n  i love to hear her speak, yet well i know\n  that music hath a far more pleasing sound\n  i grant i never saw a goddess go,--\n  my mistress, when she walks, treads on the ground\n    and yet by heaven, i think my love as rare,\n    as any she belied with false compare\n```\nThe text in brackets at the very beginning is the seed text entered in by me. The network takes that and generates the rest of the sonnet, plus some extra spaces at the end that I removed.\n\nHere is the output from running on an excerpt of Alice's Adventures in Wonderland:\n```\n[chapter] i. down the rabbit-hole\n\nalice was beginning then she\nray so menty see.\n\naf the\nhing howver be world she was considering in her feet, for it flashed across her mind that she ought to have wondered at the sides it pocts tow  th the tried to have wondered at the sides it pocts top the rabbit with pink eyes time as she fell very slowly, for she had\nplenty of time as she went lothe the down nothing to her owa get in to her that she was considering in her feet, for it flashed across her mind that she ought to have wondered at the sides it withing either a waistcoat-pocket, and to ple pfonsidering in her feet, for it flashed across her mind there she fell very slowly, for she had\nplenty so  it with pink ey her feet, for it flashed across her mind that she ought to have wondered at the sides it pocts top the rabbit with pink eyes time as she fell very slowly, for she had\nplenty of time as she went tring to look down and make out what\nshe was considering in her feet, for it flashed across her mind that she ought to have wondered at the sides it poct plap the had pe was coming to, but it was too dark to see anything then she\nlooked at the sides all seemed quite was beginning then she\nray co peer a watch\nto take out of it, and fortunately was just in time to see it pop down a large\nrabbit-hole under the well was considering in her feet, for it flashed across her mind that she ought to have wondered at the sides it pocts top the rabbit with pink eyes time as she fell very slowly, for she had\nplenty so  it with pink ey, she she to out it was too dark to tires all, be late! (when she thought it over afterwatd, but it was too dark to te was beginning then somenasy\nseen a rabbit with either a waistcoat-pocket, and to ple ppend.n ahelves thok down a jar from one of the shelves had wondel very sleepy and stupid), whether the well was coused it was labelled orange maran\naling she to her feet, for it flashed across her mind there she fell very slowly, for she had\nplenty of time as she went tr.e hed as\nshe pagllidy, nothing then she\nron to happen next. first, she tried to look down and make out what\nshe was considering in her feet, for it flashed across her mind that she ought to have wondered at the sides it poct poud there she fell past it\n```\nThere are a lot of misspelled words, but it is pretty cool nonetheless.\n\nFinally, here is the output of running the network on the entire Act I Scene I of Romeo and Juliet:\n```\n[act i]t sarn'd to the will part thee.\n\n  rom. i do beauty his sunpong sprice.\n\n  rom. i do beauty his groanse the wall.\n\n  samp. i do beaut's thess swords therefere in that is to streponse thee the wall.\n\n  samp. i do beaut's thess swords therefere if thou doth the maids, or ment to the willat them, an thet thee weart of lovers' from the strunce of his will the live his will be comes to the but whett ther theis ments i will the hat with the fair,\n    bees thee, when the wall the hat with the wall.\n\n  samp. i do beaut's thess swords therefere in that len.\n\n  ben. montague should be so fair mark.\n  sh therefore i will they will stoul of the maids having the will part thee the hat let pee i pass me not here in sparkling hath the maids hour side i sad the wall.\n\n  samp. i do beaut's thess swords therefere in that len.\n\n  ben. montague should more or here with the maids hours shown so thee with me.\n\n  samp. i do beauty his caming makes the he will.\n\n  samp. no, should montagues!\n    wher thear hears'd and will they were head the willat to stand, and moved.\n\n  ben. in sand hear the with my a wist.\n\n  samp. i do beaut's thess sword morte the wall.\n\n  samp. i do beaut's this will stans.\n\n  greg. the heads of the beauty the hasterte me my fair madk to the was the weakes starn,\n    where is to stor. what her comes is to store.\n\n  samp. i do beauty the wall the hat here in sparkling his hit tles, and montague and with me.\n\n  samp. i do betheas in paine.\n\n  samp. a dog of the fair markman.\n\n  samp. i do beaut's thess sword morte me they me what hes if othen. i will the wall.\n\n  samp. i do beaut's thess swords therefere in that me.\n\n  rom. i dis ment good the was she head the was what, and hent sword of the will part the will part the will.\n\n  samp. no, sir.\n\n  samp. no, as the weadt of the wall.\n\n  samp. i do beaut's thess sword.\n\n  rom. i dis ment good the wall.\n\n  samp. a dog of the hat heads her his comes and montague ind sen the wall.\n\n  samp. i do beaut's thess sword morte and made is that we dows thee i sang the hearty saive in strun.\n```\nAs you can see there are some repetitions that would probably disappear if the temperature is increased (which increases the randomness). Originally, I wanted the network to start predicting from the first line that says what act and scene it was, but the network started from somewhere else.\n\nIn all of these examples, the model and hyperparameters were the same. What's cool is that the network learns the structure of the text and properly adds newlines and indents for the first and third examples. Also, I got the texts from [Project Gutenberg](http:\/\/www.gutenberg.org\/).\n\nMany other examples can be found in the [tests folder](https:\/\/github.com\/Daniel-Liu-c0deb0t\/Java-Machine-Learning\/tree\/master\/src\/tests).\n\nI have a blog post on backpropagation and gradient descent equations [here](https:\/\/c0deb0t.wordpress.com\/2018\/06\/17\/the-math-for-gradient-descent-and-backpropagation\/). It has some interesting math stuff!\n\n### Image load\n\nIf you want to load image to Tensor, you can do following codes\n\n```\nImageUtils imgUtils = new ImageUtils();\nTensor imgTensor = img.readColorImageToTensor(String path, boolean convertGray)\n```\n\nAnd if you want to load many images to Tensor array, also you can do following codes\n\n```\nImageUtils imgUtils = new ImagUtils()\nTensor[] imgTensorArray = public Tensor[] readImages(String folderPath, boolean convertGray)\n```\n","42":"# Cognitive Foundry\n\n## About\n\n\nThe Cognitive Foundry is an open-source Java library for building intelligent systems with a focus on machine learning. The Foundry's development is led by Sandia National Laboratories and is released under the open-source BSD license.\n\nThe Foundry contains four primary packages: Common, Learning, Text, and Framework. Common defines many of the basic interfaces and types in the Foundry. It also includes a linear algebra package and other generally useful Java utilities. Learning contains components and algorithms for machine learning and statistics. Text contains components and algorithms for text analysis and information retrieval such as topic modeling. Framework contains a framework for building multi-level models.\n\nThe Foundry requires Java 1.8 or later.\n\n## License\n\nSee [License.txt](License.txt) for information about the license.\n\n## Binaries\n\nThe latest version of the Cognitive Foundry can be downloaded from the [GitHub site](https:\/\/github.com\/algorithmfoundry\/Foundry\/releases) or [official site](http:\/\/foundry.sandia.gov\/download.html). You can also get the jars from Maven Central using Maven or Ivy. The organization\/groupId is gov.sandia.foundry.\n\nExample Maven dependencies:\n\n```xml\n<dependencies>\n  <dependency>\n    <groupId>gov.sandia.foundry<\/groupId>\n    <artifactId>gov-sandia-cognition-common-core<\/artifactId>\n    <version>4.0.1<\/version>\n  <\/dependency>\n  <dependency>\n    <groupId>gov.sandia.foundry<\/groupId>\n    <artifactId>gov-sandia-cognition-common-data<\/artifactId>\n    <version>4.0.1<\/version>\n  <\/dependency>\n  <dependency>\n    <groupId>gov.sandia.foundry<\/groupId>\n    <artifactId>gov-sandia-cognition-learning-core<\/artifactId>\n    <version>4.0.1<\/version>\n  <\/dependency>\n  <dependency>\n    <groupId>gov.sandia.foundry<\/groupId>\n    <artifactId>gov-sandia-cognition-text-core<\/artifactId>\n    <version>4.0.1<\/version>\n  <\/dependency>\n  <dependency>\n    <groupId>gov.sandia.foundry<\/groupId>\n    <artifactId>gov-sandia-cognition-framework-core<\/artifactId>\n    <version>4.0.1<\/version>\n  <\/dependency>\n  <dependency>\n    <groupId>gov.sandia.foundry<\/groupId>\n    <artifactId>gov-sandia-cognition-framework-learning<\/artifactId>\n    <version>4.0.1<\/version>\n  <\/dependency>\n<\/dependencies>\n```\n\nExample Ivy dependencies\n```xml\n<dependencies>\n    <dependency org=\"gov.sandia.foundry\" name=\"gov-sandia-cognition-common-core\"        rev=\"4.0.1\"\/>\n    <dependency org=\"gov.sandia.foundry\" name=\"gov-sandia-cognition-common-data\"        rev=\"4.0.1\"\/>\n    <dependency org=\"gov.sandia.foundry\" name=\"gov-sandia-cognition-learning-core\"      rev=\"4.0.1\"\/>\n    <dependency org=\"gov.sandia.foundry\" name=\"gov-sandia-cognition-text-core\"          rev=\"4.0.1\"\/>\n    <dependency org=\"gov.sandia.foundry\" name=\"gov-sandia-cognition-framework-core\"     rev=\"4.0.1\"\/>\n    <dependency org=\"gov.sandia.foundry\" name=\"gov-sandia-cognition-framework-learning\" rev=\"4.0.1\"\/>\n<\/dependencies>\n```\n\n## Building\n\nTo compile the Foundry from source, you can use either the [Ant](http:\/\/ant.apache.org\/) or [Maven](http:\/\/maven.apache.org\/) build scripts.\n\nUsing Ant you can simply invoke the main build.xml file in the root directory by doing:\n```\nant clean build\n```\n\nUsing Maven you can invoke the pom.xml in the root directory by doing:\n```\nmvn clean package\n```\n\n## Modules\n\nThe Foundry has four main packages some of which have multiple modules. Each module has a jar whos ename is prefixed with ```gov-sandia-cognition-```.\n\n  * ```common-core``` - Contains base interfaces and types, including classes for linear algebra.\n  * ```common-data``` - Contains utilities for handling data.\n  * ```learning-core``` - Contains algorithms and components for machine learning and statistics.\n  * ```text-core``` - Contains algorithms and components for text analysis and Information Retrieval.\n  * ```graph-core``` - Contains algorithms and components for graphs.\n  * ```framework-core``` - Contains a framework for multi-level models.\n  * ```framework-learning``` - Contains adapters for Machine Learning in the framework.\n\n## Example Code\n\nEach of the four main packages has example code for how to get started with some of the basic components.\n\n### Common Package Examples\n  * [Evaluator Example](Components\/CommonExamples\/Source\/examples\/EvaluatorExample.java)\n  * [Matrix and Vector Example](Components\/CommonExamples\/Source\/examples\/MatrixAndVectorExample.java)\n  * [XStream Serialization Handler Example](Components\/CommonExamples\/Source\/examples\/XStreamSerializationHandlerExample.java)\n\n### Learning Package Examples\n  * [Learning Experiment Example](Components\/LearningExamples\/Source\/examples\/LearningExperimentExample.java)\n  * [K-Means Example](Components\/LearningExamples\/Source\/examples\/SimpleKMeansExample.java)\n  * [Custom Clustering Example](Components\/LearningExamples\/Source\/examples\/CustomClusteringExample.java)\n  * [Mixture of Gaussians Example](Components\/LearningExamples\/Source\/examples\/MixtureOfGaussiansExample.java)\n\n### Text Package Examples\n  * [Text Pipeline Example](Components\/TextExamples\/Source\/examples\/TextPipelineExample.java)\n\n### Framework Package Examples\n  * [Cognitive Model Example](Components\/FrameworkExamples\/Source\/examples\/BasicCognitiveModelLiteExample.java)\n\nFor more information about a specific class you can also look at the unit test for examples of how to create and use it.\n\n\n## Contacts\n\nHere are some useful contact points for the Foundry.\n\n * Official site: http:\/\/foundry.sandia.gov\n * GitHub site: https:\/\/github.com\/algorithmfoundry\/Foundry\n * Community site: http:\/\/www.cognitivefoundry.org\n * Twitter: [@AlgoFoundry](http:\/\/www.twitter.com\/AlgoFoundry)\n * Google Group: [cognitive-foundry@googlegroups.com](http:\/\/groups.google.com\/d\/forum\/cognitive-foundry)\n\n","43":"[![Build Status](https:\/\/travis-ci.org\/ClearTK\/cleartk.svg?branch=master)](https:\/\/travis-ci.org\/ClearTK\/cleartk)\n\n# Introduction #\n\nClearTK provides a framework for developing statistical natural language \nprocessing (NLP) components in Java and is built on top of Apache UIMA. It is \ndeveloped by the Center for Computational Language and Education Research \n(CLEAR) at the University of Colorado at Boulder.\n\nClearTK is built with Maven and we recommend that you build your project that\ndepends on ClearTK with Maven.  This will allow you to add dependencies for only\nthe parts of ClearTK that you are interested and automatically pull in only \nthose dependencies that those parts depend on.  The zip file you have downloaded\nis provided as a convenience to those who are unable to build with Maven.  It \nprovides jar files for each of the sub-projects of ClearTK as well as all the \ndependencies that each of those sub-projects uses.  To use ClearTK in your \nJava project, simply add all of these jar files to your classpath.  If you are\nonly interested in one (or a few) sub-project of ClearTK, then you may not want\nto add every jar file provided here.  Please consult the maven build files to \ndetermine which jar files are required for the parts of ClearTK you want to use.   \n\nPlease see the section titled \"Dependencies\" below for important licensing information.\n\n# License #\n\nCopyright (c) 2007-2014, Regents of the University of Colorado \nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. \nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution. \nNeither the name of the University of Colorado at Boulder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. \n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE. \n\n# Dependencies #\n\nClearTK depends on a variety of different open source libraries that are\nredistributed here subject to the respective licensing terms provided by \neach library.  We have been careful to use only libraries that are \ncommercially friendly.  Please see the notes below for exceptions.  For a \ncomplete listing of the dependencies and their respective licenses please\nsee the file licenses\/index.html.\n\n# GPL Dependencies #\n\nClearTK has two sub-projects that depend on GPL licensed libraries: \n * cleartk-syntax-berkeley\n * cleartk-stanford-corenlp\nNeither of these projects nor their dependencies are provided in this release.\nTo obtain these projects, please manually download them from our googlecode\nhosted maven repository:\n\nhttp:\/\/cleartk.googlecode.com\/svn\/repo\/org\/cleartk\/cleartk-syntax-berkeley\/\nhttp:\/\/cleartk.googlecode.com\/svn\/repo\/org\/cleartk\/cleartk-stanford-corenlp\/\n\n# SVMLIGHT #\n\nClearTK also has two projects called cleartk-ml-svmlight and cleartk-ml-tksvmlight\nwhich have special licensing considerations. The ClearTK project does not \nredistribute SVMlight. ClearTK does, however, facilitate the building of SVMlight \nmodels via the ClassifierBuilder interface. In order to use the implementations of\nthis interface to good effect you will need to have SVMlight installed on your\nmachine. The ClassifierBuilders for SVMlight simply call the executable \"svm_learn\"\nprovided by the SVMlight distribution. ClearTK does not use SVMlight at\nclassification time - it only uses the models that are build by SVMlight. Instead,\nClearTK provides its own code for classification that makes use of an SVMlight\ngenerated model.  This code is provided with ClearTK and is available with the\nabove BSD license as is all of the other code written for ClearTK. Therefore, be\nadvised that while ClearTK is not required (or compelled) to redistribute the code\nor license of SVMlight or to comply with it (i.e. the noncommercial license\nprovided by SVMlight is not compatible with our BSD License) - it would be very\ndifficult to use the SVMlight wrappers we provide in a commercial setting without\nobtaining a license for SVMlight directly from its authors.  \n\n# LGPL #\n\nThe cleartk-ml-mallet project depends on Mallet (http:\/\/mallet.cs.umass.edu\/),\nwhich depends on trove4j (http:\/\/trove.starlight-systems.com\/), which is\nreleased under the LGPL license. If you do not need Mallet classifiers and would\nlike to avoid the LGPL license, you can omit the cleartk-ml-mallet dependency.\n","44":"\n\nUEA Time Series Classification\n==============================\n\n.. image:: https:\/\/travis-ci.com\/uea-machine-learning\/tsml.svg?branch=master\n    :target: https:\/\/travis-ci.com\/uea-machine-learning\/tsml\n\nA `Weka <https:\/\/svn.cms.waikato.ac.nz\/svn\/weka\/branches\/stable-3-8\/>`__-compatible Java toolbox for\n**time series classification, clustering and transformation**. For the python sklearn-compatible version, see \n`sktime <https:\/\/github.com\/alan-turing-institute\/sktime>`__\n\nFind out more info about our broader work and dataset hosting for the UCR univariate and UEA multivariate time series classification archives on our `website <http:\/\/www.timeseriesclassification.com>`__.\n\nThis codebase is actively being developed for our research. The dev branch will contain the most up-to-date, but stable, code. \n\nInstallation\n------------\nWe are looking into deploying this project on Maven or Gradle in the future. For now there are two options:\n\n* download the `jar file <http:\/\/timeseriesclassification.com\/Downloads\/tsml11_3_2020.jar>`__ and include as a dependency in your project, or you can run experiments through command line, see the `examples on running experiments <https:\/\/github.com\/uea-machine-learning\/tsml\/blob\/dev\/src\/main\/java\/examples\/Ex04_ThoroughExperiments.java>`__\n* fork or download the source files and include in a project in your favourite IDE you can then construct your own experiments (see our `examples <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/dev\/src\/main\/java\/examples>`__) and implement your own classifiers.\n\nOverview\n--------\n\nThis codebase mainly represents the implementation of different algorithms in a common framework, which at the time leading up to the `Great Time Series Classification Bake Off <https:\/\/link.springer.com\/article\/10.1007\/s10618-016-0483-9>`__ in particular was a real problem, with implementations being in any of Python, C\/C++, Matlab, R, Java, etc. or even combinations thereof. \n\nWe therefore mainly provide implementations of different classifiers as well as experimental and results analysis pipelines with the hope of promoting and streamlining open source, easily comparable, and easily reproducible results, specifically within the TSC space. \n\nWhile they are obviously very important methods to study, we shall very likely not be implementing any kind of deep learning methods in our codebase, and leave those rightfully in the land of optimised languages and libraries for them, such as `sktime-dl <https:\/\/github.com\/uea-machine-learning\/sktime-dl>`__ , the Keras-enabled extension to `sktime <https:\/\/github.com\/alan-turing-institute\/sktime>`__. \n\nOur `examples <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/dev\/src\/main\/java\/examples>`__ run through the basics of using the code, however the basic layout of the codebase is this:\n\n`evaluation\/ <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/master\/src\/main\/java\/evaluation>`__ \n    contains classes for generating, storing and analysing the results of your experiments\n    \n`experiments\/ <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/master\/src\/main\/java\/experiments>`__ \n    contains classes specifying the experimental pipelines we utilise, and lists of classifier and dataset specifications. The 'main' class is `Experiments.java <https:\/\/github.com\/uea-machine-learning\/tsml\/blob\/master\/src\/main\/java\/experiments\/Experiments.java>`__, however other experiments classes exist for running on simulation datasets or for generating transforms of time series for later classification, such as with the Shapelet Transform. \n\n`tsml\/ <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/master\/src\/main\/java\/tsml>`__ and `multivariate_timeseriesweka\/ <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/master\/src\/main\/java\/multivariate_timeseriesweka>`__\n    contain the TSC algorithms we have implemented, for univariate and multivariate classification respectively. \n\n`machine_learning\/ <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/master\/src\/main\/java\/machine_learning>`__\n    contains extra algorithm implementations that are not specific to TSC, such as generalised ensembles or classifier tuners. \n\nImplemented Algorithms\n----------------------\n\nClassifiers\n```````````\n\nThe lists of implemented TSC algorithms shall continue to grow over time. These are all in addition to the standard Weka classifiers and non-TSC algorithms defined under the machine_learning package.\n\nWe have implemented the following bespoke classifiers for univariate, equal length time series classification:\n\n================  ================  ==============  =================  ==============  ================\nDistance Based    Dictionary Based  Kernel Based    Shapelet Based     Interval Based  Hybrids\n================  ================  ==============  =================  ==============  ================\nDD_DTW            BOSS              Arsenal         LearnShapelets     TSF             HIVE-COTE\nDTD_C             cBOSS             ROCKET          ShapeletTransform  TSBF            Catch22\nElasticEnsemble   TDE                               FastShapelets      LPS\nNN_CID            WEASEL                            ShapeletTree       CIF\nSAX_1NN           SAXVSM                                               DrCIF\nProximityForest   SpatialBOSS                                          RISE\nDTW_kNN           SAX_1NN                                              STSF\nFastDTW           BafOfPatterns...\nFastElasticEn...  BOSSC45\nShapeDTW_1NN      BoTSWEnsemble\nShapeDTW_SVM      BOSSSpatialPy...\nSlowDTW_1NN\nKNN\n================  ================  ==============  =================  ==============  ================\n\nAnd we have implemented the following bespoke classifiers for multivariate, equal length time series classification:\n\n========  =============================\nNN_ED_D   MultivariateShapeletTransform\nNN_ED_I   ConcatenateClassifier\nNN_DTW_D  MultivariateHiveCote\nNN_DTW_I  WEASEL+MUSE\nSTC_D     MultivariateSingleEnsemble\nNN_DTW_A  MultivariateAbstractClassifier\n\\         MultivariateAbstractEnsemble\n========  =============================\n\nClusterers\n``````````\n\nCurrently quite limited, aside from those already shipped with Weka.\n\n============================ ====\nUnsupervisedShapelets\nK-Shape\nDictClusterer\nTTC\nAbstractTimeSeriesCLusterer\n============================ ====\n\nFilters\n```````````````````````\n\nSimpleBatchFilters that take an Instances (the set of time series), transforms them\nand returns a new Instances object.\n\n===================  ===================  ===================\nACF                  ACF_PACF             ARMA\nBagOfPatternsFilter  BinaryTransform      Clipping\nCorrelation          Cosine               DerivativeFilter\nDifferences          FFT                  Hilbert\nMatrixProfile        NormalizeAttribute   NormalizeCase\nPAA                  PACF                 PowerCepstrum\nPowerSepstrum        RankOrder            RunLength\nSAX                  Sine                 SummaryStats\n===================  ===================  ===================\n\nTransformers\nWe will be shifting over to a bespoke Transformer interface\n\n=================== =======\nShapeletTransform\ncatch22\n=================== =======\n\nPaper-Supporting Branches\n-------------------------\n\nThis project acts as the general open-source codebase for our research, especially the `Great Time Series Classification Bake Off <https:\/\/link.springer.com\/article\/10.1007\/s10618-016-0483-9>`__. We are also trialling a process of creating stable branches in support of specific outputs. \n\nCurrent branches of this type are: \n\n* `paper\/cawpe\/ <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/paper\/cawpe>`__ in support of `\"A probabilistic classifier ensemble weighting scheme based on cross-validated accuracy estimates\" <https:\/\/link.springer.com\/article\/10.1007\/s10618-019-00638-y>`__\n\n* `paper\/cawpeExtension\/ <https:\/\/github.com\/uea-machine-learning\/tsml\/tree\/paper\/cawpeExtension>`__ in support of \"Mixing hetero- and homogeneous models in weighted ensembles\" (Accepted\/in-press)\n\nContributors\n------------\n\nLead: Anthony Bagnall (@TonyBagnall, `@tony_bagnall <https:\/\/twitter.com\/tony_bagnall>`__, ajb@uea.ac.uk)\n\n* James Large (@James-Large, `@jammylarge <https:\/\/twitter.com\/jammylarge>`__, james.large@uea.ac.uk)\n* Jason Lines (@jasonlines), \n* George Oastler (@goastler), \n* Matthew Middlehurst (@MatthewMiddlehurst, `@M_Middlehurst <https:\/\/twitter.com\/M_Middlehurst>`__, m.middlehurst@uea.ac.uk),\n* Michael Flynn (GitHub - `@MJFlynn <https:\/\/github.com\/MJFlynn>`__, Twitter - `@M_J_Flynn <https:\/\/twitter.com\/M_J_Flynn>`__, Email - Michael.Flynn@uea.ac.uk)\n* Aaron Bostrom (@ABostrom, `@_Groshh_ <https:\/\/twitter.com\/_Groshh_>`__, a.bostrom@uea.ac.uk), \n* Patrick Sch\u00e4fer (@patrickzib)\n* Chang Wei Tan (@ChangWeiTan)\n* Alejandro Pasos Ruiz (a.pasos-ruiz@uea.ac.uk)\n* Conor Egan (@c-eg)\n\nWe welcome anyone who would like to contribute their algorithms! \n\nLicense \n-------\n\nGNU General Public License v3.0\n","45":"# \u57fa\u4e8e\u7b80\u5355\u673a\u5668\u7b97\u6cd5\u7684\u8003\u7814\u62e9\u6821\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\n\n##  \u4f7f\u7528\u7684\u6280\u672f\uff1a\n\n###  Web\u7aef\n\nSSM\u6846\u67b6+Maven+Mysql+Tomcat\uff0c\u5b9e\u73b0\u4e86\u57fa\u672c\u7684\u8bba\u575b\u529f\u80fd\uff0c\u5305\u62ec\u53d1\u8868\u5e16\u5b50\u3001\u67e5\u770b\u5e16\u5b50\u3001\u6d4f\u89c8\u6536\u85cf\u70b9\u8d5e\u8bc4\u8bba\u3001\u767b\u5f55\u6ce8\u518c\u7b49\uff08\u9875\u9762\u6a21\u4eff\u7684\u8003\u7814\u5e2e\uff09\n\n### \u7b97\u6cd5\n\n\u4f7f\u7528Python\u8bed\u8a00\u7f16\u5199\uff0c\u5728Java\u4e2d\u4f7f\u7528JPython\u901a\u8fc7cmd\u8c03\u7528Python\u811a\u672c\u5373\u53ef\n\n\u5b9e\u73b0\u7b97\u6cd5\u5305\u62ec\u4f46\u4e0d\u5c40\u9650\u4e8e\uff1a\n\n* \u57fa\u4e8e\u7528\u6237\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u3001\u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\n\n* PageRank\u7b97\u6cd5\u3001\u57fa\u4e8e\u5185\u5bb9\u7684\u6392\u5e8f\u7b97\u6cd5\uff08\u5355\u8bcd\u9891\u5ea6\u3001\u5355\u8bcd\u8ddd\u79bb\u3001\u5355\u8bcd\u4f4d\u7f6e\uff09\n\n* \u7b80\u5355\u7684\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5bf9\u641c\u7d22\u7ed3\u679c\u8fdb\u884c\u6392\u5e8f\u4f18\u5316\n\n* \u57fa\u4e8e\u903b\u8f91\u56de\u5f52\u7684\u7b80\u5355\u56fe\u5f62\u9a8c\u8bc1\u7801\u8bc6\u522b\n\n\n* \u51b3\u7b56\u6811\u7b97\u6cd5\u5bf9\u7528\u6237\u8fdb\u884c\u5206\u7c7b \n\n### \u722c\u866b\n\n\u722c\u866b\u722c\u53d6\u7684\u6570\u636e\u4e3a\u738b\u9053\u8003\u7814\u8bba\u575b\u7684\u5e16\u5b50\u6570\u636e\uff0c\u8fd1\u4f5c\u4e3a\u5206\u6790\u4f7f\u7528\uff0c\u4e0d\u53ef\u7528\u4f5c\u5546\u4e1a\u7528\u9014\uff0c\u4f7f\u7528Python\u8bed\u8a00\u7f16\u5199\n\n####                   \u3010\u4ec5\u7528\u4e8e\u5206\u4eab\u5b66\u4e60\u4f7f\u7528\u3011\n","46":"# Machine Learning\n# Deprecated\n**This repository is deprecated! Please use [NeuralNetwork](https:\/\/github.com\/sebig3000\/NeuralNetwork) instead!**\n\nJava collection that provides Java packages for developing machine learning algorithms and that is\n- easy to use -> great for small projects or just to learn how machine learning works\n- small and simple -> easy to understand and make changes\n- lightweight (mostly because I'm a student who just started to learn how to code Java and can't code more complex :P)\n\n## Getting Started\n\n### Prerequisites\n\nThis project is written in pure vanilla Java so there is nothing needed than the standard libraries.\n\n### Installation\n\nJust add all packages with the source files in the [Source folder \/src](src) to your project and you are ready to go!\nEvery class has a main test method. After installation just run any class so you can check if the installation was successful.\n\n## Code Example\n\n### [Neural Network](src\/main\/java\/neural)\n\nInitialize a new network with a given architecture (number or inputs, number of neurons in the hidden layers and each layers activation function)\n(If you don't know what to choose, here is a rule of thumb for a average looking network: \n- number of hidden layers = 2\n- number of neurons per layer: number of inputs (except the last layer is the output layer = as many neurons as outputs)\n- activation functions: none)\n\n```\n\/\/New network\nfinal Network net = new Network(\n        2,                                    \/\/2 inputs\n        new int[]{3, 1},                      \/\/2 layers with 3 & 1 neurons\n        new Network.ActivationFunction[]{\n          Network.ActivationFunction.NONE,    \/\/both layers with ...\n          Network.ActivationFunction.NONE});  \/\/... no activation function\n```\n\nThen you can seed the weights in the network (= randomize it).\n\n```\nnet.seedWeights(-1, 1);\n```\n\nPrepare your training data and put it into a [Matrix] (src\/main\/java\/neural\/Matrix.java)\n\n```\n\/\/Generate 10 training sets\n\/\/Every row represents one training set (10 rows = 10 sets)\n\/\/Every column gets fed into the same input\/comes out of the same output\n\/\/(first column gets into the first input)\n\/\/(2 columns = 2 inputs \/ 1 column = 1 output)\nfinal Matrix trainInput = new Matrix(10, 2);\nfinal Matrix trainOutput = new Matrix(10, 1);\n\/\/Fill the training sets\n\/\/Inputs: two random numbers\n\/\/Outputs: average of these two numbers\nfinal Random rand = new Random();\nfor(int set=0; set<trainInput.getHeight(); set++) {\n  trainInput.set(set, 0, rand.nextInt(10));\n  trainInput.set(set, 1, rand.nextInt(10));\n  \n  final double out = (trainInput.get(set, 0) + trainInput.get(set, 1)) \/ 2;\n  trainOutput.set(set, 0, out);\n}\n```\n\nNow your network is ready for training!\nJust tell it how drastic the changes should be, give it the training data and if it should print it's progress to the console.\n* learning rate: higher = faster training but to high could miss the optimum, slower = better result (sometimer it goes crazy and the cost just increase, then try decreasing the laerning rate)\n* inputs: training set inputs\n* outputs: wanted outputs the network should learn from\n* printToConsole: show the progress in the console\n\n\n```\nnet.train(0.2, trainInput, trainOutput, true);\n```\n\nNow the network should be trained so let's have a look at the network itself by simply printing a basic representation and try forwarding the inputs.\n\n```\nSystem.out.println(net);\nSystem.out.println(net.forward(trainInput));\n```\n\nAnd if we would like to get the mean squared error we just call the cost function on some data:\n\n```\nSystem.out.println(net.cost(trainInput, trainOutput));\n```\n\n## Contributors\n\nThe two people who inspired me to try making my own machine learning project are Brandon Rohrer and Stephen Welch.\nBoth make awesome YouTube videos that explain how machine learning works.\n\nBrandon Rohrer:\n- YouTube https:\/\/www.youtube.com\/channel\/UCsBKTrp45lTfHa_p49I2AEQ\n- Blog: https:\/\/brohrer.github.io\/blog.html\n- GitHub: https:\/\/github.com\/brohrer\n\nStephen Welch:\n- YouTube: https:\/\/www.youtube.com\/user\/Taylorns34\n- Homepage: http:\/\/www.welchlabs.com\/\n- GitHub: https:\/\/github.com\/stephencwelch\n\n## License (MIT)\n\nMIT License\n\nCopyright (c) 2017 Sebastian G\u00f6ssl\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n","47":"> **This project is currently unmaintained.**\n\n## SnowLeopardR\nSnowLeopardR is an open-source machine learning-based anticheat project. It classfies player's \"combat style\" with the time series built from player's movements and interactions on entities.\n\nThe feature extration procedure is backed up with experiment results, and the subsequently derived dataset is testified to reveal user's movement characteristics. SnowLeopardR classifies the data by leveraging a learning vector quantization neural network[1].\n\nThe project is initially presented as SnowLeopard, which is more like a working draft. As a tutorial, it is also nice to have a bit of software engineering. Therefore, the original one has been refactored for a robust and scalable architecture and renamed to SnowLeopardR. For further information, please move to the link below.\n\n## Works Cited\n[1] T. Kohonen (1995), \"Learning vector quantization\", in M.A. Arbib (ed.), _The Handbook of Brain Theory and Neural Networks_, Cambridge, MA: MIT Press, pp. 537\u2013540\n\n## Requirements\nJava 8+\n\nMaven 3\n\n## Please refer to (the thesis)\n[https:\/\/www.spigotmc.org\/threads\/machine-learning-killaura-detection-in-minecraft.301609\/](https:\/\/www.spigotmc.org\/threads\/machine-learning-killaura-detection-in-minecraft.301609\/)\n\n## License\nGPL v3\n\n> Written with [StackEdit](https:\/\/stackedit.io\/).\n","48":"# AMIDST Toolbox ([http:\/\/www.amidsttoolbox.com](http:\/\/www.amidsttoolbox.com))\n<!-- version -->\nv.0.7.2\n\n<!-- version -->\n[![GitHub version](https:\/\/badge.fury.io\/gh\/amidst%2Ftoolbox.svg)](https:\/\/badge.fury.io\/gh\/amidst%2Ftoolbox)\n[![Build Status](https:\/\/travis-ci.org\/amidst\/toolbox.svg?branch=release-0.7.2)](https:\/\/travis-ci.org\/amidst\/toolbox)\n[![Codacy Badge](https:\/\/api.codacy.com\/project\/badge\/Grade\/71e9ce7f576e473fa7f4c6846293e9d6)](https:\/\/www.codacy.com\/app\/rafacabanas\/toolbox?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=amidst\/toolbox&amp;utm_campaign=Badge_Grade)\n[![License](https:\/\/img.shields.io\/badge\/License-Apache%202.0-blue.svg)](https:\/\/opensource.org\/licenses\/Apache-2.0)\n\n# Description<a name=\"Description\"><\/a>\n\n## Probabilistic Machine Learning\n\n\n<p align=\"center\">\n<img title=\"PGM\" src=\"https:\/\/amidst.github.io\/toolbox\/docs\/web\/figs\/pgm-crop.png\" width=\"250\">\n<\/p>\n\n\nThe AMIDST Toolbox allows you to model your problem using a flexible probabilistic language based on graphical models. \nThen you fit your model with data using a Bayesian approach to handle modeling uncertainty.\n\n## Multi-core and distributed processing\n\n<p align=\"center\">\n<img title=\"Taxonomy\" src=\"https:\/\/amidst.github.io\/toolbox\/docs\/web\/figs\/cluster-crop.png\" width=\"250\">\n<\/p>\n\nAMIDST provides tailored parallel (powered by Java 8 Streams) and distributed (powered by [Flink](https:\/\/flink.apache.org) or [Spark](http:\/\/spark.apache.org)) implementations of Bayesian parameter learning for batch and streaming data. This processing is based on flexible and [scalable message passing algorithms](http:\/\/amidst.github.io\/toolbox\/docs\/dVMP.pdf).\n\n#Features<a name=\"features\"><\/a>\n\n* **Probabilistic Graphical Models**: Specify your model using probabilistic graphical models with [latent variables](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/examples-072\/bnetworks-072\/)\nand [temporal dependencies](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/examples-072\/dbnetworks-072\/). AMIDST contains a  large list of predefined latent variable models: \n<!-- version -->\n\n\n![](http:\/\/amidst.github.io\/toolbox\/docs\/web\/figs\/amidstModels-crop.png)\n\n* **Scalable inference**: Perform inference on your probabilistic models with powerful approximate and\nscalable algorithms.\n\n* **Data Streams**: Update your models when new data is available. This makes our toolbox \nappropriate for learning from (massive) data streams.\n\n* **Large-scale Data**: Use your defined models to process massive data sets in a distributed \ncomputer cluster using **Apache Flink** or (soon) **Apache Spark**. \n\n* **Extensible**: Code your models or algorithms within AMiDST and expand the toolbox functionalities. \nFlexible toolbox for researchers performing their experimentation in machine learning.\n\n* **Interoperability**: Leverage existing functionalities and algorithms by interfacing \nto other software tools such as [Hugin](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/examples-072\/bnetworks-072\/#sec:bns:huginlink), [MOA](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/examples-072\/bnetworks-072\/#sec:bns:moalink), Weka, R, etc.\n<!-- version -->\n\n\n#Simple Code Example<a name=\"example\"><\/a>\n\n## Fitting a model with local data\n\n```java\n        \/\/Load the data\n        String filename = \".\/data.arff\";\n        DataStream<DataInstance> data = DataStreamLoader.open(filename);\n\n        \/\/Learn the model\n        Model model = new CustomGaussianMixture(data.getAttributes());\n        model.updateModel(data);\n\n        System.out.println(model.getModel());\n\n        \/\/ Save with .bn format\n        BayesianNetworkWriter.save(model.getModel(), \".\/example.bn\");\n```\n\n## Fitting a model with distributed data\n\n\n```java\n        \/\/Load the data\n        String filename = \"hdfs:\/\/dataDistributed.arff\";\n        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n        DataFlink<DataInstance> data = DataFlinkLoader.loadDataFromFolder(env, filename, false);\n\n        \/\/Learn the model\n        Model model = new CustomGaussianMixture(data.getAttributes());\n        model.updateModel(data);\n\n        System.out.println(model.getModel());\n\n        \/\/ Save with .bn format\n        BayesianNetworkWriter.save(model.getModel(), \".\/example.bn\");\n```\n\n#Real-World Uses Cases<a name=\"uses\"><\/a>\n\n## Risk prediction in credit operations\n\n\n<p align=\"center\">\n<img title=\"PGM\" src=\"https:\/\/amidst.github.io\/toolbox\/docs\/web\/figs\/creditcard-crop.png\" width=\"250\">\n<\/p>\n\nAMIDST Toolbox has been used to track concept drift and do risk prediction in credit operations, \nand as data is collected continuously and reported on a daily basis, this gives rise to a streaming data \nclassification problem. This work has been performed in collaboration with one of our partners, \nthe Spanish bank BCC. It is expected to be into production at the beginning of 2017.\n\n\n## Recognition of traffic maneuvers\n\n<p align=\"center\">\n<img title=\"PGM\" src=\"https:\/\/amidst.github.io\/toolbox\/docs\/web\/figs\/cars-crop.png\" width=\"350\">\n<\/p>\n\nAMIDST Toolbox has been used to prototype models for early recognition of traffic maneuver \nintentions. Similarly to the previous case, data is continuously collected by car on-board \nsensors giving rise to a large and quickly evolving data stream. This work has been performed \nin collaboration with one of our partners, DAIMLER. \n\n# Documentation<a name=\"documentation\"><\/a>\n\n<!-- version -->\n* [Getting Started!](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/first-steps-072\/getting-started-072\/) explains how to\ninstall the AMIDST toolbox, how this toolbox makes use of Java 8 new functional style programming\nfeatures, and why it is based on a module based architecture.\n\n* [Toolbox Functionalities](http:\/\/amidst.github.io\/toolbox\/ToolboxFunctionalities.html) describes\nthe main functionalities (i.e., data streams, BNs, DBNs, static and dynamic learning and inference\nengines, etc.) of the AMIDST toolbox.\n\n<!-- version -->\n\n* [Bayesian networks: Code Examples](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/examples-072\/bnetworks-072\/) includes\na list of source code examples explaining how to use some functionalities of the AMIDST toolbox.\n\n<!-- version -->\n* [Dynamic Bayesian networks: Code Examples](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/examples-072\/dbnetworks-072\/)\nincludes some source code examples of functionalities related to Dynamic Bayesian networks.\n\n<!-- version -->\n\n* [FlinkLink](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/examples-072\/flinklink-072\/): Code Examples includes some \nsource code examples of functionalities related to the module that integrates Apache Flink with AMIDST.\n\n<!-- version -->\n\n* [SparkLink](http:\/\/www.amidsttoolbox.com\/documentation\/0-7-2\/examples-072\/sparklink-072\/): some source code examples of \nfunctionalities related to the module that integrates Apache Spark with AMIDST.\n\n* [API JavaDoc](http:\/\/javadoc.amidsttoolbox.com\/) of the AMIDST toolbox. \n\n\n# Scalability\n\n## Multi-Core Scalablity using Java 8 Streams\n\nScalability is a main concern for the AMIDST toolbox. Java 8 streams are used to\nprovide parallel implementations of our learning algorithms. If more computation capacity is needed to process\ndata, AMIDST users can also use more CPU cores. As an example, the following figure shows how\nthe data processing capacity of our toolbox increases given the number of CPU cores when learning an\na probabilistic model (including a class variable C, two latent variables (dashed nodes), multinomial\n(blue nodes) and Gaussian (green nodes) observable variables) using the AMIDST's learning engine.\nAs can be seen, using our variational learning engine, AMIDST toolbox is able to process data in the order\nof gigabytes (GB) per hour depending on the number of available CPU cores with large and complex PGMs with\nlatent variables. Note that, these experiments were carried out on a Ubuntu Linux server with a x86_64\narchitecture and 32 cores. The size of the processed data set was measured according to the\n[Weka](www.cs.waikato.ac.nz\/ml\/weka\/)'s ARFF format.\n\n<p align=\"center\">\n<img src=\"https:\/\/amidst.github.io\/toolbox\/docs\/scalability.png\" width=\"800\">\n<\/p>\n\n\n## Distributed Scalablity using Apache Flink\n\nIf your data is really big and can not be stored in a single laptop, you can also learn \nyour probabilistic model on it by using the AMIDST distributed learning engine based on \na novel and state-of-the-art [distributed message passing scheme](http:\/\/amidst.github.io\/toolbox\/docs\/dVMP.pdf) implemented on top \nof [Apache Flink](http:\/\/flink.com). As detailed in this [paper](http:\/\/amidst.github.io\/toolbox\/docs\/dVMP.pdf), we were able to perform inference in a billion node (i.e. 10^9) probabilistic model in an Amazon's cluster with 2, 4, 8 and 16 nodes, each node containing 8 processing units. The following figure shows the scalability of our approach under these settings. \n\n<p align=\"center\">\n<img src=\"https:\/\/amidst.github.io\/toolbox\/docs\/web\/figs\/flink-scalability.png\" width=\"800\">\n<\/p>\n\n# Spark Link Module on AMIDST\n\nThis module integrates the functionality of the AMIDST toolbox with the [Apache Spark](http:\/\/spark.apache.org) platform.\n\nThe following functionality is already implemented on the **sparklink** module:\n\n* Data Sources integration: Reading and writing data from SparkSQL on AMIDST\n* Distributed Sampling of Bayesian Networks\n* Parametric learning from distributed data (Maximum Likelihood)\n\nMore information [here](https:\/\/github.com\/amidst\/toolbox\/tree\/develop\/sparklink)\n\n# Publications & Use-Cases\n\nThe following repository [https:\/\/github.com\/amidst\/toolbox-usecases](https:\/\/github.com\/amidst\/toolbox-usecases)\ncontains the source code and details about the publications and use-cases using the AMIDST toolbox.\n\n# Upcoming Developments\n\nThe AMIDST toolbox is an expanding project and upcoming developments include for instance the ongoing\nintegration of the toolbox in [Spark](http:\/\/spark.apache.org) to enlarge its scalability capacities.\nIn addition, a new link to [R](http:\/\/static.amidst.eu\/upload\/dokumenter\/Posters\/PosterUseR.pdf)\nis still in progress which will expand the AMIDST user-base.\n\n# Contributing to AMIDST\n\nAMIDST is an open source toolbox and the end-users are encouraged to upload their\ncontributions (which may include basic contributions, major extensions, and\/or use-cases)\nfollowing the indications given in this [link](http:\/\/amidst.github.io\/toolbox\/ContributingToAMIDST.html).\n\n\n\n#\u00a0Acknowledgements and License\nThis software was performed as part of the AMIDST project. AMIDST has received funding from the European Union\u2019s Seventh Framework Programme for research, technological development and demonstration under grant agreement no 619209.\n\nThis software is distributed under Apache License Version 2.0\n","49":"# MarkovComposer\n\n## Abstract\nAlgorithms, or algorithmic composition, have been used to compose music for centuries. For example, Western [punctus contra punctum](https:\/\/en.wikipedia.org\/wiki\/Counterpoint) can be sometimes reduced to algorithmic determinacy. Then, why not use fast-learning computers capable of billions of calculations per second to do what they do best, to follow algorithms?\n\n## Markov chain?\n[Markov chain](https:\/\/en.wikipedia.org\/wiki\/Markov_chain), named after Andrey Andreyevich Markov, is a (pseudo)random process of transition from one state to another. The transition is \"memoryless\" and it only depends on the current state and on the probabilities (saved in a so-called transition matrix). Sequence of events that preceeded the current state should in no way determine the transition. This \"memorylessness\" is also called Markov property. In short, transiting from one state to another is a random process based on probability.\n\n## The general idea\nMarkov chain is just plain perfect for algorithmic music compositions. Notes (128 of them) are used as possible states. For the implementation, I'm using a second order Markov chain, meaning two previous states (two previous notes) determine the next state and nothing else. All of the transition probabilites are stored in a 2^14x2^7 matrix. As input, the composer takes two integer values (0 <= n, m <= 127) representing 2 starting notes. Based on that the algorithm calculates\/generates the next note and the generation process goes *ad infinitum* (until you stop it, that is). Pitch of the notes and spacing between two notes is also stored in the Markov chain.\n\n## Samples\nJust an example what can Markov composer do. Based on a little bit of testing, there are probably a lot more, a lot better generated compositions.\n\n* [Piano 1](http:\/\/zx.rs\/mp3\/Piano1.mp3)\n* [Piano 2](http:\/\/zx.rs\/mp3\/Piano2.mp3)\n* [Piano 3](http:\/\/zx.rs\/mp3\/Piano3.mp3)\n* [Piano 4](http:\/\/zx.rs\/mp3\/Piano4.mp3)\n\n## Live demonstration\nIf you want to see MarkovComposer in action, but you don't want to mess with the Java code, you can access a web version of it [here](http:\/\/markov.zx.rs\/). Source of the web version is avaiable [here](https:\/\/github.com\/anbud\/MarkovComposerWeb).\n\n## Implementation\nA detailed explanation can be found in the following blog post: [Markov Composer - Using machine learning and a Markov chain to compose music](http:\/\/zx.rs\/3\/Markov-Composer---Using-machine-learning-and-a-Markov-chain-to-compose-music\/)\n","50":"<div align=\"center\">\n  <img src=\"https:\/\/www.tensorflow.org\/images\/tf_logo_horizontal.png\">\n<\/div>\n\n[![Python](https:\/\/img.shields.io\/pypi\/pyversions\/tensorflow.svg?style=plastic)](https:\/\/badge.fury.io\/py\/tensorflow)\n[![PyPI](https:\/\/badge.fury.io\/py\/tensorflow.svg)](https:\/\/badge.fury.io\/py\/tensorflow)\n[![DOI](https:\/\/zenodo.org\/badge\/DOI\/10.5281\/zenodo.4724125.svg)](https:\/\/doi.org\/10.5281\/zenodo.4724125)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https:\/\/img.shields.io\/badge\/api-reference-blue.svg)](https:\/\/www.tensorflow.org\/api_docs\/) |\n\n[TensorFlow](https:\/\/www.tensorflow.org\/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https:\/\/www.tensorflow.org\/resources\/tools),\n[libraries](https:\/\/www.tensorflow.org\/resources\/libraries-extensions), and\n[community](https:\/\/www.tensorflow.org\/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working on the\nGoogle Brain team within Google's Machine Intelligence Research organization to\nconduct machine learning and deep neural networks research. The system is\ngeneral enough to be applicable in a wide variety of other domains, as well.\n\nTensorFlow provides stable [Python](https:\/\/www.tensorflow.org\/api_docs\/python)\nand [C++](https:\/\/www.tensorflow.org\/api_docs\/cc) APIs, as well as\nnon-guaranteed backward compatible API for\n[other languages](https:\/\/www.tensorflow.org\/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https:\/\/groups.google.com\/a\/tensorflow.org\/forum\/#!forum\/announce).\nSee all the [mailing lists](https:\/\/www.tensorflow.org\/community\/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https:\/\/www.tensorflow.org\/install) for the\n[pip package](https:\/\/www.tensorflow.org\/install\/pip), to\n[enable GPU support](https:\/\/www.tensorflow.org\/install\/gpu), use a\n[Docker container](https:\/\/www.tensorflow.org\/install\/docker), and\n[build from source](https:\/\/www.tensorflow.org\/install\/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https:\/\/www.tensorflow.org\/install\/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https:\/\/pypi.python.org\/pypi\/tf-nightly) and\n[tf-nightly-cpu](https:\/\/pypi.python.org\/pypi\/tf-nightly-cpu) packages on PyPi.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\nb'Hello, TensorFlow!'\n```\n\nFor more examples, see the\n[TensorFlow tutorials](https:\/\/www.tensorflow.org\/tutorials\/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https:\/\/github.com\/tensorflow\/tensorflow\/issues) for\ntracking requests and bugs, please see\n[TensorFlow Discuss](https:\/\/groups.google.com\/a\/tensorflow.org\/forum\/#!forum\/discuss)\nfor general questions and discussion, and please direct specific questions to\n[Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development:\n\n[![Fuzzing Status](https:\/\/oss-fuzz-build-logs.storage.googleapis.com\/badges\/tensorflow.svg)](https:\/\/bugs.chromium.org\/p\/oss-fuzz\/issues\/list?sort=-opened&can=1&q=proj:tensorflow)\n[![CII Best Practices](https:\/\/bestpractices.coreinfrastructure.org\/projects\/1486\/badge)](https:\/\/bestpractices.coreinfrastructure.org\/projects\/1486)\n[![Contributor Covenant](https:\/\/img.shields.io\/badge\/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n\n## Continuous build status\n\nYou can find more community-supported platforms and configurations in the\n[TensorFlow SIG Build community builds table](https:\/\/github.com\/tensorflow\/build#community-supported-tensorflow-builds).\n\n### Official Builds\n\nBuild Type                    | Status                                                                                                                                                                           | Artifacts\n----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux CPU**                 | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/ubuntu-cc.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/ubuntu-cc.html)           | [PyPI](https:\/\/pypi.org\/project\/tf-nightly\/)\n**Linux GPU**                 | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/ubuntu-gpu-py3.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/ubuntu-gpu-py3.html) | [PyPI](https:\/\/pypi.org\/project\/tf-nightly-gpu\/)\n**Linux XLA**                 | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/ubuntu-xla.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/ubuntu-xla.html)         | TBA\n**macOS**                     | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/macos-py2-cc.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/macos-py2-cc.html)     | [PyPI](https:\/\/pypi.org\/project\/tf-nightly\/)\n**Windows CPU**               | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/windows-cpu.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/windows-cpu.html)       | [PyPI](https:\/\/pypi.org\/project\/tf-nightly\/)\n**Windows GPU**               | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/windows-gpu.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/windows-gpu.html)       | [PyPI](https:\/\/pypi.org\/project\/tf-nightly-gpu\/)\n**Android**                   | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/android.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/android.html)               | [Download](https:\/\/bintray.com\/google\/tensorflow\/tensorflow\/_latestVersion)\n**Raspberry Pi 0 and 1**      | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/rpi01-py3.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/rpi01-py3.html)           | [Py3](https:\/\/storage.googleapis.com\/tensorflow-nightly\/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)\n**Raspberry Pi 2 and 3**      | [![Status](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/rpi23-py3.svg)](https:\/\/storage.googleapis.com\/tensorflow-kokoro-build-badges\/rpi23-py3.html)           | [Py3](https:\/\/storage.googleapis.com\/tensorflow-nightly\/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\n**Libtensorflow MacOS CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https:\/\/storage.googleapis.com\/libtensorflow-nightly\/prod\/tensorflow\/release\/macos\/latest\/macos_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https:\/\/storage.googleapis.com\/tensorflow\/)\n**Libtensorflow Linux CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https:\/\/storage.googleapis.com\/libtensorflow-nightly\/prod\/tensorflow\/release\/ubuntu_16\/latest\/cpu\/ubuntu_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https:\/\/storage.googleapis.com\/tensorflow\/)\n**Libtensorflow Linux GPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https:\/\/storage.googleapis.com\/libtensorflow-nightly\/prod\/tensorflow\/release\/ubuntu_16\/latest\/gpu\/ubuntu_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https:\/\/storage.googleapis.com\/tensorflow\/)\n**Libtensorflow Windows CPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https:\/\/storage.googleapis.com\/libtensorflow-nightly\/prod\/tensorflow\/release\/windows\/latest\/cpu\/windows_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https:\/\/storage.googleapis.com\/tensorflow\/)\n**Libtensorflow Windows GPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https:\/\/storage.googleapis.com\/libtensorflow-nightly\/prod\/tensorflow\/release\/windows\/latest\/gpu\/windows_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https:\/\/storage.googleapis.com\/tensorflow\/)\n\n## Resources\n\n*   [TensorFlow.org](https:\/\/www.tensorflow.org)\n*   [TensorFlow Tutorials](https:\/\/www.tensorflow.org\/tutorials\/)\n*   [TensorFlow Official Models](https:\/\/github.com\/tensorflow\/models\/tree\/master\/official)\n*   [TensorFlow Examples](https:\/\/github.com\/tensorflow\/examples)\n*   [DeepLearning.AI TensorFlow Developer Professional Certificate](https:\/\/www.coursera.org\/specializations\/tensorflow-in-practice)\n*   [TensorFlow: Data and Deployment from Coursera](https:\/\/www.coursera.org\/specializations\/tensorflow-data-and-deployment)\n*   [Getting Started with TensorFlow 2 from Coursera](https:\/\/www.coursera.org\/learn\/getting-started-with-tensor-flow2)\n*   [TensorFlow: Advanced Techniques from Coursera](https:\/\/www.coursera.org\/specializations\/tensorflow-advanced-techniques)\n*   [TensorFlow 2 for Deep Learning Specialization from Coursera](https:\/\/www.coursera.org\/specializations\/tensorflow2-deeplearning)\n*   [Intro to TensorFlow for A.I, M.L, and D.L from Coursera](https:\/\/www.coursera.org\/learn\/introduction-tensorflow)\n*   [Intro to TensorFlow for Deep Learning from Udacity](https:\/\/www.udacity.com\/course\/intro-to-tensorflow-for-deep-learning--ud187)\n*   [Introduction to TensorFlow Lite from Udacity](https:\/\/www.udacity.com\/course\/intro-to-tensorflow-lite--ud190)\n*   [Machine Learning with TensorFlow on GCP](https:\/\/www.coursera.org\/specializations\/machine-learning-tensorflow-gcp)\n*   [TensorFlow Codelabs](https:\/\/codelabs.developers.google.com\/?cat=TensorFlow)\n*   [TensorFlow Blog](https:\/\/blog.tensorflow.org)\n*   [Learn ML with TensorFlow](https:\/\/www.tensorflow.org\/resources\/learn-ml)\n*   [TensorFlow Twitter](https:\/\/twitter.com\/tensorflow)\n*   [TensorFlow YouTube](https:\/\/www.youtube.com\/channel\/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow model optimization roadmap](https:\/\/www.tensorflow.org\/model_optimization\/guide\/roadmap)\n*   [TensorFlow White Papers](https:\/\/www.tensorflow.org\/about\/bib)\n*   [TensorBoard Visualization Toolkit](https:\/\/github.com\/tensorflow\/tensorboard)\n\nLearn more about the\n[TensorFlow community](https:\/\/www.tensorflow.org\/community) and how to\n[contribute](https:\/\/www.tensorflow.org\/community\/contribute).\n\n## License\n\n[Apache License 2.0](LICENSE)\n","51":"# The Algorithms - C++ # {#mainpage}\n<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site -->\n\n[![Gitpod Ready-to-Code](https:\/\/img.shields.io\/badge\/Gitpod-Ready--to--Code-blue?logo=gitpod)](https:\/\/gitpod.io\/#https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus)\n[![Language grade: C\/C++](https:\/\/img.shields.io\/lgtm\/grade\/cpp\/g\/TheAlgorithms\/C-Plus-Plus.svg?logo=lgtm&logoWidth=18)](https:\/\/lgtm.com\/projects\/g\/TheAlgorithms\/C-Plus-Plus\/context:cpp)\n[![CodeQL CI](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/actions\/workflows\/codeql_analysis.yml\/badge.svg)](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/actions\/workflows\/codeql_analysis.yml)\n[![Gitter chat](https:\/\/img.shields.io\/badge\/Chat-Gitter-ff69b4.svg?label=Chat&logo=gitter&style=flat-square)](https:\/\/gitter.im\/TheAlgorithms)\n[![contributions welcome](https:\/\/img.shields.io\/static\/v1.svg?label=Contributions&message=Welcome&color=0059b3&style=flat-square)](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/blob\/master\/CONTRIBUTING.md)\n![GitHub repo size](https:\/\/img.shields.io\/github\/repo-size\/TheAlgorithms\/C-Plus-Plus?color=red&style=flat-square)\n[![Doxygen CI](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/workflows\/Doxygen%20CI\/badge.svg)](https:\/\/TheAlgorithms.github.io\/C-Plus-Plus)\n[![Awesome CI](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/workflows\/Awesome%20CI%20Workflow\/badge.svg)](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/actions?query=workflow%3A%22Awesome+CI+Workflow%22)\n[![Income](https:\/\/img.shields.io\/liberapay\/receives\/TheAlgorithms.svg?logo=liberapay)](https:\/\/liberapay.com\/TheAlgorithms)\n[![Discord chat](https:\/\/img.shields.io\/discord\/808045925556682782.svg?logo=discord&colorB=5865F2)](https:\/\/discord.gg\/c7MnfGFGa6)\n[![Donate](https:\/\/liberapay.com\/assets\/widgets\/donate.svg)](https:\/\/liberapay.com\/TheAlgorithms\/donate)\n\n## Overview\n\nThis repository is a collection of open-source implementation of a variety of algorithms implemented in C++ and licensed under [MIT License](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/blob\/master\/LICENSE). These algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and the associated documentation are meant to provide a learning resource for educators and students. Hence, one may find more than one implementation for the same objective but using a different algorithm strategies and optimizations.\n\n## Features\n\n* The repository provides implementations of various algorithms in one of the most fundamental general purpose languages - [C++](https:\/\/en.wikipedia.org\/wiki\/C%2B%2B).\n* Well documented source code with detailed explanations provide a valuable resource for educators and students alike.\n* Each source code is atomic using [STL classes](https:\/\/en.wikipedia.org\/wiki\/Standard_Template_Library) and _no external libraries_ are required for their compilation and execution. Thus, the fundamentals of the algorithms can be studied in much depth.\n* Source codes are [compiled and tested](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/actions?query=workflow%3A%22Awesome+CI+Workflow%22) for every commit on the latest versions of three major operating systems viz., Windows, MacOS and Ubuntu (Linux) using MSVC 16 2019, AppleClang 11.0 and GNU 7.5.0 respectively. \n* Strict adherence to [C++11](https:\/\/en.wikipedia.org\/wiki\/C%2B%2B11) standard ensures portability of code to embedded systems as well like ESP32, ARM Cortex, etc. with little to no changes.\n* Self-checks within programs ensure correct implementations with confidence.\n* Modular implementations and OpenSource licensing enable the functions to be utilized conveniently in other applications.\n\n## Documentation\n\n[Online Documentation](https:\/\/TheAlgorithms.github.io\/C-Plus-Plus) is generated from the repository source codes directly. The documentation contains all resources including source code snippets, details on execution of the programs, diagrammatic representation of program flow, and links to external resources where necessary. The documentation also introduces interactive source code with links to documentation for C++ STL library functions used.\nClick on [Files menu](https:\/\/TheAlgorithms.github.io\/C-Plus-Plus\/files.html) to see the list of all the files documented with the code.\n\n[Documentation of Algorithms in C++](https:\/\/thealgorithms.github.io\/C-Plus-Plus) by [The Algorithms Contributors](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/graphs\/contributors) is licensed under [CC BY-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/?ref=chooser-v1)<br\/>\n<a href=\"https:\/\/creativecommons.org\/licenses\/by-sa\/4.0\"><img alt=\"Creative Commons License\" style=\"height:22px!important;margin-left: 3px;vertical-align:text-bottom;\" src=\"https:\/\/mirrors.creativecommons.org\/presskit\/icons\/cc.svg\" \/><img  alt=\"Credit must be given to the creator\" style=\"height:22px!important;margin-left: 3px;vertical-align:text-bottom;\" src=\"https:\/\/mirrors.creativecommons.org\/presskit\/icons\/by.svg\" \/><img alt=\"Adaptations must be shared under the same terms\" style=\"height:22px!important;margin-left: 3px;vertical-align:text-bottom;\" src=\"https:\/\/mirrors.creativecommons.org\/presskit\/icons\/sa.svg\" \/><\/a>\n\n## Contributions\n\nAs a community developed and maintained repository, we welcome new un-plagiarized quality contributions. Please read our [Contribution Guidelines](https:\/\/github.com\/TheAlgorithms\/C-Plus-Plus\/blob\/master\/CONTRIBUTING.md).\n","52":"<p align=\"center\">\n<img align=\"center\" src=\"doc\/imgs\/logo.png\", width=1600>\n<p>\n    \n--------------------------------------------------------------------------------\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](.\/README_cn.md)\n\n[![Build Status](https:\/\/travis-ci.org\/PaddlePaddle\/Paddle.svg?branch=develop)](https:\/\/travis-ci.org\/PaddlePaddle\/Paddle)\n[![Documentation Status](https:\/\/img.shields.io\/badge\/docs-latest-brightgreen.svg?style=flat)](https:\/\/paddlepaddle.org.cn\/documentation\/docs\/en\/guides\/index_en.html)\n[![Documentation Status](https:\/\/img.shields.io\/badge\/\u4e2d\u6587\u6587\u6863-\u6700\u65b0-brightgreen.svg)](https:\/\/paddlepaddle.org.cn\/documentation\/docs\/zh\/guides\/index_cn.html)\n[![Release](https:\/\/img.shields.io\/github\/release\/PaddlePaddle\/Paddle.svg)](https:\/\/github.com\/PaddlePaddle\/Paddle\/releases)\n[![License](https:\/\/img.shields.io\/badge\/license-Apache%202-blue.svg)](LICENSE)\n\nWelcome to the PaddlePaddle GitHub.\n\nPaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning frameworks, basic model libraries, end-to-end development kits, tools & components as well as service platforms.\nPaddlePaddle is originated from industrial practices with dedication and commitments to industrialization. It has been widely adopted by a wide range of sectors including manufacturing, agriculture, enterprise service, and so on while serving more than 4 million developers, 157,000 companies and generating 476,000 models. With such advantages, PaddlePaddle has helped an increasing number of partners commercialize AI.\n\n\n## Installation\n\n### Latest PaddlePaddle Release: [v2.3](https:\/\/github.com\/PaddlePaddle\/Paddle\/tree\/release\/2.3)\n\nOur vision is to enable deep learning for everyone via PaddlePaddle.\nPlease refer to our [release announcement](https:\/\/github.com\/PaddlePaddle\/Paddle\/releases) to track the latest features of PaddlePaddle.\n### Install Latest Stable Release:\n```\n# CPU\npip install paddlepaddle\n# GPU\npip install paddlepaddle-gpu\n\n```\nFor more information about installation, please view [Quick Install](https:\/\/www.paddlepaddle.org.cn\/install\/quick)\n\nNow our developers can acquire Tesla V100 online computing resources for free. If you create a program by AI Studio, you will obtain 8 hours to train models online per day. [Click here to start](https:\/\/aistudio.baidu.com\/aistudio\/index).\n\n## FOUR LEADING TECHNOLOGIES\n\n- **Agile Framework for Industrial Development of Deep Neural Networks**\n\n    The PaddlePaddle deep learning framework facilitates the development while lowering the technical burden, through leveraging a programmable scheme to architect the neural networks. It supports both declarative programming and imperative programming with both development flexibility and high runtime performance preserved.  The neural architectures could be automatically designed by algorithms with better performance than the ones designed by human experts.\n\n\n-  **Support Ultra-Large-Scale Training of Deep Neural Networks**\n\n    PaddlePaddle has made breakthroughs in ultra-large-scale deep neural networks training. It launched the world's first large-scale open-source training platform that supports the training of deep networks with 100 billion features and trillions of parameters using data sources distributed over hundreds of nodes. PaddlePaddle overcomes the online deep learning challenges for ultra-large-scale deep learning models, and further achieved real-time model updating with more than 1 trillion parameters.\n     [Click here to learn more](https:\/\/github.com\/PaddlePaddle\/Fleet)\n\n\n- **High-Performance Inference Engines for Comprehensive Deployment Environments**\n\n   PaddlePaddle is not only compatible with models trained in 3rd party open-source frameworks , but also offers complete inference products for various production scenarios. Our inference product line includes [Paddle Inference](https:\/\/paddle-inference.readthedocs.io\/en\/latest\/product_introduction\/summary.html): Native inference library for high-performance server and cloud inference; [Paddle Serving](https:\/\/github.com\/PaddlePaddle\/Serving): A service-oriented framework suitable for distributed and pipeline productions; [Paddle Lite](https:\/\/github.com\/PaddlePaddle\/Paddle-Lite): Ultra-Lightweight inference engine for mobile and IoT environments; [Paddle.js](https:\/\/www.paddlepaddle.org.cn\/paddle\/paddlejs): A frontend inference engine for browser and mini-apps. Furthermore, by great amounts of optimization with leading hardware in each scenario, Paddle inference engines outperform most of the other mainstream frameworks.\n     \n     \n- **Industry-Oriented Models and Libraries with Open Source Repositories**\n\n     PaddlePaddle includes and maintains more than 100 mainstream models that have been practiced and polished for a long time in the industry. Some of these models have won major prizes from key international competitions. In the meanwhile, PaddlePaddle has further more than 200 pre-training models (some of them with source codes) to facilitate the rapid development of industrial applications.\n     [Click here to learn more](https:\/\/github.com\/PaddlePaddle\/models)\n     \n\n## Documentation\n\nWe provide [English](https:\/\/www.paddlepaddle.org.cn\/documentation\/docs\/en\/guides\/index_en.html) and\n[Chinese](https:\/\/www.paddlepaddle.org.cn\/documentation\/docs\/zh\/guide\/index_cn.html) documentation.\n\n- [Guides](https:\/\/www.paddlepaddle.org.cn\/documentation\/docs\/en\/guides\/index_en.html)\n\n  You might want to start from how to implement deep learning basics with PaddlePaddle.\n\n- [Practice](https:\/\/www.paddlepaddle.org.cn\/documentation\/docs\/zh\/tutorial\/index_cn.html)\n\n  So far you have already been familiar with Fluid. And the next step should be building a more efficient model or inventing your original Operator. \n\n- [API Reference](https:\/\/www.paddlepaddle.org.cn\/documentation\/docs\/en\/api\/index_en.html)\n\n   Our new API enables much shorter programs.\n\n- [How to Contribute](https:\/\/www.paddlepaddle.org.cn\/documentation\/docs\/en\/guides\/08_contribution\/index_en.html)\n\n   We appreciate your contributions!\n\n## Communication\n\n- [Github Issues](https:\/\/github.com\/PaddlePaddle\/Paddle\/issues): bug reports, feature requests, install issues, usage issues, etc.\n- QQ discussion group: 441226485 (PaddlePaddle).\n- [Forums](https:\/\/ai.baidu.com\/forum\/topic\/list\/168?pageNo=1): discuss implementations, research, etc.\n    \n## Courses\n\n- [Server Deployments](https:\/\/aistudio.baidu.com\/aistudio\/course\/introduce\/19084): Courses intorducing high performance server deployments via local and remote services.\n- [Edge Deployments](https:\/\/aistudio.baidu.com\/aistudio\/course\/introduce\/22690): Courses intorducing edge deployments from mobile, IoT to web and applets.   \n\n## Copyright and License\nPaddlePaddle is provided under the [Apache-2.0 license](LICENSE).\n","53":"<img src=https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/logo\/LightGBM_logo_black_text.svg width=300 \/>\n\nLight Gradient Boosting Machine\n===============================\n\n[![Python-package GitHub Actions Build Status](https:\/\/github.com\/microsoft\/LightGBM\/workflows\/Python-package\/badge.svg?branch=master)](https:\/\/github.com\/microsoft\/LightGBM\/actions)\n[![R-package GitHub Actions Build Status](https:\/\/github.com\/microsoft\/LightGBM\/workflows\/R-package\/badge.svg?branch=master)](https:\/\/github.com\/microsoft\/LightGBM\/actions)\n[![CUDA Version GitHub Actions Build Status](https:\/\/github.com\/microsoft\/LightGBM\/workflows\/CUDA%20Version\/badge.svg?branch=master)](https:\/\/github.com\/microsoft\/LightGBM\/actions)\n[![Static Analysis GitHub Actions Build Status](https:\/\/github.com\/microsoft\/LightGBM\/workflows\/Static%20Analysis\/badge.svg?branch=master)](https:\/\/github.com\/microsoft\/LightGBM\/actions)\n[![Azure Pipelines Build Status](https:\/\/lightgbm-ci.visualstudio.com\/lightgbm-ci\/_apis\/build\/status\/Microsoft.LightGBM?branchName=master)](https:\/\/lightgbm-ci.visualstudio.com\/lightgbm-ci\/_build\/latest?definitionId=1)\n[![Appveyor Build Status](https:\/\/ci.appveyor.com\/api\/projects\/status\/1ys5ot401m0fep6l\/branch\/master?svg=true)](https:\/\/ci.appveyor.com\/project\/guolinke\/lightgbm\/branch\/master)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/lightgbm\/badge\/?version=latest)](https:\/\/lightgbm.readthedocs.io\/)\n[![Link checks](https:\/\/github.com\/microsoft\/LightGBM\/workflows\/Link%20checks\/badge.svg)](https:\/\/github.com\/microsoft\/LightGBM\/actions?query=workflow%3A%22Link+checks%22)\n[![License](https:\/\/img.shields.io\/github\/license\/microsoft\/lightgbm.svg)](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/LICENSE)\n[![Python Versions](https:\/\/img.shields.io\/pypi\/pyversions\/lightgbm.svg?logo=python&logoColor=white)](https:\/\/pypi.org\/project\/lightgbm)\n[![PyPI Version](https:\/\/img.shields.io\/pypi\/v\/lightgbm.svg?logo=pypi&logoColor=white)](https:\/\/pypi.org\/project\/lightgbm)\n[![CRAN Version](https:\/\/www.r-pkg.org\/badges\/version\/lightgbm)](https:\/\/cran.r-project.org\/package=lightgbm)\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n- Faster training speed and higher efficiency.\n- Lower memory usage.\n- Better accuracy.\n- Support of parallel, distributed, and GPU learning.\n- Capable of handling large-scale data.\n\nFor further details, please refer to [Features](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Features.rst).\n\nBenefiting from these advantages, LightGBM is being widely-used in many [winning solutions](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/examples\/README.md#machine-learning-challenge-winning-solutions) of machine learning competitions.\n\n[Comparison experiments](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Experiments.rst#comparison-experiment) on public datasets show that LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. What's more, [distributed learning experiments](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Experiments.rst#parallel-experiment) show that LightGBM can achieve a linear speed-up by using multiple machines for training in specific settings.\n\nGet Started and Documentation\n-----------------------------\n\nOur primary documentation is at https:\/\/lightgbm.readthedocs.io\/ and is generated from this repository. If you are new to LightGBM, follow [the installation instructions](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Installation-Guide.html) on that site.\n\nNext you may want to read:\n\n- [**Examples**](https:\/\/github.com\/microsoft\/LightGBM\/tree\/master\/examples) showing command line usage of common tasks.\n- [**Features**](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Features.rst) and algorithms supported by LightGBM.\n- [**Parameters**](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst) is an exhaustive list of customization you can make.\n- [**Distributed Learning**](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parallel-Learning-Guide.rst) and [**GPU Learning**](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst) can speed up computation.\n- [**Laurae++ interactive documentation**](https:\/\/sites.google.com\/view\/lauraepp\/parameters) is a detailed guide for hyperparameters.\n- [**FLAML**](https:\/\/www.microsoft.com\/en-us\/research\/project\/fast-and-lightweight-automl-for-large-scale-data\/articles\/flaml-a-fast-and-lightweight-automl-library\/) provides automated tuning for LightGBM ([code examples](https:\/\/microsoft.github.io\/FLAML\/docs\/Examples\/AutoML-for-LightGBM\/)).\n- [**Optuna Hyperparameter Tuner**](https:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258) provides automated tuning for LightGBM hyperparameters ([code examples](https:\/\/github.com\/optuna\/optuna\/tree\/master\/examples\/lightgbm)).\n- [**Understanding LightGBM Parameters (and How to Tune Them using Neptune)**](https:\/\/neptune.ai\/blog\/lightgbm-parameters-guide).\n\nDocumentation for contributors:\n\n- [**How we update readthedocs.io**](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/README.rst).\n- Check out the [**Development Guide**](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Development-Guide.rst).\n\nNews\n----\n\nPlease refer to changelogs at [GitHub releases](https:\/\/github.com\/microsoft\/LightGBM\/releases) page.\n\nSome old update logs are available at [Key Events](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Key-Events.md) page.\n\nExternal (Unofficial) Repositories\n----------------------------------\n\nFLAML (AutoML library for hyperparameter optimization): https:\/\/github.com\/microsoft\/FLAML\n\nOptuna (hyperparameter optimization framework): https:\/\/github.com\/optuna\/optuna\n\nJulia-package: https:\/\/github.com\/IQVIA-ML\/LightGBM.jl\n\nJPMML (Java PMML converter): https:\/\/github.com\/jpmml\/jpmml-lightgbm\n\nNyoka (Python PMML converter): https:\/\/github.com\/SoftwareAG\/nyoka\n\nTreelite (model compiler for efficient deployment): https:\/\/github.com\/dmlc\/treelite\n\nlleaves (LLVM-based model compiler for efficient inference): https:\/\/github.com\/siboehm\/lleaves\n\nHummingbird (model compiler into tensor computations): https:\/\/github.com\/microsoft\/hummingbird\n\ncuML Forest Inference Library (GPU-accelerated inference): https:\/\/github.com\/rapidsai\/cuml\n\ndaal4py (Intel CPU-accelerated inference): https:\/\/github.com\/intel\/scikit-learn-intelex\/tree\/master\/daal4py\n\nm2cgen (model appliers for various languages): https:\/\/github.com\/BayesWitnesses\/m2cgen\n\nleaves (Go model applier): https:\/\/github.com\/dmitryikh\/leaves\n\nONNXMLTools (ONNX converter): https:\/\/github.com\/onnx\/onnxmltools\n\nSHAP (model output explainer): https:\/\/github.com\/slundberg\/shap\n\nShapash (model visualization and interpretation): https:\/\/github.com\/MAIF\/shapash\n\ndtreeviz (decision tree visualization and model interpretation): https:\/\/github.com\/parrt\/dtreeviz\n\nSynapseML (LightGBM on Spark): https:\/\/github.com\/microsoft\/SynapseML\n\nKubeflow Fairing (LightGBM on Kubernetes): https:\/\/github.com\/kubeflow\/fairing\n\nKubeflow Operator (LightGBM on Kubernetes): https:\/\/github.com\/kubeflow\/xgboost-operator\n\nlightgbm_ray (LightGBM on Ray): https:\/\/github.com\/ray-project\/lightgbm_ray\n\nMars (LightGBM on Mars): https:\/\/github.com\/mars-project\/mars\n\nML.NET (.NET\/C#-package): https:\/\/github.com\/dotnet\/machinelearning\n\nLightGBM.NET (.NET\/C#-package): https:\/\/github.com\/rca22\/LightGBM.Net\n\nRuby gem: https:\/\/github.com\/ankane\/lightgbm-ruby\n\nLightGBM4j (Java high-level binding): https:\/\/github.com\/metarank\/lightgbm4j\n\nlightgbm-rs (Rust binding): https:\/\/github.com\/vaaaaanquish\/lightgbm-rs\n\nMLflow (experiment tracking, model monitoring framework): https:\/\/github.com\/mlflow\/mlflow\n\n`{treesnip}` (R `{parsnip}`-compliant interface): https:\/\/github.com\/curso-r\/treesnip\n\n`{mlr3extralearners}` (R `{mlr3}`-compliant interface): https:\/\/github.com\/mlr-org\/mlr3extralearners\n\nlightgbm-transform (feature transformation binding): https:\/\/github.com\/microsoft\/lightgbm-transform\n\nSupport\n-------\n\n- Ask a question [on Stack Overflow with the `lightgbm` tag](https:\/\/stackoverflow.com\/questions\/ask?tags=lightgbm), we monitor this for new questions.\n- Open **bug reports** and **feature requests** (not questions) on [GitHub issues](https:\/\/github.com\/microsoft\/LightGBM\/issues).\n\nHow to Contribute\n-----------------\n\nCheck [CONTRIBUTING](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/CONTRIBUTING.md) page.\n\nMicrosoft Open Source Code of Conduct\n-------------------------------------\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https:\/\/opensource.microsoft.com\/codeofconduct\/). For more information see the [Code of Conduct FAQ](https:\/\/opensource.microsoft.com\/codeofconduct\/faq\/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nReference Papers\n----------------\n\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. \"[LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https:\/\/papers.nips.cc\/paper\/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)\". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.\n\nQi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. \"[A Communication-Efficient Parallel Algorithm for Decision Tree](http:\/\/papers.nips.cc\/paper\/6380-a-communication-efficient-parallel-algorithm-for-decision-tree)\". Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.\n\nHuan Zhang, Si Si and Cho-Jui Hsieh. \"[GPU Acceleration for Large-scale Tree Boosting](https:\/\/arxiv.org\/abs\/1706.08359)\". SysML Conference, 2018.\n\n**Note**: If you use LightGBM in your GitHub projects, please add `lightgbm` in the `requirements.txt`.\n\nLicense\n-------\n\nThis project is licensed under the terms of the MIT license. See [LICENSE](https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/LICENSE) for additional details.\n","54":"<!--- SPDX-License-Identifier: Apache-2.0 -->\n\n<p align=\"center\"><img width=\"40%\" src=\"https:\/\/github.com\/onnx\/onnx\/raw\/main\/docs\/ONNX_logo_main.png\" \/><\/p>\n\n\n[![Build Status](https:\/\/dev.azure.com\/onnx-pipelines\/onnx\/_apis\/build\/status\/Windows-CI?branchName=main)](https:\/\/dev.azure.com\/onnx-pipelines\/onnx\/_build\/latest?definitionId=5&branchName=main)\n[![Build Status](https:\/\/dev.azure.com\/onnx-pipelines\/onnx\/_apis\/build\/status\/Linux-CI?branchName=main)](https:\/\/dev.azure.com\/onnx-pipelines\/onnx\/_build\/latest?definitionId=7&branchName=main)\n[![Build Status](https:\/\/dev.azure.com\/onnx-pipelines\/onnx\/_apis\/build\/status\/MacOS-CI?branchName=main)](https:\/\/dev.azure.com\/onnx-pipelines\/onnx\/_build\/latest?definitionId=6&branchName=main)\n[![CII Best Practices](https:\/\/bestpractices.coreinfrastructure.org\/projects\/3313\/badge)](https:\/\/bestpractices.coreinfrastructure.org\/projects\/3313)\n\n[Open Neural Network Exchange (ONNX)](https:\/\/onnx.ai) is an open ecosystem that empowers AI developers\nto choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard\ndata types. Currently we focus on the capabilities needed for inferencing (scoring).\n\nONNX is [widely supported](http:\/\/onnx.ai\/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.\n\n# Use ONNX\n* [Tutorials for creating ONNX models](https:\/\/github.com\/onnx\/tutorials).\n* [Pre-trained ONNX models](https:\/\/github.com\/onnx\/models)\n\n# Learn about the ONNX spec\n* [Overview](docs\/Overview.md)\n* [ONNX intermediate representation spec](docs\/IR.md)\n* [Versioning principles of the spec](docs\/Versioning.md)\n* [Operators documentation](docs\/Operators.md)\n* [Python API Overview](docs\/PythonAPIOverview.md)\n\n# Programming utilities for working with ONNX Graphs\n* [Shape and Type Inference](docs\/ShapeInference.md)\n* [Graph Optimization](https:\/\/github.com\/onnx\/optimizer)\n* [Opset Version Conversion](docs\/VersionConverter.md)\n\n# NOTICE: ONNX now uses main branch as default branch\nHere are the [steps](https:\/\/github.com\/onnx\/onnx\/wiki\/How-to-migrate-to-main-branch-in-local-repo) from ONNX wiki for migrating to main branch in local repo.\n\n# Contribute\nONNX is a [community project](community\/readme.md). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [Special Interest Groups](community\/sigs.md) and [Working Groups](community\/working-groups.md) to shape the future of ONNX.\n\nCheck out our [contribution guide](docs\/CONTRIBUTING.md) to get started.\n\nIf you think some operator should be added to ONNX specification, please read\n[this document](docs\/AddNewOp.md).\n\n# Discuss\nWe encourage you to open [Issues](https:\/\/github.com\/onnx\/onnx\/issues), or use [Slack](https:\/\/lfaifoundation.slack.com\/) (If you have not joined yet, please use this [link](https:\/\/join.slack.com\/t\/lfaifoundation\/shared_invite\/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA) to join the group) for more real-time discussion.\n\n# Follow Us\nStay up to date with the latest ONNX news. [[Facebook](https:\/\/www.facebook.com\/onnxai\/)] [[Twitter](https:\/\/twitter.com\/onnxai)]\n\n\n\n\n\n\n# Installation\n\n## Prerequisites\n\n```\nnumpy >= 1.16.6\nprotobuf >= 3.12.2\ntyping-extensions >= 3.6.2.1\n```\n\n## Official Python packages\nONNX released packages are published in PyPi.\n```\npip install numpy protobuf==3.16.0\npip install onnx\n```\n\n[Weekly packages](https:\/\/test.pypi.org\/project\/onnx-weekly\/) are published in test pypi to enable experimentation and early testing.\n\n\n## Conda packages\nA binary build of ONNX is available from [Conda](https:\/\/conda.io), in [conda-forge](https:\/\/conda-forge.org\/):\n```\nconda install -c conda-forge numpy protobuf==3.16.0 libprotobuf=3.16.0\nconda install -c conda-forge onnx\n```\n\nYou can also use the [onnx-dev docker image](https:\/\/hub.docker.com\/r\/onnx\/onnx-dev) for a Linux-based installation without having to worry about dependency versioning.\n\n\n## Build ONNX from Source\nBefore building from source uninstall any existing versions of onnx `pip uninstall onnx`.\n\nGenerally speaking, you need to install [protobuf C\/C++ libraries and tools](https:\/\/github.com\/protocolbuffers\/protobuf) before proceeding forward. Then depending on how you installed protobuf, you need to set environment variable CMAKE_ARGS to \"-DONNX_USE_PROTOBUF_SHARED_LIBS=ON\" or \"-DONNX_USE_PROTOBUF_SHARED_LIBS=OFF\".  For example, you may need to run the following command:\n\nLinux:\n```bash\nexport CMAKE_ARGS=\"-DONNX_USE_PROTOBUF_SHARED_LIBS=ON\"\n```\nWindows:\n```bat\nset CMAKE_ARGS=\"-DONNX_USE_PROTOBUF_SHARED_LIBS=ON\"\n```\n\nThe ON\/OFF depends on what kind of protobuf library you have. Shared libraries are files ending with \\*.dll\/\\*.so\/\\*.dylib. Static libraries are files ending with \\*.a\/\\*.lib. This option depends on how you get your protobuf library and how it was built. And it is default OFF. You don't need to run the commands above if you'd prefer to use a static protobuf library.\n\n\n### Windows\nIf you are building ONNX from source, it is recommended that you also build Protobuf locally as a static library. The version distributed with conda-forge is a DLL, but ONNX expects it to be a static library. Building protobuf locally also lets you control the version of protobuf. The tested and recommended version is 3.16.0.\n\nThe instructions in this README assume you are using Visual Studio.  It is recommended that you run all the commands from a shell started from \"x64 Native Tools Command Prompt for VS 2019\" and keep the build system generator for cmake (e.g., cmake -G \"Visual Studio 16 2019\") consistent while building protobuf as well as ONNX.\n\nYou can get protobuf by running the following commands:\n```bat\ngit clone https:\/\/github.com\/protocolbuffers\/protobuf.git\ncd protobuf\ngit checkout v3.16.0\ncd cmake\ncmake -G \"Visual Studio 16 2019\" -A x64 -DCMAKE_INSTALL_PREFIX=<protobuf_install_dir> -Dprotobuf_MSVC_STATIC_RUNTIME=OFF -Dprotobuf_BUILD_SHARED_LIBS=OFF -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_EXAMPLES=OFF .\nmsbuild protobuf.sln \/m \/p:Configuration=Release\nmsbuild INSTALL.vcxproj \/p:Configuration=Release\n```\nThen it will be built as a static library and installed to <protobuf_install_dir>. Please add the bin directory(which contains protoc.exe) to your PATH.\n\n```bat\nset PATH=<protobuf_install_dir>\/bin;%PATH%\n```\n\nPlease note: if your protobuf_install_dir contains spaces, **do not** add quotation marks around it.\n\nAlternative: if you don't want to change your PATH, you can set ONNX_PROTOC_EXECUTABLE instead.\n```bat\nset CMAKE_ARGS=-DONNX_PROTOC_EXECUTABLE=<full_path_to_protoc.exe>\n```\n\nThen you can build ONNX as:\n```\ngit clone https:\/\/github.com\/onnx\/onnx.git\ncd onnx\ngit submodule update --init --recursive\n# prefer lite proto\nset CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON\npip install -e .\n```\n### Linux\n\nFirst, you need to install protobuf.\n\nUbuntu users: the quickest way to install protobuf is to run\n\n```bash\napt-get install python3-pip python3-dev libprotobuf-dev protobuf-compiler\n```\n\nThen you can build ONNX as:\n```\nexport CMAKE_ARGS=\"-DONNX_USE_PROTOBUF_SHARED_LIBS=ON\"\ngit clone --recursive https:\/\/github.com\/onnx\/onnx.git\ncd onnx\n# prefer lite proto\nset CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON\npip install -e .\n```\n\nOtherwise, you may need to install it from source. You can use the following commands to do it:\n\nDebian\/Ubuntu:\n```\ngit clone https:\/\/github.com\/protocolbuffers\/protobuf.git\ncd protobuf\ngit checkout v3.16.0\ngit submodule update --init --recursive\nmkdir build_source && cd build_source\ncmake ..\/cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=\/usr -DCMAKE_INSTALL_SYSCONFDIR=\/etc -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release\nmake -j$(nproc)\nmake install\n```\n\nCentOS\/RHEL\/Fedora:\n```\ngit clone https:\/\/github.com\/protocolbuffers\/protobuf.git\ncd protobuf\ngit checkout v3.16.0\ngit submodule update --init --recursive\nmkdir build_source && cd build_source\ncmake ..\/cmake  -DCMAKE_INSTALL_LIBDIR=lib64 -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=\/usr -DCMAKE_INSTALL_SYSCONFDIR=\/etc -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release\nmake -j$(nproc)\nmake install\n```\n\nHere \"-DCMAKE_POSITION_INDEPENDENT_CODE=ON\" is crucial. By default static libraries are built without \"-fPIC\" flag, they are not position independent code. But shared libraries must be position independent code. Python C\/C++ extensions(like ONNX) are shared libraries. So if a static library was not built with \"-fPIC\", it can't be linked to such a shared library.\n\nOnce build is successful, update PATH to include protobuf paths.\n\nThen you can build ONNX as:\n```\ngit clone https:\/\/github.com\/onnx\/onnx.git\ncd onnx\ngit submodule update --init --recursive\n# prefer lite proto\nset CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON\npip install -e .\n```\n\n* **Mac**\n```\nexport NUM_CORES=`sysctl -n hw.ncpu`\nbrew update\nbrew install autoconf && brew install automake\nwget https:\/\/github.com\/protocolbuffers\/protobuf\/releases\/download\/v3.16.0\/protobuf-cpp-3.16.0.tar.gz\ntar -xvf protobuf-cpp-3.16.0.tar.gz\ncd protobuf-3.16.0\nmkdir build_source && cd build_source\ncmake ..\/cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release\nmake -j${NUM_CORES}\nmake install\n```\n\nOnce build is successful, update PATH to include protobuf paths.\n\nThen you can build ONNX as:\n```\ngit clone --recursive https:\/\/github.com\/onnx\/onnx.git\ncd onnx\n# prefer lite proto\nset CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON\npip install -e .\n```\n\n\n## Verify Installation\nAfter installation, run\n\n```\npython -c \"import onnx\"\n```\n\nto verify it works.\n\n\n## Common Build Options\nFor full list refer to CMakeLists.txt\n**Environment variables**\n* `USE_MSVC_STATIC_RUNTIME` should be 1 or 0, not ON or OFF. When set to 1 onnx links statically to runtime library.\n**Default**: USE_MSVC_STATIC_RUNTIME=0\n\n* `DEBUG` should be 0 or 1. When set to 1 onnx is built in debug mode. or debug versions of the dependencies, you need to open the [CMakeLists file](CMakeLists.txt) and append a letter `d` at the end of the package name lines. For example, `NAMES protobuf-lite` would become `NAMES protobuf-lited`.\n**Default**: Debug=0\n\n**CMake variables**\n* `ONNX_USE_PROTOBUF_SHARED_LIBS` should be ON or OFF.\n**Default**: ONNX_USE_PROTOBUF_SHARED_LIBS=OFF USE_MSVC_STATIC_RUNTIME=0\n`ONNX_USE_PROTOBUF_SHARED_LIBS` determines how onnx links to protobuf libraries.\n    - When set to ON - onnx will dynamically link to protobuf shared libs, PROTOBUF_USE_DLLS will be defined as described [here](https:\/\/github.com\/protocolbuffers\/protobuf\/blob\/master\/cmake\/README.md#dlls-vs-static-linking), Protobuf_USE_STATIC_LIBS will be set to OFF and `USE_MSVC_STATIC_RUNTIME` must be 0.\n    - When set to OFF - onnx will link statically to protobuf, and Protobuf_USE_STATIC_LIBS will be set to ON (to force the use of the static libraries) and `USE_MSVC_STATIC_RUNTIME` can be 0 or 1.\n\n* `ONNX_USE_LITE_PROTO` should be ON or OFF. When set to ON onnx uses lite protobuf instead of full protobuf.\n**Default**: ONNX_USE_LITE_PROTO=OFF\n\n* `ONNX_WERROR` should be ON or OFF. When set to ON warnings are treated as errors.\n**Default**: ONNX_WERROR=OFF in local builds, ON in CI and release pipelines.\n\n\n## Common Errors\n* Note: the `import onnx` command does not work from the source checkout directory; in this case you'll see `ModuleNotFoundError: No module named 'onnx.onnx_cpp2py_export'`. Change into another directory to fix this error.\n\n* Building ONNX on Ubuntu works well, but on CentOS\/RHEL and other ManyLinux systems, you might need to open the [CMakeLists file][CMakeLists] and replace all instances of `\/lib` with `\/lib64`.\n\n# Testing\n\nONNX uses [pytest](https:\/\/docs.pytest.org) as test driver. In order to run tests, you will first need to install pytest:\n\n```\npip install pytest nbval\n```\n\nAfter installing pytest, use the following command to run tests.\n\n```\npytest\n```\n\n# Development\n\nCheck out the [contributor guide](docs\/CONTRIBUTING.md) for instructions.\n\n# License\n\n[Apache License v2.0](LICENSE)\n\n# Code of Conduct\n\n[ONNX Open Source Code of Conduct](https:\/\/onnx.ai\/codeofconduct.html)\n","55":"# dlib C++ library [![Travis Status](https:\/\/travis-ci.com\/davisking\/dlib.svg?branch=master)](https:\/\/app.travis-ci.com\/github\/davisking\/dlib) [![GitHub Actions C++ Status](https:\/\/github.com\/davisking\/dlib\/actions\/workflows\/build_cpp.yml\/badge.svg)](https:\/\/github.com\/davisking\/dlib\/actions\/workflows\/build_cpp.yml) [![GitHub Actions Python Status](https:\/\/github.com\/davisking\/dlib\/actions\/workflows\/build_python.yml\/badge.svg)](https:\/\/github.com\/davisking\/dlib\/actions\/workflows\/build_python.yml)\n\nDlib is a modern C++ toolkit containing machine learning algorithms and tools for creating complex software in C++ to solve real world problems. See [http:\/\/dlib.net](http:\/\/dlib.net) for the main project documentation and API reference.\n\n\n\n## Compiling dlib C++ example programs\n\nGo into the examples folder and type:\n\n```bash\nmkdir build; cd build; cmake .. ; cmake --build .\n```\n\nThat will build all the examples.\nIf you have a CPU that supports AVX instructions then turn them on like this:\n\n```bash\nmkdir build; cd build; cmake .. -DUSE_AVX_INSTRUCTIONS=1; cmake --build .\n```\n\nDoing so will make some things run faster.\n\nFinally, Visual Studio users should usually do everything in 64bit mode.  By default Visual Studio is 32bit, both in its outputs and its own execution, so you have to explicitly tell it to use 64bits.  Since it's not the 1990s anymore you probably want to use 64bits.  Do that with a cmake invocation like this:\n```bash\ncmake .. -G \"Visual Studio 14 2015 Win64\" -T host=x64 \n```\n\n## Compiling your own C++ programs that use dlib\n\nThe examples folder has a [CMake tutorial](https:\/\/github.com\/davisking\/dlib\/blob\/master\/examples\/CMakeLists.txt) that tells you what to do.  There are also additional instructions on the [dlib web site](http:\/\/dlib.net\/compile.html).\n\nAlternatively, if you are using the [vcpkg](https:\/\/github.com\/Microsoft\/vcpkg\/) dependency manager you can download and install dlib with CMake integration in a single command:\n```bash\nvcpkg install dlib\n```\n\n## Compiling dlib Python API\n\nBefore you can run the Python example programs you must compile dlib. Type:\n\n```bash\npython setup.py install\n```\n\n\n## Running the unit test suite\n\nType the following to compile and run the dlib unit test suite:\n\n```bash\ncd dlib\/test\nmkdir build\ncd build\ncmake ..\ncmake --build . --config Release\n.\/dtest --runall\n```\n\nNote that on windows your compiler might put the test executable in a subfolder called `Release`. If that's the case then you have to go to that folder before running the test.\n\nThis library is licensed under the Boost Software License, which can be found in [dlib\/LICENSE.txt](https:\/\/github.com\/davisking\/dlib\/blob\/master\/dlib\/LICENSE.txt).  The long and short of the license is that you can use dlib however you like, even in closed source commercial software.\n\n## dlib sponsors\n\nThis research is based in part upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) under contract number 2014-14071600010. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government.\n\n","56":"Quick Links: [Installation](#supported-platforms) | [Documentation](#documentation)\n\n[![Build Status](https:\/\/travis-ci.com\/apple\/turicreate.svg?branch=master)](#)\n[![PyPI Release](https:\/\/img.shields.io\/pypi\/v\/turicreate.svg)](#)\n[![Python Versions](https:\/\/img.shields.io\/pypi\/pyversions\/turicreate.svg)](#)\n\n[<img align=\"right\" src=\"https:\/\/docs-assets.developer.apple.com\/turicreate\/turi-dog.svg\" alt=\"Turi Create\" width=\"100\">](#)\n\n# Turi Create \n\nTuri Create simplifies the development of custom machine learning models. You\ndon't have to be a machine learning expert to add recommendations, object\ndetection, image classification, image similarity or activity classification to\nyour app.\n\n* **Easy-to-use:** Focus on tasks instead of algorithms\n* **Visual:** Built-in, streaming visualizations to explore your data\n* **Flexible:** Supports text, images, audio, video and sensor data\n* **Fast and Scalable:** Work with large datasets on a single machine\n* **Ready To Deploy:** Export models to Core ML for use in iOS, macOS, watchOS, and tvOS apps\n\nWith Turi Create, you can accomplish many common ML tasks:\n\n| ML Task                 | Description                      |\n|:------------------------:|:--------------------------------:|\n| [Recommender](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/recommender\/)             | Personalize choices for users    |\n| [Image Classification](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/image_classifier\/)    | Label images                     |\n| [Drawing Classification](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/drawing_classifier)  | Recognize Pencil\/Touch Drawings and Gestures                     |\n| [Sound Classification](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/sound_classifier)  | Classify sounds                     |\n| [Object Detection](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/object_detection\/)        | Recognize objects within images  |\n| [One Shot Object Detection](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/one_shot_object_detection\/)    | Recognize 2D objects within images using a single example  |\n| [Style Transfer](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/style_transfer\/)        | Stylize images |\n| [Activity Classification](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/activity_classifier\/) | Detect an activity using sensors |\n| [Image Similarity](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/image_similarity\/)        | Find similar images              |\n| [Classifiers](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/supervised-learning\/classifier.html)             | Predict a label           |\n| [Regression](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/supervised-learning\/regression.html)              | Predict numeric values           |\n| [Clustering](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/clustering\/)              | Group similar datapoints together|\n| [Text Classifier](https:\/\/apple.github.io\/turicreate\/docs\/userguide\/text_classifier\/)         | Analyze sentiment of messages    |\n\n\nExample: Image classifier with a few lines of code\n--------------------------------------------------\n\nIf you want your app to recognize specific objects in images, you can build your own model with just a few lines of code:\n\n```python\nimport turicreate as tc\n\n# Load data \ndata = tc.SFrame('photoLabel.sframe')\n\n# Create a model\nmodel = tc.image_classifier.create(data, target='photoLabel')\n\n# Make predictions\npredictions = model.predict(data)\n\n# Export to Core ML\nmodel.export_coreml('MyClassifier.mlmodel')\n```\n \nIt's easy to use the resulting model in an [iOS application](https:\/\/developer.apple.com\/documentation\/vision\/classifying_images_with_vision_and_core_ml):\n\n<p align=\"center\"><img src=\"https:\/\/docs-assets.developer.apple.com\/published\/a2c37bce1f\/689f61a6-1087-4112-99d9-bbfb326e3138.png\" alt=\"Turi Create\" width=\"600\"><\/p>\n\nSupported Platforms\n-------------------\n\nTuri Create supports:\n\n* macOS 10.12+\n* Linux (with glibc 2.10+)\n* Windows 10 (via WSL)\n\nSystem Requirements\n-------------------\n\nTuri Create requires:\n\n* Python 2.7, 3.5, 3.6, 3.7, 3.8\n* x86\\_64 architecture\n* At least 4 GB of RAM\n\nInstallation\n------------\n\nFor detailed instructions for different varieties of Linux see [LINUX\\_INSTALL.md](LINUX_INSTALL.md).\nFor common installation issues see [INSTALL\\_ISSUES.md](INSTALL_ISSUES.md).\n\nWe recommend using virtualenv to use, install, or build Turi Create. \n\n```shell\npip install virtualenv\n```\n\nThe method for installing *Turi Create* follows the\n[standard python package installation steps](https:\/\/packaging.python.org\/installing\/).\nTo create and activate a Python virtual environment called `venv` follow these steps:\n\n```shell\n# Create a Python virtual environment\ncd ~\nvirtualenv venv\n\n# Activate your virtual environment\nsource ~\/venv\/bin\/activate\n```\nAlternatively, if you are using [Anaconda](https:\/\/www.anaconda.com\/what-is-anaconda\/), you may use its virtual environment:\n```shell\nconda create -n virtual_environment_name anaconda\nconda activate virtual_environment_name\n```\n\nTo install `Turi Create` within your virtual environment:\n```shell\n(venv) pip install -U turicreate\n```\n\nDocumentation\n-------------\n\nThe package [User Guide](https:\/\/apple.github.io\/turicreate\/docs\/userguide) and [API Docs](https:\/\/apple.github.io\/turicreate\/docs\/api) contain\nmore details on how to use Turi Create.\n\nGPU Support\n-----------\n\nTuri Create **does not require a GPU**, but certain models can be accelerated 9-13x by utilizing a GPU.\n\n| Linux                     | macOS 10.13+         | macOS 10.14+ discrete GPUs, macOS 10.15+ integrated GPUs |\n| :-------------------------|:---------------------|:---------------------------------------------------------|\n| Activity Classification   | Image Classification | Activity Classification                                  |\n| Drawing Classification    | Image Similarity     | Object Detection                                         |\n| Image Classification      | Sound Classification | One Shot Object Detection                                |\n| Image Similarity          |                      | Style Transfer                                           |\n| Object Detection          |                      |                                                          |\n| One Shot Object Detection |                      |                                                          |\n| Sound Classification      |                      |                                                          |\n| Style Transfer            |                      |                                                          |\n\nmacOS GPU support is automatic. For Linux GPU support, see [LinuxGPU.md](LinuxGPU.md).\n\nBuilding From Source\n---------------------\n\nIf you want to build Turi Create from source, see [BUILD.md](BUILD.md).\n\nContributing\n------------\n\nPrior to contributing, please review [CONTRIBUTING.md](CONTRIBUTING.md) and do\nnot provide any contributions unless you agree with the terms and conditions\nset forth in [CONTRIBUTING.md](CONTRIBUTING.md).\n\nWe want the Turi Create community to be as welcoming and inclusive as possible, and have adopted a [Code of Conduct](CODE_OF_CONDUCT.md) that we expect all community members, including contributors, to read and observe.\n","57":"[![Travis Build Status](https:\/\/api.travis-ci.org\/bulletphysics\/bullet3.png?branch=master)](https:\/\/travis-ci.org\/bulletphysics\/bullet3)\n[![Appveyor Build status](https:\/\/ci.appveyor.com\/api\/projects\/status\/6sly9uxajr6xsstq)](https:\/\/ci.appveyor.com\/project\/erwincoumans\/bullet3)\n\n# Bullet Physics SDK\n\nThis is the official C++ source code repository of the Bullet Physics SDK: real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc.\n\n![PyBullet](https:\/\/pybullet.org\/wordpress\/wp-content\/uploads\/2019\/03\/cropped-pybullet.png)\n\n## Issues ##\nThe Issue tracker was flooded with support questions and is closed until it is cleaned up. Use the [PyBullet forums](http:\/\/pybullet.org) to discuss with others.\n\n## PyBullet ##\nIt is highly recommended to use PyBullet Python bindings for improved support for robotics, reinforcement learning and VR. Use pip install pybullet and checkout the [PyBullet Quickstart Guide](https:\/\/docs.google.com\/document\/d\/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA\/edit#heading=h.2ye70wns7io3).\n\nInstallation is simple:\n```\npip3 install pybullet --upgrade --user\npython3 -m pybullet_envs.examples.enjoy_TF_AntBulletEnv_v0_2017may\npython3 -m pybullet_envs.examples.enjoy_TF_HumanoidFlagrunHarderBulletEnv_v1_2017jul\npython3 -m pybullet_envs.deep_mimic.testrl --arg_file run_humanoid3d_backflip_args.txt\n```\n\nIf you use PyBullet in your research, please cite it like this:\n\n```\n@MISC{coumans2021,\nauthor =   {Erwin Coumans and Yunfei Bai},\ntitle =    {PyBullet, a Python module for physics simulation for games, robotics and machine learning},\nhowpublished = {\\url{http:\/\/pybullet.org}},\nyear = {2016--2021}\n}\n```\n\n## Requirements for Bullet Physics C++\n\nA C++ compiler for C++ 2003. The library is tested on Windows, Linux, Mac OSX, iOS, Android,\nbut should likely work on any platform with C++ compiler. \nSome optional demos require OpenGL 2 or OpenGL 3, there are some non-graphical demos and unit tests too.\n\n## Contributors and Coding Style information\n\nhttps:\/\/docs.google.com\/document\/d\/1u9vyzPtrVoVhYqQOGNWUgjRbfwfCdIts_NzmvgiJ144\/edit\n\n## Requirements for experimental OpenCL GPGPU support\n\nThe entire collision detection and rigid body dynamics can be executed on the GPU.\n\nA high-end desktop GPU, such as an AMD Radeon 7970 or NVIDIA GTX 680 or better.\nWe succesfully tested the software under Windows, Linux and Mac OSX.\nThe software currently doesn't work on OpenCL CPU devices. It might run\non a laptop GPU but performance will not likely be very good. Note that\noften an OpenCL drivers fails to compile a kernel. Some unit tests exist to\ntrack down the issue, but more work is required to cover all OpenCL kernels.\n\n## License\n\nAll source code files are licensed under the permissive zlib license\n(http:\/\/opensource.org\/licenses\/Zlib) unless marked differently in a particular folder\/file.\n\n## Build instructions for Bullet using vcpkg\n\nYou can download and install Bullet using the [vcpkg](https:\/\/github.com\/Microsoft\/vcpkg\/) dependency manager:\n\n    git clone https:\/\/github.com\/Microsoft\/vcpkg.git\n    cd vcpkg\n    .\/bootstrap-vcpkg.sh\n    .\/vcpkg integrate install\n    .\/vcpkg install bullet3\n\nThe Bullet port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https:\/\/github.com\/Microsoft\/vcpkg) on the vcpkg repository.\n\n## Build instructions for Bullet using premake. You can also use cmake instead.\n\n**Windows**\n\nClick on build_visual_studio_vr_pybullet_double.bat and open build3\/vs2010\/0_Bullet3Solution.sln\nWhen asked, convert the projects to a newer version of Visual Studio.\nIf you installed Python in the C:\\ root directory, the batch file should find it automatically.\nOtherwise, edit this batch file to choose where Python include\/lib directories are located.\n\n**Windows Virtual Reality sandbox for HTC Vive and Oculus Rift**\n\nBuild and run the App_SharedMemoryPhysics_VR project, preferably in Release\/optimized build.\nYou can connect from Python pybullet to the sandbox using:\n\n```\nimport pybullet as p\np.connect(p.SHARED_MEMORY) #or (p.TCP, \"localhost\", 6667) or (p.UDP, \"192.168.86.10\",1234)\n```\n\n**Linux and Mac OSX gnu make**\n\nMake sure cmake is installed (sudo apt-get install cmake, brew install cmake, or https:\/\/cmake.org)\n\nIn a terminal type:\n```\n.\/build_cmake_pybullet_double.sh\n```\nThis script will invoke cmake and build in the build_cmake directory. You can find pybullet in Bullet\/examples\/pybullet.\nThe BulletExampleBrowser binary will be in Bullet\/examples\/ExampleBrowser.\n\nYou can also build Bullet using premake. There are premake executables in the build3 folder.\nDepending on your system (Linux 32bit, 64bit or Mac OSX) use one of the following lines\nUsing premake:\n```\ncd build3\n.\/premake4_linux --double gmake\n.\/premake4_linux64 --double gmake\n.\/premake4_osx --double --enable_pybullet gmake\n```\nThen\n```\ncd gmake\nmake\n```\n\nNote that on Linux, you need to use cmake to build pybullet, since the compiler has issues of mixing shared and static libraries.\n\n**Mac OSX Xcode**\n\t\nClick on build3\/xcode4.command or in a terminal window execute\n```\t\n.\/premake_osx xcode4\n```\n## Usage\n\nThe App_ExampleBrowser executables will be located in the bin folder.\nYou can just run it though a terminal\/command prompt, or by clicking it.\n\n\n```\n[--start_demo_name=\"Demo Name\"]     Start with a selected demo  \n[--mp4=moviename.mp4]               Create a mp4 movie of the window, requires ffmpeg installed\n[--mouse_move_multiplier=0.400000]  Set the mouse move sensitivity\n[--mouse_wheel_multiplier=0.01]     Set the mouse wheel sensitivity\n[--background_color_red= 0.9]       Set the red component for background color. Same for green and blue\n[--fixed_timestep= 0.0]             Use either a real-time delta time (0.0) or a fixed step size (0.016666)\n```\n\nYou can use mouse picking to grab objects. When holding the ALT or CONTROL key, you have Maya style camera mouse controls.\nPress F1 to create a series of screenshots. Hit ESCAPE to exit the demo app.\n\nCheck out the docs folder and the Bullet physics forums for further information.\n","58":"<img src=\"\/logo_assets\/vowpal-wabbits-github-logo@3x.png\" height=\"auto\" width=\"100%\" alt=\"Vowpal Wabbit\">\n\n[![Linux build status](https:\/\/img.shields.io\/azure-devops\/build\/vowpalwabbit\/3934113c-9e2b-4dbc-8972-72ab9b9b4342\/23?label=Linux%20build&logo=Azure%20Devops)](https:\/\/dev.azure.com\/vowpalwabbit\/Vowpal%20Wabbit\/_build\/latest?definitionId=23&branchName=master)\n[![Windows build status](https:\/\/img.shields.io\/azure-devops\/build\/vowpalwabbit\/3934113c-9e2b-4dbc-8972-72ab9b9b4342\/14?label=Windows%20build&logo=Azure%20Devops)](https:\/\/dev.azure.com\/vowpalwabbit\/Vowpal%20Wabbit\/_build\/latest?definitionId=14&branchName=master)\n[![MacOS build status](https:\/\/img.shields.io\/azure-devops\/build\/vowpalwabbit\/3934113c-9e2b-4dbc-8972-72ab9b9b4342\/22?label=MacOS%20build&logo=Azure%20Devops)](https:\/\/dev.azure.com\/vowpalwabbit\/Vowpal%20Wabbit\/_build\/latest?definitionId=22&branchName=master)\n\n[![codecov](https:\/\/codecov.io\/gh\/VowpalWabbit\/vowpal_wabbit\/branch\/master\/graph\/badge.svg)](https:\/\/codecov.io\/gh\/VowpalWabbit\/vowpal_wabbit)\n[![Total Alerts](https:\/\/img.shields.io\/lgtm\/alerts\/g\/JohnLangford\/vowpal_wabbit.svg?logo=lgtm&logoWidth=18)](https:\/\/lgtm.com\/projects\/g\/JohnLangford\/vowpal_wabbit\/alerts\/)\n[![Gitter chat](https:\/\/badges.gitter.im\/VowpalWabbit.svg)](https:\/\/gitter.im\/VowpalWabbit)\n\nThis is the *Vowpal Wabbit* fast online learning code.\n\n## Why Vowpal Wabbit?\nVowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning. There is a specific focus on reinforcement learning with several contextual bandit algorithms implemented and the online nature lending to the problem well. Vowpal Wabbit is a destination for implementing and maturing state of the art algorithms with performance in mind.\n\n- **Input Format.** The input format for the learning algorithm is substantially more flexible than might be expected. Examples can have features consisting of free form text, which is interpreted in a bag-of-words way. There can even be multiple sets of free form text in different namespaces.\n- **Speed.** The learning algorithm is fast -- similar to the few other online algorithm implementations out there. There are several optimization algorithms available with the baseline being sparse gradient descent (GD) on a loss function.\n- **Scalability.** This is not the same as fast. Instead, the important characteristic here is that the memory footprint of the program is bounded independent of data. This means the training set is not loaded into main memory before learning starts. In addition, the size of the set of features is bounded independent of the amount of training data using the hashing trick.\n- **Feature Interaction.** Subsets of features can be internally paired so that the algorithm is linear in the cross-product of the subsets. This is useful for ranking problems. The alternative of explicitly expanding the features before feeding them into the learning algorithm can be both computation and space intensive, depending on how it's handled.\n\n[Visit the wiki to learn more.](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/wiki)\n\n## Getting Started\nFor the most up to date instructions for getting started on Windows, MacOS or Linux [please see the wiki](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/wiki\/Getting-started). This includes:\n\n- [Installing with a package manager](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/wiki\/Getting-started)\n- [Dependencies](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/wiki\/Dependencies)\n- [Building](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/wiki\/Building)\n- [Tutorial](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/wiki\/Tutorial)\n","59":"# TensorFlow Serving\n\n[![Ubuntu Build Status](https:\/\/storage.googleapis.com\/tensorflow-serving-kokoro-build-badges-bucket\/ubuntu.svg)](https:\/\/storage.googleapis.com\/tensorflow-serving-kokoro-build-badges-bucket\/ubuntu.html)\n[![Ubuntu Build Status at TF HEAD](https:\/\/storage.googleapis.com\/tensorflow-serving-kokoro-build-badges-bucket\/ubuntu-tf-head.svg)](https:\/\/storage.googleapis.com\/tensorflow-serving-kokoro-build-badges-bucket\/ubuntu-tf-head.html)\n![Docker CPU Nightly Build Status](https:\/\/storage.googleapis.com\/tensorflow-serving-kokoro-build-badges-bucket\/docker-cpu-nightly.svg)\n![Docker GPU Nightly Build Status](https:\/\/storage.googleapis.com\/tensorflow-serving-kokoro-build-badges-bucket\/docker-gpu-nightly.svg)\n\n----\nTensorFlow Serving is a flexible, high-performance serving system for\nmachine learning models, designed for production environments. It deals with\nthe *inference* aspect of machine learning, taking models after *training* and\nmanaging their lifetimes, providing clients with versioned access via\na high-performance, reference-counted lookup table.\nTensorFlow Serving provides out-of-the-box integration with TensorFlow models,\nbut can be easily extended to serve other types of models and data.\n\nTo note a few features:\n\n-   Can serve multiple models, or multiple versions of the same model\n    simultaneously\n-   Exposes both gRPC as well as HTTP inference endpoints\n-   Allows deployment of new model versions without changing any client code\n-   Supports canarying new versions and A\/B testing experimental models\n-   Adds minimal latency to inference time due to efficient, low-overhead\n    implementation\n-   Features a scheduler that groups individual inference requests into batches\n    for joint execution on GPU, with configurable latency controls\n-   Supports many *servables*: Tensorflow models, embeddings, vocabularies,\n    feature transformations and even non-Tensorflow-based machine learning\n    models\n\n## Serve a Tensorflow model in 60 seconds\n```bash\n# Download the TensorFlow Serving Docker image and repo\ndocker pull tensorflow\/serving\n\ngit clone https:\/\/github.com\/tensorflow\/serving\n# Location of demo models\nTESTDATA=\"$(pwd)\/serving\/tensorflow_serving\/servables\/tensorflow\/testdata\"\n\n# Start TensorFlow Serving container and open the REST API port\ndocker run -t --rm -p 8501:8501 \\\n    -v \"$TESTDATA\/saved_model_half_plus_two_cpu:\/models\/half_plus_two\" \\\n    -e MODEL_NAME=half_plus_two \\\n    tensorflow\/serving &\n\n# Query the model using the predict API\ncurl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n    -X POST http:\/\/localhost:8501\/v1\/models\/half_plus_two:predict\n\n# Returns => { \"predictions\": [2.5, 3.0, 4.5] }\n```\n\n## End-to-End Training & Serving Tutorial\n\nRefer to the official Tensorflow documentations site for [a complete tutorial to train and serve a Tensorflow Model](https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple).\n\n\n## Documentation\n\n### Set up\n\nThe easiest and most straight-forward way of using TensorFlow Serving is with\nDocker images. We highly recommend this route unless you have specific needs\nthat are not addressed by running in a container.\n\n*   [Install Tensorflow Serving using Docker](tensorflow_serving\/g3doc\/docker.md)\n    *(Recommended)*\n*   [Install Tensorflow Serving without Docker](tensorflow_serving\/g3doc\/setup.md)\n    *(Not Recommended)*\n*   [Build Tensorflow Serving from Source with Docker](tensorflow_serving\/g3doc\/building_with_docker.md)\n*   [Deploy Tensorflow Serving on Kubernetes](tensorflow_serving\/g3doc\/serving_kubernetes.md)\n\n### Use\n\n#### Export your Tensorflow model\n\nIn order to serve a Tensorflow model, simply export a SavedModel from your\nTensorflow program.\n[SavedModel](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/python\/saved_model\/README.md)\nis a language-neutral, recoverable, hermetic serialization format that enables\nhigher-level systems and tools to produce, consume, and transform TensorFlow\nmodels.\n\nPlease refer to [Tensorflow documentation](https:\/\/www.tensorflow.org\/guide\/saved_model#save_and_restore_models)\nfor detailed instructions on how to export SavedModels.\n\n#### Configure and Use Tensorflow Serving\n\n* [Follow a tutorial on Serving Tensorflow models](tensorflow_serving\/g3doc\/serving_basic.md)\n* [Configure Tensorflow Serving to make it fit your serving use case](tensorflow_serving\/g3doc\/serving_config.md)\n* Read the [Performance Guide](tensorflow_serving\/g3doc\/performance.md)\nand learn how to [use TensorBoard to profile and optimize inference requests](tensorflow_serving\/g3doc\/tensorboard.md)\n* Read the [REST API Guide](tensorflow_serving\/g3doc\/api_rest.md)\nor [gRPC API definition](https:\/\/github.com\/tensorflow\/serving\/tree\/master\/tensorflow_serving\/apis)\n* [Use SavedModel Warmup if initial inference requests are slow due to lazy initialization of graph](tensorflow_serving\/g3doc\/saved_model_warmup.md)\n* [If encountering issues regarding model signatures, please read the SignatureDef documentation](tensorflow_serving\/g3doc\/signature_defs.md)\n* If using a model with custom ops, [learn how to serve models with custom ops](tensorflow_serving\/g3doc\/custom_op.md)\n\n### Extend\n\nTensorflow Serving's architecture is highly modular. You can use some parts\nindividually (e.g. batch scheduling) and\/or extend it to serve new use cases.\n\n* [Ensure you are familiar with building Tensorflow Serving](tensorflow_serving\/g3doc\/building_with_docker.md)\n* [Learn about Tensorflow Serving's architecture](tensorflow_serving\/g3doc\/architecture.md)\n* [Explore the Tensorflow Serving C++ API reference](https:\/\/www.tensorflow.org\/tfx\/serving\/api_docs\/cc\/)\n* [Create a new type of Servable](tensorflow_serving\/g3doc\/custom_servable.md)\n* [Create a custom Source of Servable versions](tensorflow_serving\/g3doc\/custom_source.md)\n\n## Contribute\n\n\n**If you'd like to contribute to TensorFlow Serving, be sure to review the\n[contribution guidelines](CONTRIBUTING.md).**\n\n\n## For more information\n\nPlease refer to the official [TensorFlow website](http:\/\/tensorflow.org) for\nmore information.\n","60":"# InterpretML - Alpha Release\n\n![License](https:\/\/img.shields.io\/github\/license\/microsoft\/interpret.svg?style=flat-square)\n![Python Version](https:\/\/img.shields.io\/pypi\/pyversions\/interpret.svg?style=flat-square)\n![Package Version](https:\/\/img.shields.io\/pypi\/v\/interpret.svg?style=flat-square)\n![Build Status](https:\/\/img.shields.io\/azure-devops\/build\/ms\/interpret\/293\/develop.svg?style=flat-square)\n![Coverage](https:\/\/img.shields.io\/azure-devops\/coverage\/ms\/interpret\/293\/develop.svg?style=flat-square)\n![LGTM Grade](https:\/\/img.shields.io\/lgtm\/grade\/python\/github\/interpretml\/interpret?style=flat-square)\n![Maintenance](https:\/\/img.shields.io\/maintenance\/yes\/2022?style=flat-square)\n<br\/>\n> ### In the beginning machines learned in darkness, and data scientists struggled in the void to explain them. \n> ### Let there be light.\n\nInterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model's global behavior, or understand the reasons behind individual predictions.\n\nInterpretability is essential for:\n- Model debugging - Why did my model make this mistake?\n- Feature Engineering - How can I improve my model?\n- Detecting fairness issues - Does my model discriminate?\n- Human-AI cooperation - How can I understand and trust the model's decisions?\n- Regulatory compliance - Does my model satisfy legal requirements?\n- High-risk applications - Healthcare, finance, judicial, ...\n\n![](https:\/\/github.com\/interpretml\/interpretml.github.io\/blob\/master\/interpret-highlight.gif)\n\n# Installation\n\nPython 3.6+ | Linux, Mac, Windows\n```sh\npip install interpret\n```\n\n# Introducing the Explainable Boosting Machine (EBM)\n\nEBM is an interpretable model developed at Microsoft Research<sup>[*](#citations)<\/sup>. It uses modern machine learning techniques like bagging, gradient boosting, and automatic interaction detection to breathe new life into traditional GAMs (Generalized Additive Models). This makes EBMs as accurate as state-of-the-art techniques like random forests and gradient boosted trees. However, unlike these blackbox models, EBMs produce exact explanations and are editable by domain experts.\n\n| Dataset\/AUROC | Domain  | Logistic Regression | Random Forest | XGBoost         | Explainable Boosting Machine |\n|---------------|---------|:-------------------:|:-------------:|:---------------:|:----------------------------:|\n| Adult Income  | Finance | .907\u00b1.003           | .903\u00b1.002     | .927\u00b1.001       | **_.928\u00b1.002_**              |\n| Heart Disease | Medical | .895\u00b1.030           | .890\u00b1.008     | .851\u00b1.018       | **_.898\u00b1.013_**              |\n| Breast Cancer | Medical | **_.995\u00b1.005_**     | .992\u00b1.009     | .992\u00b1.010       | **_.995\u00b1.006_**              |\n| Telecom Churn | Business| .849\u00b1.005           | .824\u00b1.004     | .828\u00b1.010       | **_.852\u00b1.006_**              |\n| Credit Fraud  | Security| .979\u00b1.002           | .950\u00b1.007     | **_.981\u00b1.003_** | **_.981\u00b1.003_**              |\n\n[*Notebook for reproducing table*](https:\/\/nbviewer.jupyter.org\/github\/interpretml\/interpret\/blob\/master\/benchmarks\/EBM%20Classification%20Comparison.ipynb)\n\n# Supported Techniques\n\n| Interpretability Technique  | Type               |\n|-----------------------------|--------------------|\n| [Explainable Boosting](https:\/\/interpret.ml\/docs\/ebm.html)        | glassbox model     |\n| [Decision Tree](https:\/\/interpret.ml\/docs\/dt.html)                | glassbox model     |\n| [Decision Rule List](https:\/\/interpret.ml\/docs\/dr.html)           | glassbox model     |\n| [Linear\/Logistic Regression](https:\/\/interpret.ml\/docs\/lr.html)   | glassbox model     |\n| [SHAP Kernel Explainer](https:\/\/interpret.ml\/docs\/shap.html)      | blackbox explainer |\n| [LIME](https:\/\/interpret.ml\/docs\/lime.html)                       | blackbox explainer |\n| [Morris Sensitivity Analysis](https:\/\/interpret.ml\/docs\/msa.html) | blackbox explainer |\n| [Partial Dependence](https:\/\/interpret.ml\/docs\/pdp.html)          | blackbox explainer |\n\n# Train a glassbox model\n\nLet's fit an Explainable Boosting Machine\n\n```python\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\nebm = ExplainableBoostingClassifier()\nebm.fit(X_train, y_train)\n\n# or substitute with LogisticRegression, DecisionTreeClassifier, RuleListClassifier, ...\n# EBM supports pandas dataframes, numpy arrays, and handles \"string\" data natively.\n```\n\nUnderstand the model\n```python\nfrom interpret import show\n\nebm_global = ebm.explain_global()\nshow(ebm_global)\n```\n![Global Explanation Image](.\/examples\/python\/assets\/readme_ebm_global_specific.PNG?raw=true)\n\n<br\/>\n\nUnderstand individual predictions\n```python\nebm_local = ebm.explain_local(X_test, y_test)\nshow(ebm_local)\n```\n![Local Explanation Image](.\/examples\/python\/assets\/readme_ebm_local_specific.PNG?raw=true)\n\n<br\/>\n\nAnd if you have multiple model explanations, compare them\n```python\nshow([logistic_regression_global, decision_tree_global])\n```\n![Dashboard Image](.\/examples\/python\/assets\/readme_dashboard.PNG?raw=true)\n\n<br\/>\n\nIf you need to keep your data private, use Differentially Private EBMs (see [DP-EBMs](http:\/\/proceedings.mlr.press\/v139\/nori21a\/nori21a.pdf))\n\n```python\nfrom interpret.privacy import DPExplainableBoostingClassifier, DPExplainableBoostingRegressor\n\ndp_ebm = DPExplainableBoostingClassifier(epsilon=1, delta=1e-5) # Specify privacy parameters\ndp_ebm.fit(X_train, y_train)\n\nshow(dp_ebm.explain_global()) # Identical function calls to standard EBMs\n```\n\n<br\/>\n<br\/>\n\nFor more information, see the [documentation](https:\/\/interpret.ml\/docs\/getting-started.html).\n<br\/>\n<br\/>\n\n# Acknowledgements\n\nInterpretML was originally created by (equal contributions): Samuel Jenkins, Harsha Nori, Paul Koch, and Rich Caruana\n\nEBMs are fast derivative of GA2M, invented by: Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker\n\nMany people have supported us along the way. Check out [ACKNOWLEDGEMENTS.md](.\/ACKNOWLEDGEMENTS.md)!\n\nWe also build on top of many great packages. Please check them out!\n\n[plotly](https:\/\/github.com\/plotly\/plotly.py) |\n[dash](https:\/\/github.com\/plotly\/dash) |\n[scikit-learn](https:\/\/github.com\/scikit-learn\/scikit-learn) |\n[lime](https:\/\/github.com\/marcotcr\/lime) |\n[shap](https:\/\/github.com\/slundberg\/shap) |\n[salib](https:\/\/github.com\/SALib\/SALib) |\n[skope-rules](https:\/\/github.com\/scikit-learn-contrib\/skope-rules) |\n[treeinterpreter](https:\/\/github.com\/andosa\/treeinterpreter) |\n[gevent](https:\/\/github.com\/gevent\/gevent) |\n[joblib](https:\/\/github.com\/joblib\/joblib) |\n[pytest](https:\/\/github.com\/pytest-dev\/pytest) |\n[jupyter](https:\/\/github.com\/jupyter\/notebook)\n\n# <a name=\"citations\">Citations<\/a>\n\n<details open>\n  <summary><strong>InterpretML<\/strong><\/summary>\n  <hr\/>\n  <details open>\n    <summary>\n      <em>\"InterpretML: A Unified Framework for Machine Learning Interpretability\" (H. Nori, S. Jenkins, P. Koch, and R.\n        Caruana 2019)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{nori2019interpretml,\n  title={InterpretML: A Unified Framework for Machine Learning Interpretability},\n  author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},\n  journal={arXiv preprint arXiv:1909.09223},\n  year={2019}\n}\n<\/pre>\n    <a href=\"https:\/\/arxiv.org\/pdf\/1909.09223.pdf\">Paper link<\/a>\n  <\/details>\n  <hr\/>\n<\/details>\n\n<details>\n  <summary><strong>Explainable Boosting<\/strong><\/summary>\n  <hr\/>\n  <details>\n    <summary>\n      <em>\"Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission\" (R. Caruana,\n        Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad 2015)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@inproceedings{caruana2015intelligible,\n  title={Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},\n  author={Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},\n  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n  pages={1721--1730},\n  year={2015},\n  organization={ACM}\n}\n<\/pre>\n    <a href=\"https:\/\/www.microsoft.com\/en-us\/research\/wp-content\/uploads\/2017\/06\/KDD2015FinalDraftIntelligibleModels4HealthCare_igt143e-caruanaA.pdf\">Paper link<\/a>\n  <\/details>\n\n  <details>\n    <summary>\n      <em>\"Accurate intelligible models with pairwise interactions\" (Y. Lou, R. Caruana, J. Gehrke, and G. Hooker\n        2013)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@inproceedings{lou2013accurate,\n  title={Accurate intelligible models with pairwise interactions},\n  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},\n  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},\n  pages={623--631},\n  year={2013},\n  organization={ACM}\n}\n<\/pre>\n    <a href=\"http:\/\/www.cs.cornell.edu\/~yinlou\/papers\/lou-kdd13.pdf\">Paper link<\/a>\n  <\/details>\n  <details>\n    <summary>\n      <em>\"Intelligible models for classification and regression\" (Y. Lou, R. Caruana, and J. Gehrke 2012)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@inproceedings{lou2012intelligible,\n  title={Intelligible models for classification and regression},\n  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes},\n  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},\n  pages={150--158},\n  year={2012},\n  organization={ACM}\n}\n\n<\/pre>\n    <a href=\"https:\/\/www.cs.cornell.edu\/~yinlou\/papers\/lou-kdd12.pdf\">Paper link<\/a>\n  <\/details>\n\n  <details>\n    <summary>\n      <em>\"Axiomatic Interpretability for Multiclass Additive Models\" (X. Zhang, S. Tan, P. Koch, Y. Lou, U. Chajewska, and R. Caruana 2019)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@inproceedings{zhang2019axiomatic,\n  title={Axiomatic Interpretability for Multiclass Additive Models},\n  author={Zhang, Xuezhou and Tan, Sarah and Koch, Paul and Lou, Yin and Chajewska, Urszula and Caruana, Rich},\n  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n  pages={226--234},\n  year={2019},\n  organization={ACM}\n}\n<\/pre>\n    <a href=\"https:\/\/arxiv.org\/pdf\/1810.09092.pdf\">Paper link<\/a>\n  <\/details>\n\n  <details>\n    <summary>\n      <em>\"Distill-and-compare: auditing black-box models using transparent model distillation\" (S. Tan, R. Caruana, G. Hooker, and Y. Lou 2018)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@inproceedings{tan2018distill,\n  title={Distill-and-compare: auditing black-box models using transparent model distillation},\n  author={Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},\n  booktitle={Proceedings of the 2018 AAAI\/ACM Conference on AI, Ethics, and Society},\n  pages={303--310},\n  year={2018},\n  organization={ACM}\n}\n<\/pre>\n    <a href=\"https:\/\/arxiv.org\/pdf\/1710.06169\">Paper link<\/a>\n  <\/details>\n\n  <details>\n    <summary>\n      <em>\"Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models\" (B. Lengerich, S. Tan, C. Chang, G. Hooker, R. Caruana 2019)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{lengerich2019purifying,\n  title={Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},\n  author={Lengerich, Benjamin and Tan, Sarah and Chang, Chun-Hao and Hooker, Giles and Caruana, Rich},\n  journal={arXiv preprint arXiv:1911.04974},\n  year={2019}\n}\n<\/pre>\n    <a href=\"https:\/\/arxiv.org\/pdf\/1911.04974.pdf\">Paper link<\/a>\n  <\/details>\n\n  <details>\n    <summary>\n      <em>\"Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning\" (H. Kaur, H. Nori, S. Jenkins, R. Caruana, H. Wallach, J. Wortman Vaughan 2020)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@inproceedings{kaur2020interpreting,\n  title={Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},\n  author={Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},\n  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\n  pages={1--14},\n  year={2020}\n}\n<\/pre>\n    <a href=\"http:\/\/www-personal.umich.edu\/~harmank\/Papers\/CHI2020_Interpretability.pdf\">Paper link<\/a>\n  <\/details>\n\n  <details>\n    <summary>\n      <em>\"How Interpretable and Trustworthy are GAMs?\" (C. Chang, S. Tan, B. Lengerich, A. Goldenberg, R. Caruana 2020)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{chang2020interpretable,\n  title={How Interpretable and Trustworthy are GAMs?},\n  author={Chang, Chun-Hao and Tan, Sarah and Lengerich, Ben and Goldenberg, Anna and Caruana, Rich},\n  journal={arXiv preprint arXiv:2006.06466},\n  year={2020}\n}\n<\/pre>\n    <a href=\"https:\/\/arxiv.org\/pdf\/2006.06466.pdf\">Paper link<\/a>\n  <\/details>\n\n  <hr\/>\n<\/details>\n\n<details>\n  <summary><strong>Differential Privacy<\/strong><\/summary>\n  <hr\/>\n  <details>\n    <summary>\n      <em>\"Accuracy, Interpretability, and Differential Privacy via Explainable Boosting\" (H. Nori, R. Caruana, Z. Bu, J. Shen, J. Kulkarni 2021)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@inproceedings{pmlr-v139-nori21a,\n  title = \t {Accuracy, Interpretability, and Differential Privacy via Explainable Boosting},\n  author =       {Nori, Harsha and Caruana, Rich and Bu, Zhiqi and Shen, Judy Hanwen and Kulkarni, Janardhan},\n  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n  pages = \t {8227--8237},\n  year = \t {2021},\n  volume = \t {139},\n  series = \t {Proceedings of Machine Learning Research},\n  publisher =    {PMLR}\n}\n<\/pre>\n    <a href=\"http:\/\/proceedings.mlr.press\/v139\/nori21a\/nori21a.pdf\">Paper link<\/a>\n  <\/details>\n  <hr\/>\n<\/details>\n\n<details>\n  <summary><strong>LIME<\/strong><\/summary>\n  <hr\/>\n  <details>\n    <summary>\n      <em>\"Why should i trust you?: Explaining the predictions of any classifier\" (M. T. Ribeiro, S. Singh, and C. Guestrin 2016)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@inproceedings{ribeiro2016should,\n  title={Why should i trust you?: Explaining the predictions of any classifier},\n  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},\n  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},\n  pages={1135--1144},\n  year={2016},\n  organization={ACM}\n}\n<\/pre>\n    <a href=\"https:\/\/arxiv.org\/pdf\/1602.04938.pdf\">Paper link<\/a>\n  <\/details>\n  <hr\/>\n<\/details>\n\n<details>\n  <summary><strong>SHAP<\/strong><\/summary>\n  <hr\/>\n  <details>\n    <summary>\n      <em>\"A Unified Approach to Interpreting Model Predictions\" (S. M. Lundberg and S.-I. Lee 2017)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@incollection{NIPS2017_7062,\n title = {A Unified Approach to Interpreting Model Predictions},\n author = {Lundberg, Scott M and Lee, Su-In},\n booktitle = {Advances in Neural Information Processing Systems 30},\n editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {4765--4774},\n year = {2017},\n publisher = {Curran Associates, Inc.},\n url = {http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions.pdf}\n}\n<\/pre>\n    <a href=\"http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions.pdf\">Paper link<\/a>\n  <\/details>\n  <details>\n    <summary>\n      <em>\"Consistent individualized feature attribution for tree ensembles\" (Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In 2018)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{lundberg2018consistent,\n  title={Consistent individualized feature attribution for tree ensembles},\n  author={Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In},\n  journal={arXiv preprint arXiv:1802.03888},\n  year={2018}\n}\n<\/pre>\n    <a href=\"https:\/\/arxiv.org\/pdf\/1802.03888\">Paper link<\/a>\n  <\/details>\n  <details>\n    <summary>\n      <em>\"Explainable machine-learning predictions for the prevention of hypoxaemia during surgery\" (S. M. Lundberg et al. 2018)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{lundberg2018explainable,\n  title={Explainable machine-learning predictions for the prevention of hypoxaemia during surgery},\n  author={Lundberg, Scott M and Nair, Bala and Vavilala, Monica S and Horibe, Mayumi and Eisses, Michael J and Adams, Trevor and Liston, David E and Low, Daniel King-Wai and Newman, Shu-Fang and Kim, Jerry and others},\n  journal={Nature Biomedical Engineering},\n  volume={2},\n  number={10},\n  pages={749},\n  year={2018},\n  publisher={Nature Publishing Group}\n}\n<\/pre>\n    <a href=\"https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6467492\/pdf\/nihms-1505578.pdf\">Paper link<\/a>\n  <\/details>\n  <hr\/>\n<\/details>\n\n<details>\n  <summary><strong>Sensitivity Analysis<\/strong><\/summary>\n  <hr\/>\n  <details>\n    <summary>\n      <em>\"SALib: An open-source Python library for Sensitivity Analysis\" (J. D. Herman and W. Usher 2017)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{herman2017salib,\n  title={SALib: An open-source Python library for Sensitivity Analysis.},\n  author={Herman, Jonathan D and Usher, Will},\n  journal={J. Open Source Software},\n  volume={2},\n  number={9},\n  pages={97},\n  year={2017}\n}\n<\/pre>\n    <a href=\"https:\/\/www.researchgate.net\/profile\/Will_Usher\/publication\/312204236_SALib_An_open-source_Python_library_for_Sensitivity_Analysis\/links\/5ac732d64585151e80a39547\/SALib-An-open-source-Python-library-for-Sensitivity-Analysis.pdf?origin=publication_detail\">Paper link<\/a>\n  <\/details>\n  <details>\n    <summary>\n      <em>\"Factorial sampling plans for preliminary computational experiments\" (M. D. Morris 1991)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{morris1991factorial,\n  title={},\n  author={Morris, Max D},\n  journal={Technometrics},\n  volume={33},\n  number={2},\n  pages={161--174},\n  year={1991},\n  publisher={Taylor \\& Francis Group}\n}\n<\/pre>\n    <a href=\"https:\/\/abe.ufl.edu\/Faculty\/jjones\/ABE_5646\/2010\/Morris.1991%20SA%20paper.pdf\">Paper link<\/a>\n  <\/details>\n  <hr\/>\n<\/details>\n\n<details>\n  <summary><strong>Partial Dependence<\/strong><\/summary>\n  <hr\/>\n  <details>\n    <summary>\n      <em>\"Greedy function approximation: a gradient boosting machine\" (J. H. Friedman 2001)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{friedman2001greedy,\n  title={Greedy function approximation: a gradient boosting machine},\n  author={Friedman, Jerome H},\n  journal={Annals of statistics},\n  pages={1189--1232},\n  year={2001},\n  publisher={JSTOR}\n}\n    <\/pre>\n    <a href=\"https:\/\/projecteuclid.org\/download\/pdf_1\/euclid.aos\/1013203451\">Paper link<\/a>\n  <\/details>\n  <hr\/>\n<\/details>\n\n\n\n<details>\n  <summary><strong>Open Source Software<\/strong><\/summary>\n  <hr\/>\n  <details>\n    <summary>\n      <em>\"Scikit-learn: Machine learning in Python\" (F. Pedregosa et al. 2011)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{pedregosa2011scikit,\n  title={Scikit-learn: Machine learning in Python},\n  author={Pedregosa, Fabian and Varoquaux, Ga{\\\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},\n  journal={Journal of machine learning research},\n  volume={12},\n  number={Oct},\n  pages={2825--2830},\n  year={2011}\n}\n<\/pre>\n    <a href=\"http:\/\/www.jmlr.org\/papers\/volume12\/pedregosa11a\/pedregosa11a.pdf\">Paper link<\/a>\n  <\/details>\n<details>\n    <summary>\n      <em>\"Collaborative data science\" (Plotly Technologies Inc. 2015)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@online{plotly, \n  author = {Plotly Technologies Inc.}, \n  title = {Collaborative data science}, \n  publisher = {Plotly Technologies Inc.}, \n  address = {Montreal, QC}, \n  year = {2015}, \n  url = {https:\/\/plot.ly} }\n  <\/pre>\n    <a href=\"https:\/\/plot.ly\">Link<\/a>\n<\/details>\n<details>\n    <summary>\n      <em>\"Joblib: running python function as pipeline jobs\" (G. Varoquaux and O. Grisel 2009)<\/em>\n    <\/summary>\n    <br\/>\n    <pre>\n@article{varoquaux2009joblib,\n  title={Joblib: running python function as pipeline jobs},\n  author={Varoquaux, Ga{\\\"e}l and Grisel, O},\n  journal={packages. python. org\/joblib},\n  year={2009}\n}\n  <\/pre>\n    <a href=\"https:\/\/joblib.readthedocs.io\/en\/latest\/\">Link<\/a>\n<\/details>\n  \n  \n  <hr\/>\n<\/details>\n\n# Videos\n\n- [The Science Behind InterpretML: Explainable Boosting Machine](https:\/\/www.youtube.com\/watch?v=MREiHgHgl0k)\n- [How to Explain Models with InterpretML Deep Dive](https:\/\/www.youtube.com\/watch?v=WwBeKMQ0-I8)\n- [Black-Box and Glass-Box Explanation in Machine Learning](https:\/\/youtu.be\/7uzNKY8pEhQ)\n- [Explainable AI explained!  By-design interpretable models with Microsofts InterpretML](https:\/\/www.youtube.com\/watch?v=qPn9m30ojfc)\n- [Interpreting Machine Learning Models with InterpretML](https:\/\/www.youtube.com\/watch?v=ERNuFfsknhk)\n\n# External links\n\n- [Interpretable or Accurate? Why Not Both?](https:\/\/towardsdatascience.com\/interpretable-or-accurate-why-not-both-4d9c73512192)\n- [The Explainable Boosting Machine. As accurate as gradient boosting, as interpretable as linear regression.](https:\/\/towardsdatascience.com\/the-explainable-boosting-machine-f24152509ebb)\n- [Performance And Explainability With EBM](https:\/\/blog.oakbits.com\/ebm-algorithm.html)\n- [InterpretML: Another Way to Explain Your Model](https:\/\/towardsdatascience.com\/interpretml-another-way-to-explain-your-model-b7faf0a384f8)\n- [A gentle introduction to GA2Ms, a white box model](https:\/\/blog.fiddler.ai\/2019\/06\/a-gentle-introduction-to-ga2ms-a-white-box-model)\n- [Model Interpretation with Microsoft\u2019s Interpret ML](https:\/\/medium.com\/@sand.mayur\/model-interpretation-with-microsofts-interpret-ml-85aa0ad697ae)\n- [Explaining Model Pipelines With InterpretML](https:\/\/medium.com\/@mariusvadeika\/explaining-model-pipelines-with-interpretml-a9214f75400b)\n- [Explain Your Model with Microsoft\u2019s InterpretML](https:\/\/medium.com\/@Dataman.ai\/explain-your-model-with-microsofts-interpretml-5daab1d693b4)\n- [On Model Explainability: From LIME, SHAP, to Explainable Boosting](https:\/\/everdark.github.io\/k9\/notebooks\/ml\/model_explain\/model_explain.nb.html)\n- [Dealing with Imbalanced Data (Mortgage loans defaults)](https:\/\/mikewlange.github.io\/ImbalancedData-\/index.html)\n- [The right way to compute your Shapley Values](https:\/\/towardsdatascience.com\/the-right-way-to-compute-your-shapley-values-cfea30509254)\n- [The Art of Sprezzatura for Machine Learning](https:\/\/towardsdatascience.com\/the-art-of-sprezzatura-for-machine-learning-e2494c0db727)\n\n# Papers that use or compare EBMs\n\n- [GAM(E) CHANGER OR NOT? AN EVALUATION OF INTERPRETABLE MACHINE LEARNING MODELS](https:\/\/arxiv.org\/pdf\/2204.09123.pdf)\n- [Revealing the Galaxy-Halo Connection Through Machine Learning](https:\/\/arxiv.org\/pdf\/2204.10332.pdf)\n- [Explainable Artificial Intelligence for COVID-19 Diagnosis Through Blood Test Variables](https:\/\/link.springer.com\/article\/10.1007\/s40313-021-00858-y)\n- [Using Explainable Boosting Machines (EBMs) to Detect Common Flaws in Data](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-030-93736-2_40)\n- [Explainable Boosting Machines for Slope Failure Spatial Predictive Modeling](https:\/\/www.mdpi.com\/2072-4292\/13\/24\/4991\/htm)\n- [Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health](https:\/\/arxiv.org\/pdf\/2109.13770.pdf)\n- [Identifying main and interaction effects of risk factors to predict intensive care admission in patients hospitalized with COVID-19](https:\/\/www.medrxiv.org\/content\/10.1101\/2020.06.30.20143651v1.full.pdf)\n- [Neural Additive Models: Interpretable Machine Learning with Neural Nets](https:\/\/arxiv.org\/pdf\/2004.13912.pdf)\n- [NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning](https:\/\/arxiv.org\/pdf\/2106.01613.pdf)\n- [Integrating Co-Clustering and Interpretable Machine Learning for the Prediction of Intravenous Immunoglobulin Resistance in Kawasaki Disease](https:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?tp=&arnumber=9097874)\n- [GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions](https:\/\/arxiv.org\/pdf\/2003.07132v1.pdf)\n- [Interpretable Prediction of Goals in Soccer](http:\/\/statsbomb.com\/wp-content\/uploads\/2019\/10\/decroos-interpretability-statsbomb.pdf)\n- [Extending the Tsetlin Machine with Integer-Weighted Clauses for Increased Interpretability](https:\/\/arxiv.org\/pdf\/2005.05131.pdf)\n- [In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction](https:\/\/arxiv.org\/pdf\/2005.04176.pdf)\n- [Development and Validation of an Interpretable 3-day Intensive Care Unit Readmission Prediction Model Using Explainable Boosting Machines](https:\/\/www.medrxiv.org\/content\/10.1101\/2021.11.01.21265700v1.full.pdf)\n- [Explainable Boosting Machine for Predicting Alzheimer\u2019s Disease from MRI Hippocampal Subfields](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-030-86993-9_31)\n- [Impact of Accuracy on Model Interpretations](https:\/\/arxiv.org\/pdf\/2011.09903.pdf)\n\n# Books that include EBMs\n\n- [Interpretable Machine Learning with Python](https:\/\/www.amazon.com\/Interpretable-Machine-Learning-Python-hands\/dp\/180020390X)\n- [Explainable Artificial Intelligence: An Introduction to Interpretable Machine Learning](https:\/\/www.amazon.com\/Explainable-Artificial-Intelligence_-An-Introduction-to-Interpretable-XAI\/dp\/3030833550)\n- [Machine Learning for High-Risk Applications](https:\/\/www.oreilly.com\/library\/view\/machine-learning-for\/9781098102425\/)\n\n# External tools\n\n- [EBM to Onnx converter by SoftAtHome](https:\/\/github.com\/interpretml\/ebm2onnx)\n- [GAM Changer](https:\/\/github.com\/interpretml\/gam-changer)\n- [ML 2 SQL (experimental)](https:\/\/github.com\/kaspersgit\/ml_2_sql)\n\n# Contact us\n\nThere are multiple ways to get in touch:\n- Email us at interpret@microsoft.com\n- Or, feel free to raise a GitHub issue\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n<br\/>\n\n> ### If a tree fell in your random forest, would anyone notice?\n","61":"\n\n# Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine\n\nDSSTNE (pronounced \"Destiny\") is an open source software library for training and deploying recommendation\nmodels with sparse inputs, fully connected hidden layers, and sparse outputs. Models with weight matrices\nthat are too large for a single GPU can still be trained on a single host. DSSTNE has been used at Amazon\nto generate personalized product recommendations for our customers at Amazon's scale. It is designed for\nproduction deployment of real-world applications which need to emphasize speed and scale over experimental \nflexibility.\n\nDSSTNE was built with a number of features for production recommendation workloads:\n\n* **Multi-GPU Scale**: Training and prediction\nboth scale out to use multiple GPUs, spreading out computation\nand storage in a model-parallel fashion for each layer.\n* **Large Layers**: Model-parallel scaling enables larger networks than\nare possible with a single GPU.\n* **Sparse Data**: DSSTNE is optimized for fast performance on sparse datasets, common in recommendation \nproblems. Custom GPU kernels perform sparse computation on the GPU, without filling in lots of zeroes.\n\n## Benchmarks\n* scottlegrand@ reported [near-linear scaling with multiple GPUs] on the MovieLens recommendation problem \n(https:\/\/medium.com\/@scottlegrand\/first-dsstne-benchmarks-tldr-almost-15x-faster-than-tensorflow-393dbeb80c0f#.ghe74fu1q)\n* Directions on how to run a benchmark can be found in [here](benchmarks\/Benchmark.md)\n\n## Scaling up\n* [Using Spark in AWS EMR and Dockers in AWS ECS ](http:\/\/blogs.aws.amazon.com\/bigdata\/post\/TxGEL8IJ0CAXTK\/Generating-Recommendations-at-Amazon-Scale-with-Apache-Spark-and-Amazon-DSSTNE)\n    \n\n## License\n[License](LICENSE)\n\n\n \n \n\n## Setup\n* Follow [Setup](docs\/getting_started\/setup.md) for step by step instructions on installing and setting up DSSTNE\n\n## User Guide\n* Check [User Guide](docs\/getting_started\/userguide.md) for detailed information about the features in DSSTNE\n\n## Examples\n* Check [Examples](docs\/getting_started\/examples.md) to start trying your first Neural Network Modeling using DSSTNE\n\n## Q&A\n[FAQ](FAQ.md)\n","62":"![Flashlight: Fast, Flexible Machine Learning in C++](.\/logo.svg)\n\n<hr\/>\n\n[**Quickstart**](#quickstart)\n| [**Installation**](#building-and-installing)\n| [**Documentation**](https:\/\/fl.readthedocs.io\/en\/latest\/)\n| [**Citing**](#citing)\n\n[![CircleCI](https:\/\/circleci.com\/gh\/flashlight\/flashlight.svg?style=shield)](https:\/\/app.circleci.com\/pipelines\/github\/flashlight\/flashlight)\n[![Documentation Status](https:\/\/img.shields.io\/readthedocs\/fl.svg)](https:\/\/fl.readthedocs.io\/en\/latest\/)\n[![Docker Image Build Status](https:\/\/img.shields.io\/github\/workflow\/status\/flashlight\/flashlight\/Publish%20Docker%20images?label=docker%20image%20build)](https:\/\/hub.docker.com\/r\/flml\/flashlight\/tags)\n[![Join the chat at https:\/\/gitter.im\/flashlight-ml\/community](https:\/\/img.shields.io\/gitter\/room\/flashlight-ml\/community)](https:\/\/gitter.im\/flashlight-ml\/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n[![codecov](https:\/\/codecov.io\/gh\/flashlight\/flashlight\/branch\/master\/graph\/badge.svg?token=rBp4AilMc0)](https:\/\/codecov.io\/gh\/flashlight\/flashlight)\n\n[![Docker Image for CUDA backend](https:\/\/img.shields.io\/docker\/image-size\/flml\/flashlight\/cuda-latest?label=docker%20%28cuda%29&logo=docker)](https:\/\/hub.docker.com\/r\/flml\/flashlight\/tags?page=1&ordering=last_updated&name=cuda-latest)\n[![Docker Image for CPU backend](https:\/\/img.shields.io\/docker\/image-size\/flml\/flashlight\/cpu-latest?label=docker%20%28cpu%29&logo=docker)](https:\/\/hub.docker.com\/r\/flml\/flashlight\/tags?page=1&ordering=last_updated&name=cpu-latest)\n\n[![Install CUDA backend with vcpkg](https:\/\/img.shields.io\/badge\/dynamic\/json?color=orange&label=get%20%28cuda%29&query=name&url=https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2Fvcpkg%2Fmaster%2Fports%2Fflashlight-cuda%2Fvcpkg.json&prefix=vcpkg%20install%20)](https:\/\/vcpkg.info\/port\/flashlight-cuda)\n[![Install CPU backend with vcpkg](https:\/\/img.shields.io\/badge\/dynamic\/json?color=orange&label=get%20%28cpu%29&query=name&url=https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2Fvcpkg%2Fmaster%2Fports%2Fflashlight-cpu%2Fvcpkg.json&prefix=vcpkg%20install%20)](https:\/\/vcpkg.info\/port\/flashlight-cpu)\n\n\nFlashlight is a fast, flexible machine learning library written entirely in C++\nfrom the Facebook AI Research and the creators of Torch, TensorFlow, Eigen and\nDeep Speech. Its core features include:\n- **Modifiability to the core** including [internal APIs for tensor computation](flashlight\/fl\/tensor\/README.md).\n- **A small footprint**, with the core clocking in at under 10 MB and 20k lines of C++.\n- **High-performance defaults** featuring fust-in-time kernel compilation with modern C++ via the [*ArrayFire*](https:\/\/github.com\/arrayfire\/arrayfire)\ntensor library.\n- An emphasis on efficiency and scale.\n\nNative support in C++ and simple extensibility makes Flashlight a powerful research framework that's *hackable to its core* and enables fast iteration on new experimental setups and algorithms with little unopinionation and without sacrificing performance. In a single repository, Flashlight provides [apps](https:\/\/github.com\/flashlight\/flashlight\/tree\/master\/flashlight\/app) for research across multiple domains:\n- [Automatic speech recognition](https:\/\/github.com\/flashlight\/flashlight\/tree\/master\/flashlight\/app\/asr) (formerly [wav2letter](https:\/\/github.com\/flashlight\/wav2letter\/) project) \u2014\u00a0[Documentation](flashlight\/app\/asr) | [Tutorial](flashlight\/app\/asr\/tutorial)\n- [Image classification](flashlight\/app\/imgclass)\n- [Object detection](flashlight\/app\/objdet)\n- [Language modeling](flashlight\/app\/lm)\n\n\n### Project Layout\n\nFlashlight is broken down into a few parts:\n- [**`flashlight\/lib`**](flashlight\/lib) contains kernels and standalone utilities for sequence losses, beam search decoding, text processing, and more.\n- [**`flashlight\/fl`**](flashlight\/fl) is the core tensor interface and neural network library using the [ArrayFire](https:\/\/github.com\/arrayfire\/arrayfire) tensor library by default.\n- [**`flashlight\/pkg`**](flashlight\/pkg) are domain packages for speech, vision, and text built on the core.\n- [**`flashlight\/app`**](flashlight\/app) are applications of the core library to machine learning across domains.\n\n## Quickstart\n\nFirst, [build and install Flashlight](#building-and-installing) and [link it to your own project](#building-your-own-project-with-flashlight).\n\n[`Sequential`](https:\/\/fl.readthedocs.io\/en\/latest\/modules.html#sequential) forms a sequence of Flashlight [`Module`](https:\/\/fl.readthedocs.io\/en\/latest\/modules.html#module)s for chaining computation.\n\n<details><summary>Implementing a simple convnet is easy.<\/summary>\n\n```c++\n#include <flashlight\/fl\/flashlight.h>\n\nSequential model;\n\nmodel.add(View(fl::Shape({IM_DIM, IM_DIM, 1, -1})));\nmodel.add(Conv2D(\n    1 \/* input channels *\/,\n    32 \/* output channels *\/,\n    5 \/* kernel width *\/,\n    5 \/* kernel height *\/,\n    1 \/* stride x *\/,\n    1 \/* stride y *\/,\n    PaddingMode::SAME; \/* padding mode *\/,\n    PaddingMode::SAME; \/* padding mode *\/));\nmodel.add(ReLU());\nmodel.add(Pool2D(\n    2 \/* kernel width *\/,\n    2 \/* kernel height *\/,\n    2 \/* stride x *\/,\n    2 \/* stride y *\/));\nmodel.add(Conv2D(32, 64, 5, 5, 1, 1, PaddingMode::SAME, PaddingMode::SAME));\nmodel.add(ReLU());\nmodel.add(Pool2D(2, 2, 2, 2));\nmodel.add(View(fl::Shape({7 * 7 * 64, -1})));\nmodel.add(Linear(7 * 7 * 64, 1024));\nmodel.add(ReLU());\nmodel.add(Dropout(0.5));\nmodel.add(Linear(1024, 10));\nmodel.add(LogSoftmax());\n```\n\nPerforming forward and backward computation is straightforwards:\n```c++\nauto output = model.forward(input);\nauto loss = categoricalCrossEntropy(output, target);\nloss.backward();\n```\n\n<\/details>\n\nSee the [MNIST example](https:\/\/fl.readthedocs.io\/en\/latest\/mnist.html) for a full tutorial including a training loop and dataset abstractions.\n\n[`Variable`](https:\/\/fl.readthedocs.io\/en\/latest\/variable.html) is the base Flashlight tensor that operates on [ArrayFire `array`s](http:\/\/arrayfire.org\/docs\/classaf_1_1array.htm). Tape-based [Automatic differentiation in Flashlight](https:\/\/fl.readthedocs.io\/en\/latest\/autograd.html) is simple and works as you'd expect.\n\n<details><summary>Autograd Example<\/summary>\n\n```c++\nauto A = Variable(fl::rand({1000, 1000}), true \/* calcGrad *\/);\nauto B = 2.0 * A;\nauto C = 1.0 + B;\nauto D = log(C);\nD.backward(); \/\/ populates A.grad() along with gradients for B, C, and D.\n```\n\n<\/details>\n\n## Building and Installing\n[**Install with `vcpkg`**](#library-installation-with-vcpkg) | [**With Docker**](#building-and-running-flashlight-with-docker) | [**From Source**](#building-from-source) | [**From Source with `vcpkg`**](#from-source-build-with-vcpkg) | [**Build Your Project with Flashlight**](#building-your-own-project-with-flashlight)\n\n### Requirements\nAt minimum, compilation requires:\n- A C++ compiler with good C++17 support (e.g. gcc\/g++ >= 7)\n- [CMake](https:\/\/cmake.org\/) \u2014 version 3.10 or later, and ``make``\n- A Linux-based operating system.\n\nSee the [full dependency](#dependencies) list for more details if [building from source](#building-from-source).\n\nInstructions for building\/installing Python bindings [can be found here](bindings\/python\/README.md).\n\n### Flashlight Build Setups\n\nFlashlight can be broken down into several components as [described above](#project-layout). Each component can be incrementally built by specifying the correct [build options](#build-options).\n\nThere are two ways to work with Flashlight:\n1. **As an installed library** that you link to with your own project. This is best for building standalone applications dependent on Flashlight.\n2. **With in-source development** where the Flashlight project source is changed and rebuilt. This is best if customizing\/hacking the core framework or the Flashlight-provided [app binaries](flashlight\/app).\n\nFlashlight can be built in one of two ways:\n1. [**With `vcpkg`**](#installing-flashlight-with-vcpkg), a [C++ package manager](https:\/\/github.com\/microsoft\/vcpkg).\n2. [**From source**](#building-from-source) by installing dependencies as needed.\n\n### Installing Flashlight with `vcpkg`\n#### Library Installation with `vcpkg`\n\nFlashlight is most-easily built and installed with `vcpkg`. Both the CUDA and CPU backends are supported with `vcpkg`. For either backend, first install [Intel MKL](https:\/\/software.intel.com\/content\/www\/us\/en\/develop\/tools\/oneapi\/base-toolkit\/download.html). For the CUDA backend, install [`CUDA` >= 9.2](https:\/\/developer.nvidia.com\/cuda-downloads), [`cuDNN`](https:\/\/docs.nvidia.com\/deeplearning\/cudnn\/install-guide\/index.html), and [`NCCL`](https:\/\/docs.nvidia.com\/deeplearning\/nccl\/install-guide\/index.html). Then, after [installing `vcpkg`](https:\/\/github.com\/microsoft\/vcpkg#getting-started), install the libraries and core with:\n```shell\n.\/vcpkg\/vcpkg install flashlight-cuda # CUDA backend, OR\n.\/vcpkg\/vcpkg install flashlight-cpu  # CPU backend\n```\nTo install [Flashlight apps](flashlight\/app), check the features available for installation by running `.\/vcpkg search flashlight-cuda` or `.\/vcpkg search flashlight-cpu`. Each app is a \"feature\": for example, `.\/vcpkg install flashlight-cuda[asr]` installs the ASR app with the CUDA backend.\n\nBelow is the currently-supported list of features (for each of [`flashlight-cuda`](https:\/\/vcpkg.info\/port\/flashlight-cuda) and [`flashlight-cpu`](https:\/\/vcpkg.info\/port\/flashlight-cpu)):\n```\nflashlight-{cuda\/cpu}[lib]      # Flashlight libraries\nflashlight-{cuda\/cpu}[nn]       # Flashlight neural net library\nflashlight-{cuda\/cpu}[asr]      # Flashlight speech recognition app\nflashlight-{cuda\/cpu}[lm]       # Flashlight language modeling app\nflashlight-{cuda\/cpu}[imgclass] # Flashlight image classification app\n```\n\nFlashlight [app binaries](flashlight\/app) are also built for the selected features and are installed into the `vcpkg` install tree's `tools` directory.\n\n[Integrating Flashlight into your own project](#with-a-vcpkg-flashlight-installation) with is simple using `vcpkg`'s [CMake toolchain integration](https:\/\/vcpkg.readthedocs.io\/en\/latest\/examples\/installing-and-using-packages\/#cmake).\n\n#### From-Source Build with `vcpkg`\n\nFirst, install the dependencies for your backend of choice using `vcpkg` (click to expand the below):\n\n<details><summary>Installing CUDA Backend Dependencies with vcpkg<\/summary>\n\nTo build the Flashlight CUDA backend from source using dependencies installed with `vcpkg`, install [`CUDA` >= 9.2](https:\/\/developer.nvidia.com\/cuda-downloads), [`cuDNN`](https:\/\/docs.nvidia.com\/deeplearning\/cudnn\/install-guide\/index.html), [`NCCL`](https:\/\/docs.nvidia.com\/deeplearning\/nccl\/install-guide\/index.html), and [Intel MKL](https:\/\/software.intel.com\/content\/www\/us\/en\/develop\/tools\/oneapi\/base-toolkit\/download.html), then build the rest of the dependencies for the CUDA backend based on which Flashlight features you'd like to build:\n```shell\n.\/vcpkg install \\\n    cuda intel-mkl fftw3 cub kenlm                \\ # if building flashlight libraries\n    arrayfire[cuda] cudnn nccl openmpi cereal stb \\ # if building the flashlight neural net library\n    gflags glog                                   \\ # if building any flashlight apps\n    libsndfile                                    \\ # if building the flashlight asr app\n    gtest                                           # optional, if building tests\n```\n<\/details>\n\n<details><summary>Installing CPU Backend Dependencies with vcpkg<\/summary>\n\nTo build the Flashlight CPU backend from source using dependencies installed with `vcpkg`, install [Intel MKL](https:\/\/software.intel.com\/content\/www\/us\/en\/develop\/tools\/oneapi\/base-toolkit\/download.html), then build the rest of the dependencies for the CPU backend based on which Flashlight features you'd like to build:\n```shell\n.\/vcpkg install \\\n    intel-mkl fftw3 kenlm                              \\ # for flashlight libraries\n    arrayfire[cpu] gloo[mpi] openmpi onednn cereal stb \\ # for the flashlight neural net library\n    gflags glog                                        \\ # for the flashlight runtime pkg (any flashlight apps using it)\n    libsndfile                                         \\ # for the flashlight speech pkg\n    gtest                                                # optional, for tests\n```\n\n<\/details>\n\n##### Build Using the `vcpkg` Toolchain File\nTo build Flashlight from source with these dependencies, clone the repository:\n```shell\ngit clone https:\/\/github.com\/flashlight\/flashlight.git && cd flashlight\nmkdir -p build && cd build\n```\nThen, build from source using `vcpkg`'s [CMake toolchain](https:\/\/github.com\/microsoft\/vcpkg\/blob\/master\/docs\/users\/integration.md#cmake-toolchain-file-recommended-for-open-source-cmake-projects):\n```shell\ncmake .. \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DFL_BUILD_ARRAYFIRE=ON \\\n    -DCMAKE_TOOLCHAIN_FILE=[path to your vcpkg clone]\/scripts\/buildsystems\/vcpkg.cmake\nmake -j$(nproc)\nmake install -j$(nproc) # only if you want to install Flashlight for external use\n```\nTo build a subset of Flashlight's features, see the [build options](#build-options) below.\n\n### Building from Source\nTo build from source, first install the below [dependencies](#dependencies). Most are available with your system's local package manager.\n\nSome dependencies marked below are downloaded and installed automatically if not found on the local system. `FL_BUILD_STANDALONE` determines this behavior \u2014 if disabled, dependencies won't be downloaded and built when building Flashlight.\n\n**Once all dependencies are installed**, clone the repository:\n```shell\ngit clone https:\/\/github.com\/flashlight\/flashlight.git && cd flashlight\nmkdir -p build && cd build\n```\nThen build all Flashlight components with:\n```\ncmake .. -DCMAKE_BUILD_TYPE=Release -DFL_BUILD_ARRAYFIRE=ON [...build options]\nmake -j$(nproc)\nmake install\n```\nSetting the `MKLROOT` environment variable (`export MKLROOT=\/opt\/intel\/oneapi\/mkl\/latest` or `export MKLROOT=\/opt\/intel\/mkl` on most Linux-based systems) can help CMake find Intel MKL if not initially found.\n\nTo build a smaller subset of Flashlight features\/apps, see the [build options](#build-options) below for a complete list of options.\n\nTo install Flashlight in a custom directory, use CMake's [`CMAKE_INSTALL_PREFIX`](https:\/\/cmake.org\/cmake\/help\/v3.10\/variable\/CMAKE_INSTALL_PREFIX.html) argument. Flashlight libraries can be built as shared libraries using CMake's [`BUILD_SHARED_LIBS`](https:\/\/cmake.org\/cmake\/help\/v3.10\/variable\/BUILD_SHARED_LIBS.html) argument.\n\nFlashlight uses modern CMake and `IMPORTED` targets for most dependencies. If a dependency isn't found, passing `-D<package>_DIR` to your `cmake` command or exporting `<package>_DIR` as an environment variable equal to the path to `<package>Config.cmake` can help locate dependencies on your system. See [the documentation](https:\/\/cmake.org\/cmake\/help\/v3.10\/command\/find_package.html) for more details. If CMake is failing to locate a package, check to see if a corresponding [issue](https:\/\/github.com\/flashlight\/flashlight\/issues) has already been created before creating your own.\n\n#### Dependencies\n\nDependencies marked with `*` are automatically downloaded and built from source if not found on the system. Setting `FL_BUILD_STANDALONE` to `OFF` disables this behavior.\n\nDependencies marked with `^` are required if building with distributed training enabled (`FL_BUILD_DISTRIBUTED` \u2014\u00a0see the [build options](#build-options) below). Distributed training is required for all apps.\n\nDependencies marked with `\u2020` are installable via `vcpkg`. See the [instructions for installing those dependencies](#from-source-build-with-vcpkg) above for doing a Flashlight from-source build.\n\n<div class=\"tg-wrap\"><table>\n<thead>\n  <tr>\n    <th>Component<\/th>\n    <th>Backend<\/th>\n    <th>Dependencies<\/th>\n  <\/tr>\n<\/thead>\n<tbody>\n  <tr>\n    <td rowspan=\"2\"> Audio library (fl_lib_audio) <\/td>\n    <td>CUDA<\/td>\n    <td><a href=\"https:\/\/developer.nvidia.com\/cuda-downloads\">CUDA<\/a> &gt;= 9.2, <a href=\"https:\/\/github.com\/nvidia\/cub\">CUB<\/a>*\u2020 (if CUDA &lt; 11)<\/td>\n  <\/tr>\n  <tr>\n    <td>CPU<\/td>\n    <td>A BLAS library (<a href=\"https:\/\/software.intel.com\/content\/www\/us\/en\/develop\/tools\/oneapi\/base-toolkit\/download.html\">Intel MKL<\/a> &gt;= 2018, OpenBLAS\u2020, etc)<\/td>\n  <\/tr>\n  <tr>\n    <td rowspan=\"3\">core<\/td>\n    <td>Any<\/td>\n    <td><a href=\"https:\/\/github.com\/arrayfire\/arrayfire#installation\">ArrayFire<\/a> &gt;= 3.7.3\u2020, an MPI library^(<a href=\"https:\/\/www.open-mpi.org\/\">OpenMPI<\/a>\u2020, etc),&nbsp;&nbsp;<a href=\"https:\/\/github.com\/USCiLab\/cereal\">cereal<\/a>*\u2020 &gt;= 1.3.0, <a href=\"https:\/\/github.com\/nothings\/stb\">stb<\/a>*\u2020<\/td>\n  <\/tr>\n  <tr>\n    <td>CUDA<\/td>\n    <td><a href=\"https:\/\/developer.nvidia.com\/cuda-downloads\">CUDA<\/a> &gt;= 9.2, <a href=\"https:\/\/developer.nvidia.com\/nccl\">NCCL<\/a>^, <a href=\"https:\/\/developer.nvidia.com\/cuDNN\">cuDNN<\/a><\/td>\n  <\/tr>\n  <tr>\n    <td>CPU<\/td>\n    <td><a href=\"https:\/\/github.com\/oneapi-src\/oneDNN\">oneDNN<\/a>\u2020 &gt;= 2.0, <a href=\"https:\/\/github.com\/facebookincubator\/gloo\">gloo<\/a> (<a href=\"https:\/\/github.com\/facebookincubator\/gloo\/blob\/01e2c2660cd43963ce1fe3e21220ac01f07d9a4b\/docs\/rendezvous.md#using-mpi\">with MPI<\/a>)*^\u2020<\/td>\n  <\/tr>\n  <tr>\n    <td> Runtime package (fl_pkg_runtime)  <\/td>\n    <td>Any<\/td>\n    <td><a href=\"https:\/\/github.com\/google\/glog\">Google Glog<\/a>\u2020, <a href=\"https:\/\/github.com\/gflags\/gflags\">Gflags<\/a>\u2020<\/td>\n  <\/tr>\n  <tr>\n    <td> Speech package (fl_pkg_speech) <\/td>\n    <td>Any<\/td>\n    <td><a href=\"https:\/\/github.com\/libsndfile\/libsndfile\">libsndfile<\/a>*\u2020 &gt;= 10.0.28, a BLAS library (<a href=\"https:\/\/software.intel.com\/content\/www\/us\/en\/develop\/tools\/oneapi\/base-toolkit\/download.html\">Intel MKL<\/a> &gt;= 2018, OpenBLAS\u2020, etc)<\/td>\n  <\/tr>\n  <tr>\n    <td>app: imgclass<\/td>\n    <td>Any<\/td>\n    <td>-<\/td>\n  <\/tr>\n  <tr>\n    <td>app: objdet<\/td>\n    <td>Any<\/td>\n    <td>-<\/td>\n  <\/tr>\n  <tr>\n    <td>app: lm<\/td>\n    <td>Any<\/td>\n    <td>-<\/td>\n  <\/tr>\n  <tr>\n    <td>tests<\/td>\n    <td>Any<\/td>\n    <td><a href=\"https:\/\/github.com\/google\/googletest\">Google Test (gtest, with gmock)<\/a>*\u2020 &gt;= 1.10.0<\/td>\n  <\/tr>\n<\/tbody>\n<\/table><\/div>\n\n#### Build Options\nThe Flashlight CMake build accepts the following build options (prefixed with `-D` when running CMake from the command line):\n\n<style type=\"text\/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-cly1{text-align:left;vertical-align:middle}\n.tg .tg-g7sd{border-color:inherit;font-weight:bold;text-align:left;vertical-align:middle}\n.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n<\/style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-g7sd\"><span style=\"font-weight:bold\">Name<\/span><\/th>\n    <th class=\"tg-yla0\"><span style=\"font-weight:bold\">Options<\/span><\/th>\n    <th class=\"tg-yla0\"><span style=\"font-weight:bold\">Default Value<\/span><\/th>\n    <th class=\"tg-yla0\"><span style=\"font-weight:bold\">Description<\/span><\/th>\n  <\/tr>\n<\/thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_ARRAYFIRE<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build Flashlight with the ArrayFire backend.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_STANDALONE<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Downloads\/builds some dependencies if not found.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_LIBRARIES<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build the Flashlight libraries.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_CORE<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build the Flashlight neural net library.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_DISTRIBUTED<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build with distributed training; required for apps.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_CONTRIB<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build contrib APIs subject to breaking changes.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_APPS<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build applications (see below).<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_APP_ASR<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build the automatic speech recognition application.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_APP_IMGCLASS<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build the image classification application.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_APP_LM<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build the language modeling application.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_APP_ASR_TOOLS<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build automatic speech recognition app tools.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_TESTS<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build tests.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_EXAMPLES<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">ON<\/td>\n    <td class=\"tg-cly1\">Build examples.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">FL_BUILD_EXPERIMENTAL<\/td>\n    <td class=\"tg-0lax\">ON, OFF<\/td>\n    <td class=\"tg-0lax\">OFF<\/td>\n    <td class=\"tg-cly1\">Build experimental components.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">CMAKE_BUILD_TYPE<\/td>\n    <td class=\"tg-0lax\">See <a href=\"https:\/\/cmake.org\/cmake\/help\/v3.10\/variable\/CMAKE_BUILD_TYPE.html\">docs<\/a>.<\/td>\n    <td class=\"tg-0lax\">Debug<\/td>\n    <td class=\"tg-cly1\">See the <a href=\"https:\/\/cmake.org\/cmake\/help\/v3.10\/variable\/CMAKE_BUILD_TYPE.html\">CMake documentation<\/a>.<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-cly1\">CMAKE_INSTALL_PREFIX<\/td>\n    <td class=\"tg-cly1\">[Directory]<\/td>\n    <td class=\"tg-cly1\">See <a href=\"https:\/\/cmake.org\/cmake\/help\/v3.10\/variable\/CMAKE_INSTALL_PREFIX.html\">docs<\/a>.<\/td>\n    <td class=\"tg-cly1\">See the <a href=\"https:\/\/cmake.org\/cmake\/help\/v3.10\/variable\/CMAKE_INSTALL_PREFIX.html\">CMake documentation<\/a>.<\/td>\n  <\/tr>\n<\/tbody>\n<\/table>\n\n### Building Your Own Project with Flashlight\nFlashlight is most-easily linked to using CMake. Flashlight exports the following CMake targets when installed:\n- `flashlight::fl_lib_common` \u2014 contains flashlight libraries for common headers and symbols used throughout flashlight.\n- `flashlight::fl_lib_set` \u2014 contains flashlight libraries for headers and symbols pertaining to sets.\n- `flashlight::fl_lib_sequence` \u2014 contains flashlight libraries for headers and symbols pertaining to sequences.\n- `flashlight::fl_lib_audio` \u2014 contains flashlight libraries for headers and symbols pertaining to audio.\n- `flashlight::fl_lib_text` \u2014 contains flashlight libraries for headers and symbols pertaining to text.\n- `flashlight::flashlight` \u2014 contains flashlight libraries as well as the flashlight core autograd and neural network library.\n- `flashlight::fl_pkg_runtime` \u2014 contains flashlight core as well as common utilities for training (logging \/ flags \/ distributed utils).\n- `flashlight::fl_pkg_vision` \u2014 contains flashlight core as well as common utilities for vision pipelines.\n- `flashlight::fl_pkg_text` \u2014 contains flashlight core as well as common utilities for dealing with text data.\n- `flashlight::fl_pkg_speech` \u2014 contains flashlight core as well as common utilities for dealing with speech data.\n- `flashlight::fl_pkg_halide` \u2014 contains flashlight core and extentions to easily interface with halide.\n\nGiven a simple `project.cpp` file that includes and links to Flashlight:\n```c++\n#include <iostream>\n\n#include <flashlight\/fl\/flashlight.h>\n\nint main() {\n fl::Variable v(fl::full({1}, 1.), true);\n auto result = v + 10;\n std::cout << \"Tensor value is \" << result.tensor() << std::endl; \/\/ 11.000\n return 0;\n}\n```\n\nThe following CMake configuration links Flashlight and sets include directories:\n\n```cmake\ncmake_minimum_required(VERSION 3.10)\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nadd_executable(myProject project.cpp)\n\nfind_package(flashlight CONFIG REQUIRED)\ntarget_link_libraries(myProject PRIVATE flashlight::flashlight)\n```\n\n#### With a `vcpkg` Flashlight Installation\n\nIf you installed Flashlight with `vcpkg`, the above CMake configuration for `myProject` can be built by running:\n```shell\ncd project && mkdir build && cd build\ncmake .. \\\n  -DCMAKE_TOOLCHAIN_FILE=[path to vcpkg clone]\/scripts\/buildsystems\/vcpkg.cmake \\\n  -DCMAKE_BUILD_TYPE=Release\nmake -j$(nproc)\n```\n\n#### With a From-Source Flashlight Installation\n\nIf using a from-source installation of Flashlight, Flashlight will be found automatically by CMake:\n```shell\ncd project && mkdir build && cd build\ncmake .. -DCMAKE_BUILD_TYPE=Release\nmake -j$(nproc)\n```\nIf Flashlight is installed in a custom location using a `CMAKE_INSTALL_PREFIX`, passing `-Dflashlight_DIR=[install prefix]\/share\/flashlight\/cmake` as an argument to your `cmake` command can help CMake find Flashlight.\n\n### Building and Running Flashlight with Docker\nFlashlight and its dependencies can also be built with the provided Dockerfiles \u2014 see the accompanying [Docker documentation](.docker) for more information.\n\n### Contributing and Contact\nContact: vineelkpratap@fb.com, awni@fb.com, jacobkahn@fb.com, qiantong@fb.com, antares@fb.com, padentomasello@fb.com,\njcai@fb.com,  gab@fb.com, vitaliy888@fb.com, locronan@fb.com\n\nFlashlight is being very actively developed. See\n[CONTRIBUTING](CONTRIBUTING.md) for more on how to help out.\n\n#### Acknowledgments\nSome of Flashlight's code is derived from\n[arrayfire-ml](https:\/\/github.com\/arrayfire\/arrayfire-ml\/).\n\n## Citing\nYou can cite [Flashlight](https:\/\/arxiv.org\/abs\/2201.12465) using:\n```\n@misc{kahn2022flashlight,\n      title={Flashlight: Enabling Innovation in Tools for Machine Learning},\n      author={Jacob Kahn and Vineel Pratap and Tatiana Likhomanenko and Qiantong Xu and Awni Hannun and Jeff Cai and Paden Tomasello and Ann Lee and Edouard Grave and Gilad Avidov and Benoit Steiner and Vitaliy Liptchinsky and Gabriel Synnaeve and Ronan Collobert},\n      year={2022},\n      eprint={2201.12465},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n## License\nFlashlight is under an MIT license. See [LICENSE](LICENSE) for more information.\n","63":"<h2 align=\"center\">\n  <a href=\"http:\/\/mlpack.org\"><img\nsrc=\"https:\/\/cdn.rawgit.com\/mlpack\/mlpack.org\/e7d36ed8\/mlpack-black.svg\" style=\"background-color:rgba(0,0,0,0);\" height=230 alt=\"mlpack: a fast, flexible machine learning library\"><\/a>\n  <br>a fast, flexible machine learning library<br>\n<\/h2>\n\n<h5 align=\"center\">\n  <a href=\"https:\/\/mlpack.org\">Home<\/a> |\n  <a href=\"https:\/\/www.mlpack.org\/docs.html\">Documentation<\/a> |\n  <a href=\"https:\/\/www.mlpack.org\/doc\/mlpack-git\/doxygen\/index.html\">Doxygen<\/a> |\n  <a href=\"https:\/\/www.mlpack.org\/community.html\">Community<\/a> |\n  <a href=\"https:\/\/www.mlpack.org\/questions.html\">Help<\/a> |\n  <a href=\"https:\/\/webchat.freenode.net\/?channels=mlpack\">IRC Chat<\/a>\n<\/h5>\n\n<p align=\"center\">\n  <a href=\"https:\/\/dev.azure.com\/mlpack\/mlpack\/_build?definitionId=1\"><img alt=\"Azure DevOps builds (job)\" src=\"https:\/\/img.shields.io\/azure-devops\/build\/mlpack\/84320e87-76e3-4b6e-8b6e-3adaf6b36eed\/1\/master?job=Linux&label=Linux%20Build&style=flat-square\"><\/a>\n  <a href=\"https:\/\/opensource.org\/licenses\/BSD-3-Clause\"><img src=\"https:\/\/img.shields.io\/badge\/License-BSD%203--Clause-blue.svg?style=flat-square\" alt=\"License\"><\/a>\n  <a href=\"http:\/\/numfocus.org\/donate-to-mlpack\"><img src=\"https:\/\/img.shields.io\/badge\/sponsored%20by-NumFOCUS-orange.svg?style=flat-square&colorA=E1523D&colorB=007D8A\" alt=\"NumFOCUS\"><\/a>\n<\/p>\n\n<p align=\"center\">\n  <em>\n    Download:\n    <a href=\"https:\/\/www.mlpack.org\/files\/mlpack-3.4.2.tar.gz\">current stable version (3.4.2)<\/a>\n  <\/em>\n<\/p>\n\n**mlpack** is an intuitive, fast, and flexible C++ machine learning library with\nbindings to other languages.  It is meant to be a machine learning analog to\nLAPACK, and aims to implement a wide array of machine learning methods and\nfunctions as a \"swiss army knife\" for machine learning researchers.  In addition\nto its powerful C++ interface, mlpack also provides command-line programs,\nPython bindings, Julia bindings, Go bindings and R bindings.\n\n[\/\/]: # (numfocus-fiscal-sponsor-attribution)\n\nmlpack uses an [open governance model](.\/GOVERNANCE.md) and is fiscally\nsponsored by [NumFOCUS](https:\/\/numfocus.org\/).  Consider making a\n[tax-deductible donation](https:\/\/numfocus.org\/donate-to-mlpack) to help the\nproject pay for developer time, professional services, travel, workshops, and a\nvariety of other needs.\n\n<div align=\"center\">\n  <a href=\"https:\/\/numfocus.org\/\">\n    <img height=\"60px\"\n         src=\"https:\/\/raw.githubusercontent.com\/numfocus\/templates\/master\/images\/numfocus-logo.png\"\n         align=\"center\">\n  <\/a>\n<\/div>\n<br>\n\n### 0. Contents\n\n  1. [Introduction](#1-introduction)\n  2. [Citation details](#2-citation-details)\n  3. [Dependencies](#3-dependencies)\n  4. [Building mlpack from source](#4-building-mlpack-from-source)\n  5. [Running mlpack programs](#5-running-mlpack-programs)\n  6. [Using mlpack from Python](#6-using-mlpack-from-python)\n  7. [Further documentation](#7-further-documentation)\n  8. [Bug reporting](#8-bug-reporting)\n\n###  1. Introduction\n\nThe mlpack website can be found at https:\/\/www.mlpack.org and it contains\nnumerous tutorials and extensive documentation.  This README serves as a guide\nfor what mlpack is, how to install it, how to run it, and where to find more\ndocumentation. The website should be consulted for further information:\n\n  - [mlpack homepage](https:\/\/www.mlpack.org\/)\n  - [mlpack documentation](https:\/\/www.mlpack.org\/docs.html)\n  - [Tutorials](https:\/\/www.mlpack.org\/doc\/mlpack-git\/doxygen\/tutorials.html)\n  - [Development Site (Github)](https:\/\/www.github.com\/mlpack\/mlpack\/)\n  - [API documentation (Doxygen)](https:\/\/www.mlpack.org\/doc\/mlpack-git\/doxygen\/index.html)\n\n### 2. Citation details\n\nIf you use mlpack in your research or software, please cite mlpack using the\ncitation below (given in BibTeX format):\n\n    @article{mlpack2018,\n        title     = {mlpack 3: a fast, flexible machine learning library},\n        author    = {Curtin, Ryan R. and Edel, Marcus and Lozhnikov, Mikhail and\n                     Mentekidis, Yannis and Ghaisas, Sumedh and Zhang,\n                     Shangtong},\n        journal   = {Journal of Open Source Software},\n        volume    = {3},\n        issue     = {26},\n        pages     = {726},\n        year      = {2018},\n        doi       = {10.21105\/joss.00726},\n        url       = {https:\/\/doi.org\/10.21105\/joss.00726}\n    }\n\nCitations are beneficial for the growth and improvement of mlpack.\n\n### 3. Dependencies\n\nmlpack has the following dependencies:\n\n      Armadillo      >= 9.800\n      Boost (math_c99, spirit) >= 1.58.0\n      CMake          >= 3.6\n      ensmallen      >= 2.10.0\n      cereal         >= 1.1.2\n\nAll of those should be available in your distribution's package manager.  If\nnot, you will have to compile each of them by hand.  See the documentation for\neach of those packages for more information.\n\nIf you would like to use or build the mlpack Python bindings, make sure that the\nfollowing Python packages are installed:\n\n      setuptools\n      cython >= 0.24\n      numpy\n      pandas >= 0.15.0\n\nIf you would like to build the Julia bindings, make sure that Julia >= 1.3.0 is\ninstalled.\n\nIf you would like to build the Go bindings, make sure that Go >= 1.11.0 is\ninstalled with this package:\n\n     Gonum\n\nIf you would like to build the R bindings, make sure that R >= 4.0 is\ninstalled with these R packages.\n\n     Rcpp >= 0.12.12\n     RcppArmadillo >= 0.8.400.0\n     RcppEnsmallen >= 0.2.10.0\n     BH >= 1.58\n     roxygen2\n\nIf the STB library headers are available, image loading support will be\ncompiled.\n\nIf you are compiling Armadillo by hand, ensure that LAPACK and BLAS are enabled.\n\n### 4. Building mlpack from source\n\nThis document discusses how to build mlpack from source. These build directions\nwill work for any Linux-like shell environment (for example Ubuntu, macOS,\nFreeBSD etc). However, mlpack is in the repositories of many Linux distributions\nand so it may be easier to use the package manager for your system.  For example,\non Ubuntu, you can install the mlpack library and command-line executables (e.g.\nmlpack_pca, mlpack_kmeans etc.) with the following command:\n\n    $ sudo apt-get install libmlpack-dev mlpack-bin\n\nOn Fedora or Red Hat (EPEL):\n\n    $ sudo dnf install mlpack-devel mlpack-bin\n\n*Note*: Older Ubuntu versions may not have the most recent version of mlpack\navailable---for instance, at the time of this writing, Ubuntu 16.04 only has\nmlpack 3.4.2 available.  Options include upgrading your Ubuntu version, finding\na PPA or other non-official sources, or installing with a manual build.\n\n*Note*: If you are using RHEL7\/CentOS 7, gcc 4.8 is too old to compile mlpack.\nOne option is to use `devtoolset-8`; see\n[here](https:\/\/www.softwarecollections.org\/en\/scls\/rhscl\/devtoolset-8\/) for more\ninformation.\n\nThere are some useful pages to consult in addition to this section:\n\n  - [Building mlpack From Source](https:\/\/www.mlpack.org\/doc\/mlpack-git\/doxygen\/build.html)\n  - [Building mlpack From Source on Windows](https:\/\/www.mlpack.org\/doc\/mlpack-git\/doxygen\/build_windows.html)\n\nmlpack uses CMake as a build system and allows several flexible build\nconfiguration options. You can consult any of the CMake tutorials for\nfurther documentation, but this tutorial should be enough to get mlpack built\nand installed.\n\nFirst, unpack the mlpack source and change into the unpacked directory.  Here we\nuse mlpack-x.y.z where x.y.z is the version.\n\n    $ tar -xzf mlpack-x.y.z.tar.gz\n    $ cd mlpack-x.y.z\n\nThen, make a build directory.  The directory can have any name, but 'build' is\nsufficient.\n\n    $ mkdir build\n    $ cd build\n\nThe next step is to run CMake to configure the project.  Running CMake is the\nequivalent to running `.\/configure` with autotools. If you run CMake with no\noptions, it will configure the project to build with no debugging symbols and\nno profiling information:\n\n    $ cmake ..\/\n\nOptions can be specified to compile with debugging information and profiling information:\n\n    $ cmake -D DEBUG=ON -D PROFILE=ON ..\/\n\nOptions are specified with the -D flag.  The allowed options include:\n\n    DEBUG=(ON\/OFF): compile with debugging symbols\n    PROFILE=(ON\/OFF): compile with profiling symbols\n    ARMA_EXTRA_DEBUG=(ON\/OFF): compile with extra Armadillo debugging symbols\n    BOOST_ROOT=(\/path\/to\/boost\/): path to root of boost installation\n    ARMADILLO_INCLUDE_DIR=(\/path\/to\/armadillo\/include\/): path to Armadillo headers\n    ARMADILLO_LIBRARY=(\/path\/to\/armadillo\/libarmadillo.so): Armadillo library\n    BUILD_CLI_EXECUTABLES=(ON\/OFF): whether or not to build command-line programs\n    BUILD_PYTHON_BINDINGS=(ON\/OFF): whether or not to build Python bindings\n    PYTHON_EXECUTABLE=(\/path\/to\/python_version): Path to specific Python executable\n    PYTHON_INSTALL_PREFIX=(\/path\/to\/python\/): Path to root of Python installation\n    BUILD_JULIA_BINDINGS=(ON\/OFF): whether or not to build Julia bindings\n    JULIA_EXECUTABLE=(\/path\/to\/julia): Path to specific Julia executable\n    BUILD_GO_BINDINGS=(ON\/OFF): whether or not to build Go bindings\n    GO_EXECUTABLE=(\/path\/to\/go): Path to specific Go executable\n    BUILD_GO_SHLIB=(ON\/OFF): whether or not to build shared libraries required by Go bindings\n    BUILD_R_BINDINGS=(ON\/OFF): whether or not to build R bindings\n    R_EXECUTABLE=(\/path\/to\/R): Path to specific R executable\n    BUILD_TESTS=(ON\/OFF): whether or not to build tests\n    BUILD_SHARED_LIBS=(ON\/OFF): compile shared libraries and executables as\n        opposed to static libraries\n    DISABLE_DOWNLOADS=(ON\/OFF): whether to disable all downloads during build\n    ENSMALLEN_INCLUDE_DIR=(\/path\/to\/ensmallen\/include): path to include directory\n       for ensmallen\n    STB_IMAGE_INCLUDE_DIR=(\/path\/to\/stb\/include): path to include directory for\n       STB image library\n    USE_OPENMP=(ON\/OFF): whether or not to use OpenMP if available\n    BUILD_DOCS=(ON\/OFF): build Doxygen documentation, if Doxygen is available\n       (default ON)\n\nFor example, to build mlpack library and CLI bindings statically the following\ncommand can be used:\n\n    $ cmake  -D BUILD_SHARED_LIBS=OFF ..\/\n\nOther tools can also be used to configure CMake, but those are not documented\nhere.  See [this section of the build guide](https:\/\/www.mlpack.org\/doc\/mlpack-git\/doxygen\/build.html#build_config)\nfor more details, including a full list of options, and their default values.\n\nBy default, command-line programs will be built, and if the Python dependencies\n(Cython, setuptools, numpy, pandas) are available, then Python bindings will\nalso be built.  OpenMP will be used for parallelization when possible by\ndefault.\n\nOnce CMake is configured, building the library is as simple as typing 'make'.\nThis will build all library components and bindings.\n\n    $ make\n\nIf you do not want to build everything in the library, individual components\nof the build can be specified:\n\n    $ make mlpack_pca mlpack_knn mlpack_kfn\n\nIf you want to build the tests, just make the `mlpack_test` target, and use\n`ctest` to run the tests:\n\n    $ make mlpack_test\n    $ ctest .\n\nIf the build fails and you cannot figure out why, register an account on Github\nand submit an issue. The mlpack developers will quickly help you figure it out:\n\n[mlpack on Github](https:\/\/www.github.com\/mlpack\/mlpack\/)\n\nAlternately, mlpack help can be found in IRC at `#mlpack` on chat.freenode.net.\n\nIf you wish to install mlpack to `\/usr\/local\/include\/mlpack\/`, `\/usr\/local\/lib\/`,\nand `\/usr\/local\/bin\/`, make sure you have root privileges (or write permissions\nto those three directories), and simply type\n\n    $ make install\n\nYou can now run the executables by name; you can link against mlpack with\n    `-lmlpack`\nand the mlpack headers are found in\n    `\/usr\/local\/include\/mlpack\/`\nand if Python bindings were built, you can access them with the `mlpack`\npackage in Python.\n\nIf running the programs (i.e. `$ mlpack_knn -h`) gives an error of the form\n\n    error while loading shared libraries: libmlpack.so.2: cannot open shared object file: No such file or directory\n\nthen be sure that the runtime linker is searching the directory where\n`libmlpack.so` was installed (probably `\/usr\/local\/lib\/` unless you set it\nmanually).  One way to do this, on Linux, is to ensure that the\n`LD_LIBRARY_PATH` environment variable has the directory that contains\n`libmlpack.so`.  Using bash, this can be set easily:\n\n    export LD_LIBRARY_PATH=\"\/usr\/local\/lib\/:$LD_LIBRARY_PATH\"\n\n(or whatever directory `libmlpack.so` is installed in.)\n\n### 5. Running mlpack programs\n\nAfter building mlpack, the executables will reside in `build\/bin\/`.  You can call\nthem from there, or you can install the library and (depending on system\nsettings) they should be added to your PATH and you can call them directly.  The\ndocumentation below assumes the executables are in your PATH.\n\nConsider the 'mlpack_knn' program, which finds the k nearest neighbors in a\nreference dataset of all the points in a query set.  That is, we have a query\nand a reference dataset. For each point in the query dataset, we wish to know\nthe k points in the reference dataset which are closest to the given query\npoint.\n\nAlternately, if the query and reference datasets are the same, the problem can\nbe stated more simply: for each point in the dataset, we wish to know the k\nnearest points to that point.\n\nEach mlpack program has extensive help documentation which details what the\nmethod does, what each of the parameters is, and how to use them:\n\n```shell\n$ mlpack_knn --help\n```\n\nRunning `mlpack_knn` on one dataset (that is, the query and reference\ndatasets are the same) and finding the 5 nearest neighbors is very simple:\n\n```shell\n$ mlpack_knn -r dataset.csv -n neighbors_out.csv -d distances_out.csv -k 5 -v\n```\n\nThe `-v (--verbose)` flag is optional; it gives informational output.  It is not\nunique to `mlpack_knn` but is available in all mlpack programs.  Verbose\noutput also gives timing output at the end of the program, which can be very\nuseful.\n\n### 6. Using mlpack from Python\n\nIf mlpack is installed to the system, then the mlpack Python bindings should be\nautomatically in your PYTHONPATH, and importing mlpack functionality into Python\nshould be very simple:\n\n```python\n>>> from mlpack import knn\n```\n\nAccessing help is easy:\n\n```python\n>>> help(knn)\n```\n\nThe API is similar to the command-line programs.  So, running `knn()`\n(k-nearest-neighbor search) on the numpy matrix `dataset` and finding the 5\nnearest neighbors is very simple:\n\n```python\n>>> output = knn(reference=dataset, k=5, verbose=True)\n```\n\nThis will store the output neighbors in `output['neighbors']` and the output\ndistances in `output['distances']`.  Other mlpack bindings function similarly,\nand the input\/output parameters exactly match those of the command-line\nprograms.\n\n### 7. Further documentation\n\nThe documentation given here is only a fraction of the available documentation\nfor mlpack.  If doxygen is installed, you can type `make doc` to build the\ndocumentation locally.  Alternately, up-to-date documentation is available for\nolder versions of mlpack:\n\n  - [mlpack homepage](https:\/\/www.mlpack.org\/)\n  - [mlpack documentation](https:\/\/www.mlpack.org\/docs.html)\n  - [Tutorials](https:\/\/www.mlpack.org\/doc\/mlpack-git\/doxygen\/tutorials.html)\n  - [Development Site (Github)](https:\/\/www.github.com\/mlpack\/mlpack\/)\n  - [API documentation (Doxygen)](https:\/\/www.mlpack.org\/doc\/mlpack-git\/doxygen\/index.html)\n\nTo learn about the development goals of mlpack in the short- and medium-term\nfuture, see the [vision document](https:\/\/www.mlpack.org\/papers\/vision.pdf).\n\n### 8. Bug reporting\n\n   (see also [mlpack help](https:\/\/www.mlpack.org\/questions.html))\n\nIf you find a bug in mlpack or have any problems, numerous routes are available\nfor help.\n\nGithub is used for bug tracking, and can be found at\nhttps:\/\/github.com\/mlpack\/mlpack\/.\nIt is easy to register an account and file a bug there, and the mlpack\ndevelopment team will try to quickly resolve your issue.\n\nIn addition, mailing lists are available.  The mlpack discussion list is\navailable at\n\n  [mlpack discussion list](http:\/\/lists.mlpack.org\/mailman\/listinfo\/mlpack)\n\nand the git commit list is available at\n\n  [commit list](http:\/\/lists.mlpack.org\/mailman\/listinfo\/mlpack-git)\n\nLastly, the IRC channel `#mlpack` on Freenode can be used to get help.\n","64":"<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/xlearn_logo.png\" width = \"400\"\/>\n\n[![Hex.pm](https:\/\/img.shields.io\/hexpm\/l\/plug.svg)](.\/LICENCE)\n[![Project Status](https:\/\/img.shields.io\/badge\/version-0.4.4-green.svg)]()\n\n## What is xLearn?\n\nxLearn is a ***high performance***, ***easy-to-use***, and ***scalable*** machine learning package that contains linear model (LR), factorization machines (FM), and field-aware factorization machines (FFM), all of which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data. Many real world datasets deal with high dimensional sparse feature vectors like a recommendation system where the number of categories and users is on the order of millions. In that case, if you are the user of liblinear, libfm, and libffm, now xLearn is your another better choice.\n\n[Get Started! (English)](http:\/\/xlearn-doc.readthedocs.io\/en\/latest\/index.html)\n\n[Get Started! (\u4e2d\u6587)](http:\/\/xlearn-doc-cn.readthedocs.io\/en\/latest\/index.html)\n\n### Performance\n\n<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/speed.png\" width = \"800\"\/>\n\nxLearn is developed by high-performance C++ code with careful design and optimizations. Our system is designed to maximize CPU and memory utilization, provide cache-aware computation, and support lock-free learning. By combining these insights, xLearn is 5x-13x faster compared to similar systems.\n\n### Ease-of-use\n\n<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/code.png\" width = \"600\"\/>\n\nxLearn does not rely on any third-party library and users can just clone the code and compile it by using cmake. Also, xLearn supports very simple Python and CLI interface for data scientists, and it also offers many useful features that have been widely used in machine learning and data mining competitions, such as cross-validation, early-stop, etc.\n\n### Scalability\n\n<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/scalability.png\" width = \"650\"\/>\n\nxLearn can be used for solving large-scale machine learning problems. First, xLearn supports out-of-core training, which can handle very large data (TB) by just leveraging the disk of a PC. In addition, xLearn supports distributed training, which scales beyond billions of example across many machines by using the Parameter Server framework.\n\n## How to Contribute\n\nxLearn has been developed and used by many active community members. Your help is very valuable to make it better for everyone.\n\n * Please contribute if you find any bug in xLearn.\n * Contribute new features you want to see in xLearn.\n * Contribute to the tests to make it more reliable.\n * Contribute to the documents to make it clearer for everyone.\n * Contribute to the examples to share your experience with other users.\n * Open issue if you met problems during development.\n\nNote that, please post iusse and contribution in *English* so that everyone can get help from them.\n\n### Contributors (rank randomly)\n\n<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/10520307.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/11278017.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/1289856.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/13925796.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/15322665.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/1842965.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/21072881.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/22660103.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/2387719.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/25626965.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/3086744.png\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/3928409.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/4606937.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/6054101.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/6161143.png\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/7145046.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/7608904.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/27916175.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/7608904.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/1443518.png\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/9783213.png\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/4609798.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/11628637.png\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/1726448.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/32598525.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/3285618.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/8625634.jpeg\" width = \"40\"\/><img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/11938898.png\" width = \"40\"\/>\n\n\n## For Enterprise Users and Call for Sponsors\n\nIf you are enterprise users and find xLearn is useful in your work, please let us know, and we are glad to add your company logo here. We also welcome you become a sponsor to make this project better.\n\n<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/tencent.png\" width = \"200\"\/>\n<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/stategrid.jpg\" width = \"200\"\/>\n<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/xiaodaka.png\" width = \"200\"\/>\n<img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/beikezhaofang.png\" width = \"200\"\/>\n\n## What's New\n\n - 2019-10-13 [Andrew Kane](https:\/\/github.com\/ankane) add [Ruby bindings](https:\/\/github.com\/ankane\/xlearn) for xLearn!\n\n - 2019-4-25 xLearn 0.4.4 version release. Main update:\n\n    * Support Python DMatrix\n    * Better Windows support\n    * Fix bugs in previous version\n\n - 2019-3-25 xLearn 0.4.3 version release. Main update:\n    * Fix bugs in previous version\n\n - 2019-3-12 xLearn 0.4.2 version release. Main update:\n    * Release Windows version of xLearn\n\n - 2019-1-30 xLearn 0.4.1 version release. Main update:\n    * More flexible data reader\n\n - 2018-11-22 xLearn 0.4.0 version release. Main update:\n\n    * Fix bugs in previous version\n    * Add online learning for xLearn\n\n - 2018-11-10 xLearn 0.3.8 version release. Main update:\n\n    * Fix bugs in previous version.\n    * Update early-stop mechanism.\n\n - 2018-11-08. xLearn gets 2000 star! Congs!\n\n - 2018-10-29 xLearn 0.3.7 version release. Main update:\n\n    * Add incremental Reader, which can save 50% memory cost.\n\n - 2018-10-22 xLearn 0.3.5 version release. Main update:\n\n    * Fix bugs in 0.3.4.\n\n - 2018-10-21 xLearn 0.3.4 version release. Main update:\n\n    * Fix bugs in on-disk training.\n    * Support new file format.\n\n - 2018-10-14 xLearn 0.3.3 version release. Main update:\n\n    * Fix segmentation fault in prediction task.\n    * Update early-stop meachnism.\n\n - 2018-09-21 xLearn 0.3.2 version release. Main update:\n\n    * Fix bugs in previous version\n    * New TXT format for model output\n\n - 2018-09-08 xLearn uses the new logo:\n\n <img src=\"https:\/\/github.com\/aksnzhy\/xLearn\/raw\/master\/img\/xlearn_logo.png\" width = \"300\"\/>\n\n - 2018-09-07 The [Chinese document](http:\/\/xlearn-doc-cn.readthedocs.io\/en\/latest\/index.html) is available now!\n\n - 2018-03-08 xLearn 0.3.0 version release. Main update:\n\n    * Fix bugs in previous version\n    * Solved the memory leak problem for on-disk learning\n    * Support TXT model checkpoint\n    * Support Scikit-Learn API\n\n - 2017-12-18 xLearn 0.2.0 version release. Main update:\n\n    * Fix bugs in previous version\n    * Support pip installation\n    * New Documents\n    * Faster FTRL algorithm\n\n - 2017-11-24 The first version (0.1.0) of xLearn release !\n","65":"\n> **\u26a0 Important**\n> Starting from the next release (22.05): 'master' branch will be changed to 'main' following our inclusive language update, more information [here](https:\/\/arm-software.github.io\/ComputeLibrary\/latest\/contribution_guidelines.xhtml#S5_0_inc_lang).\n\n<br>\n<div align=\"center\">\n <img src=\"https:\/\/raw.githubusercontent.com\/ARM-software\/ComputeLibrary\/gh-pages\/ACL_logo.png\"\/><br><br>\n<\/div>\n\n# Compute Library ![](https:\/\/img.shields.io\/badge\/latest_release-22.02-green)\n\n\nThe Compute Library is a collection of low-level machine learning functions optimized for Arm\u00ae Cortex\u00ae-A and Arm\u00ae Mali\u2122 GPUs architectures.<br>\n\nThe library provides superior performance to other open source alternatives and immediate support for new Arm\u00ae technologies e.g. SVE2.\n\nKey Features:\n\n- Open source software available under a permissive MIT license\n- Over 100 machine learning functions for CPU and GPU\n- Multiple convolution algorithms (GeMM, Winograd, FFT, Direct and indirect-GeMM)\n- Support for multiple data types: FP32, FP16, INT8, UINT8, BFLOAT16\n- Micro-architecture optimization for key ML primitives\n- Highly configurable build options enabling lightweight binaries\n- Advanced optimization techniques such as kernel fusion, Fast math enablement and texture utilization\n- Device and workload specific tuning using OpenCL tuner and GeMM optimized heuristics\n\n<br>\n\n| Repository  | Link |\n| ----------- | ----------- |\n| Release     | https:\/\/github.com\/arm-software\/ComputeLibrary       |\n| Development | https:\/\/review.mlplatform.org\/#\/admin\/projects\/ml\/ComputeLibrary        |\n\n<br>\n\n## Documentation\n[![Documentation](https:\/\/img.shields.io\/badge\/documentation-22.02-green)](https:\/\/arm-software.github.io\/ComputeLibrary\/latest)\n\n> Note: The documentation includes the reference API, changelogs, build guide, contribution guide, errata, etc.\n\n<br>\n\n## Pre-built binaries\nAll the binaries can be downloaded from [here](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases) or from the tables below.\n\n<br>\n\n| Platform    | Operating System | Release archive (Download) |\n| ----------- | ----------- | ----------- |\n| Raspberry Pi 4 | Linux 32bit | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-armv7a-neon.tar.gz) |\n| Raspberry Pi 4 | Linux 64bit | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-neon.tar.gz) |\n| Odroid N2 | Linux 64bit | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-neon.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-opencl-blue)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-cl.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-neon+cl-yellowgreen)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-neon-cl.tar.gz) |\n| HiKey960 | Linux 64bit | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-neon.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-opencl-blue)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-cl.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-neon+cl-yellowgreen)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-neon-cl.tar.gz) |\n\n<br>\n\n| Architecture    | Operating System | Release archive (Download) |\n| ----------- | ----------- | ----------- |\n| armv7 | Android | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-armv7a-neon.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-opencl-blue)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-armv7a-cl.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-neon+cl-yellowgreen)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-armv7a-neon-cl.tar.gz) |\n| armv7 | Linux | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-armv7a-neon.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-opencl-blue)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-armv7a-cl.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-neon+cl-yellowgreen)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-armv7a-neon-cl.tar.gz) |\n| arm64-v8a | Android | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-arm64-v8a-neon.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-opencl-blue)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-arm64-v8a-cl.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-neon+cl-yellowgreen)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-arm64-v8a-neon-cl.tar.gz) |\n| arm64-v8a | Linux | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-neon.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-opencl-blue)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-cl.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-neon+cl-yellowgreen)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8a-neon-cl.tar.gz) |\n| arm64-v8.2-a | Android | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-arm64-v8.2-a-neon.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-opencl-blue)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-arm64-v8.2-a-cl.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-neon+cl-yellowgreen)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-android-arm64-v8.2-a-neon-cl.tar.gz) |\n| arm64-v8.2-a | Linux | [![](https:\/\/img.shields.io\/badge\/build-neon-orange)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8.2-a-neon.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-opencl-blue)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8.2-a-cl.tar.gz) [![](https:\/\/img.shields.io\/badge\/build-neon+cl-yellowgreen)](https:\/\/github.com\/ARM-software\/ComputeLibrary\/releases\/download\/v22.02\/arm_compute-v22.02-bin-linux-arm64-v8.2-a-neon-cl.tar.gz) |\n\n<br>\n\n\n## Supported Architectures\/Technologies\n\n- Arm\u00ae CPUs:\n    - Arm\u00ae Cortex\u00ae-A processor family using Arm\u00ae Neon\u2122 technology\n    - Arm\u00ae Cortex\u00ae-R processor family with Armv8-R AArch64 architecture using Arm\u00ae Neon\u2122 technology\n    - Arm\u00ae Cortex\u00ae-X1 processor using Arm\u00ae Neon\u2122 technology\n\n- Arm\u00ae Mali\u2122 GPUs:\n    - Arm\u00ae Mali\u2122-G processor family\n    - Arm\u00ae Mali\u2122-T processor family\n\n- x86\n\n<br>\n\n## Supported Systems\n\n- Android\u2122\n- Bare Metal\n- Linux\u00ae\n- macOS\u00ae\n- Tizen\u2122\n\n<br>\n\n## Resources\n- [Tutorial: Running AlexNet on Raspberry Pi with Compute Library](https:\/\/community.arm.com\/processors\/b\/blog\/posts\/running-alexnet-on-raspberry-pi-with-compute-library)\n- [Gian Marco's talk on Performance Analysis for Optimizing Embedded Deep Learning Inference Software](https:\/\/www.embedded-vision.com\/platinum-members\/arm\/embedded-vision-training\/videos\/pages\/may-2019-embedded-vision-summit)\n- [Gian Marco's talk on optimizing CNNs with Winograd algorithms at the EVS](https:\/\/www.embedded-vision.com\/platinum-members\/arm\/embedded-vision-training\/videos\/pages\/may-2018-embedded-vision-summit-iodice)\n- [Gian Marco's talk on using SGEMM and FFTs to Accelerate Deep Learning](https:\/\/www.embedded-vision.com\/platinum-members\/arm\/embedded-vision-training\/videos\/pages\/may-2016-embedded-vision-summit-iodice)\n\n<br>\n\n## How to contribute\n\nContributions to the Compute Library are more than welcome. If you are interested on contributing, please have a look at our [how to contribute guidelines](https:\/\/arm-software.github.io\/ComputeLibrary\/latest\/contribution_guidelines.xhtml).\n\n### Developer Certificate of Origin (DCO)\nBefore the Compute Library accepts your contribution, you need to certify its origin and give us your permission. To manage this process we use the Developer Certificate of Origin (DCO) V1.1 (https:\/\/developercertificate.org\/)\n\nTo indicate that you agree to the the terms of the DCO, you \"sign off\" your contribution by adding a line with your name and e-mail address to every git commit message:\n\n```Signed-off-by: John Doe <john.doe@example.org>```\n\nYou must use your real name, no pseudonyms or anonymous contributions are accepted.\n\n### Public mailing list\nFor technical discussion, the ComputeLibrary project has a public mailing list: acl-dev@lists.linaro.org\nThe list is open to anyone inside or outside of Arm to self subscribe.  In order to subscribe, please visit the following website:\nhttps:\/\/lists.linaro.org\/mailman\/listinfo\/acl-dev\n\n<br>\n\n## License and Contributions\n\nThe software is provided under MIT license. Contributions to this project are accepted under the same license.\n\n<br>\n\n## Trademarks and Copyrights\n\nAndroid is a trademark of Google LLC.\n\nArm, Cortex, Mali and Neon are registered trademarks or trademarks of Arm Limited (or its subsidiaries) in the US and\/or elsewhere.\n\nLinux\u00ae is the registered trademark of Linus Torvalds in the U.S. and other countries.\n\nMac and macOS are trademarks of Apple Inc., registered in the U.S. and other\ncountries.\n\nTizen is a registered trademark of The Linux Foundation.\n\n","66":"# ![Tensor Comprehensions](docs\/source\/_static\/img\/tc-logo-full-color-with-text-2.png)\n\nTensor Comprehensions (TC) is a fully-functional C++ library to *automatically* synthesize high-performance machine learning kernels using [Halide](https:\/\/github.com\/halide\/Halide), [ISL](http:\/\/isl.gforge.inria.fr\/) and NVRTC or LLVM. TC additionally provides basic integration with Caffe2 and PyTorch. We provide more details in our paper on [arXiv](https:\/\/arxiv.org\/abs\/1802.04730).\n\nThis library is designed to be highly portable, machine-learning-framework agnostic and only requires a simple tensor library with memory allocation, offloading and synchronization capabilities.\n\nFor now, we have integrated TC with [Caffe2](https:\/\/github.com\/caffe2\/caffe2) and [PyTorch](https:\/\/github.com\/pytorch\/pytorch\/).\n\n# A simple example\n\nThe following illustrates a short but powerful feature of the library: the capacity to JIT-compile high-performance machine learning kernels on demand, for specific sizes.\n\n```python\nimport tensor_comprehensions as tc\nimport torch\nlang = \"\"\"\ndef tensordot(float(N, C1, C2, H, W) I0, float(N, C2, C3, H, W) I1) -> (O) {\n    O(n, c1, c3, h, w) +=! I0(n, c1, c2, h, w) * I1(n, c2, c3, h, w)\n}\n\"\"\"\nN, C1, C2, C3, H, W = 32, 512, 8, 2, 28, 28\ntensordot = tc.define(lang, name=\"tensordot\")\nI0, I1 = torch.randn(N, C1, C2, H, W).cuda(), torch.randn(N, C2, C3, H, W).cuda()\nbest_options = tensordot.autotune(I0, I1, cache=True)\nout = tensordot(I0, I1, options=best_options)\n```\n\nAfter a few generations of `autotuning` on a 2-GPU P100 system, we see results resembling:\n\n![Autotuning Sample](docs\/source\/_static\/img\/autotuning.png)\n\nIn C++ a minimal autotuning example resembles the [following](tc\/examples\/tensordot.cc):\n```cpp\nTEST(TensorDot, SimpleAutotune) {\n  \/\/ 1. Define and setup the TC compilation unit with CUDA memory\n  \/\/ management backed by ATen tensors.\n  std::string tc = R\"TC(\ndef tensordot(float(N, C1, C2, H, W) I0,\n              float(N, C2, C3, H, W) I1)  -> (O)\n{\n    O(n, c1, c3, h, w) +=! I0(n, c1, r_c2, h, w) * I1(n, r_c2, c3, h, w)\n}\n  )TC\";\n\n  \/\/ 2. Allocate tensors with random data.\n  at::Tensor I0 = at::CUDA(at::kFloat).rand({32,  8, 16, 17, 25});\n  at::Tensor I1 = at::CUDA(at::kFloat).rand({32, 16, 2, 17, 25});\n\n  \/\/ 3. Run autotuning with evolutionary search starting from a naive option.\n  auto naiveOptions = Backend::MappingOptionsType::makeNaiveMappingOptions();\n  tc::aten::ATenAutotuner<tc::CudaBackend, tc::autotune::GeneticSearch>\n      geneticAutotuneATen(tc);\n  auto bestOption =\n      geneticAutotuneATen.tune(\"tensordot\", {I0, I1}, {naiveOptions});\n\n  \/\/ 4. Compile and run the TC with the best option after allocating output\n  \/\/    tensors.\n  auto pExecutor =\n      tc::aten::compile<Backend>(tc, \"tensordot\", {I0, I1}, bestOption[0]);\n  auto outputs = tc::aten::prepareOutputs(tc, \"tensordot\", {I0, I1});\n  auto timings = tc::aten::profile(*pExecutor, {I0, I1}, outputs);\n  std::cout << \"tensordot size I0: \" << I0.sizes() << \", \"\n            << \"size I1: \" << I1.sizes()\n            << \" ran in: \" << timings.kernelRuntime.toMicroSeconds() << \"us\\n\";\n}\n```\n\nNote that we only need to **autotune a TC once** to obtain reasonable mapping options\nthat can translate to other problem sizes for a given TC as the following snippet\nillustrates:\n```cpp\n\/\/ 5. Reuse bestOptions from autotuning on another kernel\nfor (auto sizes : std::vector<std::pair<at::IntList, at::IntList>>{\n         {{4, 9, 7, 16, 14}, {4, 7, 3, 16, 14}},\n         {{8, 5, 11, 10, 10}, {8, 11, 16, 10, 10}},\n     }) {\n  at::Tensor I0 = makeATenTensor<Backend>(sizes.first);\n  at::Tensor I1 = makeATenTensor<Backend>(sizes.second);\n  auto pExecutor =\n      tc::aten::compile<Backend>(tc, \"tensordot\", {I0, I1}, bestOption[0]);\n  auto outputs = tc::aten::prepareOutputs(tc, \"tensordot\", {I0, I1});\n  auto timings = tc::aten::profile(*pExecutor, {I0, I1}, outputs);\n  std::cout << \"tensordot size I0: \" << I0.sizes() << \", \"\n            << \"size I1: \" << I1.sizes()\n            << \" ran in: \" << timings.kernelRuntime.toMicroSeconds()\n            << \"us\\n\";\n}\n```\n\nPutting it all together, one may see:\n```shell\n> build$ .\/examples\/example_simple\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from TensorDot\n[ RUN      ] TensorDot.SimpleAutotune\nGeneration 0    Jobs(Compiled, GPU)\/total  (10, 10)\/10   (best\/median\/worst)us: 226\/4238\/7345\nGeneration 1    Jobs(Compiled, GPU)\/total  (10, 10)\/10   (best\/median\/worst)us: 220\/221\/233\nGeneration 2    Jobs(Compiled, GPU)\/total  (10, 10)\/10   (best\/median\/worst)us: 220\/221\/234\ntensordot size I0: [16, 8, 16, 17, 25], size I1: [16, 16, 2, 17, 25] ran in: 239us\ntensordot size I0: [4, 9, 7, 16, 14], size I1: [4, 7, 3, 16, 14] ran in: 56us\ntensordot size I0: [8, 5, 11, 10, 10], size I1: [8, 11, 16, 10, 10] ran in: 210us\n[       OK ] TensorDot.SimpleAutotune (27812 ms)\n[----------] 1 test from TensorDot (27812 ms total)\n\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (27812 ms total)\n[  PASSED  ] 1 test.\n```\n\nWe have not yet characterized the precise fraction of peak performance we obtain but it is not uncommon to obtain 80%+ of peak shared memory bandwidth after autotuning. Solid register-level optimizations are still in the work but TC in its current form already addresses the productivity gap between the needs of research and the needs of production. Which is why we are excited to share it with the entire community and bring this collaborative effort in the open.\n\n# Documentation\n\n**General**: You can find detailed information about Tensor Comprehensions [here](https:\/\/facebookresearch.github.io\/TensorComprehensions\/).\n\n**C++ API**: We also provide documentation for our C++ API which can can be found [here](https:\/\/facebookresearch.github.io\/TensorComprehensions\/api\/)\n\n# Installation\n\n## Binaries\n\nWe provide conda package for making it easy to install and use TC binary. Please refer to our documentation\n[here](https:\/\/facebookresearch.github.io\/TensorComprehensions\/framework\/pytorch_integration\/getting_started.html) for instructions.\n\n## From Source\n\nYou can find documentation [here](https:\/\/facebookresearch.github.io\/TensorComprehensions\/) which contains instructions for building TC via docker, conda packages or in non-conda environment.\n\n# Communication\n\n* **Email**: tensorcomp@fb.com\n* **GitHub issues**: bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* **Slack**: For discussion around framework integration, build support, collaboration, etc. join our slack channel https:\/\/tensorcomprehensions.herokuapp.com\/.\n\n# Code of Conduct\nSee the [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) file for more details.\n\n# License\nTensor Comprehensions is distributed under a permissive Apache v2.0 license, see the [LICENSE](LICENSE) file for more details.\n\n# Contributing\nSee the [CONTRIBUTING.md](CONTRIBUTING.md) file for more details.\n","67":"Introduction\n============\n\nThe [Simd Library](http:\/\/ermig1979.github.io\/Simd) is a free open source image processing and machine learning library, designed for C and C++ programmers. \nIt provides many useful high performance algorithms for image processing such as: \npixel format conversion, image scaling and filtration, extraction of statistic information from images, motion detection,\nobject detection (HAAR and LBP classifier cascades) and classification, neural network.\n\nThe algorithms are optimized with using of different SIMD CPU extensions. \nIn particular the library supports following CPU extensions: \nSSE, AVX and AVX-512 for x86\/x64, VMX(Altivec) and VSX(Power7) for PowerPC (big-endian), NEON for ARM.\n\nThe Simd Library has C API and also contains useful C++ classes and functions to facilitate access to C API. \nThe library supports dynamic and static linking, 32-bit and 64-bit Windows and Linux, \nMSVS, G++ and Clang compilers, MSVS project and CMake build systems.\n\nLibrary folder's structure\n==========================\n\nThe Simd Library has next folder's structure:\n\n* `simd\/src\/Simd\/` - contains source codes of the library.\n* `simd\/src\/Test\/` - contains test framework of the library.\n* `simd\/src\/Use\/` - contains the use examples of the library.\n* `simd\/prj\/vs2015\/` - contains project files of Microsoft Visual Studio 2015.\n* `simd\/prj\/vs2017\/` - contains project files of Microsoft Visual Studio 2017.\n* `simd\/prj\/vs2019\/` - contains project files of Microsoft Visual Studio 2019.\n* `simd\/prj\/cmd\/` - contains additional scripts needed for building of the library in Windows.\n* `simd\/prj\/cmake\/` - contains files of CMake build systems.\n* `simd\/prj\/sh\/` - contains additional scripts needed for building of the library in Linux.\n* `simd\/prj\/txt\/` - contains text files needed for building of the library.\n* `simd\/data\/cascade\/` - contains OpenCV cascades (HAAR and LBP).\n* `simd\/data\/image\/` - contains image samples.\n* `simd\/data\/network\/` - contains examples of trained networks.\n* `simd\/docs\/` - contains documentation of the library.\n\nBuilding the library for Windows\n================================\n\nTo build the library and test application for Windows 32\/64 you need to use Microsoft Visual Studio 2019 (or 2015\/2017). \nThe project files are in the directory: \n\n`simd\/prj\/vs2019\/`\n\nBy default the library is built as a DLL (Dynamic Linked Library).\nYou also may build it as a static library. \nTo do this you must change appropriate property (Configuration Type) of **Simd** project and also uncomment `#define SIMD_STATIC` in file:\n\n`simd\/src\/Simd\/SimdConfig.h`\n\nAlso in order to build the library you can use CMake and MinGW:\n\n\tmkdir build\n\tcd build\n\tcmake ..\\prj\\cmake -DSIMD_TOOLCHAIN=\"your_toolchain\\bin\\g++\" -DSIMD_TARGET=\"x86_64\" -DCMAKE_BUILD_TYPE=\"Release\" -G \"MinGW Makefiles\"\n\tmingw32-make\n\nBuilding the library for Linux\n==============================\n\nTo build the library and test application for Linux 32\/64 you need to use CMake build systems.\nFiles of CMake build systems are placed in the directory:\n\n`simd\/prj\/cmake\/`\n\t\nThe library can be built for x86\/x64, PowerPC(64, big-endian) and ARM(32\/64) platforms using the G++ or Clang compilers.\nUsing the native compiler (g++) for the current platform is simple:\n\n\tmkdir build\n\tcd build\n\tcmake ..\/prj\/cmake -DSIMD_TOOLCHAIN=\"\" -DSIMD_TARGET=\"\"\n\tmake\n\t\nTo build the library for PowerPC(64, big-endian) and ARM(32\/64) platforms you can also use a toolchain for cross compilation.\nThere is an example of using for PowerPC (64 bit, big-endian):\n\n\tmkdir build\n\tcd build\n\tcmake ..\/prj\/cmake -DSIMD_TOOLCHAIN=\"\/your_toolchain\/usr\/bin\/powerpc-linux-gnu-g++\" -DSIMD_TARGET=\"ppc64\" -DCMAKE_BUILD_TYPE=\"Release\"\n\tmake\n\nFor ARM (32 bit):\n\n\tmkdir build\n\tcd build\n\tcmake ..\/prj\/cmake -DSIMD_TOOLCHAIN=\"\/your_toolchain\/usr\/bin\/arm-linux-gnueabihf-g++\" -DSIMD_TARGET=\"arm\" -DCMAKE_BUILD_TYPE=\"Release\"\n\tmake\n\nAnd for ARM (64 bit):\n\n\tmkdir build\n\tcd build\n\tcmake ..\/prj\/cmake -DSIMD_TOOLCHAIN=\"\/your_toolchain\/usr\/bin\/aarch64-linux-gnu-g++\" -DSIMD_TARGET=\"aarch64\" -DCMAKE_BUILD_TYPE=\"Release\"\n\tmake\n\nAs result the library and the test application will be built in the current directory.\n\nThere are addition build parameters:\n\n* `SIMD_AVX512` - Enable of AVX-512 (AVX-512F, AVX-512CD, AVX-512VL, AVX-512DQ, AVX-512BW) CPU extensions. It is switched on by default.\n* `SIMD_AVX512VNNI` - Enable of AVX-512-VNNI CPU extensions. It is switched on by default.\n* `SIMD_TEST` - Build test framework. It is switched on by default.\n* `SIMD_INFO` - Print build information. It is switched on by default.\n* `SIMD_PERF` - Enable of internal performance statistic. It is switched off by default.\n* `SIMD_SHARED` - Build as SHARED library. It is switched off by default.\n* `SIMD_GET_VERSION` - Call scipt to get Simd Library version. It is switched on by default.\n* `SIMD_SYNET` - Enable optimizations for Synet framework. It is switched on by default.\n* `SIMD_SINT8_DEBUG` - Enable debug INT8 capabilities for Synet framework. It is switched on by default.\n* `SIMD_HIDE` - Hide internal functions of Simd Library. It is switched off by default.\n* `SIMD_TEST_FLAGS` - Addition compiler flags to build test framework.\n\nUsing the library\n=================\n\nIf you use the library from C code you must include:\n\t\n    #include \"Simd\/SimdLib.h\"\n\nAnd to use the library from C++ code you must include:\n\n    #include \"Simd\/SimdLib.hpp\"\n\nIn order to use [Simd::Detection](http:\/\/ermig1979.github.io\/Simd\/help\/struct_simd_1_1_detection.html) you must include:\n\n    #include \"Simd\/SimdDetection.hpp\"\n\t\nIn order to use [Simd::Neural](http:\/\/ermig1979.github.io\/Simd\/help\/namespace_simd_1_1_neural.html) you must include:\n\n    #include \"Simd\/SimdNeural.hpp\"\n\t\nIn order to use [Simd::Motion](http:\/\/ermig1979.github.io\/Simd\/help\/namespace_simd_1_1_motion.html) you must include:\n\n    #include \"Simd\/SimdMotion.hpp\"\n\t\nInteraction with OpenCV\n=======================\n\nIf you need to use mutual conversion between Simd and OpenCV types you just have to define macro `SIMD_OPENCV_ENABLE` before including of Simd headers:\n    \n    #include <opencv2\/core\/core.hpp>\n    #define SIMD_OPENCV_ENABLE\n    #include \"Simd\/SimdLib.hpp\"\n\nAnd you can convert next types:\n\t\n* `cv::Point`, `cv::Size` <--> `Simd::Point`.\n* `cv::Rect` <--> `Simd::Rectangle`.\n* `cv::Mat` <--> `Simd::View`.\n\t\nTest Framework\n==============\n\nThe test suite is needed for testing of correctness of work of the library and also for its performance testing.\nThere is a set of tests for every function from API of the library. \nThere is an example of test application using:\n\n\t.\/Test -m=a -tt=1 -f=Sobel -ot=log.txt\n\nWhere next parameters were used:\n\n* `-m=a` - a auto checking mode which includes performance testing (only for library built in Release mode). \nIn this case different implementations of each functions will be compared between themselves \n(for example a scalar implementation and implementations with using of different SIMD instructions such as SSE2, AVX2, and other).\nAlso it can be `-m=c` (creation of test data for cross-platform testing), `-m=v` (cross-platform testing with using of early prepared test data)\nand `-m=s` (running of special tests).\n* `-tt=1` - a number of test threads.\n* `-fi=Sobel` - an include filter. In current case will be tested only functions which contain word 'Sobel' in their names. \nIf you miss this parameter then full testing will be performed.\nYou can use several filters - function name has to satisfy at least one of them.\n* `-ot=log.txt` - a file name with test report (in TEXT file format). The test's report also will be output to console.\n    \nAlso you can use parameters:\n\n* `-help` or `-?` in order to print help message.\n* `-r=..\/..` to set project root directory.\n* `-pa=1` to print alignment statistics.\n* `-pi=1` to print internal statistics (Cmake parameter SIMD_PERF must be ON).\n* `-c=512` a number of channels in test image for performance testing.\n* `-h=1080` a height of test image for performance testing.\n* `-w=1920` a width of test image for performance testing.\n* `-oh=log.html` - a file name with test report (in HTML file format).\t\n* `-s=sample.avi` a video source (See `Simd::Motion` test).\n* `-o=output.avi` an annotated video output (See `Simd::Motion` test).\n* `-wt=1` a thread number used to parallelize algorithms.\n* `-fe=Abs` an exclude filter to exclude some tests.\n* `-mt=100` a minimal test execution time (in milliseconds).\n* `-lc=1` to litter CPU cache between test runs.\n* `-ri=city.jpg` a name of real image used in some tests. The image have to be placed in `.\/data\/image` directory.\n\n","68":"\n<div align=center><img src=\".\/images\/openmldb_logo.png\" width=\"400\" \/><\/div>\n\n[![build status](https:\/\/github.com\/4paradigm\/openmldb\/actions\/workflows\/cicd.yaml\/badge.svg?branch=openmldb)](https:\/\/github.com\/4paradigm\/openmldb\/actions\/workflows\/cicd.yaml)\n[![docker pulls](https:\/\/img.shields.io\/docker\/pulls\/4pdosc\/openmldb.svg)](https:\/\/hub.docker.com\/r\/4pdosc\/openmldb)\n[![slack](https:\/\/img.shields.io\/badge\/Slack-Join%20Slack-blue)](https:\/\/join.slack.com\/t\/hybridsql-ws\/shared_invite\/zt-ozu3llie-K~hn9Ss1GZcFW2~K_L5sMg)\n[![discuss](https:\/\/img.shields.io\/badge\/Discuss-Ask%20Questions-blue)](https:\/\/github.com\/4paradigm\/OpenMLDB\/discussions)\n[![codecov](https:\/\/codecov.io\/gh\/4paradigm\/OpenMLDB\/branch\/main\/graph\/badge.svg?token=OMPII8NGN2)](https:\/\/codecov.io\/gh\/4paradigm\/OpenMLDB)\n[![release](https:\/\/img.shields.io\/github\/v\/release\/4paradigm\/OpenMLDB?color=lime)](https:\/\/github.com\/4paradigm\/OpenMLDB\/releases)\n[![license](https:\/\/img.shields.io\/github\/license\/4paradigm\/OpenMLDB?color=orange)](https:\/\/github.com\/4paradigm\/OpenMLDB\/blob\/main\/LICENSE)\n[![gitee](https:\/\/img.shields.io\/badge\/Gitee-mirror-lightyellow)](https:\/\/gitee.com\/paradigm4\/OpenMLDB)\n[![maven central](https:\/\/img.shields.io\/maven-central\/v\/com.4paradigm.openmldb\/openmldb-batch)](https:\/\/mvnrepository.com\/artifact\/com.4paradigm.openmldb\/openmldb-batch)\n[![maven central](https:\/\/img.shields.io\/maven-central\/v\/com.4paradigm.openmldb\/openmldb-jdbc)](https:\/\/mvnrepository.com\/artifact\/com.4paradigm.openmldb\/openmldb-jdbc)\n[![pypi](https:\/\/img.shields.io\/pypi\/v\/openmldb)](https:\/\/pypi.org\/project\/openmldb\/)\n\n**English | [\u4e2d\u6587](README_cn.md)**\n\n### OpenMLDB is an open-source machine learning database that provides a feature platform enabling consistent features for training and inference.\n\n## 1. Our Philosophy\n\nFor the artificial intelligence (AI) engineering, 95% of the time and effort is consumed by data related workloads. In order to tackle this challenge, tech giants spend thousands of hours on building in-house data and feature platforms to address engineering issues such as data leakage, feature backfilling, and efficiency. The other small and medium-sized enterprises have to purchase expensive SaaS tools and data governance services. \n\nOpenMLDB is an open-source machine learning database that is committed to solving the data and feature challenges. OpenMLDB has been deployed in hundreds of real-world enterprise applications. It prioritizes the capability of feature engineering using SQL for open-source, which offers a feature platform enabling consistent features for training and inference.\n\n## 2. A Feature Platform for ML Applications\n\nReal-time features are essential for many machine learning applications, such as real-time personalized recommendation and risk analytics. However, a feature engineering script developed by data scientists (Python scripts in most cases) cannot be directly deployed into production for online inference because it usually cannot meet the engineering requirements, such as low latency, high throughput and high availability. Therefore, a engineering team needs to be involved to refactor and optimize the source code using database or C++ to ensure its efficiency and robustness. As there are two teams and two toolchains involved for the development and deployment life cycle, the verification for consistency is essential, which usually costs a lot of time and human power. \n\nOpenMLDB is particularly designed as a feature platform for ML applications to accomplish the mission of **Development as Deployment**, to significantly reduce the cost from the offline training to online inference. Based on OpenMLDB, there are three steps only for the entire life cycle:\n\n- Step 1: Offline development of feature engineering script based on SQL\n- Step 2: SQL online deployment using just one command\n- Step 3: Online data source configuration to import real-time data\n\nWith those three steps done, the system is ready to serve real-time features, and highly optimized to achieve low latency and high throughput for production.\n\n<p align=\"center\">\n <img src=\"docs\/en\/about\/images\/workflow.png\" alt=\"image-20211103103052253\" width=800 \/>\n<\/p>\n\nIn order to achieve the goal of Development as Deployment, OpenMLDB is designed to provide consistent features for training and inference. The figure above shows the high-level architecture of OpenMLDB, which consists of four key components: (1) SQL as the unified programming language; (2) The real-time SQL engine for for extra-low latency services; (3) The batch SQL engine based on [a tailored Spark distribution](https:\/\/github.com\/4paradigm\/spark); (4) The unified execution plan generator to bridge the batch and real-time SQL engines to guarantee the consistency.\n\n## 3. Highlights\n\n**Consistent Features for Training and Inference:** Based on the unified execution plan generator, correct and consistent features are produced for offline training and online inference, providing hassle-free time travel without data leakage.\n\n**Real-Time Features with Ultra-Low Latency**: The real-time SQL engine is built from scratch and particularly optimized for time series data. It can achieve the response time of a few milliseconds only to produce real-time features, which significantly outperforms other commercial in-memory database systems (Figures 9 & 10, [the VLDB 2021 paper](http:\/\/vldb.org\/pvldb\/vol14\/p799-chen.pdf)).\n\n**Define Features as SQL**: SQL is used as the unified programming language to define and manage features. SQL is further enhanced for feature engineering, such as the extended syntax `LAST JOIN` and `WINDOW UNION`.\n\n**Production-Ready for ML Applications**: Production features are seamlessly integrated to support enterprise-grade ML applications, including distributed storage and computing, fault recovery, high availability, seamless scale-out, smooth upgrade, monitoring, heterogeneous memory support, and so on.\n\n## 4. FAQ\n\n1. **What are use cases of OpenMLDB?**\n   \n   At present, it is mainly positioned as a feature platform for ML applications, with the strength of low-latency real-time features. It provides the capability of Development as Deployment to significantly reduce the cost for machine learning applications. On the other hand, OpenMLDB contains an efficient and fully functional time-series database, which is used in finance, IoT and other fields.\n   \n2. **How does OpenMLDB evolve?**\n   \n   OpenMLDB originated from the commercial product of [4Paradigm](https:\/\/www.4paradigm.com\/) (a leading artificial intelligence service provider). In 2021, the core team has abstracted, enhanced and developed community-friendly features based on the commercial product; and then makes it publicly available as an open-source project to benefit more enterprises to achieve successful digital transformations at low cost. Before the open-source, it had been successfully deployed in hundreds of real-world ML applications together with 4Paradigm's other commercial products.\n   \n3. **Is OpenMLDB a feature store?**\n   \n   OpenMLDB is more than a feature store to provide features for ML applications. OpenMLDB is capable of producing real-time features in a few milliseconds. Nowadays, most feature stores in the market serve online features by syncing features pre-computed at offline. But they are unable to produce low latency real-time features. By comparison, OpenMLDB is taking advantage of its optimized online SQL engine, to efficiently produce real-time features in a few milliseconds.\n   \n4. **Why does OpenMLDB choose SQL to define and manage features?**\n   \n   SQL (with extension) has the elegant syntax but yet powerful expression ability. SQL based programming experience flattens the learning curve of using OpenMLDB, and further makes it easier for collaboration and sharing.\n\n## 5. Build & Install\n\n:point_right: [Read more](https:\/\/openmldb.ai\/docs\/en\/main\/deploy\/index.html)\n\n## 6. QuickStart\n\n**Cluster and Standalone Versions**\n\nOpenMLDB has two versions with different deployment options, which are *cluster version* and *standalone version*. The cluster version is suitable for large-scale applications and ready for production. On the other hand, the lightweight standalone version running on a single node is ideal for evaluation and demonstration. The cluster and standalone versions have the same functionalities but with different limitations for particular functions. Please refer to [this document](https:\/\/openmldb.ai\/docs\/en\/main\/tutorial\/standalone_vs_cluster.html)  for details. \n\n**Getting Started with OpenMLDB**\n\n:point_right: [OpenMLDB QuickStart](https:\/\/openmldb.ai\/docs\/en\/main\/quickstart\/openmldb_quickstart.html)\n\n## 7. Use Cases\n\nWe are building a list of real-world use cases based on OpenMLDB to demonstrate how it can fit into your business. \n\n| Application                                                  | Tools                                       | Brief Introduction                                           |\n| ------------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------ |\n| [New York City Taxi Trip Duration](https:\/\/openmldb.ai\/docs\/zh\/main\/use_case\/taxi_tour_duration_prediction.html) | OpenMLDB, LightGBM                          | This is a challenge from Kaggle to predict the total ride duration of taxi trips in New York City. You can read [more detail here](https:\/\/www.kaggle.com\/c\/nyc-taxi-trip-duration\/). It demonstrates using the open-source tools OpenMLDB + LightGBM to build an end-to-end machine learning applications easily. |\n| [Using the Pulsar connector to import real-time data streams](https:\/\/openmldb.ai\/docs\/en\/main\/use_case\/pulsar_openmldb_connector_demo.html) | OpenMLDB, Pulsar, Pulsar OpenMLDB Connector | Apache Pulsar is a cloud-native streaming platform. Based on the [Pulsar OpenMLDB connector](https:\/\/pulsar.apache.org\/docs\/en\/next\/io-connectors\/#jdbc-openmldb) , we are able to seamlessly import real-time data streams from Pulsar to OpenMLDB as the online data sources. |\n\n## 8. Documentation\n\n- Chinese documentations: [https:\/\/openmldb.ai\/docs\/zh](https:\/\/openmldb.ai\/docs\/zh\/)\n- English documentations: [https:\/\/openmldb.ai\/docs\/en\/](https:\/\/openmldb.ai\/docs\/en\/)\n\n## 9. Roadmap\n\nPlease refer to our [public Roadmap page](https:\/\/github.com\/4paradigm\/OpenMLDB\/projects\/10).\n\nFurthermore, there are a few important features on the development roadmap but have not been scheduled yet. We appreciate any feedbacks on those features.\n\n- A cloud-native OpenMLDB\n- Automatic feature extraction\n- Optimization based on heterogeneous storage and computing resources\n- A lightweight OpenMLDB for edge computing\n\n## 10. Contributors\n\nWe really appreciate the contribution from our community.\n\n- If you are interested to contribute, please read our [Contribution Guideline](CONTRIBUTING.md) for more details. \n- If you are a new contributor, you may get start with [the list of issues labeled with `good first issue`](https:\/\/github.com\/4paradigm\/OpenMLDB\/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22).\n- If you have experience of OpenMLDB development, or want to tackle a challenge that may take 1-2 weeks, you may find [the list of issues labeled with `call-for-contributions`](https:\/\/github.com\/4paradigm\/OpenMLDB\/issues?q=is%3Aopen+is%3Aissue+label%3Acall-for-contributions).\n\nLet's clap hands for our community contributors :clap:\n\n<a href=\"https:\/\/github.com\/4paradigm\/openmldb\/graphs\/contributors\">\n  <img src=\"https:\/\/contrib.rocks\/image?repo=4paradigm\/openmldb\" width=600\/>\n<\/a>\n\n## 11. Community\n\n- Website: [https:\/\/openmldb.ai\/en](https:\/\/openmldb.ai\/en)\n\n- Email: [contact@openmldb.ai](mailto:contact@openmldb.ai)\n\n- [Slack](https:\/\/join.slack.com\/t\/openmldb\/shared_invite\/zt-ozu3llie-K~hn9Ss1GZcFW2~K_L5sMg) \n\n- [GitHub Issues](https:\/\/github.com\/4paradigm\/OpenMLDB\/issues) and [GitHub Discussions](https:\/\/github.com\/4paradigm\/OpenMLDB\/discussions): The GitHub Issues is used to report bugs and collect new feature requirements. The GitHub Discussions is open to any discussions related to OpenMLDB.\n\n- [Blogs (English)](https:\/\/openmldb.medium.com\/)\n\n- [Blogs (Chinese)](https:\/\/www.zhihu.com\/column\/c_1417199590352916480)\n\n- Public drives maintained by the PMC: [English](https:\/\/drive.google.com\/drive\/folders\/1T5myyLVe--I9b77Vg0Y8VCYH29DRujUL) |  [\u4e2d\u6587](https:\/\/go005qabor.feishu.cn\/drive\/folder\/fldcn3W5i52QmWqgJzRlHvxFf2d)\n\n- [Mailing list for developers](https:\/\/groups.google.com\/g\/openmldb-developers)\n\n- WeChat Groups (Chinese):\n\n  <img src=\"images\/wechat.png\" alt=\"img\" width=120 \/>  \n\n## 12. Publications\n\n- Cheng Chen, Jun Yang, Mian Lu, Taize Wang, Zhao Zheng, Yuqiang Chen, Wenyuan Dai, Bingsheng He, Weng-Fai Wong, Guoan Wu, Yuping Zhao, and Andy Rudoff. *[Optimizing in-memory database engine for AI-powered on-line decision augmentation using persistent memory](http:\/\/vldb.org\/pvldb\/vol14\/p799-chen.pdf)*. International Conference on Very Large Data Bases (VLDB) 2021.\n\n## 13. [The User List](https:\/\/github.com\/4paradigm\/OpenMLDB\/discussions\/707)\n\nWe are building [a user list](https:\/\/github.com\/4paradigm\/OpenMLDB\/discussions\/707) to collect feedback from the community. We really appreciate it if you can provide your use cases, comments, or any feedback when using OpenMLDB. We want to hear from you! \n","69":"---\n# IBM Fully Homomorphic Encryption (HELayers) SDK for Linux\n\n![HELayers](Documentation\/Images\/fhe.jpg)\n\nLast year, we introduced FHE to Linux with our FHE Toolkit. Today, we are announcing the next evolution of the FHE Toolkit called IBM HElayers, a software development kit (SDK) for the practical and efficient execution of encrypted workloads using fully homomorphic encrypted data. HElayers enables application developers and data scientists to seamlessly use advanced privacy preserving techniques without having to be a specialist in cryptography - all while working in a newly integrated Python environment.\u00a0\n\nHELayers is packaged as Docker containers that make it easier to get started and experimenting with FHE.  It is written in C++ and includes a Python API that enables data scientists and application developers to easily use the power of FHE by supporting a wide array of analytics such as linear regression, logistic regression, and neural networks.  \n\nIt is delivered as an open platform that is capable of using the latest FHE schemes and libraries for a given use case. HElayers currently ships with 15 tutorials and sample applications that highlight the basics of FHE and how to use this technology in a practical way. Sample applications include credit card fraud detection, encrypted database search, heart disease detection, image classification and more.  \n\nIn this repository is a script that you can run to install and run the pre-packaged SDKs, cpp or python, as a docker container.\n\n     .\/StartHELayers.sh \n\nIf you are instead looking for the IBM Fully Homomorphic Encryption Toolkit for Linux README, it can be found [here](README_TOOLKIT.md).\n\n\n### Try it for yourself\n\nTo run the script in this reposistory, clone the repo, open a Terminal and navigate to the root of this repo (`cd fhe-toolkit-linux\/`).  \n\nTo run the Python HELayers:\n\n     .\/StartHELayers.sh python\n\nFor the C++ version:\n\n     .\/StartHELayers.sh cpp\n\nThe script will pull & download the latest version that works on your machine's architecture. It will run locally in its own container for you to view in your web browser.  The script will tell you where to point your browser to in the Terminal after completing the setup. The c++ version uses VSCode integrated into the browser for an IDE, using port 8443, and the python relies on a self-contained jupyter notebook on port 8888.\n\nTo try HELayers out from Dockerhub directly, you can use these links below:\n\n* Python\n   * [HELayers Python x86](https:\/\/hub.docker.com\/r\/ibmcom\/helayers-pylab)\n   * [HELayers Python s390x](https:\/\/hub.docker.com\/r\/ibmcom\/helayers-pylab-s390x)\n* C++\n   * [HELayers C++ x86](https:\/\/hub.docker.com\/r\/ibmcom\/helayers-lab)\n   * [HELayers C++ s390x](https:\/\/hub.docker.com\/r\/ibmcom\/helayers-lab-s390x)\n\n### Take a look at our in-depth walkthrough video on how to download and get started with HElayers.\n\n[![Fully Homomorphic Encryption](http:\/\/img.youtube.com\/vi\/_bEMWffloas\/0.jpg)](https:\/\/www.youtube.com\/watch?v=_bEMWffloas \"Getting Started with HELayers\")\n\nTo learn more about FHE in general, and what it can be used for, you can check out our [FAQ\/Content Solutions page](https:\/\/www.ibm.com\/support\/z-content-solutions\/fully-homomorphic-encryption\/ ).\n\n## System Requirements\n\nHElayers is a Linux based Docker container and can be implemented entirely in software; there is no hardware dependency. The only software that is required is Docker version 19 or higher with the necessary user privileges to run `docker` commands. It is also assumed you have a working internet connection. \n\nRuntimes tested include Linux, Linux on IBM Z (z15 and LinuxONE III and later), z\/OS Container Extensions (v2.4 & v2.5), Windows 10 subsystem for Linux and MacOS. Today, the only distribution we support is Ubuntu 20.04 which is included as the base image. \n\nOlder versions have not been thoroughly tested and there are no plans to support older versions in the future.\n\n\n## License\n\nThis image is provided under a community edition license for non-commercial use. Customers who want to work directly with IBM Research, access advanced features, and plan for commercial-grade deployment using HElayers can engage through the Premium Edition Program by contacting the IBM FHE team at FHEstart@us.ibm.com.\n\n## Feedback Survey\n\nHELayers is the next step in the advancement of FHE at IBM.  If you are moving from the Toolkit, you will see familiar code that helped get your applications up and running built around the HElib library.  We have extended that and now offer much more in this edition.  We invite all the Toolkit users to try out HELayers and let us know what you think by taking the survey below and providing feedback on how we can improve and serve you better.  We will only use your feedback to improve the HELayers experience and inform future IBM security-focused products and services.  IBM will not share your response data with any third parties.  We look forward to your valuable feedback to improve the IBM FHE experience. \n\n[https:\/\/www.surveygizmo.com\/s3\/6494169\/IBM-HElayers-SDK-Survey](https:\/\/www.surveygizmo.com\/s3\/6494169\/IBM-HElayers-SDK-Survey)\n\n\n","70":"## The Edge Machine Learning library\n\nThis repository provides code for machine learning algorithms for edge devices\ndeveloped at [Microsoft Research\nIndia](https:\/\/www.microsoft.com\/en-us\/research\/project\/resource-efficient-ml-for-the-edge-and-endpoint-iot-devices\/). \n\nMachine learning models for edge devices need to have a small footprint in\nterms of storage, prediction latency, and energy. One instance of where such \nmodels are desirable is resource-scarce devices and sensors in the Internet \nof Things (IoT) setting. Making real-time predictions locally on IoT devices \nwithout connecting to the cloud requires models that fit in a few kilobytes.\n\n### Contents\nAlgorithms that shine in this setting in terms of both model size and compute, namely:\n - **Bonsai**: Strong and shallow non-linear tree based classifier.\n - **ProtoNN**: **Proto**type based k-nearest neighbors (k**NN**) classifier. \n - **EMI-RNN**: Training routine to recover the critical signature from time series data for faster and accurate RNN predictions.\n - **Shallow RNN**: A meta-architecture for training RNNs that can be applied to streaming data.\n - **FastRNN & FastGRNN - FastCells**: **F**ast, **A**ccurate, **S**table and **T**iny (**G**ated) RNN cells.\n - **DROCC**: **D**eep **R**obust **O**ne-**C**lass **C**lassfiication for training robust anomaly detectors.\n - **RNNPool**: An efficient non-linear pooling operator for RAM constrained inference.\n\nThese algorithms can train models for classical supervised learning problems\nwith memory requirements that are orders of magnitude lower than other modern\nML algorithms. The trained models can be loaded onto edge devices such as IoT\ndevices\/sensors, and used to make fast and accurate predictions completely\noffline.\n\nA tool that adapts models trained by above algorithms to be inferred by fixed point arithmetic.\n - **SeeDot**: Floating-point to fixed-point quantization tool.\n\nApplications demonstrating usecases of these algorithms:\n - **GesturePod**: Gesture recognition pipeline for microcontrollers.\n - **MSC-RNN**: Multi-scale cascaded RNN for analyzing Radar data.\n\n### Organization\n - The `tf` directory contains the `edgeml_tf` package which specifies these architectures in TensorFlow,\n   and `examples\/tf` contains sample training routines for these algorithms.\n - The `pytorch` directory contains the `edgeml_pytorch` package which specifies these architectures in PyTorch,\n   and `examples\/pytorch` contains sample training routines for these algorithms.\n - The `cpp` directory has training and inference code for `Bonsai` and `ProtoNN` algorithms in C++.\n - The `applications` directory has code\/demonstrations of applications of the EdgeML algorithms. \n - The `tools\/SeeDot` directory has the quantization tool to generate fixed-point inference code.  \n - The `c_reference` directory contains the inference code (floating-point or quantized) for various algorithms in C.\n\nPlease see install\/run instructions in the README pages within these directories.\n\n### Details and project pages\nFor details, please see our\n [project page](https:\/\/microsoft.github.io\/EdgeML\/), \n [Microsoft Research page](https:\/\/www.microsoft.com\/en-us\/research\/project\/resource-efficient-ml-for-the-edge-and-endpoint-iot-devices\/),\nthe ICML '17 publications on [Bonsai](\/docs\/publications\/Bonsai.pdf) and\n[ProtoNN](\/docs\/publications\/ProtoNN.pdf) algorithms, \nthe NeurIPS '18 publications on [EMI-RNN](\/docs\/publications\/emi-rnn-nips18.pdf) and\n[FastGRNN](\/docs\/publications\/FastGRNN.pdf),\nthe PLDI '19 publication on [SeeDot compiler](\/docs\/publications\/SeeDot.pdf),\nthe UIST '19 publication on [Gesturepod](\/docs\/publications\/GesturePod-UIST19.pdf), \nthe BuildSys '19 publication on [MSC-RNN](\/docs\/publications\/MSCRNN.pdf),\nthe NeurIPS '19 publication on [Shallow RNNs](\/docs\/publications\/Sha-RNN.pdf),\nthe ICML '20 publication on [DROCC](\/docs\/publications\/drocc.pdf),\nand the NeurIPS '20 publication on [RNNPool](\/docs\/publications\/RNNPool.pdf).\n\n\nAlso checkout the [ELL](https:\/\/github.com\/Microsoft\/ELL) project which can\nprovide optimized binaries for some of the ONNX models trained by this library.\n\n### Contributors:\nCode for algorithms, applications and tools contributed by:\n  - [Don Dennis](https:\/\/dkdennis.xyz)\n  - [Yash Gaurkar](https:\/\/github.com\/mr-yamraj\/)\n  - [Sridhar Gopinath](http:\/\/www.sridhargopinath.in\/)\n  - [Sachin Goyal](https:\/\/saching007.github.io\/)\n  - [Chirag Gupta](https:\/\/aigen.github.io\/)\n  - [Moksh Jain](https:\/\/github.com\/MJ10)\n  - [Shikhar Jaiswal](https:\/\/shikharj.github.io\/)\n  - [Ashish Kumar](https:\/\/ashishkumar1993.github.io\/)\n  - [Aditya Kusupati](https:\/\/adityakusupati.github.io\/)\n  - [Chris Lovett](https:\/\/github.com\/lovettchris)\n  - [Shishir Patil](https:\/\/shishirpatil.github.io\/)\n  - [Oindrila Saha](https:\/\/github.com\/oindrilasaha)\n  - [Harsha Vardhan Simhadri](http:\/\/harsha-simhadri.org)\n\n[Contributors](https:\/\/microsoft.github.io\/EdgeML\/People) to this project. New contributors welcome.\n\nPlease [email us](mailto:edgeml@microsoft.com) your comments, criticism, and questions.\n\nIf you use software from this library in your work, please use the BibTex entry below for citation.\n\n```\n@misc{edgeml04,\n   author = {{Dennis, Don Kurian and Gaurkar, Yash and Gopinath, Sridhar and Goyal, Sachin \n              and Gupta, Chirag and Jain, Moksh and Jaiswal, Shikhar and Kumar, Ashish and\n              Kusupati, Aditya and  Lovett, Chris and Patil, Shishir G and Saha, Oindrila and\n              Simhadri, Harsha Vardhan}},\n   title = {{EdgeML: Machine Learning for resource-constrained edge devices}},\n   url = {https:\/\/github.com\/Microsoft\/EdgeML},\n   version = {0.4},\n}\n```\n\n### Microsoft Open Source Code of Conduct This project has adopted the\n[Microsoft Open Source Code of\nConduct](https:\/\/opensource.microsoft.com\/codeofconduct\/). For more information\nsee the [Code of Conduct\nFAQ](https:\/\/opensource.microsoft.com\/codeofconduct\/faq\/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional\nquestions or comments.\n","71":"## ml4a-ofx\n\nA collection of real-time interactive applications and associated scripts for working with machine learning. \n\nAll apps contained here require [openFrameworks](http:\/\/www.openframeworks.cc) to run, as well as a number of addons, listed below.\n\n### Organization\n\nThe openFrameworks apps are provided as source code, and can be built and compiled by using the project generator that comes with openFrameworks.\n\nSeveral of these applications are coupled with python scripts which do some analysis of media (feature extraction, t-SNE, etc), whose results are then imported into your ofApp via JSON or some other means for further processing. Some of them can be replicated entirely within openFrameworks, and wherever possible (for example, t-SNE) such applications are also provided. The advantage of the scripts is that they can be used in other environments as well, and are much easier to port or implement from the preeminent machine learning frameworks. \n\n### Applications\n\nThe [apps](https:\/\/github.com\/ml4a\/ml4a-ofx\/tree\/master\/apps) folder contains all of the individual openFrameworks apps, and their descriptions and usage instructions.\n\n### Osc Modules\n\nThe [osc-modules](https:\/\/github.com\/ml4a\/ml4a-ofx\/tree\/master\/osc-modules) folder contains a set of applications which send or receive OSC in order to facilitate communication with other applications, and are very easily used alongside [Wekinator](http:\/\/www.wekinator.org). \n\n### Datasets\n\nSome of the applications work on image\/audio\/text datasets. Example datasets are provided and can be downloaded with the provided scripts (see `download_images.py` for image example). Additionally, some require pre-trained models which can also be downloaded. For the ones which require VGG weights, you can download that [here](https:\/\/drive.google.com\/file\/d\/0Bz7KyqmuGsilT0J5dmRCM0ROVHc\/view?usp=sharing).\n\n### Addons\n\nAll of the included applications (both in the apps and osc-modules folders) require some combination of the following addons. The readme for each app lists the addons required individually. The following is a complete list: \n- [ofxAbletonLive](https:\/\/github.com\/genekogan\/ofxAbletonLive)\n- [ofxAssignment](https:\/\/github.com\/kylemcdonald\/ofxAssignment)\n- [ofxAudioUnit](https:\/\/github.com\/admsyn\/ofxAudioUnit)\n- [ofxCv](https:\/\/github.com\/kylemcdonald\/ofxCv)\n- [ofxCcv](https:\/\/github.com\/kylemcdonald\/ofxCcv)\n- [ofxDarknet](https:\/\/github.com\/mrzl\/ofxDarknet)\n- [ofxFaceTracker2](https:\/\/github.com\/HalfdanJ\/ofxFaceTracker2)\n- [ofxGrt](https:\/\/github.com\/nickgillian\/ofxGrt)\n- [ofxJSON](https:\/\/github.com\/jeffcrouse\/ofxJSON)\n- [ofxLeapMotion2](https:\/\/github.com\/genekogan\/ofxLeapMotion2)\n- [ofxLearn](https:\/\/github.com\/genekogan\/ofxLearn)\n- [ofxMaxim](https:\/\/github.com\/falcon4ever\/ofxMaxim)\n- [ofxOpenNI](https:\/\/github.com\/gameoverhack\/ofxOpenNI)\n- [ofxScreenGrab](https:\/\/github.com\/genekogan\/ofxScreenGrab)\n- [ofxTemplateMatch](https:\/\/github.com\/genekogan\/ofxTemplateMatching)\n- [ofxTSNE](https:\/\/github.com\/genekogan\/ofxTSNE)\n\nThe addons also make use of ofxGui, ofxOsc, and ofxOpenCv, which are included in openFrameworks by default.\n\nThese addons are not currently used in ml4a-ofx, but are also relevant and may be wrapped into examples in the future: \n- [ofxMSATensorflow](https:\/\/github.com\/memo\/ofxMSATensorFlow)\n- [ofxDlib](https:\/\/github.com\/bakercp\/ofxDlib)\n- [ofxCaffe](https:\/\/github.com\/Geekrick88\/ofxCaffe)\n\n## Getting started\n\nThese are some steps to get you up and running with ml4a-ofx:\n\n### Prepare your project\n\n0. Make sure you've read and followed the [openFrameworks setup guides](http:\/\/openframeworks.cc\/download\/) relevant for your system. Going through them makes sure the foundation for all ml4a-ofx apps is working correctly.\n1. This guide assumes that you have:\n   - One folder with the `openFrameworks` setup in it. This is the folder in which you would test your `openFrameworks` setup as described above.\n   - One different folder in which you have the `ml4a-ofx` content. This folder does *not* have to be within the `openFrameworks` folder.\n1. For your project, you most certainly want to use an existing `ml4a-ofx` app as a starting point. So simply duplicate any of the folders within `ml4a-ofx\/apps` and give the folder a custom name. Or work in the folder of the example project if you do not plan to re-use it anyways.\n1. Check the `readme.md` file of your project. It always contains the required addons you need to get your app running. Follow the links of each addon, check the installation guide and follow each one.\n   - Some of the apps need example data to work correctly. As you probably have enough space on your computer anyways, open a terminal window, go into the `ml4a-ofx` folder and run `sh setup.sh`. This will download the necessary data and place it in the `data` folder.\n   - All addons currently used across the examples are mentioned in the [Addons list](#addons).\n   - Be careful: Put the required addons into the `addons` subfolder of your `openFrameworks` folder to work correctly.\n\n### Setup your project\n\n#### On a Mac\n\nYou can use the `projectGenerator` app that is included in `openFrameworks`.\n\n- Set the base path to your `openFrameworks` folder as described in the app. This has to be done on the first time only.\n- Click on the `create\/update` tab\n- Import your project by clicking the `import` button, selecting your project's folder. This should be located in your `ml4a-ofx` folder. Your project name, project path and the addons list should update accordingly. If any of the addons is missing, the interface will notify you of that. Since you prepared your project in the above described steps, all should be good here. Otherwise, check the \"Prepare your project\" part again.\n- Click \"Update\" to get the project setup running. The interface doesn't show it explicitly, but this gets the project generator running. It should\u2122 result in a success message. You can then click \"Open in IDE\" to directly open the project in XCode. Also, feel free to close the `projectGenerator` again since you don't need it anymore for now.\n- Within XCode, wait a few moments to index and process the files of the project. You can then compile the project by pressing the \"Play\" button or `CMD + R`.\n- Cross your fingers :fingers_crossed: and hope your project compiles :wink:\n\nIf you run into any errors: The `openFrameworks setup guides` might have some explanations or possible steps. Otherwise, it always helps to google the error (seriously!) and try to figure out what's going on. In case you get completely stuck, you might find help here by [opening an issue](https:\/\/github.com\/ml4a\/ml4a-ofx\/issues\/new). Include any error message you find and try to explain the problem as good as possible. This will help resolve any issue a lot.\n\n#### On Windows\n\n[Feel free to add documentation here]\n\n#### On Linux\n\n[Feel free to add documentation here]","72":"mshadow: Matrix Shadow\n======\nThis code base has been donated to the Apache MXNet project per [#373](https:\/\/github.com\/dmlc\/mshadow\/issues\/373), and repo is deprecated. Future development should continue in Apache MXNet.\n\n[![Build Status](https:\/\/travis-ci.org\/dmlc\/mshadow.svg?branch=master)](https:\/\/travis-ci.org\/dmlc\/mshadow)\n\nMShadow is a lightweight CPU\/GPU Matrix\/Tensor Template Library in C++\/CUDA. The goal of mshadow is to support ***efficient***,\n***device invariant*** and ***simple*** tensor library for machine learning project that aims for maximum performance and control, while also emphasize simplicity.\n\nMShadow also provides interface that allows writing Multi-GPU and distributed deep learning programs in an easy and unified way.\n\n* [Contributors](https:\/\/github.com\/tqchen\/mshadow\/graphs\/contributors)\n* [Tutorial](guide)\n* [Documentation](doc)\n* [Parameter Server Interface for GPU Tensor](guide\/mshadow-ps)\n\nFeatures\n--------\n* Efficient: all the expression you write will be lazily evaluated and compiled into optimized code\n  - No temporal memory allocation will happen for expression you write\n  - mshadow will generate specific kernel for every expression you write in compile time.\n* Device invariant: you can write one code and it will run on both CPU and GPU\n* Simple: mshadow allows you to write machine learning code using expressions.\n* Whitebox: put a float* into the Tensor struct and take the benefit of the package, no memory allocation is happened unless explicitly called\n* Lightweight library: light amount of code to support frequently used functions in machine learning\n* Extendable: user can write simple functions that plugs into mshadow and run on GPU\/CPU, no experience in CUDA is required.\n* MultiGPU and Distributed ML: mshadow-ps interface allows user to write efficient MultiGPU and distributed programs in an unified way.\n\nVersion\n-------\n* This version mshadow-2.x, there are a lot of changes in the interface and it is not backward compatible with mshadow-1.0\n  - If you use older version of cxxnet, you will need to use the legacy mshadow code\n* For legacy code, refer to [Here](https:\/\/github.com\/tqchen\/mshadow\/releases\/tag\/v1.1)\n* Change log in [CHANGES.md](CHANGES.md)\n\nProjects Using MShadow\n----------------------\n* [MXNet: Efficient and Flexible Distributed Deep Learning Framework](https:\/\/github.com\/dmlc\/mxnet)\n* [CXXNet: A lightweight  C++ based deep learnig framework](https:\/\/github.com\/dmlc\/cxxnet)\n","73":"# ML++\n\nMachine learning is a vast and exiciting discipline, garnering attention from specialists of many fields. Unfortunately, for C++ programmers and enthusiasts, there appears to be a lack of support in the field of machine learning. To fill that void and give C++ a true foothold in the ML sphere, this library was written. The intent with this library is for it to act as a crossroad between low-level developers and machine learning engineers.\n\n<p align=\"center\">\n    <img src=\"https:\/\/user-images.githubusercontent.com\/78002988\/119920911-f3338d00-bf21-11eb-89b3-c84bf7c9f4ac.gif\" \n    width = 600 height = 400>\n<\/p>\n\n## Installation\nBegin by downloading the header files for the ML++ library. You can do this by cloning the repository and extracting the MLPP directory within it:\n```\ngit clone https:\/\/github.com\/novak-99\/MLPP\n```\nNext, execute the \"buildSO.sh\" shell script:\n```\nsudo .\/buildSO.sh\n```\nAfter doing so, maintain the ML++ source files in a local directory and include them in this fashion: \n```cpp\n#include \"MLPP\/Stat\/Stat.hpp\" \/\/ Including the ML++ statistics module. \n\nint main(){\n...\n}\n```\nFinally, after you have concluded creating a project, compile it using g++:\n```\ng++ main.cpp \/usr\/local\/lib\/MLPP.so --std=c++17\n```\n\n## Usage\nPlease note that ML++ uses the ```std::vector<double>``` data type for emulating vectors, and the ```std::vector<std::vector<double>>``` data type for emulating matrices.\n\nBegin by including the respective header file of your choice.\n```cpp\n#include \"MLPP\/LinReg\/LinReg.hpp\"\n```\nNext, instantiate an object of the class. Don't forget to pass the input set and output set as parameters.\n```cpp\nLinReg model(inputSet, outputSet);\n```\nAfterwards, call the optimizer that you would like to use. For iterative optimizers such as gradient descent, include the learning rate, epoch number, and whether or not to utilize the UI panel. \n```cpp\nmodel.gradientDescent(0.001, 1000, 0);\n```\nGreat, you are now ready to test! To test a singular testing instance, utilize the following function:\n```cpp\nmodel.modelTest(testSetInstance);\n```\nThis will return the model's singular prediction for that example. \n\nTo test an entire test set, use the following function: \n```cpp\nmodel.modelSetTest(testSet);\n```\nThe result will be the model's predictions for the entire dataset.\n\n\n## Contents of the Library\n1. ***Regression***\n    1. Linear Regression \n    2. Logistic Regression\n    3. Softmax Regression\n    4. Exponential Regression\n    5. Probit Regression\n    6. CLogLog Regression\n    7. Tanh Regression\n2. ***Deep, Dynamically Sized Neural Networks***\n    1. Possible Activation Functions\n        - Linear\n        - Sigmoid\n        - Softmax\n        - Swish\n        - Mish\n        - SinC\n        - Softplus\n        - Softsign\n        - CLogLog\n        - Logit\n        - Gaussian CDF\n        - RELU\n        - GELU\n        - Sign\n        - Unit Step \n        - Sinh\n        - Cosh\n        - Tanh\n        - Csch\n        - Sech\n        - Coth\n        - Arsinh\n        - Arcosh\n        - Artanh\n        - Arcsch\n        - Arsech\n        - Arcoth\n    2. Possible Optimization Algorithms\n        - Batch Gradient Descent\n        - Mini-Batch Gradient Descent \n        - Stochastic Gradient Descent \n        - Gradient Descent with Momentum\n        - Nesterov Accelerated Gradient\n        - Adagrad Optimizer \n        - Adadelta Optimizer \n        - Adam Optimizer \n        - Adamax Optimizer \n        - Nadam Optimizer \n        - AMSGrad Optimizer \n        - 2nd Order Newton-Raphson Optimizer*\n        - Normal Equation*\n        <p><\/p>\n        *Only available for linear regression\n    3. Possible Loss Functions\n        - MSE\n        - RMSE \n        - MAE\n        - MBE\n        - Log Loss\n        - Cross Entropy\n        - Hinge Loss\n        - Wasserstein Loss\n    4. Possible Regularization Methods\n        - Lasso\n        - Ridge\n        - ElasticNet\n        - Weight Clipping\n    5. Possible Weight Initialization Methods\n        - Uniform \n        - Xavier Normal\n        - Xavier Uniform\n        - He Normal\n        - He Uniform\n        - LeCun Normal\n        - LeCun Uniform\n    6. Possible Learning Rate Schedulers\n        - Time Based \n        - Epoch Based\n        - Step Based\n        - Exponential \n3. ***Prebuilt Neural Networks***\n    1. Multilayer Peceptron\n    2. Autoencoder\n    3. Softmax Network\n4. ***Generative Modeling***\n    1. Tabular Generative Adversarial Networks\n    2. Tabular Wasserstein Generative Adversarial Networks\n5. ***Natural Language Processing***\n    1. Word2Vec (Continous Bag of Words, Skip-Gram)\n    2. Stemming\n    3. Bag of Words\n    4. TFIDF\n    5. Tokenization \n    6. Auxiliary Text Processing Functions\n6. ***Computer Vision***\n    1. The Convolution Operation\n    2. Max, Min, Average Pooling\n    3. Global Max, Min, Average Pooling\n    4. Prebuilt Feature Detectors\n        - Horizontal\/Vertical Prewitt Filter\n        - Horizontal\/Vertical Sobel Filter\n        - Horizontal\/Vertical Scharr Filter\n        - Horizontal\/Vertical Roberts Filter\n        - Gaussian Filter\n        - Harris Corner Detector\n7. ***Principal Component Analysis***\n8. ***Naive Bayes Classifiers***\n    1. Multinomial Naive Bayes\n    2. Bernoulli Naive Bayes \n    3. Gaussian Naive Bayes\n9. ***Support Vector Classification***\n    1. Primal Formulation (Hinge Loss Objective) \n    2. Dual Formulation (Via Lagrangian Multipliers)\n10. ***K-Means***\n11. ***k-Nearest Neighbors***\n12. ***Outlier Finder (Using z-scores)***\n13. ***Matrix Decompositions***    \n    1. SVD Decomposition\n    2. Cholesky Decomposition\n        - Positive Definiteness Checker \n    3. QR Decomposition\n14. ***Numerical Analysis***\n    1. Numerical Diffrentiation \n        - Univariate Functions \n        - Multivariate Functions \n    2. Jacobian Vector Calculator\n    3. Hessian Matrix Calculator\n    4. Function approximator\n        - Constant Approximation\n        - Linear Approximation \n        - Quadratic Approximation\n        - Cubic Approximation\n    5. Diffrential Equations Solvers \n        - Euler's Method \n        - Growth Method\n15. ***Mathematical Transforms***\n    1. Discrete Cosine Transform\n16. ***Linear Algebra Module***\n17. ***Statistics Module***\n18. ***Data Processing Module***\n    1. Setting and Printing Datasets \n    2. Available Datasets\n        1. Wisconsin Breast Cancer Dataset\n            - Binary\n            - SVM \n        2. MNIST Dataset\n            - Train\n            - Test\n        3. Iris Flower Dataset\n        4. Wine Dataset\n        5. California Housing Dataset\n        6. Fires and Crime Dataset (Chicago)\n    3. Feature Scaling \n    4. Mean Normalization\n    5. One Hot Representation\n    6. Reverse One Hot Representation\n    7. Supported Color Space Conversions \n        - RGB to Grayscale\n        - RGB to HSV\n        - RGB to YCbCr\n        - RGB to XYZ\n        - XYZ to RGB\n19. ***Utilities***\n    1. TP, FP, TN, FN function\n    2. Precision\n    3. Recall \n    4. Accuracy\n    5. F1 score\n\n\n## What's in the Works? \nML++, like most frameworks, is dynamic, and constantly changing. This is especially important in the world of ML, as new algorithms and techniques are being developed day by day. Here are a couple of things currently being developed for ML++:\n    <p>\n    - Convolutional Neural Networks \n    <\/p>\n    <p>\n    - Kernels for SVMs \n    <\/p>\n    <p>\n    - Support Vector Regression\n    <\/p>    \n    \n## Citations\nVarious different materials helped me along the way of creating ML++, and I would like to give credit to several of them here. [This](https:\/\/www.tutorialspoint.com\/cplusplus-program-to-compute-determinant-of-a-matrix) article by TutorialsPoint was a big help when trying to implement the determinant of a matrix, and [this](https:\/\/www.geeksforgeeks.org\/adjoint-inverse-matrix\/) article by GeeksForGeeks was very helpful when trying to take the adjoint and inverse of a matrix.\n","74":"Veles\n=====\nDistributed platform for rapid Deep learning application development\n--------------------------------------------------------------------\nConsists of: \n\n*  Platform - https:\/\/github.com\/Samsung\/veles\n\n*  Znicz Plugin - [Neural Network engine](https:\/\/github.com\/Samsung\/veles.znicz)\n\n*  Mastodon - [Veles <-> Java bridge for Hadoop etc.](https:\/\/github.com\/Samsung\/veles.mastodon)\n\n*  SoundFeatureExtraction - [audio feature extraction library](https:\/\/github.com\/Samsung\/veles.sound_feature_extraction)\n\nHome page: https:\/\/velesnet.ml\n\nNamed after https:\/\/en.wikipedia.org\/wiki\/Veles_(god)\n\nReleased under Apache 2.0 license. Copyright \u00a9 Samsung Electronics Co., Ltd., 2013-2015.\n","75":"# **THIS REPOSITORY IS DEPRECATED.**\n# THE FUNCTIONALITY HAS MOVED TO [Turi Create](https:\/\/github.com\/apple\/turicreate). \n\n____\n\nSFrame\n======\n<a href=\"https:\/\/pypi.python.org\/pypi\/SFrame\">\n   <img src=\"https:\/\/img.shields.io\/pypi\/v\/SFrame.svg\" alt=\"latest release\" \/>\n<\/a>\n<a href=\"https:\/\/travis-ci.org\/turi-code\/SFrame\">\n   <img src=\"https:\/\/travis-ci.org\/turi-code\/SFrame.svg?branch=master\" alt=\"travis build status\" \/>\n<\/a>\n\nScalable tabular (**SFrame**, **SArray**) and graph (**SGraph**) data-structures built for out-of-core data analysis. \n\nThe SFrame package provides the complete implementation of:\n - SFrame\n - SArray\n - SGraph \n - The C++ SDK surface area (gl_sframe, gl_sarray, gl_sgraph)\n\nIntroduction\n------------\n\nThe SFrame contains the open source components [GraphLab Create](https:\/\/turi.com\/products\/create\/) from [Turi](https:\/\/turi.com). \n\nSome documentation to help get started:\n- [Getting started with SFrame](https:\/\/turi.com\/learn\/gallery\/notebooks\/introduction_to_sframes.html)\n- [SFrame user guide](https:\/\/turi.com\/learn\/userguide\/sframe\/tabular-data.html)\n- [SGraph user guide](https:\/\/turi.com\/learn\/userguide\/sgraph\/sgraph.html) \n\nFor more details on GraphLab Create (including documentation and tutorials) see http:\/\/turi.com.\n\nSome of the key features of this package are.\n\n- A scalable column compressed disk-backed dataframe optimized for machine learning and data science needs.\n- Designed for both **tabular** (SFrame, SArray) as well as **graph** data (SGraph)\n- Support for **strictly typed** columns (int, float, str, datetime), **weakly typed** columns (schema free lists, dictionaries) as well as **specialized types** such as Image.\n- Uniform support for **missing data**.\n- Query optimization and Lazy evaluation.\n- A C++ API (gl_sarray, gl_sframe, gl_sgraph) with direct native access via the C++ SDK.\n- A Python API (SArray, SFrame, SGraph) with an indirect access via an interprocess layer.\n\nLicense\n-------\nThe SFrame Package is licensed under a BSD license. See [license](LICENSE) file.\n\nInstallation\n------------\nWe release an updated version about once every 1.5 months or so. You can download\nthe most recent version directly from [pypi](https:\/\/pypi.python.org\/pypi\/SFrame)\nor using pip:\n\n    pip install -U sframe\n\n**Requirements**\n\nSFrame requires a 64-bit operating system.\n\n**Operating Systems**\n- Mac OS X: 10.8+\n- Linux: Any distribution with GLIBC >= 2.11\n - Ubuntu >= 11.04 \n - Debian >= 6 \n - RHEL >= 6 \n - SLES >= 11\n- Windows (7, 8, 10, Server 2012 R2)\n\n**Python**\n- Python 2.7.x\n- Pyhton 3.4.x\n- **Note**: Unfortunately, Python 3.5.x is currently not supported. This is coming soon.\n\nBuild Dependencies\n------------------\nTo simplify the development process, we use a pre-built dependency toolkit we\ncall dato-deps which prepackages all compile-time dependencies like boost, curl,\netc, some with patches which we should contribute back.  On Linux, we also\nprepackage the entire compiler toolchain. On configuration, dato-deps is\ndownloaded and unpacked into the deps\/ directory\n\n### OS X\nAt least OS X 10.9. OS X 10.10 preferred.\n\n* On OS X: Apple XCode 6 Command Line Tools [Required]\n  +  \"clang --version\" should report at least\n     \"Apple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)\"\n\n* ccache [Optional]\n   + Not required, but highly recommended\n\n* cmake >= 3.2 [Optional]\n   + Not required, but highly recommended\n\n### Linux\n\n* *nix build tools: patch, make [Required]\n   +  Should come with most Mac\/Linux systems by default. Recent Ubuntu versions\n   will require installing the build-essential package.\n\n* zlib [Required]\n   +   Comes with most Mac\/Linux systems by default. Recent Ubuntu version will\n   require the zlib1g-dev package.\n\n* ccache [Optional]\n   + Not required, but highly recommended\n\n* cmake >= 3.2 [Optional]\n   + Not required, but highly recommended\n\n### Windows\n\nThis is somewhat more involving. See the wiki.\n\n\nCompiling\n---------\nAfter you have done a git pull, cd into the SFrame directory and run the configure script:\n\n    cd SFrame\n    .\/configure\n\nRunning configure will create two sub-directories, *release* and *debug*.  Select \neither of these modes\/directories and navigate to the *oss_src\/unity* subdirectory:\n\n    cd <repo root>\/debug\/oss_src\/unity\/python\n   \n   or\n   \n    cd <repo root>\/release\/oss_src\/unity\/python\n\nRunning **make** will build the project, according to the mode selected. \n\nWe recommend using make's parallel build feature to accelerate the compilation\nprocess. For instance:\n\n    make -j 4\n\nWill perform up to 4 build tasks in parallel. The number of tasks to run in\nparallel should be roughly equal to the number of CPUs you have.\n\nRunning in the Build Tree\n-------------------------\nTo avoid complicated interactions with python installed in your system, we\ninstall an entire miniconda toolchain into deps\/.\n\nIn order to use the dev build that you have just compiled, some environment\nvariables will need to be set.  This can be done by sourcing a script. You'll\nneed to pass the script either \"debug\" or \"release\" depending on the type of\nbuild you have compiled:\n  \n    source <repo root>\/oss_local_scripts\/python_env.sh [debug | release ]\n\nThis will activate the miniconda toolchain, and you can run python directly\nand import sframe, etc.\n \nRunning Unit Tests\n------------------\n\n### Running Python unit tests\n\nThere is a script that makes it easy to run the python unit test. You will just need to call it and pass it\nyour build type (either \"debug\" or \"release).\n\n    <repo root>\/oss_local_scripts\/run_python_test.sh [debug | release]\n\nAlternatively, you can run nosetests directly. For instance, if I am using\nthe debug build directory:\n\nActivate the python environment\n\n    source <repo root>\/oss_local_scripts\/python_env.sh debug\n\nSwitch the build output directory and run nosetests\n\n    cd <repo root>\/debug\/src\/unity\/python\n    nosetests -s -v sframe\/test\n\n### Running C++ Units Tests\n \nThere are also C++ tests. To compile and run them, do the following:\n\n    cd <repo root>\/[debug | release]\/oss_test\n    make\n    ctest\n  \nCompilation and Execution Summary\n---------------------------------\nTo summarize the above, to just build and run SFrame in the debug build directory:\n\n    .\/configure\n    cd debug\/oss_src\/unity\n    make -j4\n\n    # where root is the checked out SFrame directory\n    source <root>\/oss_local_scripts\/python_env.sh debug\n\n    # import sframe should work here\n    python\n\n\nPackaging\n---------\nTo build an egg for your platform, run\n\n    <repo root>\/oss_local_scripts\/make_egg.sh\n\n(See --help for options)\n\nSDK\n---\nSee: https:\/\/github.com\/turi-code\/GraphLab-Create-SDK\n","76":"# TensorFlow.js\n\n[![build status](https:\/\/travis-ci.org\/node-tensorflow\/node-tensorflow.svg)](https:\/\/travis-ci.org\/node-tensorflow\/node-tensorflow)\n\nNode-tensorflow is a NodeJS API for utilizing Google's open-source machine learning library TensorFlow.\nThis project aims to allow more people easy-to-use access to the TensorFlow library inside of NodeJS while still having performance and stability in mind. The project is in early design stage and we appreciate anyone who would like to help!\n\n## Contributing\nWe currently need all help we can get. We are specially looking for people with C++ knowledge!\nFor more information, please join the slack conversation.\nhttps:\/\/tensor-flow-talk-invite.herokuapp.com\/\n\n## Get Started\n\nThere is a proof of idea of using SWIG interface. A simple test is made for checking the version of Tensorflow.\n```\n$ npm run _preinstall\n$ npm install\n$ npm test\n\n# Tensorflow Version\nok 1 should be equal\n\n1..1\n# tests 1\n# pass  1\n\n# ok\n```\n\n## Current Progress\n\nIn our first proposal, we have defined the roadmap of building a Node.js API to utilize the core functions of Tensorflow. In order to control the graph execution from C++, we have to use Python API to build the computation graph and load the graph using C++ Session API. We are currently working on the Tensorflow interface using SWIG first which drives the C++ core API to a Node.js binding. Python API also provides Optimizers, Tensor Transformations, Type Casting, and some other useful features. We will have further discussion once the interface is built.\n\n### Roadmap:\n\n+ In progress\n\nC++ core library <-> SWIG <-> Node.js binding <-> Python API <-> End Users\n\n+ Final stage:\n\nC++ core library <-> SWIG <-> Node.js binding <-> Node.js API <-> End Users\n\n## Discussion of interface for different languages\n\n+ [C# API](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/18)\n+ [Node.js API](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/37)\n+ [GoLang Library](https:\/\/groups.google.com\/a\/tensorflow.org\/d\/topic\/discuss\/GMd-Am_u9KE\/discussion)\n+ [Ruby API](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/50)\n+ [Rust API](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/388)\n+ [Swig to Java](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/5)\n","77":"# Windows Machine Learning\n\nWindows Machine Learning is a high-performance machine learning inference API that is powered by [ONNX Runtime](https:\/\/onnxruntime.ai\/) and [DirectML](https:\/\/docs.microsoft.com\/en-us\/windows\/ai\/directml\/dml).\n\n\n[![Alt text](https:\/\/i.ytimg.com\/vi\/8MCDSlm326U\/hq720.jpg?sqp=-oaymwEcCOgCEMoBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLCFThAhMuWU3UrHtPjc4Ttz9SSkpQ)](https:\/\/www.youtube.com\/watch?v=8MCDSlm326U)\n\nThe Windows ML API is a [Windows Runtime Component](https:\/\/docs.microsoft.com\/en-us\/windows\/uwp\/winrt-components\/) and is suitable for high-performance, low-latency applications such as frameworks, games, and other real-time applications as well as applications built with high-level languages.\n\n\nThis repo contains Windows Machine Learning samples and tools that demonstrate how to build machine learning powered scenarios into Windows applications.\n\n- [Getting Started with Windows ML](#getting-started-with-windows-ml)\n- [Model Samples](#model-samples)\n- [Advanced Scenario Samples](#advanced-scenario-samples)\n- [Developer Tools](#developer-tools)\n- [Feedback](#feedback)\n- [External Links](#external-links)\n- [Contributing](#contributing)\n\nFor additional information on Windows ML, including step-by-step tutorials and how-to guides, please visit the [Windows ML documentation](https:\/\/docs.microsoft.com\/en-us\/windows\/ai\/).\n\n\n| Sample\/Tool | Status |\n|---------|--------------|\n| All Samples | [![Build Status](https:\/\/microsoft.visualstudio.com\/WindowsAI\/_apis\/build\/status\/WinML\/Samples\/Github%20Public%20Samples%20Build%20RS5?branchName=master)](https:\/\/microsoft.visualstudio.com\/WindowsAI\/_build\/latest?definitionId=39302&branchName=master) |\n| WinmlRunner | [![Build Status](https:\/\/microsoft.visualstudio.com\/WindowsAI\/_apis\/build\/status\/WInMLRunner%20CI%20Build?branchName=master)](https:\/\/microsoft.visualstudio.com\/WindowsAI\/_build\/latest?definitionId=38654&branchName=master) |\n| WinML Dashboard | [![Build Status](https:\/\/microsoft.visualstudio.com\/WindowsAI\/_apis\/build\/status\/WinML\/WinMLTools\/WinML%20Dashboard%20CI%20Build%20(Github)?branchName=master)](https:\/\/microsoft.visualstudio.com\/WindowsAI\/_build\/latest?definitionId=39375&branchName=master) |\n\n## Getting Started with Windows ML\n\n**Prerequisites**\n- [Visual Studio 2017 Version 15.7.4 or Newer](https:\/\/developer.microsoft.com\/en-us\/windows\/downloads)\n\n\nWindows ML offers machine learning inferencing via the inbox Windows SDK as well as a redistributable NuGet package. The table below highlights the availability, distribution, language support, servicing, and forward compatibility aspects of the In-Box and NuGet package for Windows ML.\n\n| |In-Box|\tNuGet|\n|-|-----|------|\n|Availability|\t[Windows 10 - Build 17763 (RS5) or Newer](https:\/\/www.microsoft.com\/en-us\/software-download\/windows10)<br\/>For more detailed information about version support, checkout our [docs](https:\/\/docs.microsoft.com\/en-us\/uwp\/api\/windows.ai.machinelearning?view=winrt-22000).\t| [Windows 8.1 or Newer](https:\/\/www.microsoft.com\/en-us\/software-download\/windows8ISO)<br\/>**NOTE**: Some APIs (ie: VideoFrame) are not available on older OSes.|\n|Windows SDK|\t[Windows SDK - Build 17763 (RS5) or Newer](https:\/\/developer.microsoft.com\/en-us\/windows\/downloads\/sdk-archive\/) |\t[Windows SDK - Build 17763 (RS5) or Newer](https:\/\/developer.microsoft.com\/en-us\/windows\/downloads\/sdk-archive\/)\n|Distribution|\tBuilt into Windows |\tPackage and distribute as part of your application\n|Servicing|\tMicrosoft-driven (customers benefit automatically)\t| Developer-driven\n|Forward| compatibility\tAutomatically rolls forward with new features\t| Developer needs to update package manually\n\nLearn more [here](https:\/\/docs.microsoft.com\/en-us\/windows\/ai\/windows-ml\/get-started).\n\n## Model Samples\nIn this section you will find various model samples for a variety of scenarios across the different Windows ML API offerings.\n\n**Image Classification**\n\nA subdomain of computer vision in which an algorithm looks at an image and assigns it a tag from a collection of predefined tags or categories that it has been trained on.\n\n| Windows App Type <br\/>Distribution | UWP<br\/>In-Box |  UWP<br\/>NuGet | Desktop<br\/>In-Box | Desktop<br\/>NuGet |\n|------------|------------------------------------|--------------------------------------|------------------------------------|--------------------------------------|\n| [AlexNet](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/alexnet)                                | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [CaffeNet](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/caffenet)                                  | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [DenseNet](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/densenet-121)                              | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [EfficientNet](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/efficientnet-lite4)             | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [Emoji8](https:\/\/blogs.windows.com\/windowsdeveloper\/2018\/11\/16\/introducing-emoji8\/)     | [\u2714\ufe0fC#](https:\/\/github.com\/microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/Emoji8\/UWP\/cs) |                                                 |\n| [GoogleNet](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/inception_and_googlenet\/googlenet)        | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [InceptionV1](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/inception_and_googlenet\/inception_v1)      | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [InceptionV2](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/inception_and_googlenet\/inception_v2)      | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [MNIST](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/mnist)      | [\u2714\ufe0fC++\/CX](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/MNIST\/UWP)<br\/>[\u2714\ufe0fC#](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/MNIST\/Tutorial\/cs)<br\/>    |                                                 |\n| [MobileNetV2](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/mobilenet)                              | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [RCNN](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/rcnn_ilsvrc13)                        | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [ResNet50](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/resnet)                          | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [ShuffleNetV1](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/shufflenet)                              | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [ShuffleNetV2](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/shufflenet)                          | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [SqueezeNet](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/squeezenet) | [\u2714\ufe0fC#](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/SqueezeNetObjectDetection\/UWP\/cs)<br\/>[\u2714\ufe0fJavaScript](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/SqueezeNetObjectDetection\/UWP\/cs)<br\/>        |                     |[\u2714\ufe0fC++\/WinRT](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/SqueezeNetObjectDetection\/Desktop\/cpp)<br\/> [\u2714\ufe0fC# .NET5](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/SqueezeNetObjectDetection\/NET5)<br\/>[\u2714\ufe0fC# .NET Core 2](https:\/\/github.com\/microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/SqueezeNetObjectDetection\/NETCore\/cs)<br\/>|[\u2714\ufe0fC++\/WinRT](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/SqueezeNetObjectDetection\/Desktop\/cpp)<br\/>[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>[\u2714\ufe0fRust](https:\/\/github.com\/microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/RustSqueezenet)<br\/>|\n| [VGG19](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/vgg)                                          | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [VGG19bn](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/vgg)                                       | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n| [ZFNet512](https:\/\/github.com\/onnx\/models\/tree\/master\/vision\/classification\/zfnet-512)                                 | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n\n**Style Transfer**\n\nA computer vision technique that allows us to recompose the content of an image in the style of another.\n\n| Windows App Type <br\/>Distribution | UWP<br\/>In-Box |  UWP<br\/>NuGet | Desktop<br\/>In-Box | Desktop<br\/>NuGet |\n|------------|------------------------------------|--------------------------------------|------------------------------------|--------------------------------------|\n| FNSCandy   | [\u2714\ufe0fC# - FNS Style Transfer](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/FNSCandyStyleTransfer)<br\/>[\u2714\ufe0fC# - Real-Time Style Transfer](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/StyleTransfer)<br\/>           |                                                 |\n\n<!--\n**Object Detection**\n\n\n|            | Store App<br\/>Inbox API |  Store App<br\/>NuGet API | Desktop App<br\/>Inbox API |  Desktop App<br\/>NuGet API |\n|------------|------------------------------------|--------------------------------------|------------------------------------|--------------------------------------|\n| [YoloV4](https:\/\/github.com\/onnx\/models\/raw\/master\/vision\/object_detection_segmentation\/yolov4\/model\/yolov4.onnx)                         | | ||[\u2714\ufe0fC# .NET5 - Samples Gallery](Samples\/WinMLSamplesGallery)<br\/>|\n\n-->\n\n## Advanced Scenario Samples\n\nThese advanced samples show how to use various binding and evaluation features in Windows ML:\n\n- **[Custom Tensorization](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/CustomTensorization)**: A Windows Console Application (C++\/WinRT) that shows how to do custom tensorization.\n\n- **[Custom Operator (CPU)](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/CustomOperator)**: A desktop app that defines multiple custom cpu operators. One of these is a debug operator which we invite you to integrate into your own workflow.\n\n- **[Adapter Selection](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Samples\/AdapterSelection)**: A desktop app that demonstrates how to choose a specific device adapter for running your model.\n\n- **[Plane Identifier](https:\/\/github.com\/Microsoft\/Windows-AppConsult-Samples-UWP\/tree\/master\/PlaneIdentifier)**: a UWP app and a WPF app packaged with the Desktop Bridge, sharing the same model trained using [Azure Custom Vision service](https:\/\/customvision.ai\/). For step-by-step instructions for this sample, please see the blog post [Upgrade your WinML application to the latest bits](https:\/\/blogs.msdn.microsoft.com\/appconsult\/2018\/11\/06\/upgrade-your-winml-application-to-the-latest-bits\/).\n\n- **[Custom Vision and Windows ML](https:\/\/github.com\/Microsoft\/Windows-AppConsult-Samples-UWP\/tree\/master\/PlaneIdentifier)**: The tutorial shows how to train a neural network model to classify images of food using Azure Custom Vision service, export the model to ONNX format, and deploy the model in a Windows Machine Learning application running locally on Windows device. \n\n- **[ML.NET and Windows ML](https:\/\/github.com\/Microsoft\/Windows-AppConsult-Samples-UWP\/tree\/master\/PlaneIdentifier)**: This tutorial shows you how to train a neural network model to classify images of food using ML.NET Model Builder, export the model to ONNX format, and deploy the model in a Windows Machine Learning application running locally on a Windows device. \n\n- **[PyTorch Data Analysis](https:\/\/github.com\/Microsoft\/Windows-AppConsult-Samples-UWP\/tree\/master\/PlaneIdentifier)**: The tutorial shows how to solve a classification task with a neural network using the PyTorch library, export the model to ONNX format and deploy the model with the Windows Machine Learning application that can run on any Windows device.\n\n- **[PyTorch Image Classification](https:\/\/github.com\/Microsoft\/Windows-AppConsult-Samples-UWP\/tree\/master\/PlaneIdentifier)**: The tutorial shows how to train an image classification neural network model using PyTorch, export the model to the ONNX format, and deploy it in a Windows Machine Learning application running locally on your Windows device.\n\n- **[YoloV4 Object Detection](https:\/\/github.com\/Microsoft\/Windows-AppConsult-Samples-UWP\/tree\/master\/PlaneIdentifier)**: This tutorial shows how to build a UWP C# app that uses the YOLOv4 model to detect objects in video streams.\n\n## Developer Tools\n\n- **Model Conversion**\n\n  Windows ML provides inferencing capabilities powered by the ONNX Runtime engine. As such, all models run in Windows ML must be converted to the [ONNX Model format](https:\/\/github.com\/onnx\/onnx). Models built and trained in source frameworks like TensorFlow or PyTorch must be converted to ONNX. Check out the documentation for how to convert to an ONNX model:\n    - https:\/\/onnxruntime.ai\/docs\/tutorials\/mobile\/model-conversion.html\n    - https:\/\/docs.microsoft.com\/en-us\/windows\/ai\/windows-ml\/tutorials\/pytorch-convert-model\n    - [WinMLTools](https:\/\/pypi.org\/project\/winmltools\/): a Python tool for converting models from different machine learning toolkits into ONNX for use with Windows ML.\n\n- **Model Optimization**\n\n  Models may need further optimizations applied post conversion to support advanced features like batching and quantization. Check out the following tools for optimizing your model:\n\n  - **[WinML Dashboard (Preview)](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Tools\/WinMLDashboard)**: a GUI-based tool for viewing, editing, converting, and validating machine learning models for Windows ML inference engine. This tool can be used to enable free dimensions on models that were built with fixed dimensions. [Download Preview Version](https:\/\/github.com\/microsoft\/Windows-Machine-Learning\/releases)\n\n\n  - **[Graph Optimizations](https:\/\/onnxruntime.ai\/docs\/performance\/graph-optimizations.html#:~:text=ONNX%20Runtime%20provides%20various%20graph%20optimizations%20to%20improve,to%20more%20complex%20node%20fusions%20and%20layout%20optimizations):** Graph optimizations are essentially graph-level transformations, ranging from small graph simplifications and node eliminations to more complex node fusions and layout optimizations.\n\n  - **[Graph Quantization](https:\/\/onnxruntime.ai\/docs\/performance\/quantization.html#:~:text=Quantization%20in%20ONNX%20Runtime%20refers%20to%208%20bit,the%20floating%20point%20numbers%20to%20a%20quantization%20space.)**: Quantization in ONNX Runtime refers to 8 bit linear quantization of an ONNX model.\n\n- **Model Validation**\n  - **[WinMLRunner](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/tree\/master\/Tools\/WinMLRunner)**: a command-line tool that can run .onnx or .pb models where the input and output variables are tensors or images. It is a very handy tool to quickly validate an ONNX model. It will attempt to load, bind, and evaluate a model and print out helpful messages. It also captures performance measurements.\n\n    [Download x64 Exe](https:\/\/github.com\/microsoft\/Windows-Machine-Learning\/releases)\n\n- **Model Integration**\n  - **[WinML Code Generator (mlgen)](https:\/\/marketplace.visualstudio.com\/items?itemName=WinML.mlgen)**: a Visual Studio extension to help you get started using WinML APIs on UWP apps by generating a template code when you add a trained ONNX file into the UWP project. From the template code you can load a model, create a session, bind inputs, and evaluate with wrapper codes. See [docs](https:\/\/docs.microsoft.com\/en-us\/windows\/ai\/mlgen) for more info.\n\n    Download for [VS 2017](https:\/\/marketplace.visualstudio.com\/items?itemName=WinML.mlgen), [VS 2019](https:\/\/marketplace.visualstudio.com\/items?itemName=WinML.MLGenV2)\n\n  - **[WinML Samples Gallery](Samples\/WinMLSamplesGallery):** explore a variety of ML integration scenarios and models.\n\n  - Check out the [Model Samples](#model-samples) and [Advanced Scenario Samples](#advanced-scenarios) to learn how to use Windows ML in your application.\n  \n\n\n\n## Feedback\n- For issues, file a bug on [GitHub Issues](https:\/\/github.com\/Microsoft\/Windows-Machine-Learning\/issues).\n- Ask questions on [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/windows-machine-learning).\n- Vote for popular feature requests on [Windows Developer Feedback](https:\/\/wpdev.uservoice.com\/forums\/110705-universal-windows-platform?category_id=341035) or include your own request.\n\n## External Links\n - [ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator](https:\/\/github.com\/microsoft\/onnxruntime\/).\n - [ONNX: Open Neural Network Exchange Project](https:\/\/onnx.ai\/).\n\n## Contributing\n\nWe're always looking for your help to fix bugs and improve the samples. Create a pull request, and we'll be happy to take a look.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https:\/\/opensource.microsoft.com\/codeofconduct\/).\nFor more information see the [Code of Conduct FAQ](https:\/\/opensource.microsoft.com\/codeofconduct\/faq\/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n","78":"Distributed Machine Learning Common Codebase\n============================================\n\n[![Build Status](https:\/\/github.com\/dmlc\/dmlc-core\/workflows\/continuous%20build\/badge.svg)](https:\/\/github.com\/dmlc\/dmlc-core\/actions)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/dmlc-core\/badge\/?version=latest)](http:\/\/dmlc-core.readthedocs.org\/en\/latest\/)\n[![GitHub license](http:\/\/dmlc.github.io\/img\/apache2.svg)](.\/LICENSE)\n\n\nDMLC-Core is the backbone library to support all DMLC projects, offers the bricks to build efficient and scalable distributed machine learning libraries.\n\nDeveloper Channel [![Join the chat at https:\/\/gitter.im\/dmlc\/dmlc-core](https:\/\/badges.gitter.im\/Join%20Chat.svg)](https:\/\/gitter.im\/dmlc\/dmlc-core?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n\nWhat's New\n----------\n* [Note on Parameter Module for Machine Learning](http:\/\/dmlc-core.readthedocs.org\/en\/latest\/parameter.html)\n\n\nContents\n--------\n* [Documentation and Tutorials](http:\/\/dmlc-core.readthedocs.org\/en\/latest\/)\n* [Contributing](#contributing)\n\nKnown Issues\n------------\n* RecordIO format is not portable across different processor endians. So it is not possible to save RecordIO file on a x86 machine and then load it on a SPARC machine, because x86 is little endian while SPARC is big endian.\n\n\nContributing\n------------\n\nContributing to dmlc-core is welcomed! dmlc-core follows google's C style guide. If you are interested in contributing, take a look at [feature wishlist](https:\/\/github.com\/dmlc\/dmlc-core\/labels\/feature%20wishlist) and open a new issue if you like to add something.\n\n* DMLC-Core uses C++11 standard. Ensure that your C++ compiler supports C++11.\n* Try to introduce minimum dependency when possible\n\n### CheckList before submit code\n* Type ```make lint``` and fix all the style problems.\n* Type ```make doc``` and fix all the warnings.\n\nNOTE\n----\ndeps:\n\nlibcurl4-openssl-dev\n","79":"Multiverso\n==========\n[![Build Status](https:\/\/travis-ci.org\/Microsoft\/Multiverso.svg?branch=master)](https:\/\/travis-ci.org\/Microsoft\/Multiverso)\n\nMultiverso is a parameter server based framework for training machine learning models on big data with numbers of machines. It is currently a standard C++ library and provides a series of friendly programming interfaces, and it is extended to support calling from python and Lua programs. With such easy-to-use APIs, machine learning researchers and practitioners do not need to worry about the system routine issues such as distributed model storage and operation, inter-process and inter-thread communication, multi-threading management, and so on.\nInstead, they are able to focus on the core machine learning logics: data, model, and training.\n\nFor more details, please view our website [http:\/\/www.dmtk.io](http:\/\/www.dmtk.io).\n\nBuild\n----------\n\n**Linux** (Tested on Ubuntu 14.04)\n\n```\nsudo apt-get install libopenmpi-dev openmpi-bin build-essential cmake git\ngit clone https:\/\/github.com\/Microsoft\/multiverso.git --recursive && cd multiverso\nmkdir build && cd build\ncmake .. && make && sudo make install\n```\n\n**Windows**\n\nOpen the `Multiverso.sln` with [Visual Studio 2013]() and build.\n\nRelated Projects\n----------\n\nCurrent distributed systems based on multiverso:\n\n* [lightLDA](https:\/\/github.com\/Microsoft\/lightlda): Scalable, fast, lightweight system for large scale topic modeling\n* [distributed_word_embedding](https:\/\/github.com\/Microsoft\/multiverso\/tree\/master\/Applications\/WordEmbedding) Distributed system for word embedding\n* [distributed_word_embedding(deprecated)](https:\/\/github.com\/Microsoft\/distributed_word_embedding) Distributed system for word embedding\n* [distributed_skipgram_mixture(deprecated)](https:\/\/github.com\/Microsoft\/distributed_skipgram_mixture) Distributed skipgram mixture for multi-sense word embedding\n\nMicrosoft Open Source Code of Conduct\n------------\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https:\/\/opensource.microsoft.com\/codeofconduct\/). For more information see the [Code of Conduct FAQ](https:\/\/opensource.microsoft.com\/codeofconduct\/faq\/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n","80":"Jubatus\n=======\n\n.. image:: https:\/\/api.travis-ci.org\/jubatus\/jubatus.svg?branch=master\n    :target: https:\/\/api.travis-ci.org\/jubatus\/jubatus\n\nThe Jubatus library is an online machine learning framework which runs in distributed environment.\n\nSee http:\/\/jubat.us\/ for details.\n\nQuick Start\n-----------\n\nWe officially support Red Hat Enterprise Linux (RHEL) 6.2 or later (64-bit) and Ubuntu Server 14.04 LTS \/ 16.04 LTS \/ 18.04 LTS (64-bit).\nOn supported systems, you can install all components of Jubatus using binary packages.\n\nSee `QuickStart <http:\/\/jubat.us\/en\/quickstart.html>`_ for detailed description.\n\nRed Hat Enterprise Linux 6.2 or later (64-bit)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRun the following command to register Jubatus Yum repository to the system.\n\n::\n\n  \/\/ For RHEL 6\n  $ sudo rpm -Uvh http:\/\/download.jubat.us\/yum\/rhel\/6\/stable\/x86_64\/jubatus-release-6-2.el6.x86_64.rpm\n\n  \/\/ For RHEL 7\n  $ sudo rpm -Uvh http:\/\/download.jubat.us\/yum\/rhel\/7\/stable\/x86_64\/jubatus-release-7-2.el7.x86_64.rpm\n\nThen install ``jubatus`` and ``jubatus-client`` package.\n\n::\n\n  $ sudo yum install jubatus jubatus-client\n\nNow Jubatus is installed in ``\/usr\/bin\/juba*``.\n\n::\n\n  $ jubaclassifier -f \/usr\/share\/jubatus\/example\/config\/classifier\/pa.json\n\nUbuntu Server (64-bit)\n~~~~~~~~~~~~~~~~~~~~~~\n\nWrite the following line to ``\/etc\/apt\/sources.list.d\/jubatus.list`` to register Jubatus Apt repository to the system.\n\n::\n\n  \/\/ For Ubuntu 12.04 (Precise) - Deprecated (unsupported)\n  deb http:\/\/download.jubat.us\/apt\/ubuntu\/precise binary\/\n\n  \/\/ For Ubuntu 14.04 (Trusty)\n  deb http:\/\/download.jubat.us\/apt\/ubuntu\/trusty binary\/\n\n  \/\/ For Ubuntu 16.04 (Xenial)\n  deb http:\/\/download.jubat.us\/apt\/ubuntu\/xenial binary\/\n\n  \/\/ For Ubuntu 18.04 (Bionic)\n  deb [trusted=yes] http:\/\/download.jubat.us\/apt\/ubuntu\/bionic\/binary \/\n\nNow install ``jubatus`` package.\n\n::\n\n  $ sudo apt-get update\n  $ sudo apt-get install jubatus\n\nNow Jubatus is installed in ``\/opt\/jubatus\/bin\/juba*``.\n\n::\n\n  $ source \/opt\/jubatus\/profile\n  $ jubaclassifier -f \/opt\/jubatus\/share\/jubatus\/example\/config\/classifier\/pa.json\n\nOther Platforms\n~~~~~~~~~~~~~~~\n\nFor other platforms, refer to the `documentation <http:\/\/jubat.us\/en\/developers\/build.html>`_.\n\nLicense\n-------\n\nLGPL 2.1\n\nThird-party libraries included in Jubatus\n-----------------------------------------\n\nJubatus source tree includes following third-party library.\n\n- cmdline_ (under BSD 3-Clause License)\n\n.. _cmdline: https:\/\/github.com\/tanakh\/cmdline\n\nJubatus requires `jubatus_core <https:\/\/github.com\/jubatus\/jubatus_core\/>`_ library. jubatus_core contains Eigen and fork of pficommon. Eigen is licensed under MPL2 (partially in LGPL 2.1 or 2.1+). The fork of pficommon is licensed under New BSD License.\n\nUpdate history\n--------------\n\nUpdate history can be found from `ChangeLog <https:\/\/github.com\/jubatus\/jubatus\/blob\/master\/ChangeLog.rst>`_ or `WikiPage <https:\/\/github.com\/jubatus\/jubatus\/wiki\/ChangeLog>`_.\n\nContributors\n------------\n\nPatches contributed by `those people <https:\/\/github.com\/jubatus\/jubatus\/contributors>`_.\n","81":"# captcha-break\ncaptcha break based on opencv2, tesseract-ocr and some machine learning algorithm.\n\n## Types\n### Basic[[cpp](.\/basic\/cpp)][[python](.\/basic\/python)]\n![](.\/basic\/basic.jpg)  \nThe simplest captcha breaking.\n\n### CSDN[[cpp](.\/csdn\/cpp)][[python](.\/csdn\/python)]\n![](.\/csdn\/csdn.png)  \nCAPTCHA from http:\/\/download.csdn.net\/\n\n### SubMail[[cpp](.\/submail\/cpp)]\n![](.\/submail\/submail.png)   \nCAPTCHA from http:\/\/submail.cn\/sms\n\n### Weibo.cn[[cpp](.\/weibo.cn\/cpp)][[python](.\/weibo.cn\/python)]\n![](.\/weibo.cn\/weibo.cn.png)  \nCAPTCHA from http:\/\/login.weibo.cn\/login\/.  \n(Note: This website has changed now, and the captcha is not available!)\n\n### JiKeXueYuan[[python](.\/jikexueyuan\/python)]\n![](.\/jikexueyuan\/jikexueyuan.png)   \nCAPTCHA of http:\/\/passport.jikexueyuan.com\/sso\/verify\n\n### Weibo.com[[python3](.\/weibo.com)]\n![](.\/weibo.com\/weibo.com.png)  \nCAPTCHA of [http:\/\/login.sina.com.cn\/cgi\/pin.php?r=8787878&s=0](http:\/\/login.sina.com.cn\/cgi\/pin.php?r=8787878&s=0)\n\n\n## License\nMIT\n","82":"# Matterport3D\n\n![Matterport3d](img\/teaser.jpg)\n\nThe Matterport3D V1.0 dataset contains data captured throughout 90 properties with a Matterport Pro Camera.\n\nThis repository includes the raw data for the dataset plus derived data, annotated  data, and scripts\/models for several scene understanding tasks.\n\nVisit the main [website](https:\/\/niessner.github.io\/Matterport) for updates and to browse the data.\n\n## Paper\n\n[**Matterport3D: Learning from RGB-D Data in Indoor Environments**](https:\/\/arxiv.org\/abs\/1709.06158)\n\nIf you use the Matterport3D data or code please cite:\n\n```\n@article{Matterport3D,\n  title={{Matterport3D}: Learning from {RGB-D} Data in Indoor Environments},\n  author={Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},\n  journal={International Conference on 3D Vision (3DV)},\n  year={2017}\n}\n```\n\n## Data\n\nThe dataset consists of several types of annotations: color and depth images, camera poses, textured 3D meshes, building floor plans and region annotations, object instance semantic annotations.  For details see the [data organization](data_organization.md) document.\n\nTo download the dataset, you must indicate that you agree to the terms of use by signing the [Terms of Use](http:\/\/kaldir.vc.in.tum.de\/matterport\/MP_TOS.pdf) agreement form and sending it to: [matterport3d@googlegroups.com](mailto:matterport3d@googlegroups.com).  We will then provide download access to the dataset.\n\n\n## Benchmark tasks\n\nUsing the Matterport3D data, we present several benchmark tasks: image keypoint matching, view overlap prediction, surface normal estimation, region type classification, and semantic voxel labeling.  See the [tasks](tasks) directory for details.\n\n\n## Tools\n\nWe provide code for loading and viewing the data.  See the [code](code) directory for details.\n\n## License\n\nThe data is released under the [Matterport3D Terms of Use](http:\/\/kaldir.vc.in.tum.de\/matterport\/MP_TOS.pdf), and the code is released under the MIT license.\n\n","83":"![NeoML](NeoML\/docs\/images\/NeoML_logo.png)\r\n\r\n![Desktop Build Status](https:\/\/img.shields.io\/azure-devops\/build\/abbyyopensource\/401f7fe0-92d9-411d-9839-60d3455fa1c0\/2\/master?label=Desktop%20Build)\r\n![Python Build](https:\/\/img.shields.io\/azure-devops\/build\/abbyyopensource\/401f7fe0-92d9-411d-9839-60d3455fa1c0\/14\/master?label=Python%20Build)\r\n![iOS Build](https:\/\/img.shields.io\/azure-devops\/build\/abbyyopensource\/401f7fe0-92d9-411d-9839-60d3455fa1c0\/15\/master?label=iOS%20build)\r\n![Android Build](https:\/\/img.shields.io\/azure-devops\/build\/abbyyopensource\/401f7fe0-92d9-411d-9839-60d3455fa1c0\/15\/master?label=Android%20build)\r\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/neoml\/badge\/?version=latest)](https:\/\/neoml.readthedocs.io\/en\/latest\/?badge=latest)\r\n\r\n**NeoML** is an end-to-end machine learning framework that allows you to build, train, and deploy ML models. This framework is used by ABBYY engineers for computer vision and natural language processing tasks, including image preprocessing, classification, document layout analysis, OCR, and data extraction from structured and unstructured documents.\r\n\r\nKey features:\r\n\r\n- Neural networks with support for over 100 layer types\r\n- Traditional machine learning: 20+ algorithms (classification, regression, clustering, etc.)\r\n- CPU and GPU support, fast inference\r\n- ONNX support\r\n- Languages: Python, C++, Java, Objective-C\r\n- Cross-platform: the same code can be run on Windows, Linux, macOS, iOS, and Android\r\n\r\n## Contents\r\n\r\n<!-- TOC -->\r\n\r\n- [Build and install](#build-and-install)\r\n\t- [Supported platforms](#supported-platforms)\r\n\t- [Third party](#third-party)\r\n\t- [Build fully functional C++ version](#build-fully-functional-c++-version)\r\n\t- [Build inference versions for Java and Objective-C](#build-inference-versions-for-java-and-objective-c)\r\n- [Getting started](#getting-started)\r\n- [API description](#api-description)\r\n\t- [Basic principles](#basic-principles)\r\n\t\t- [Platform independence](#platform-independence)\r\n\t\t- [Math engines independence](#math-engines-independence)\r\n\t\t- [Multi-threading support](#multi-threading-support)\r\n\t\t- [ONNX support](#onnx-support)\r\n\t\t- [Serialization format](#serialization-format)\r\n\t\t- [GPU support](#gpu-support)\r\n\t\t- [FineObj](#fineobj)\r\n\t- [C++ interface](#c++-interface)\r\n\t\t- [Algorithms library NeoML](#algorithms-library-neoml)\r\n\t\t- [NeoMathEngine](#neomathengine)\t\r\n\t- [Python module](#python-module)\r\n\t- [Java interface](#java-interface)\r\n\t- [Objective-C interface](#objective-c-interface)\r\n- [License](#license)\r\n\r\n<!-- \/TOC -->\r\n\r\n## Build and install\r\n\r\n### Supported platforms\r\n\r\nThe full \u0421++ library version has been tested on the platforms:\r\n\r\n|Target OS|Compiler|Architecture|\r\n|----------|----------|------------------------------|\r\n|Windows 7+ (CPU and GPU)|MSVC 2019+|x86, x86_64|\r\n|Ubuntu 14+ (CPU)|gcc 5.4+|x86_64|\r\n|MacOS 10.11+ (CPU)|Apple clang 12+|arm64, x86_64|\r\n|iOS 11+ (CPU, GPU)|Apple clang 12+|arm64-v8a, x86_64|\r\n|Android 5.0+ (CPU), Android 7.0+ (GPU)|clang 7+|armeabi-v7a, arm64-v8a, x86, x86_64|\r\n\r\nThe inference Java and Objective-C library versions have been tested on the platforms:\r\n\r\n|Target OS|Compiler|Architecture|\r\n|----------|----------|------------------------------|\r\n|iOS 11+ (CPU, GPU)|Apple clang 12+|arm64-v8a, x86_64|\r\n|Android 5.0+ (CPU), Android 7.0+ (GPU)|clang 7+|armeabi-v7a, arm64-v8a, x86, x86_64|\r\n\r\n### Third party\r\n\r\nThe library is built with [CMake](https:\/\/cmake.org\/download\/) (recommended versions 3.18 and later).\r\n\r\nFor best CPU performance on Windows, Linux and macOS we use [Intel MKL](https:\/\/software.intel.com\/en-us\/mkl).\r\n\r\nWhen processing on a GPU, you can optionally use [CUDA](https:\/\/developer.nvidia.com\/cuda-downloads) (version 11.2 upd.1) on Windows or Linux and [Vulkan](https:\/\/vulkan.lunarg.com\/sdk\/home) (version 1.1.130 and later) on Windows, Linux or Android.\r\n\r\nWe also use Google [Test](https:\/\/github.com\/google\/googletest) for testing and Google [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) for working with ONNX model format.\r\n\r\nWe use very convinous generator of JIT code [xbyak](https:\/\/github.com\/herumi\/xbyak) for speeding up some convolutions on x86_64 processors.\r\n\r\n### Build fully functional C++ version\r\n\r\nSee [here](NeoML\/docs\/en\/Installation\/cpp.md) for instructions on building the C++ library version for different platforms.\r\n\r\n### Build inference versions for Java and Objective-C\r\n\r\nSee [here](NeoML\/docs\/en\/Installation\/inference.md) for instructions on building the Java and Objective-C versions that would only run the trained neural networks.\r\n\r\n## Getting started\r\n\r\nSeveral tutorials with sample code will help you start working with the library:\r\n\r\n- [Train and use a simple network](NeoML\/docs\/en\/Tutorial\/SimpleNet.md)\r\n- [Classification with gradient boosting](NeoML\/docs\/en\/Tutorial\/News20Classification.md)\r\n- [Data clustering with k-means algorithm](NeoML\/docs\/en\/Tutorial\/IrisClustering.md)\r\n\r\n## API description\r\n\r\n### Basic principles\r\n\r\nThe library was developed with these principles in mind:\r\n\r\n#### Platform independence\r\n\r\nThe user interface is completely separated from the low-level calculations implemented by a math engine. \r\n\r\nThe only thing you have to do is to specify at the start the type of the math engine that will be used for calculations. You can also choose to select the math engine automatically, based on the device configuration detected.\r\n\r\nThe rest of your machine learning code will be the same regardless of the math engine you choose.\r\n\r\n#### Math engines independence\r\n\r\nEach network works with one math engine instance, and all its layers should have been created with the same math engine. If you have chosen a GPU math engine, it will perform all calculations. This means you may not choose to use a CPU for \"light\" calculations like adding vectors and a GPU for \"heavy\" calculations like multiplying matrices. We have introduced this restriction to avoid unnecessary synchronizations and data exchange between devices.\r\n\r\n#### Multi-threading support\r\n\r\nThe [math engine interface](NeoML\/docs\/en\/API\/NN\/MathEngine.md) is thread-safe; the same instance may be used in different networks and different threads.\r\n\r\nNote that this may entail some synchronization overhead.\r\n\r\nHowever, the [neural network implementation](NeoML\/docs\/en\/API\/NN\/Dnn.md) is not thread-safe; the network may run only in one thread.\r\n\r\n#### ONNX support\r\n\r\n**NeoML** library also works with the models created by other frameworks, as long as they support the [ONNX](https:\/\/onnx.ai\/) format. See [the description of import API](NeoML\/docs\/en\/Onnx.md). However, you cannot export a NeoML-trained model into ONNX format.\r\n\r\n#### Serialization format\r\n\r\nThe library uses its own binary format (implemented by `CArchive`, `CArchiveFile`) to save and load the trained models. \r\n\r\n#### GPU support\r\n\r\nProcessing on GPU often helps significantly improve performance of mathematical operations. The **NeoML** library uses GPU both for training and running the models. This is an optional setting and depends on the hardware and software capabilities of your system. \r\n\r\nTo work on GPU, the library requires:\r\n\r\n- Windows: NVIDIA\u00ae GPU card with CUDA\u00ae 11.2 upd. 1 support.\r\n- iOS: Apple GPU A7+.\r\n- Android: devices with Vulkan 1.0 support.\r\n- Linux\/macOS: no support for GPU processing as yet. \r\n\r\n#### FineObj\r\n\r\nThe **NeoML** library originates in ABBYY internal infrastructure. For various reasons ABBYY uses a cross-platform framework called FineObj. Because of this, the open library version uses some of this framework primitives. See the [common classes description](NeoML\/docs\/en\/API\/Common\/README.md).\r\n\r\n### C++ interface \r\n\r\n**NeoML** contains two C++ libraries:\r\n\r\n#### Algorithms library NeoML\r\n\r\nThe library provides C++ objects that implement various high-level algorithms. It consists of several parts:\r\n\r\n- [Neural networks](NeoML\/docs\/en\/API\/NN\/README.md)\r\n- [Classification and regression algorithms](NeoML\/docs\/en\/API\/ClassificationAndRegression\/README.md)\r\n- [Clustering algorithms](NeoML\/docs\/en\/API\/Clustering\/README.md)\r\n- [Auxiliary algorithms](NeoML\/docs\/en\/API\/Algorithms\/README.md)\r\n\r\n#### NeoMathEngine\r\n\r\nThe math engine used for calculations is a separate module that implements the low-level mathematical functions used in the algorithms library. The user can also call these functions but usually never needs to.\r\n\r\nThis module has different implementations for different platforms. In particular, there is an implementation that uses a GPU for calculations.\r\n\r\nThe math engine is also a set of C++ interfaces described [here](NeoML\/docs\/en\/API\/NN\/MathEngine.md).\r\n\r\n### Python module\r\n\r\nSee the extensive documentation of the Python module on [readthedocs.io](https:\/\/neoml.readthedocs.io\/en\/latest\/index.html).\r\n\r\n### Java interface\r\n\r\nTo work with the inference version of the library in Java and Kotlin we provide a [Java interface](NeoML\/docs\/en\/Wrappers\/Java.md).\r\n\r\n### Objective-C interface\r\n\r\nTo work with the inference version of the library in Swift and Objective-C we provide an [Objective-C interface](NeoML\/docs\/en\/Wrappers\/ObjectiveC.md).\r\n\r\n## License\r\nCopyright \u00a9 2016-2020 ABBYY Production LLC. Licensed under the Apache License, Version 2.0. See [the license file](LICENSE).\r\n","84":"# NVVL is part of DALI!\n[DALI (Nvidia Data Loading Library)](https:\/\/developer.nvidia.com\/dali) incorporates NVVL functionality and offers much more than that, so it is recommended to switch to it.\nDALI source code is also open source and available on the [GitHub](https:\/\/github.com\/NVIDIA\/DALI).\nUp to date documentation can be found [here](https:\/\/docs.nvidia.com\/deeplearning\/sdk\/dali-developer-guide\/docs\/index.html).\nNVVL project will still be available on the GitHub but it won't be maintained. All issues and request for the future please [submit in the DALI repository](https:\/\/github.com\/NVIDIA\/DALI\/issues).\n\n# NVVL\nNVVL (**NV**IDIA **V**ideo **L**oader) is a library to load random\nsequences of video frames from compressed video files to facilitate\nmachine learning training. It uses FFmpeg's libraries to parse and\nread the compressed packets from video files and the video decoding\nhardware available on NVIDIA GPUs to off-load and accelerate the\ndecoding of those packets, providing a ready-for-training tensor in\nGPU device memory. NVVL can additionally perform data augmentation\nwhile loading the frames. Frames can be scaled, cropped, and flipped\nhorizontally using the GPUs dedicated texture mapping units. Output\ncan be in RGB or YCbCr color space, normalized to [0, 1] or [0, 255],\nand in `float`, `half`, or `uint8` tensors.\n\n**Note that, while we hope you find NVVL useful, it is example code\nfrom a research project performed by a small group of NVIDIA researchers.\nWe will do our best to answer questions and fix small bugs as they come\nup, but it is not a supported NVIDIA product and is for the most part\nprovided as-is.**\n\nUsing compressed video files instead of individual frame image files\nsignificantly reduces the demands on the storage and I\/O systems\nduring training. Storing video datasets as video files consumes an\norder of magnitude less disk space, allowing for larger datasets to\nboth fit in system RAM as well as local SSDs for fast access. During\nloading fewer bytes must be read from disk. Fitting on smaller, faster\nstorage and reading fewer bytes at load time allievates the bottleneck\nof retrieving data from disks, which will only get worse as GPUs get\nfaster. For the dataset used in our example project, H.264 compressed\n`.mp4` files were nearly 40x smaller than storing frames as `.png`\nfiles.\n\nUsing the hardware decoder on NVIDIA GPUs to decode images\nsignificantly reduces the demands on the host CPU. This means fewer\nCPU cores need to be dedicated to data loading during training. This\nis especially important in servers with a large number of GPUs per\nCPU, such as the in the NVIDIA DGX-2 server, but also provides\nbenefits for other platforms. When training our example project on a\nNVIDIA DGX-1, the CPU load when using NVVL was 50-60% of the load seen\nwhen using a normal dataloader for `.png` files.\n\nMeasurements that quantify the performance advantages of using NVVL\nare detailed in our [super resolution example\nproject](\/examples\/pytorch_superres).\n\nMost users will want to use the deep learning framework wrappers\nprovided rather than using the library directly. Currently a wrapper\nfor PyTorch is provided (PR's for other frameworks are welcome). See\nthe [PyTorch wrapper README](\/pytorch\/README.md) for documentation on\nusing the PyTorch wrapper. Note that it is not required to build or\ninstall the C++ library before building the PyTorch wrapper (its\nsetup scripts will do so for you).\n\n# Building and Installing\n\nNVVL depends on the following:\n- CUDA Toolkit. We have tested versions 8.0 and later but earlier\n  versions may work. NVVL will perform better with CUDA 9.0 or\n  later<sup id=\"a1\">[1](#f1)<\/sup>.\n- FFmpeg's libavformat, libavcodec, libavfilter, and libavutil. These\n  can be installed from source as in the [example\n  Dockerfiles](\/docker) or from the Ubuntu 16.04 packages\n  `libavcodec-dev libavfilter-dev libavformat-dev\n  libavutil-dev`. Other distributions should have similar packages.\n\nAdditionally, building from source requires CMake version 3.8 or above\nand some examples optionally make use of some libraries from OpenCV if\nthey are installed.\n\nThe [docker](docker) directory contains Dockerfiles that can be used\nas a starting point for creating an image to build or use the NVVL\nlibrary. The [example's docker directory](examples\/pytorch\/docker) has\nan example Dockerfile that actually builds and installs the NVVL\nlibrary.\n\nCMake 3.8 and above provides builtin CUDA language support that NVVL's\nbuild system uses. Since CMake 3.8 is relatively new and not yet in\nwidely used Linux distribution, it may be required to install a new\nversion of CMake.  The easiest way to do so is to make use of their\npackage on PyPI:\n\n```\npip install cmake\n```\n\nAlternatively, or if `pip` isn't available, you can install to\n`\/usr\/local` from a binary distribution:\n\n```sh\nwget https:\/\/cmake.org\/files\/v3.10\/cmake-3.10.2-Linux-x86_64.sh\n\/bin\/sh cmake-3.10.2-Linux-x86_64.sh --prefix=\/usr\/local\n```\n\nSee https:\/\/cmake.org\/download\/ for more options.\n\nBuilding and installing NVVL follows the typical CMake pattern:\n\n```sh\nmkdir build && cd build\ncmake ..\nmake -j\nsudo make install\n```\n\nThis will install `libnvvl.so` and development headers into\nappropriate subdirectores under `\/usr\/local`. CMake can be passed the\nfollowing options using `cmake .. -DOPTION=Value`:\n\n- `CUDA_ARCH` - Name of a CUDA architecture to generate device code\n  for, seperated via a semicolon. Valid options are `Kepler`,\n  `Maxwell`, `Pascal`, and `Volta`. You can also use specific\n  architecture names such as `sm_61`. Default is\n  `Maxwell;Pascal;Volta`.\n\n- `CMAKE_CUDA_FLAGS` - A string of arguments to pass to `nvcc`. In\n  particular, you can decide to link against the static or shared\n  runtime library using `-cudart shared` or `-cudart static`. You can\n  also use this for finer control of code generation than `CUDA_ARCH`,\n  see the `nvcc` documentation. Default is `-cudart shared`.\n\n- `WITH_OPENCV` - Set this to 1 to build the examples with the\n  optional OpenCV functionality.\n\n- `CMAKE_INSTALL_PREFIX` - Install directory. Default is\n  `\/usr\/local`.\n\n- `CMAKE_BUILD_TYPE` - `Debug` or `Release` build.\n\nSee the [CMake documentation](https:\/\/cmake.org\/cmake\/help\/v3.8\/) for\nmore options.\n\nThe examples in `doc\/examples` can be built using the `examples` target:\n```\nmake examples\n```\n\nFinally, if Doxygen is installed, API documentation can be built using\nthe `doc` target:\n```\nmake doc\n```\nThis will build html files in `doc\/html`.\n\n# Preparing Data\n\nNVVL supports the H.264 and HEVC (H.265) video codecs in any container\nformat that FFmpeg is able to parse.  Video codecs only store certain\nframes, called keyframes or intra-frames, as a complete image in the\ndata stream. All other frames require data from other frames, either\nbefore or after it in time, to be decoded. In order to decode a\nsequence of frames, it is necessary to start decoding at the keyframe\nbefore the sequence, and continue past the sequence to the next\nkeyframe after it. This isn't a problem when streaming sequentially\nthrough a video; however, when decoding small sequences of frames\nrandomly throughout the video, a large gap between keyframes results in\nreading and decoding a large amount of frames that are never used.\n\nThus, to get good performance when randomly reading short sequences\nfrom a video file, it is necessary to encode the file with frequent\nkey frames. We've found setting the keyframe interval to the length of\nthe sequences you will be reading provides a good compromise between\nfilesize and loading performance. Also, NVVL's seeking logic doesn't\nsupport open GOPs in HEVC streams. To set the keyframe interval to `X`\nwhen using `ffmpeg`:\n\n- For `libx264` use `-g X`\n- For `libx265` use `-x265-params \"keyint=X:no-open-gop=1\"`\n\nThe pixel format of the video must also be yuv420p to be supported by\nthe hardware decoder. This is done by passing `-pix_fmt yuv420p` to\n`ffmpeg`. You should also remove any extra audio or video streams from\nthe video file by passing `-map v:0` to ffmpeg after the input but\nbefore the output.\n\nFor example to transcode to H.264:\n```\nffmpeg -i original.mp4 -map v:0 -c:v libx264 -crf 18 -pix_fmt yuv420p -g 5 -profile:v high prepared.mp4\n```\n\n# Basic Usage\nThis section describes the usage of the base C\/C++ library, for usage\nof the PyTorch wrapper, see the [README](\/pytorch\/README.md) in the\npytorch directory.\n\nThe library provides both a C++ and C interface. See the examples in\n[doc\/examples](doc\/examples) for brief example code on how to use the\nlibrary. [extract_frames.cpp](doc\/examples\/extract_frames.cpp)\ndemonstrates the C++ interface and\n[extract_frames_c.c](doc\/examples\/extract_frames_c.c) the C\ninterface. The API documentation built with `make doc` is the\ncanonical reference for the API.\n\nThe basic flow is to create a `VideoLoader` object, tell it which\nframe sequences to read, and then give it buffers in device memory to\nput the decoded sequences into. In C++, creating a video loader is\nstraight forward:\n\n```C++\nauto loader = NVVL::VideoLoader{device_id};\n```\n\nYou can then tell it which sequences to read via `read_sequence`:\n\n```C++\nloader.read_sequence(filename, frame_num, sequence_length);\n\n```\n\nTo receive the frames from the decoder, it is necessary to create a\n`PictureSequence` to tell it how and where you want the decoded frames\nprovided. First, create a `PictureSequence`, providing a count of the\nnumber of frames to receive from the decoder. Note that the count here\ndoes not need to be the same as the sequence_length provided to\n`read_sequence`; you can read a large sequence of frames and receive\nthem as multiple tensors, or read multiple smaller sequences and\nreceive them concatenated as a single tensor.\n\n```C++\nauto seq = PictureSequence{sequence_count};\n```\n\nYou now create \"Layers\" in the sequence to provide the destination for\nthe frames. Each layer can be a different type, have different\nprocessing, and contain different frames from the received\nsequence. First, create a `PictureSequence::Layer` of the desired\ntype:\n\n```C++\nauto pixels = PictureSequence::Layer<float>{};\n```\n\nNext, fill in the pointer to the data and other details. See the\ndocumentation in [PictureSequence.h](include\/PictureSequence.h) for a\ndescription of all the available options.\n\n```C++\nfloat* data = nullptr;\nsize_t pitch = 0;\ncudaMallocPitch(&data, &pitch,\n                crop_width * sizeof(float),\n                crop_height * sequence_count * 3);\npixels.data = data;\npixels.desc.count = sequence_count;\npixels.desc.channels = 3;\npixels.desc.width = crop_width;\npixels.desc.height = crop_height;\npixels.desc.scale_width = scale_width;\npixels.desc.scale_height = scale_height;\npixels.desc.horiz_flip = false;\npixels.desc.normalized = true;\npixels.desc.color_space = ColorSpace_RGB;\npixels.desc.stride.x = 1;\npixels.desc.stride.y = pitch \/ sizeof(float);\npixels.desc.stride.c = pixels.desc.stride.y * crop_height;\npixels.desc.stride.n = pixels.desc.stride.c * 3;\n```\n\nNote that here we have set the strides such that the dimensions are\n\"nchw\", we could have done \"nhwc\" or any other dimension order by\nsetting the strides appropriately. Also note that the strides in the\nlayer description are number of elements, not number of bytes.\n\nWe now add this layer to our `PictureSequence`, and send it to the loader:\n\n```C++\nseq.set_layer(\"pixels\", pixels);\nloader.receive_frames(seq);\n```\n\nThis call to `receive_frames` will be\nasynchronous. `receive_frames_sync` can be used if synchronous reading\nis desired. When we are ready to use the frames we can insert a wait\nevent into the CUDA stream we are using for our computation:\n\n```C++\nseq.wait(stream);\n```\n\nThis will insert a wait event into the stream `stream`, causing any\nfurther kernels launched on `stream` to wait until the data is\nready.\n\nThe C interface follows a very similar pattern, see\n[doc\/examples\/extract_frames_c.c](doc\/examples\/extract_frames_c.c)\nfor an example.\n\n# Reference\nIf you find this library useful in your work, please cite it in your\npublications using the following BibTeX entry:\n\n```\n@misc{nvvl,\n  author = {Jared Casper and Jon Barker and Bryan Catanzaro},\n  title = {NVVL: NVIDIA Video Loader},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https:\/\/github.com\/NVIDIA\/nvvl}}\n}\n```\n\n# Footnotes\n\n<b id=\"f1\">[1]<\/b> Specifically, with nvidia kernel modules version\n384 and later, which come with CUDA 9.0+, CUDA kernels launched by\nNVVL will run asynchronously on a separate stream. With earlier kernel\nmodules, all CUDA kernels are launched on the default stream. [\u21a9](#a1)\n","85":"<p float=\"left\">\n   <img src=\"https:\/\/fastmachinelearning.github.io\/hls4ml\/img\/logo.jpg\" alt=\"hls4ml\" width=\"400\"\/>\n<\/p>\n\n[![DOI](https:\/\/zenodo.org\/badge\/108329371.svg)](https:\/\/zenodo.org\/badge\/latestdoi\/108329371)\n[![PyPI version](https:\/\/badge.fury.io\/py\/hls4ml.svg)](https:\/\/badge.fury.io\/py\/hls4ml)\n[![Supported Python versions](https:\/\/img.shields.io\/pypi\/pyversions\/hls4ml.svg)](https:\/\/pypi.org\/project\/hls4ml\/)\n\nA package for machine learning inference in FPGAs. We create firmware implementations of machine learning algorithms using high level synthesis language (HLS). We translate traditional open-source machine learning package models into HLS that can be configured for your use-case!\n\n**Contact:** hls4ml.help@gmail.com\n\n# Documentation & Tutorial\n\nFor more information visit the webpage: [https:\/\/fastmachinelearning.org\/hls4ml\/](https:\/\/fastmachinelearning.org\/hls4ml\/)\n\nDetailed tutorials on how to use `hls4ml`'s various functionalities can be found [here](https:\/\/github.com\/hls-fpga-machine-learning\/hls4ml-tutorial).\n\n# Installation\n```\npip install hls4ml\n```\n\nTo install the extra dependencies for profiling: \n\n```\npip install hls4ml[profiling]\n```\n\n# Getting Started\n### Creating an HLS project\n```Python\nimport hls4ml\n\n#Fetch a keras model from our example repository\n#This will download our example model to your working directory and return an example configuration file\nconfig = hls4ml.utils.fetch_example_model('KERAS_3layer.json')\n\nprint(config) #You can print the configuration to see some default parameters\n\n#Convert it to a hls project\nhls_model = hls4ml.converters.keras_to_hls(config)\n\n# Print full list of example models if you want to explore more\nhls4ml.utils.fetch_example_list()\n```\n\n### Building a project with Xilinx Vivado HLS (after downloading and installing from [here](https:\/\/www.xilinx.com\/products\/design-tools\/vivado\/integration\/esl-design.html))\nNote: Vitis HLS is not yet supported. Vivado HLS versions between 2018.2 and 2020.1 are recommended.\n\n```Python\n#Use Vivado HLS to synthesize the model\n#This might take several minutes\nhls_model.build()\n\n#Print out the report if you want\nhls4ml.report.read_vivado_report('my-hls-test')\n```\n","86":"# Puffer\n\nPuffer ([puffer.stanford.edu](https:\/\/puffer.stanford.edu)) is a free and open-source live TV streaming website,\nand also a research study at Stanford University using machine learning to\nimprove video streaming.\n\nMore details can be found\non the [website](https:\/\/puffer.stanford.edu\/faq\/),\nin our [research paper](https:\/\/www.usenix.org\/conference\/nsdi20\/presentation\/yan)\n(*Community Award* winner at NSDI 2020),\nand in the [documentation](https:\/\/github.com\/StanfordSNR\/puffer\/wiki\/Documentation).\n","87":"# bytefish\/opencv #\n\nThis repository contains OpenCV code and documents.\n\nMore (maybe) here: [https:\/\/www.bytefish.de](https:\/\/www.bytefish.de).\n\n## colormaps ##\n\nAn implementation of various colormaps for OpenCV2 C++ in order to enhance visualizations. Feel free to fork and add your own colormaps.\n\n### Related posts ###\n\n* https:\/\/bytefish.de\/blog\/colormaps_in_opencv\n  \n## misc ##\n\nSample code that doesn't belong to a specific project. \n\n* Skin Color detection\n* PCA\n* TanTriggs Preprocessing\n\n## machinelearning ##\n\nDocument and sourcecode about OpenCV C++ machine learning API including:\n\n* Support Vector Machines\n* Multi Layer Perceptron\n* Normal Bayes\n* k-Nearest-Neighbor\n* Decision Tree\n\n### Related posts ###\n  \n* https:\/\/www.bytefish.de\/blog\/machine_learning_opencv\n\n## eigenfaces ##\n\nEigenfaces implementation using the OpenCV2 C++ API. There's a very basic function for loading the dataset, you probably want to make this a bit more sophisticated. The dataset is available at [http:\/\/www.cl.cam.ac.uk\/research\/dtg\/attarchive\/facedatabase.html](http:\/\/www.cl.cam.ac.uk\/research\/dtg\/attarchive\/facedatabase.html).\n\n### Related posts ###\n\n* https:\/\/www.bytefish.de\/blog\/pca_in_opencv\n* https:\/\/www.bytefish.de\/blog\/eigenfaces\n* https:\/\/www.bytefish.de\/blog\/fisherfaces\n  \n## lbp ##\n\nImplements various Local Binary Patterns with the OpenCV2 C++ API:\n  \n* Original LBP\n* Circular LBP (also known as Extended LBP)\n* Variance-based LBP\n\nBasic code for spatial histograms and histogram matching with a chi-square distance is included, but it's not finished right now. There's a tiny demo application you can experiment with.\n\n### Related posts ###\n\n* https:\/\/www.bytefish.de\/blog\/local_binary_patterns\n* https:\/\/www.bytefish.de\/blog\/numpy_performance\/\n  \n## lda ##\n\nFisherfaces implementation with the OpenCV2 C++ API. \n\n### Related posts ###\n\n* https:\/\/www.bytefish.de\/blog\/fisherfaces\n* https:\/\/www.bytefish.de\/blog\/lda_in_opencv\n* https:\/\/www.bytefish.de\/blog\/fisherfaces_in_opencv\n","88":"<img src=\"\/docs\/source\/_static\/img\/logo.png\" width=\"300\">\n\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/euclidesdb\/badge\/?version=latest)](http:\/\/euclidesdb.readthedocs.io\/en\/latest\/?badge=latest) \n[![License](https:\/\/img.shields.io\/badge\/License-Apache%202.0-blue.svg)](https:\/\/opensource.org\/licenses\/Apache-2.0)\n![GitHub release](https:\/\/img.shields.io\/github\/release\/perone\/euclidesdb.svg)\n\n![cpp](https:\/\/forthebadge.com\/images\/badges\/made-with-c-plus-plus.svg)\n\n# Welcome to EuclidesDB\nEuclidesDB is a multi-model machine learning feature database that is tight coupled with PyTorch and provides a backend for including and querying data on the model feature space.\n\n## Getting Started\n- [Official Documentation](http:\/\/euclidesdb.readthedocs.io)\n","89":"# MLDB is the Machine Learning Database\n\nMLDB is an open source SQL database designed for machine learning that was developed by [MLDB.ai](http:\/\/mldb.ai\/).\nSince the sale of MLDB.ai to [Element AI](http:\/\/elementai.com) in 2017, it's no longer\na commercially supported product, instead it's being developed by a very small number\nof people in their spare time as an open source research project.\n\n*The former MLDB Enterprise Edition, the MLDB Docker Containers, and the MLDB Hub\nare no longer being maintained.  Please don't use them.*\n\n[![Join the chat at https:\/\/gitter.im\/mldbai\/mldb](https:\/\/badges.gitter.im\/mldbai\/mldb.svg)](https:\/\/gitter.im\/mldbai\/mldb?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nMLDB is an open-source database designed for machine learning. \nYou can install it as a command-line tool wherever you want and either run as a script or send it commands over a RESTful API to \nstore data, explore it using SQL, then train machine learning models and expose them as APIs. More information is available at http:\/\/mldb.ai\n\nThis repository contains the source code for MLDB, which can be used to [build MLDB](Building.md).  Building MLDB is the\n_only_ way to get an up to date version.  It can be built and run on Linux or OSX, on Intel, ARM or Apple processors.  The\nCI\/CD pipeline is hosted on [GitLab](https:\/\/gitlab.com\/mldbai\/mldb\/).\n\nPlease [create a Github Issue](https:\/\/github.com\/mldbai\/mldb\/issues\/new) or [chat with us on Gitter](https:\/\/gitter.im\/mldbai\/mldb) if you have any questions or encounter problems while building or running MLDB.  Be mindful that it's open source and that everyone working on it has\na day job.\n\n## What's interesting about MLDB?\n\nMLDB contains some interesting concepts:\n* A dialect of SQL that is useful for machine learning\n* High efficiency implementations of data loading, training of classical ML algorithms and predition endpoints\n* Dataset abstractions that can effectivly model many kinds of real-world data (tabular, sparse, behavioral, logfiles, ...)\n* A data model and type system designed for ML, including nested structures, embeddings and tensors as data types\n* Everything-is-a-table, allowing manipulation and introspection of ML models\n* Lock-free and high performance REST endpoints\n* Extensibility via plugins, in C++, Python and Javascript\n\nIt is used to explore the following research topics:\n* High memory efficient data storage\n* High speed training of ML algorithms\n* Memory mappable data structures\n* Abstractions for compute-independent processing\n\nCurrently, MLDB is being rearchitected as a much smaller core with all of the other functionality implemented as plugins,\nand designed to run on a broader set of deployment platforms.\n\nThe ultimate vision for MLDB is as a machine-learning \"anti-plaform\": MLDB will make it easy to create\nand deploy machine learning solutions by allowing them to be manipulated and transformed outside of the\nplatforms on which they are created and specialized to their runtime environment.\n\n## Documentation\n\nRaw Markdown documentation files are located under `container_files\/public_html\/doc` and you can browse them on Github or you can browse the full-rendered version at https:\/\/docs.mldb.ai.  This documentation is for the last commercial release, and so is out of date, but is still generally helpful.\n\n## Copyright & License (Apache License v2.0)\n\nMLDB is \u00a9 2016 mldb.ai Inc (and its successors) and the Contributors, and is distributed under the [Apache License, version 2.0](LICENSE), except for the contents of the `ext` directory, which contains (possibly) modified versions of other open-source software components, each of which is distributed under its own, Apache-compatible license and lists its own copyright information.  Source code of each component is available via its Git submodule, and any changes to those components in the `mldbai` GitHub organization are implicitly available under the same license as the modified work.\n","90":"([\u7b80\u4f53\u4e2d\u6587](.\/README_CN.md)|English)\n\n<p align=\"center\">\n    <br>\n<img src='doc\/images\/serving_logo.png' width = \"600\" height = \"130\">\n    <br>\n<p>\n\n<p align=\"center\">\n    <br>\n    <a href=\"https:\/\/travis-ci.com\/PaddlePaddle\/Serving\">\n        <img alt=\"Build Status\" src=\"https:\/\/img.shields.io\/travis\/com\/PaddlePaddle\/Serving\/develop?style=flat-square\">\n        <img alt=\"Docs\" src=\"https:\/\/img.shields.io\/badge\/docs-\u4e2d\u6587\u6587\u6863-brightgreen?style=flat-square\">\n        <img alt=\"Release\" src=\"https:\/\/img.shields.io\/badge\/release-0.8.0-blue?style=flat-square\">\n        <img alt=\"Python\" src=\"https:\/\/img.shields.io\/badge\/python-3.6\/3.7\/3.8\/3.9-blue?style=flat-square\">\n        <img alt=\"License\" src=\"https:\/\/img.shields.io\/github\/license\/PaddlePaddle\/Serving?color=blue&style=flat-square\">\n        <img alt=\"Forks\" src=\"https:\/\/img.shields.io\/github\/forks\/PaddlePaddle\/Serving?color=yellow&style=flat-square\">\n        <img alt=\"Issues\" src=\"https:\/\/img.shields.io\/github\/issues\/PaddlePaddle\/Serving?color=yellow&style=flat-square\">\n        <img alt=\"Contributors\" src=\"https:\/\/img.shields.io\/github\/contributors\/PaddlePaddle\/Serving?color=orange&style=flat-square\">\n        <img alt=\"Community\" src=\"https:\/\/img.shields.io\/badge\/join-Wechat,QQ-orange?style=flat-square\">\n    <\/a>\n    <br>\n<p>\n\n***\n\nThe goal of Paddle Serving is to provide high-performance, flexible and easy-to-use industrial-grade online inference services for machine learning developers and enterprises.Paddle Serving supports multiple protocols such as RESTful, gRPC, bRPC, and provides inference solutions under a variety of hardware and multiple operating system environments, and many famous pre-trained model examples. The core features are as follows:\n\n\n- Integrate high-performance server-side inference engine paddle Inference and mobile-side engine paddle Lite. Models of other machine learning platforms (Caffe\/TensorFlow\/ONNX\/PyTorch) can be migrated to paddle through [x2paddle](https:\/\/github.com\/PaddlePaddle\/X2Paddle).\n- There are two frameworks, namely high-performance C++ Serving and high-easy-to-use Python pipeline. The C++ Serving is based on the bRPC network framework to create a high-throughput, low-latency inference service, and its performance indicators are ahead of competing products. The Python pipeline is based on the gRPC\/gRPC-Gateway network framework and the Python language to build a highly easy-to-use and high-throughput inference service. How to choose which one please see [Techinical Selection](doc\/Serving_Design_EN.md#21-design-selection).\n- Support multiple [protocols](doc\/C++_Serving\/Inference_Protocols_CN.md) such as HTTP, gRPC, bRPC, and provide C++, Python, Java language SDK.\n- Design and implement a high-performance inference service framework for asynchronous pipelines based on directed acyclic graph (DAG), with features such as multi-model combination, asynchronous scheduling, concurrent inference, dynamic batch, multi-card multi-stream inference, request cache, etc. \n- Adapt to a variety of commonly used computing hardwares, such as x86 (Intel) CPU, ARM CPU, Nvidia GPU, Kunlun XPU, HUAWEI Ascend 310\/910, HYGON DCU\u3001Nvidia Jetson etc. \n- Integrate acceleration libraries of Intel MKLDNN and  Nvidia TensorRT, and low-precision and quantitative inference.\n- Provide a model security deployment solution, including encryption model deployment, and authentication mechanism, HTTPs security gateway, which is used in practice.\n- Support cloud deployment, provide a deployment case of Baidu Cloud Intelligent Cloud kubernetes cluster.\n- Provide more than 40 classic pre-model deployment examples, such as PaddleOCR, PaddleClas, PaddleDetection, PaddleSeg, PaddleNLP, PaddleRec and other suites, and more models continue to expand.\n- Supports distributed deployment of large-scale sparse parameter index models, with features such as multiple tables, multiple shards, multiple copies, local high-frequency cache, etc., and can be deployed on a single machine or clouds.\n- Support service monitoring, provide prometheus-based performance statistics and port access\n\n\n<h2 align=\"center\">Tutorial<\/h2>\n\n\n- AIStudio tutorial(Chinese) : [Paddle Serving\u670d\u52a1\u5316\u90e8\u7f72\u6846\u67b6](https:\/\/www.paddlepaddle.org.cn\/tutorials\/projectdetail\/2538249)\n- Video tutorial(Chinese) : [\u6df1\u5ea6\u5b66\u4e60\u670d\u52a1\u5316\u90e8\u7f72-\u4ee5\u4e92\u8054\u7f51\u5e94\u7528\u4e3a\u4f8b](https:\/\/aistudio.baidu.com\/aistudio\/course\/introduce\/19084)\n- Edge AI solution(Chinese) : [\u57fa\u4e8ePaddle Serving&\u767e\u5ea6\u667a\u80fd\u8fb9\u7f18BIE\u7684\u8fb9\u7f18AI\u89e3\u51b3\u65b9\u6848](https:\/\/mp.weixin.qq.com\/s\/j0EVlQXaZ7qmoz9Fv96Yrw)\n\n<p align=\"center\">\n    <img src=\"doc\/images\/demo.gif\" width=\"700\">\n<\/p>\n\n<h2 align=\"center\">Documentation<\/h2>\n\n\n> Set up\n\nThis chapter guides you through the installation and deployment steps. It is strongly recommended to use Docker to deploy Paddle Serving. If you do not use docker, ignore the docker-related steps. Paddle Serving can be deployed on cloud servers using Kubernetes, running on many commonly hardwares such as ARM CPU, Intel CPU, Nvidia GPU, Kunlun XPU. The latest development kit of the develop branch is compiled and generated every day for developers to use.\n\n- [Install Paddle Serving using docker](doc\/Install_EN.md)\n- [Build Paddle Serving from Source with Docker](doc\/Compile_EN.md)\n- [Deploy Paddle Serving on Kubernetes(Chinese)](doc\/Run_On_Kubernetes_CN.md)\n- [Deploy Paddle Serving with Security gateway(Chinese)](doc\/Serving_Auth_Docker_CN.md)\n- Deploy on more hardwares[[ARM CPU\u3001\u767e\u5ea6\u6606\u4ed1](doc\/Run_On_XPU_EN.md)\u3001[\u534e\u4e3a\u6607\u817e](doc\/Run_On_NPU_CN.md)\u3001[\u6d77\u5149DCU](doc\/Run_On_DCU_CN.md)\u3001[Jetson](doc\/Run_On_JETSON_CN.md)]\n- [Docker Images](doc\/Docker_Images_EN.md)\n- [Latest Wheel packages](doc\/Latest_Packages_CN.md)\n\n> Use\n\nThe first step is to call the model save interface to generate a model parameter configuration file (.prototxt), which will be used on the client and server. The second step, read the configuration and startup parameters and start the service. According to API documents and your case, the third step is to write client requests based on the SDK, and test the inference service.\n\n- [Quick Start](doc\/Quick_Start_EN.md)\n- [Save a servable model](doc\/Save_EN.md)\n- [Description of configuration and startup parameters](doc\/Serving_Configure_EN.md)\n- [Guide for RESTful\/gRPC\/bRPC APIs(Chinese)](doc\/C++_Serving\/Introduction_CN.md#42-\u591a\u8bed\u8a00\u591a\u534f\u8baeClient)\n- [Infer on quantizative models](doc\/Low_Precision_EN.md)\n- [Data format of classic models(Chinese)](doc\/Process_data_CN.md)\n- [Prometheus(Chinese)](doc\/Prometheus_CN.md)\n- [C++ Serving(Chinese)](doc\/C++_Serving\/Introduction_CN.md) \n  - [Protocols(Chinese)](doc\/C++_Serving\/Inference_Protocols_CN.md)\n  - [Hot loading models](doc\/C++_Serving\/Hot_Loading_EN.md)\n  - [A\/B Test](doc\/C++_Serving\/ABTest_EN.md)\n  - [Encryption](doc\/C++_Serving\/Encryption_EN.md)\n  - [Analyze and optimize performance(Chinese)](doc\/C++_Serving\/Performance_Tuning_CN.md)\n  - [Benchmark(Chinese)](doc\/C++_Serving\/Benchmark_CN.md)\n  - [Multiple models in series(Chinese)](doc\/C++_Serving\/2+_model.md)\n  - [Request Cache(Chinese)](doc\/C++_Serving\/Request_Cache_CN.md)\n- [Python Pipeline](doc\/Python_Pipeline\/Pipeline_Design_EN.md)\n  - [Analyze and optimize performance](doc\/Python_Pipeline\/Performance_Tuning_EN.md)\n  - [TensorRT dynamic Shape](doc\/TensorRT_Dynamic_Shape_EN.md)\n  - [Benchmark(Chinese)](doc\/Python_Pipeline\/Benchmark_CN.md)\n  - Our Paper: [JiZhi: A Fast and Cost-Effective Model-As-A-Service System for\nWeb-Scale Online Inference at Baidu](https:\/\/arxiv.org\/pdf\/2106.01674.pdf)\n- Client SDK\n  - [Python SDK(Chinese)](doc\/C++_Serving\/Introduction_CN.md#42-\u591a\u8bed\u8a00\u591a\u534f\u8baeClient)\n  - [JAVA SDK](doc\/Java_SDK_EN.md)\n  - [C++ SDK(Chinese)](doc\/C++_Serving\/Introduction_CN.md#42-\u591a\u8bed\u8a00\u591a\u534f\u8baeClient)\n- [Large-scale sparse parameter server](doc\/Cube_Local_EN.md)\n\n<br>\n\n> Developers\n\nFor Paddle Serving developers, we provide extended documents such as custom OP, level of detail(LOD) processing.\n- [Custom Operators](doc\/C++_Serving\/OP_EN.md)\n- [Processing LoD Data](doc\/LOD_EN.md)\n- [FAQ(Chinese)](doc\/FAQ_CN.md)\n\n<h2 align=\"center\">Model Zoo<\/h2>\n\n\nPaddle Serving works closely with the Paddle model suite, and implements a large number of service deployment examples, including image classification, object detection, language and text recognition, Chinese part of speech, sentiment analysis, content recommendation and other types of examples,  for a total of 45 models.\n\n<p align=\"center\">\n\n| PaddleOCR | PaddleDetection | PaddleClas | PaddleSeg | PaddleRec | Paddle NLP | \n| :----:  | :----: | :----: | :----: | :----: | :----: | \n| 8 | 12 | 14 | 2 | 3 | 6 | \n\n<\/p>\n\nFor more model examples, read [Model zoo](doc\/Model_Zoo_EN.md)\n\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/PaddlePaddle\/PaddleOCR\/blob\/release\/2.3\/doc\/imgs_results\/PP-OCRv2\/PP-OCRv2-pic003.jpg?raw=true\" width=\"345\"\/> \n  <img src=\"doc\/images\/detection.png\" width=\"350\">\n<\/p>\n\n\n<h2 align=\"center\">Community<\/h2>\n\nIf you want to communicate with developers and other users? Welcome to join us, join the community through the following methods below.\n\n### Wechat\n- WeChat scavenging\n\n<p align=\"center\">\n  <img src=\"doc\/images\/wechat_group_1.jpeg\" width=\"250\">\n<\/p>\n\n### QQ\n- QQ Group(Group No.\uff1a697765514)\n\n<p align=\"center\">\n  <img src=\"doc\/images\/qq_group_1.png\" width=\"200\">\n<\/p>\n\n\n> Contribution\n\nIf you want to contribute code to Paddle Serving, please reference [Contribution Guidelines](doc\/Contribute_EN.md)\n- Thanks to [@loveululu](https:\/\/github.com\/loveululu) for providing python API of Cube.\n- Thanks to [@EtachGu](https:\/\/github.com\/EtachGu) in updating run docker codes.\n- Thanks to [@BeyondYourself](https:\/\/github.com\/BeyondYourself) in complementing the gRPC tutorial, updating the FAQ doc and modifying the mdkir command\n- Thanks to [@mcl-stone](https:\/\/github.com\/mcl-stone) in updating faster_rcnn benchmark\n- Thanks to [@cg82616424](https:\/\/github.com\/cg82616424) in updating the unet benchmark  modifying resize comment error\n- Thanks to [@cuicheng01](https:\/\/github.com\/cuicheng01) for providing 11 PaddleClas models\n- Thanks to [@Jiaqi Liu](https:\/\/github.com\/LiuChiachi) for supporting prediction for string list input\n- Thanks to [@Bin Lu](https:\/\/github.com\/Intsigstephon) for adding pp-shitu example\n\n> Feedback\n\nFor any feedback or to report a bug, please propose a [GitHub Issue](https:\/\/github.com\/PaddlePaddle\/Serving\/issues).\n\n> License\n\n[Apache 2.0 License](https:\/\/github.com\/PaddlePaddle\/Serving\/blob\/develop\/LICENSE)\n","91":"# Reverb\n![PyPI - Python Version](https:\/\/img.shields.io\/pypi\/pyversions\/dm-reverb)\n[![PyPI version](https:\/\/badge.fury.io\/py\/dm-reverb.svg)](https:\/\/badge.fury.io\/py\/dm-reverb)\n\nReverb is an efficient and easy-to-use data storage and transport system\ndesigned for machine learning research. Reverb is primarily used as an\nexperience replay system for distributed reinforcement learning algorithms but\nthe system also supports multiple data structure representations such as FIFO,\nLIFO, and priority queues.\n\n## Table of Contents\n\n-   [Installation](#installation)\n-   [Quick Start](#quick-start)\n-   [Detailed Overview](#detailed-overview)\n    -   [Tables](#tables)\n    -   [Item Selection Strategies](#item-selection-strategies)\n    -   [Rate Limiting](#rate-limiting)\n    -   [Sharding](#sharding)\n    -   [Checkpointing](#checkpointing)\n-   [Citation](#citation)\n\n## Installation\n\nPlease keep in mind that Reverb is not hardened for production use, and while we\ndo our best to keep things in working order, things may break or segfault.\n\n> :warning: Reverb currently only supports Linux based OSes.\n\nThe recommended way to install Reverb is with `pip`. We also provide instructions\nto build from source using the same docker images we use for releases.\n\nTensorFlow can be installed separately or as part of the `pip` install.\nInstalling TensorFlow as part of the install ensures compatibility.\n\n```shell\n$ pip install dm-reverb[tensorflow]\n\n# Without Tensorflow install and version dependency check.\n$ pip install dm-reverb\n```\n\n### Nightly builds\n\n[![PyPI version](https:\/\/badge.fury.io\/py\/dm-reverb-nightly.svg)](https:\/\/badge.fury.io\/py\/dm-reverb-nightly)\n\n```shell\n$ pip install dm-reverb-nightly[tensorflow]\n\n# Without Tensorflow install and version dependency check.\n$ pip install dm-reverb-nightly\n\n```\n\n### Debug builds\n\nStarting with version 0.6.0, debug builds of Reverb are uploaded to Google Cloud\nStorage. The builds can be downloaded or installed directly via `pip` following\nthe patterns below. `gsutils` can be used to navigate the directory structure\nto ensure the files are there, e.g.\n`gsutil ls gs:\/\/rl-infra-builds\/dm_reverb\/builds\/dbg`. To build your own debug\nbinary, see the\n[build instructions](https:\/\/github.com\/deepmind\/reverb\/tree\/master\/reverb\/pip_package#create-a-stable-reverb-release).\n\nFor Python 3.7:\n\n```shell\n$ export reverb_version=0.6.0\n# Python 3.7\n$ export python_version=36\n$ pip install https:\/\/storage.googleapis.com\/rl-infra-builds\/dm_reverb\/builds\/dbg\/$reverb_version\/dm_reverb-$reverb_version-cp$python_version-cp${python_version}m-manylinux2010_x86_64.whl\n```\n\nFor python 3.8 and 3.9 follow this pattern:\n\n```shell\n$ export reverb_version=0.6.0\n# Python 3.9\n$ export python_version=39\n$ pip install https:\/\/storage.googleapis.com\/rl-infra-builds\/dm_reverb\/builds\/dbg\/$reverb_version\/dm_reverb-$reverb_version-cp$python_version-cp$python_version-manylinux2010_x86_64.whl\n```\n\n### Build from source\n\n[This guide](reverb\/pip_package\/README.md#how-to-develop-and-build-reverb-with-the-docker-containers)\ndetails how to build Reverb from source.\n\n## Quick Start\n\nStarting a Reverb server is as simple as:\n\n```python\nimport reverb\n\nserver = reverb.Server(tables=[\n    reverb.Table(\n        name='my_table',\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=100,\n        rate_limiter=reverb.rate_limiters.MinSize(1)),\n    ],\n)\n```\n\nCreate a client to communicate with the server:\n\n```python\nclient = reverb.Client(f'localhost:{server.port}')\nprint(client.server_info())\n```\n\nWrite some data to the table:\n\n```python\n# Creates a single item and data element [0, 1].\nclient.insert([0, 1], priorities={'my_table': 1.0})\n```\n\nAn item can also reference multiple data elements:\n\n```python\n# Appends three data elements and inserts a single item which references all\n# of them as {'a': [2, 3, 4], 'b': [12, 13, 14]}.\nwith client.trajectory_writer(num_keep_alive_refs=3) as writer:\n  writer.append({'a': 2, 'b': 12})\n  writer.append({'a': 3, 'b': 13})\n  writer.append({'a': 4, 'b': 14})\n\n  # Create an item referencing all the data.\n  writer.create_item(\n      table='my_table',\n      priority=1.0,\n      trajectory={\n          'a': writer.history['a'][:],\n          'b': writer.history['b'][:],\n      })\n\n  # Block until the item has been inserted and confirmed by the server.\n  writer.flush()\n```\n\nThe items we have added to Reverb can be read by sampling them:\n\n```python\n# client.sample() returns a generator.\nprint(list(client.sample('my_table', num_samples=2)))\n```\n\nContinue with the\n[Reverb Tutorial](https:\/\/github.com\/deepmind\/reverb\/tree\/master\/examples\/demo.ipynb)\nfor an interactive tutorial.\n\n## Detailed overview\n\nExperience replay has become an important tool for training off-policy\nreinforcement learning policies. It is used by algorithms such as\n[Deep Q-Networks (DQN)][DQN], [Soft Actor-Critic (SAC)][SAC],\n[Deep Deterministic Policy Gradients (DDPG)][DDPG], and\n[Hindsight Experience Replay][HER], ... However building an efficient, easy to\nuse, and scalable replay system can be challenging. For good performance Reverb\nis implemented in C++ and to enable distributed usage it provides a gRPC service\nfor adding, sampling, and updating the contents of the tables. Python clients\nexpose the full functionality of the service in an easy to use fashion.\nFurthermore native TensorFlow ops are available for performant integration with\nTensorFlow and `tf.data`.\n\nAlthough originally designed for off-policy reinforcement learning, Reverb's\nflexibility makes it just as useful for on-policy reinforcement -- or even\n(un)supervised learning. Creative users have even used Reverb to store and\ndistribute frequently updated data (such as model weights), acting as an\nin-memory light-weight alternative to a distributed file system where each table\nrepresents a file.\n\n### Tables\n\nA Reverb `Server` consists of one or more tables. A table holds items, and each\nitem references one or more data elements. Tables also define sample and\nremoval [selection strategies](#item-selection-strategies), a maximum item\ncapacity, and a [rate limiter](#rate-limiting).\n\nMultiple items can reference the same data element, even if these items exist in\ndifferent tables. This is because items only contain references to data elements\n(as opposed to a copy of the data itself). This also means that a data element\nis only removed when there exists no item that contains a reference to it.\n\nFor example, it is possible to set up one Table as a Prioritized Experience\nReplay (PER) for transitions (sequences of length 2), and another Table as a\n(FIFO) queue of sequences of length 3. In this case the PER data could be used\nto train DQN, and the FIFO data to train a transition model for the environment.\n\n![Using multiple tables](docs\/images\/multiple_tables_example.png)\n\nItems are automatically removed from the Table when one of two conditions are\nmet:\n\n1.  Inserting a new item would cause the number of items in the Table to exceed\n    its maximum capacity. Table's removal strategy is used to determine which\n    item to remove.\n\n1.  An item has been sampled more than the maximum number of times permitted by\n    the Table's rate limiter. Such item is deleted.\n\nData elements not referenced anymore by any item are also deleted.\n\nUsers have full control over how data is sampled and removed from Reverb\ntables. The behavior is primarily controlled by the\n[item selection strategies](#item-selection-strategies) provided to the `Table`\nas the `sampler` and `remover`. In combination with the\n[`rate_limiter`](#rate-limiting) and `max_times_sampled`, a wide range of\nbehaviors can be achieved. Some commonly used configurations include:\n\n**Uniform Experience Replay**\n\nA set of `N=1000` most recently inserted items are maintained. By setting\n`sampler=reverb.selectors.Uniform()`, the probability to select an item is the\nsame for all items. Due to `reverb.rate_limiters.MinSize(100)`, sampling\nrequests will block until 100 items have been inserted. By setting\n`remover=reverb.selectors.Fifo()` when an item needs to be removed the oldest\nitem is removed first.\n\n```python\nreverb.Table(\n     name='my_uniform_experience_replay_buffer',\n     sampler=reverb.selectors.Uniform(),\n     remover=reverb.selectors.Fifo(),\n     max_size=1000,\n     rate_limiter=reverb.rate_limiters.MinSize(100),\n)\n```\n\nExamples of algorithms that make use of uniform experience replay include [SAC]\nand [DDPG].\n\n**Prioritized Experience Replay**\n\nA set of `N=1000` most recently inserted items. By setting\n`sampler=reverb.selectors.Prioritized(priority_exponent=0.8)`, the probability\nto select an item is proportional to the item's priority.\n\nNote: See [Schaul, Tom, et al.][PER] for the algorithm used in this\nimplementation of Prioritized Experience Replay.\n\n```python\nreverb.Table(\n     name='my_prioritized_experience_replay_buffer',\n     sampler=reverb.selectors.Prioritized(0.8),\n     remover=reverb.selectors.Fifo(),\n     max_size=1000,\n     rate_limiter=reverb.rate_limiters.MinSize(100),\n)\n```\n\nExamples of algorithms that make use of Prioritized Experience Replay are DQN\n(and its variants), and\n[Distributed Distributional Deterministic Policy Gradients][D4PG].\n\n**Queue**\n\nCollection of up to `N=1000` items where the oldest item is selected and removed\nin the same operation. If the collection contains 1000 items then insert calls\nare blocked until it is no longer full, if the collection is empty then sample\ncalls are blocked until there is at least one item.\n\n```python\nreverb.Table(\n    name='my_queue',\n    sampler=reverb.selectors.Fifo(),\n    remover=reverb.selectors.Fifo(),\n    max_size=1000,\n    max_times_sampled=1,\n    rate_limiter=reverb.rate_limiters.Queue(size=1000),\n)\n\n# Or use the helper classmethod `.queue`.\nreverb.Table.queue(name='my_queue', max_size=1000)\n```\n\nExamples of algorithms that make use of Queues are\n[IMPALA](https:\/\/arxiv.org\/abs\/1802.01561) and asynchronous implementations of\n[Proximal Policy Optimization](https:\/\/arxiv.org\/abs\/1707.06347).\n\n### Item selection strategies\n\nReverb defines several selectors that can be used for item sampling or removal:\n\n-   **Uniform:** Sample uniformly among all items.\n-   **Prioritized:** Samples proportional to stored priorities.\n-   **FIFO:** Selects the oldest data.\n-   **LIFO:** Selects the newest data.\n-   **MinHeap:** Selects data with the lowest priority.\n-   **MaxHeap:** Selects data with the highest priority.\n\nAny of these strategies can be used for sampling or removing items from a\nTable. This gives users the flexibility to create customized Tables that best\nfit their needs.\n\n### Rate Limiting\n\nRate limiters allow users to enforce conditions on when items can be inserted\nand\/or sampled from a Table. Here is a list of the rate limiters that are\ncurrently available in Reverb:\n\n-   **MinSize:** Sets a minimum number of items that must be in the Table before\n    anything can be sampled.\n-   **SampleToInsertRatio:** Sets that the average ratio of inserts to samples\n    by blocking insert and\/or sample requests. This is useful for controlling\n    the number of times each item is sampled before being removed.\n-   **Queue:** Items are sampled exactly once before being removed.\n-   **Stack:** Items are sampled exactly once before being removed.\n\n### Sharding\n\nReverb servers are unaware of each other and when scaling up a system to a multi\nserver setup data is not replicated across more than one node. This makes Reverb\nunsuitable as a traditional database but has the benefit of making it trivial to\nscale up systems where some level of data loss is acceptable.\n\nDistributed systems can be horizontally scaled by simply increasing the number\nof Reverb servers. When used in combination with a gRPC compatible load\nbalancer, the address of the load balanced target can simply be provided to a\nReverb client and operations will automatically be distributed across the\ndifferent nodes. You'll find details about the specific behaviors in the\ndocumentation of the relevant methods and classes.\n\nIf a load balancer is not available in your setup or if more control is required\nthen systems can still be scaled in almost the same way. Simply increase the\nnumber of Reverb servers and create separate clients for each server.\n\n### Checkpointing\n\nReverb supports checkpointing; the state and content of Reverb servers can be\nstored to permanent storage. While pointing, the `Server` serializes all of its\ndata and metadata needed to reconstruct it. During this process the `Server`\nblocks all incoming insert, sample, update, and delete requests.\n\nCheckpointing is done with a call from the Reverb `Client`:\n\n```python\n# client.checkpoint() returns the path the checkpoint was written to.\ncheckpoint_path = client.checkpoint()\n```\n\nTo restore the `reverb.Server` from a checkpoint:\n\n```python\ncheckpointer = reverb.checkpointers.DefaultCheckpointer(path=checkpoint_path)\n# The arguments passed to `tables=` must be the same as those used by the\n# `Server` that wrote the checkpoint.\nserver = reverb.Server(tables=[...], checkpointer=checkpointer)\n```\n\nRefer to\n[tfrecord_checkpointer.h](https:\/\/github.com\/deepmind\/reverb\/tree\/master\/reverb\/cc\/platform\/tfrecord_checkpointer.h)\nfor details on the implementation of checkpointing in Reverb.\n\n## Starting Reverb using `reverb_server` (beta)\n\nInstalling `dm-reverb` using `pip` will install a `reverb_server` script, which\naccepts its config as a textproto. For example:\n\n```bash\n$ reverb_server --config=\"\nport: 8000\ntables: {\n  table_name: \\\"my_table\\\"\n  sampler: {\n    fifo: true\n  }\n  remover: {\n    fifo: true\n  }\n  max_size: 200 max_times_sampled: 5\n  rate_limiter: {\n    min_size_to_sample: 1\n    samples_per_insert: 1\n    min_diff: $(python3 -c \"import sys; print(-sys.float_info.max)\")\n    max_diff: $(python3 -c \"import sys; print(sys.float_info.max)\")\n  }\n}\"\n```\n\nThe `rate_limiter` config is equivalent to the Python expression `MinSize(1)`,\nsee `rate_limiters.py`.\n\n\n## Citation\n\nIf you use this code, please cite the\n[Reverb paper](https:\/\/arxiv.org\/abs\/2102.04736) as\n\n```\n@misc{cassirer2021reverb,\n      title={Reverb: A Framework For Experience Replay},\n      author={Albin Cassirer and Gabriel Barth-Maron and Eugene Brevdo and Sabela Ramos and Toby Boyd and Thibault Sottiaux and Manuel Kroiss},\n      year={2021},\n      eprint={2102.04736},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n<!-- Links to papers go here -->\n\n[D4PG]: https:\/\/arxiv.org\/abs\/1804.08617\n[DDPG]: https:\/\/arxiv.org\/abs\/1509.02971\n[DQN]: https:\/\/www.nature.com\/articles\/nature14236\n[HER]: https:\/\/arxiv.org\/abs\/1707.01495\n[PER]: https:\/\/arxiv.org\/abs\/1511.05952\n[SAC]: https:\/\/arxiv.org\/abs\/1801.01290\n","92":"# Rabit: Reliable Allreduce and Broadcast Interface\n[![Build Status](https:\/\/travis-ci.org\/dmlc\/rabit.svg?branch=master)](https:\/\/travis-ci.org\/dmlc\/rabit)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/rabit\/badge\/?version=latest)](http:\/\/rabit.readthedocs.org\/)\n\n## Recent developments of Rabit have been moved into [dmlc\/xgboost](https:\/\/github.com\/dmlc\/xgboost). See discussion in [dmlc\/xgboost#5995](https:\/\/github.com\/dmlc\/xgboost\/issues\/5995).\n\nrabit is a light weight library that provides a fault tolerant interface of Allreduce and Broadcast. It is designed to support easy implementations of distributed machine learning programs, many of which fall naturally under the Allreduce abstraction. The goal of rabit is to support ***portable*** , ***scalable*** and ***reliable*** distributed machine learning programs.\n\n* [Tutorial](guide)\n* [API Documentation](http:\/\/homes.cs.washington.edu\/~tqchen\/rabit\/doc)\n* You can also directly read the [interface header](include\/rabit.h)\n* [XGBoost](https:\/\/github.com\/dmlc\/xgboost)\n  - Rabit is one of the backbone library to support distributed XGBoost\n\n## Features\nAll these features comes from the facts about small rabbit:)\n* Portable: rabit is light weight and runs everywhere\n  - Rabit is a library instead of a framework, a program only needs to link the library to run\n  - Rabit only replies on a mechanism to start program, which was provided by most framework\n  - You can run rabit programs on many platforms, including Yarn(Hadoop), MPI using the same code\n* Scalable and Flexible: rabit runs fast\n  * Rabit program use Allreduce to communicate, and do not suffer the cost between iterations of MapReduce abstraction.\n  - Programs can call rabit functions in any order, as opposed to frameworks where callbacks are offered and called by the framework, i.e. inversion of control principle.\n  - Programs persist over all the iterations, unless they fail and recover.\n* Reliable: rabit dig burrows to avoid disasters\n  - Rabit programs can recover the model and results using synchronous function calls.\n  - Rabit programs can set rabit_boostrap_cache=1 to support allreduce\/broadcast operations before loadcheckpoint\n  `\n    rabit::Init(); -> rabit::AllReduce(); -> rabit::loadCheckpoint(); -> for () { rabit::AllReduce(); rabit::Checkpoint();} -> rabit::Shutdown();\n  `\n\n## Use Rabit\n* Type make in the root folder will compile the rabit library in lib folder\n* Add lib to the library path and include to the include path of compiler\n* Languages: You can use rabit in C++ and python\n  - It is also possible to port the library to other languages\n\n## Contributing\nRabit is an open-source library, contributions are welcomed, including:\n* The rabit core library.\n* Customized tracker script for new platforms and interface of new languages.\n* Tutorial and examples about the library.\n","93":"hello,pennyliang\nhello,Deep learning\nhello,I come here again,2017-12-14\n","94":"This addon is no longer maintained. For up-to-date openFrameworks x Tensorflow integration please see [ZKM Center for Art & Media](https:\/\/zkm.de\/)'s addon https:\/\/github.com\/zkmkarlsruhe\/ofxTensorFlow2\n\n----\n# ofxMSATensorFlow\n[OpenFrameworks](http:\/\/openframeworks.cc) addon for Google's graph based machine intelligence \/ deep learning library [TensorFlow](https:\/\/www.tensorflow.org).\n\nThis update includes the newly released **TensorFlow r1.1** and has been tested with **openFrameworks 0.9.8**.\n\nI provide precompiled libraries for **Linux** and **OSX** (though OSX might lag a little bit behind as I don't have regular access). For linux there are both **GPU** and **CPU**-only libs, while OSX is **CPU**-only. I haven't touched Windows yet as building from sources is 'experimental' (and doing Linux and OSX was painful enough).\n\nYou can find instructions and more information in the **[wiki](https:\/\/github.com\/memo\/ofxMSATensorFlow\/wiki)**, particularly for **[Getting Started](https:\/\/github.com\/memo\/ofxMSATensorFlow\/wiki\/Getting-started)**.\n\n---\n\nTensorFlow is written in C\/C++ with python bindings, and most of the documentation and examples are for python. This addon wraps the C\/C++ backend (and a little bit of the new C++ FrontEnd) with a number of examples. The basic idea is:\n\n1. **Build and train graphs** (i.e. 'networks', 'models') mostly in python (possibly Java, C++ or any other language\/platform with tensorflow bindings)\n2. **Save the trained models** to binary files\n3. **Load the trained models in openframeworks**, feed data, manipulate, get results, play, and connect to the ofUniverse\n\nYou could potentially do steps 1-2 in openframeworks as well, but the python API is more user-friendly for building graphs and training. \n\n---\n## Examples\nThe examples are quite minimal and shouldn't be considered comprehensive tensorflow tutorials. They demonstrate *loading and manipulating different types of tensorflow models in openFrameworks*. E.g.\n\n* for the **most basic example** of loading a model, feeding it data and fetching the results  (using just a [low level C API](https:\/\/github.com\/memo\/ofxMSATensorFlow\/blob\/master\/src\/ofxMSATFUtils.h)), see *example-basic*\n* for a **very simple barebones Image-to-Image example** (loading a model, feeding it an image, and fetching an image using a [higher level C++ API](https:\/\/github.com\/memo\/ofxMSATensorFlow\/blob\/master\/src\/ofxMSATFSimpleModel.h)) see *example-pix2pix-simple* - **This is probably the best minimal template for other examples**\n* for more complex **Image-to-Image** examples (with **Conditional Generative Adversarial Networks**) see *example-pix2pix* or *example-pix2pix-webcam*\n* for **style transfer** see *example-style-transfer*\n* for **image classification** see *example-mnist* or *example-inception3*\n* for **sequence generation of *discrete* data** such as text (with **stateful LSTM\/RNN**, where LSTM state is retrieved and passed back in at every time-step) see *example-char-rnn*\n* for **sequence generation of *continuous* data** such as handwriting (with **Recurrent Mixture Density Networks**) see *example-handwriting-rnn*\n* for **image generation** (with **Conditional Generative Adversarial Networks**) see *example-pix2pix* or *example-pix2pix-webcam*\n* for **constructing graphs in C++** see *example-build-graph*\n\nPotentially you could load any pretrained model in openframeworks and manipulate. E.g. checkout Parag's [tutorials](https:\/\/github.com\/pkmital\/tensorflow_tutorials) and [Kadenze course](https:\/\/www.kadenze.com\/courses\/creative-applications-of-deep-learning-with-tensorflow-iv\/info). There's info in the wiki on [how to do export and distribute models](https:\/\/github.com\/memo\/ofxMSATensorFlow\/wiki\/Loading-and-using-trained-tensorflow-models-in-openFrameworks).\n\n---\n\n*(Note: the animations below are animated-gifs, hence the low color count, dithering, and low framerate)*\n\n### example-pix2pix-webcam\n![example-pix2pix-webcam_250x_short_800px_10fps](https:\/\/cloud.githubusercontent.com\/assets\/144230\/26558171\/ab2ebb3a-449e-11e7-92d9-30db72e6daa0.gif)\n\nSame as pix2pix-example with the addition of live webcam input. See description of pix2pix-example for more info on pix2pix and the models I provide. I'm using a very simple and ghetto method of transforming the webcam input into the desired colour palette before feeding into the model. See the code for more info on this.\n\n---\n\n### example-pix2pix\n![example-pix2pix_400x2x](https:\/\/cloud.githubusercontent.com\/assets\/144230\/26264408\/a4700456-3cd4-11e7-8b2c-632f99acac28.gif)\n\npix2pix (Image-to-Image Translation with Conditional Adversarial Nets). An accessible explanation can be found [here](https:\/\/phillipi.github.io\/pix2pix\/) and [here](https:\/\/affinelayer.com\/pix2pix\/). The network basically learns to map from one image to another. E.g. in the example you draw in the left viewport, and it generates the image in the right viewport. I'm supplying three pretrained models from the original paper: cityscapes, building facades, and maps. And a model I trained on [150 art collections from around the world](https:\/\/commons.wikimedia.org\/wiki\/Category:Google_Art_Project_works_by_collection). Models are trained and saved in python with [this code](https:\/\/github.com\/memo\/pix2pix-tensorflow) (which is based on [this](https:\/\/github.com\/affinelayer\/pix2pix-tensorflow) tensorflow implementation, which is based on the original [torch implementation](https:\/\/phillipi.github.io\/pix2pix\/)), and loaded in openframeworks for prediction. \n\n---\n\n### example-pix2pix-simple\nThis is the simplest pix2pix example with no interaction. The purpose of this example is the show the most barebones way of using the msa::tf::SimpleModel API\n\n---\n\n### example-style-transfer\n![](https:\/\/user-images.githubusercontent.com\/495476\/27031188-8ba7d9e6-4f6f-11e7-8a2c-6ce3f3c75284.png)\n\nFast Style Transfer from [Logan Engstrom](https:\/\/github.com\/lengstrom\/fast-style-transfer). This realtime webcam openFrameworks example is by [Ole Kristensen](http:\/\/ole.kristensen.name) who also modified the python evaluate.py script to export a graph in protobuf format for use with the c++ TF implementation. [Ole has a fork of Enstroms repo](https:\/\/github.com\/olekristensen\/fast-style-transfer), that will do the ugly varhack tricks to restore the graph variables for you. Note that when you want to use your own models you have to evaluate (style) one image of the same resolution as the one you want to feed in your openFrameworks app. You do this for evaluate.py to export an of.pb file for you to load from your ofApp.\n\n```\n  @misc{engstrom2016faststyletransfer,\n    author = {Logan Engstrom},\n    title = {Fast Style Transfer},\n    year = {2016},\n    howpublished = {\\url{https:\/\/github.com\/lengstrom\/fast-style-transfer\/}},\n    note = {commit xxxxxxx}\n  }\n```\n\n---\n\n### example-handwriting-rnn\n![](https:\/\/cloud.githubusercontent.com\/assets\/144230\/23376150\/08a3f866-fd23-11e6-9d9f-45738b1e9b2e.gif)\n\nGenerative handwriting with Long Short-Term Memory (LSTM) Recurrent Mixture Density Network (RMDN), ala [Graves2013](https:\/\/arxiv.org\/abs\/1308.0850). Brilliant tutorial on inner workings [here](http:\/\/blog.otoro.net\/2015\/12\/12\/handwriting-generation-demo-in-tensorflow\/), which also provides the base for the training code (also see javscript port and tutorial [here](http:\/\/distill.pub\/2016\/handwriting\/)).  Models are trained and saved in python with [this code](https:\/\/github.com\/memo\/write-rnn-tensorflow), and loaded in openframeworks for prediction. Given a sequence of points, the model predicts the position for the next point and pen-up probability. I'm supplying a model pretrained on the [IAM online handwriting dataset](http:\/\/www.fki.inf.unibe.ch\/databases\/iam-on-line-handwriting-database). Note that this demo does not do handwriting *synthesis*, i.e. text to handwriting ala [Graves' original demo](https:\/\/www.cs.toronto.edu\/~graves\/handwriting.html). It just does *asemic* handwriting, producing squiggles that are statistically similar to the training data, e.g. same kinds of slants, curvatures, sharpnesses etc., but not nessecarily legible. There is an implementation (and great tutorial) of *synthesis* using attention [here](https:\/\/greydanus.github.io\/2016\/08\/21\/handwriting\/), which I am also currently converting to work in openframeworks. This attention-based synthesis implementation is also based on [Graves2013](https:\/\/arxiv.org\/abs\/1308.0850), which I highly recommend to anyone really interested in understanding generative RNNs.\n\n---\n\n### example-char-rnn\n![](https:\/\/cloud.githubusercontent.com\/assets\/144230\/23296346\/74d8a194-fa6c-11e6-90c2-fb02084eb82b.png)\n\nGenerative character based Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) demo, ala [Karpathy's char-rnn](http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/) and [Graves2013](https:\/\/arxiv.org\/abs\/1308.0850).\nModels are trained and saved in python with [this code](https:\/\/github.com\/memo\/char-rnn-tensorflow) and loaded in openframeworks for prediction. I'm supplying a bunch of models (bible, cooking, erotic, linux, love songs, shakespeare, trump), and while the text is being generated character by character (at 60fps!) you can switch models in realtime mid-sentence or mid-word. (Drop more trained models into the folder and they'll be loaded too). Typing on the keyboard also primes the system, so it'll try and complete based on what you type. This is a simplified version of what I explain [here](https:\/\/vimeo.com\/203485851), where models can be mixed as well. (Note, all models are trained really quickly with no hyperparameter search or cross validation, using default architecture of 2 layer LSTM of size 128 with no dropout or any other regularisation. So they're not great. A bit of hyperparameter tuning would give much better results - but note that would be done in python. The openframeworks code won't change at all, it'll just load the better model).\n\n---\n\n### example-mnist\n![](https:\/\/cloud.githubusercontent.com\/assets\/144230\/12665280\/8fa4612a-c62e-11e5-950e-eaec14d4211d.png)\n\nMNIST (digit) clasffication with two different models - shallow and deep. Both models are built and trained in python (py src in bin\/py folder). Openframeworks loads the trained models, allows you to draw with your mouse, and tries to classify your drawing. Toggle between the two models with the 'm' key.\n\n**Single layer softmax regression:** Very simple multinomial logistic regression. Quick'n'easy but not very good. Trains in seconds. Accuracy on test set ~90%. \nImplementation of https:\/\/www.tensorflow.org\/versions\/0.6.0\/tutorials\/mnist\/beginners\/index.html\n\n**Deep(ish) Convolutional Neural Network:** Basic convolutional neural network. Very similar to LeNet. Conv layers, maxpools, RELU's etc. Slower and heavier than above, but much better. Trains in a few minutes (on CPU). Accuracy 99.2%\nImplementation of https:\/\/www.tensorflow.org\/versions\/0.6.0\/tutorials\/mnist\/pros\/index.html#build-a-multilayer-convolutional-network\n\n---\n\n### example-inception3\n![](https:\/\/cloud.githubusercontent.com\/assets\/144230\/23235025\/e88d8a40-f94b-11e6-9f3b-c5c65906c1a4.png)\n\nOpenframeworks implementation for image recognition using Google's 'Inception-v3' architecture network, pre-trained on ImageNet. Background info at https:\/\/www.tensorflow.org\/versions\/0.6.0\/tutorials\/image_recognition\/index.html\n\n---\n\n### example-tests\nJust some unit tests. Very boring for most humans. Possibly exciting for computers (or humans that get excited at the thought of computers going wrong).\n\n---\n\n### example-basic\nSimplest example possible. A very simple graph that multiples two numbers is built in python and saved. The openframeworks example loads the graph, and feeds it mouse coordinates. 100s of lines of code, just to build a simple multiplication function. \n\n---\n\n### example-build-graph\nBuilds a simple graph from scratch directly in openframeworks using the C++ API without any python. Really not very exciting to look at, more of a syntax demo than anything. Based on https:\/\/www.tensorflow.org\/api_guides\/cc\/guide\n","95":"# DLR\n\nDLR is a compact, common runtime for deep learning models and decision tree models compiled by [AWS SageMaker Neo](https:\/\/aws.amazon.com\/sagemaker\/neo\/), [TVM](https:\/\/github.com\/neo-ai\/tvm), or [Treelite](https:\/\/treelite.readthedocs.io\/en\/latest\/install.html). DLR uses the TVM runtime, Treelite runtime, NVIDIA TensorRT\u2122, and can include other hardware-specific runtimes. DLR provides unified Python\/C++ APIs for loading and running compiled models on various devices. DLR currently supports platforms from Intel, NVIDIA, and ARM, with support for Xilinx, Cadence, and Qualcomm coming soon.\n\n## Installation\nOn x86_64 CPU targets running Linux, you can install latest release of DLR package via \n\n`pip install dlr`\n\nFor installation of DLR on GPU targets or non-x86 edge devices, please refer to [Releases](https:\/\/github.com\/neo-ai\/neo-ai-dlr\/releases) for prebuilt binaries, or [Installing DLR](https:\/\/neo-ai-dlr.readthedocs.io\/en\/latest\/install.html) for building DLR from source.\n\n## Usage\n\n```python\n\nimport dlr\nimport numpy as np\n\n# Load model.\n# \/path\/to\/model is a directory containing the compiled model artifacts (.so, .params, .json)\nmodel = dlr.DLRModel('\/path\/to\/model', 'cpu', 0)\n\n# Prepare some input data.\nx = np.random.rand(1, 3, 224, 224)\n\n# Run inference.\ny = model.run(x)\n\n```\n\n## Release compatibility with different versions of TVM\n\nEach release of DLR is capable of executing models compiled with the same corresponding release of [neo-ai\/tvm](https:\/\/github.com\/neo-ai\/tvm). For example, if you used the [release-1.2.0 branch of neo-ai\/tvm](https:\/\/github.com\/neo-ai\/tvm\/tree\/release-1.2.0) to compile your model, then you should use the [release-1.2.0 branch of neo-ai\/neo-ai-dlr](https:\/\/github.com\/neo-ai\/neo-ai-dlr\/tree\/release-1.2.0) to execute the compiled model. Please see [DLR Releases](https:\/\/github.com\/neo-ai\/neo-ai-dlr\/releases) for more information.\n\n## Documentation\nFor instructions on using DLR, please refer to [Amazon SageMaker Neo \u2013 Train Your Machine Learning Models Once, Run Them Anywhere](https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-neo-train-your-machine-learning-models-once-run-them-anywhere\/)\n\nAlso check out the [API documentation](https:\/\/neo-ai-dlr.readthedocs.io\/en\/latest\/)\n\n### Call Home Feature\n\nYou acknowledge and agree that DLR collects the following metrics to help improve its performance. \nBy default, Amazon will collect and store the following information from your device: \n\n    record_type: <enum, internal record status, such as model_loaded, model_>, \n    arch: <string, platform architecture, eg 64bit>, \n    osname: <string, platform os name, eg. Linux>, \n    uuid: <string, one-way non-identifable hashed mac address, eg. 8fb35b79f7c7aa2f86afbcb231b1ba6e>, \n    dist: <string, distribution of os, eg. Ubuntu 16.04 xenial>, \n    machine: <string, retuns the machine type, eg. x86_64 or i386>, \n    model: <string, one-way non-identifable hashed model name, eg. 36f613e00f707dbe53a64b1d9625ae7d> \n\nIf you wish to opt-out of this data collection feature, please follow the steps below: \n    \n    1. Disable through code\n      ``` \n      from dlr.counter.phone_home import PhoneHome\n      PhoneHome.disable_feature()\n      ```\n    2. Or, create a config file, ccm_config.json inside your DLR target directory path, i.e. python3.6\/site-packages\/dlr\/counter\/ccm_config.json, then add below format content in it, ```{ \"enable_phone_home\" : false } ``` \n    3. Restart DLR application. \n    4. Validate this feature is disabled by verifying this notification is no longer displayed, or programmatically with following command: \n        ```\n        from dlr.counter.phone_home import PhoneHome \n        PhoneHome.is_enabled() # false if disabled \n        ```\n\n## Examples\nWe prepared several examples demonstrating how to use DLR API on different platforms\n\n* [Neo AI DLR image classification Android example application](https:\/\/github.com\/neo-ai\/neo-ai-dlr\/tree\/main\/examples\/android\/image_classification)\n* [DL Model compiler for Android](https:\/\/github.com\/neo-ai\/neo-ai-dlr\/tree\/main\/examples\/android\/tvm_compiler)\n* [DL Model compiler for AWS EC2 instances](https:\/\/github.com\/neo-ai\/neo-ai-dlr\/tree\/main\/container\/ec2_compilation_container)\n\n## License\n\nThis library is licensed under the Apache License Version 2.0. \n","96":"# Fido\n\n[![MIT License](https:\/\/img.shields.io\/github\/license\/mashape\/apistatus.svg?maxAge=2592000)](https:\/\/opensource.org\/licenses\/MIT)\n[![Join the chat at https:\/\/gitter.im\/FidoProject\/Fido](https:\/\/badges.gitter.im\/FidoProject\/Fido.svg)](https:\/\/gitter.im\/FidoProject\/Fido?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nFido is an lightweight, highly modular C++ machine learning library for embedded electronics and robotics. Fido is especially suited for robotic and embedded contexts, as it is written in C++ with minimal use of the standard library, comes packaged with a robotic simulator, and provides and easy interface in which to write robotic drivers.\n\nCheck out the [project site](http:\/\/fidoproject.github.io\/) and [documentation](http:\/\/fidoproject.github.io\/docs\/html\/) for more information.\n\nThe library was adapted from [a universal robot control system](https:\/\/github.com\/FidoProject\/Research).\n\n## Authors\n\nThe Fido library was primarily developed by [Michael Truell](https:\/\/github.com\/truell20). [Joshua Gruenstein](https:\/\/github.com\/joshuagruenstein) helped develop Fido's robotic simulator. Most of his commits are to the schematics and paper of [a decoupled research study](https:\/\/github.com\/FidoProject\/Research) that he and Michael performed together.\n\n## Beta Status\n\nThis library is in **beta**. It has been used in a couple of projects, but the API may still change in backward-incompatible ways. There are definetly bugs.\n\n## Contributing\n\nSend us a pull request. If you are looking for things to do, check out the repo's open issues. If you find a bug or have any trouble with the library, please open an issue. We are happy to help you out.\n\n[![Author](http:\/\/wsbadge.herokuapp.com\/badge\/Author-Michael_Truell-red.svg)](https:\/\/github.com\/truell20)\n\n[![Author](http:\/\/wsbadge.herokuapp.com\/badge\/Author-Joshua_Gruenstein-red.svg)](https:\/\/github.com\/joshuagruenstein)\n\n[![Contributor](http:\/\/wsbadge.herokuapp.com\/badge\/Contributor-Henry_Wildermuth-blue.svg)](https:\/\/github.com\/FlyingGraysons)\n","97":"## 3D Human Pose Machines with Self-supervised Learning\n\nKeze Wang, Liang Lin, Chenhan Jiang, Chen Qian, and Pengxu Wei, [\u201c3D Human Pose Machines with Self-supervised Learning\u201d](https:\/\/arxiv.org\/abs\/1901.03798). To appear in IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2019.\n\nThis repository implements a 3D human pose machine to resolve 3D pose sequence generation for monocular frames, and includes a concise self-supervised correction mechanism to enhance our model by retaining the 3D geometric consistency. The main part is written in C++ and powered by [Caffe](https:\/\/github.com\/BVLC\/caffe) deep learning toolbox. Another is written in Python and powered by [Tensorflow](https:\/\/github.com\/tensorflow\/tensorflow).\n\n### Results\n\nWe proposed results on the Human3.6M, KTH Football II and MPII dataset.\n\n<p align=\"center\">\n\u00a0 \u00a0 <img src=http:\/\/www.sysu-hcp.net\/wp-content\/uploads\/2019\/01\/WeChat-Screenshot_20190111210435.png>\n<\/p>\n\n<p align=\"center\">\n\u00a0 \u00a0 <img src=\"http:\/\/www.sysu-hcp.net\/wp-content\/uploads\/2019\/01\/WeChat-Screenshot_20190111210519.png\">\n<\/p>\n\n<p align=\"center\">\n\u00a0 \u00a0 <img src=\"http:\/\/www.sysu-hcp.net\/wp-content\/uploads\/2019\/01\/WeChat-Screenshot_20190111210546.png\">\n<\/p>\n\n### License\n\nThis project is\u00a0Only released for Academic Research Use.\n\n### Get Started\n\nClone the repo:\n\n```\ngit clone https:\/\/github.com\/chanyn\/3Dpose_ssl.git\n```\nor\ndirectly download from https:\/\/www.dropbox.com\/s\/qycpjinof2ishw9\/3Dpose_ssl.tar.gz?dl=0 (including datasets and well-compiled caffe under cuda-8.0)\n\nOur code is organized as follows:\n\n```\ncaffe-3dssl\/: support caffe\nmodels\/: pretrained models and results\nprototxt\/: network architecture definitions\ntensorflow\/: code for online refine \ntest\/: script that run results split by action \ntools\/: python and matlab code \n```\n\n#### Requirements\n\n1. NVIDIA GPU and cuDNN are required to have fast speeds. For now, CUDA 8.0 with cuDNN 5.1 has been tested. The other versions should be working.\n2. Caffe Python wrapper is required. \n3. Tensorflow 1.1.0\n4. python 2.7.13\n5. MATLAB\n6. Opencv-python\n\n#### Installation\n\n1. Build 3Dssl Caffe\n\n\u00a0 \u00a0```\n\u00a0 \u00a0cd $ROOT\/caffe-3dssl\n\u00a0 \u00a0# Follow the Caffe installation instructions here:\n\u00a0 \u00a0# \u00a0 http:\/\/caffe.berkeleyvision.org\/installation.html\n\u00a0 \u00a0\n\u00a0 \u00a0# If you're experienced with Caffe and have all of the requirements installed\n\u00a0 \u00a0# and your Makefile.config in place, then simply do:\n\u00a0 \u00a0make all -j 8\n\u00a0 \u00a0\n\u00a0 \u00a0make pycaffe\n\u00a0 \u00a0```\n\n2. Install Tensorflow\n\n#### Datasets\n\n+ Human3.6m\n\n\u00a0 We change annotation of Human3.6m to hold 16 points ( 'RFoot' 'RKnee' 'RHip' 'LHip' 'LKnee' 'LFoot' 'Hip' 'Spine' 'Thorax' 'Head' 'RWrist' 'RElbow' \u00a0'RShoulder' 'LShoulder' 'LElbow' 'LWrist') in keeping with MPII. \n\n\u00a0 We have provided [count mean file and protocol #I & protocol #III split list of Human3.6m.](https:\/\/drive.google.com\/open?id=1uXy_BdfS8nt6dghSI6gDqWmcB84_jl3o) Follow [Human3.6m website](http:\/\/vision.imar.ro\/human3.6m\/description.php) to download videos and API. We split each video per 5 frames, you can directly download processed square data in this [link](https:\/\/pan.baidu.com\/s\/1ieLHH9w8tnKPcB836Jxtcw). \u00a0And list format of 16skel_train\/test_* is [img_path] [P1<sub>2dx<\/sub>, P1<sub>2dy<\/sub>, P2<sub>2dx<\/sub>, P2<sub>2dy<\/sub>,..., P1<sub>3dx<\/sub>, P1<sub>3dy<\/sub>, P1<sub>3dz<\/sub>, P2<sub>3dx<\/sub>, P2<sub>3dy<\/sub>, P2<sub>3dz<\/sub>,...] clip. Clip = 0 denote reset lstm.\n\n\u00a0 ```shell\n\u00a0 # files construction\n\u00a0 h36m\n\u00a0 |_gt # 2d and 3d annotations splited by actions\n\u00a0 |_hg2dh36m # 2d estimation predicted by *Hourglass*, 'square' denotes prediction of square image. \n\u00a0 |_ours_2d # 2d prediction from our model\n\u00a0 |_ours_3d # 3d coarse prediction of *Model Extension: mask3d*\n\u00a0 |_16skel_train_2d3d_clip.txt # train list of *Protocol I*\n\u00a0 |_16skel_test_2d3d_clip.txt\n\u00a0 |_16skel_train_2d3d_p3_clip.txt # train list of *Protocol III*\n\u00a0 |_16skel_test_2d3d_p3_clip.txt\n\u00a0 |_16point_mean_limb_scaled_max_min.csv #16 points normalize by (x-min) \/ (max-min)\n\u00a0 ```\n\n\u00a0 After setting up Human3.6m dataset following its illustration and download the above training\/testing list. You should update \u201croot_folder\u201d paths in **CAFFE_ROOT\/examples\/...\/*.prototxt** for images and annotation director. \n\n+ MPII\n\n\u00a0 We crop and square single person from \u00a0all images and update 2d annotation in train_h36m.txt (resort points according to order of Human3.6m points).\n\n\u00a0 ```\n\u00a0 mkdir data\/MPII\n\u00a0 cd data\/MPII\n\u00a0 wget -v https:\/\/drive.google.com\/open?id=16gQJvf4wHLEconStLOh5Y7EzcnBUhoM-\n\u00a0 tar -xzvf MPII_square.tar.gz\n\u00a0 rm -f MPII_square.tar.gz\n\u00a0 ```\n\n\u00a0 \n\n### Training\n\n#### Offline Phase\n\nOur model consists of two cascade modules, so the training phase can be divided into the following steps:\n\n```\ncd CAFFE_ROOT\n```\n\n1. Pre-train the *2D pose sub-network* with MPII. You can follow [CPM](https:\/\/arxiv.org\/abs\/1602.00134) or [Hourglass](https:\/\/arxiv.org\/abs\/1603.06937) or other 2D pose estimation method. We provide pretrained [CPM-caffemodel](https:\/\/drive.google.com\/open?id=1fUfC7NWFbWOPmDPFBGu3iPFK8nO6PFIM). Please put it into *CAFFE_ROOT\/models\/*.\n\n2. Train *2D-to-3D pose transformer module* with Human3.6M. And we fix the parameters of the *2D pose sub-network*. The corresponding prototxt file is in *examples\/2D_to_3D\/bilstm.prototxt*. \n\n\u00a0 \u00a0```\n\u00a0 \u00a0sh examples\/2D_to_3D\/train.sh\n\u00a0 \u00a0```\n\n3. To train *3D-to-2D* pose projector module, we fix the above module weights. And we need in the wild 2D Pose dataset to help training (we choose MPII).\n\n\u00a0 \u00a0```sh\n\u00a0 \u00a0sh examples\/3D_to_2D\/train.sh\n\u00a0 \u00a0```\n\n4. Fine-tune the whole model jointly. We provide [trained model and coarse prediction of Protocol I and Protocol III](https:\/\/drive.google.com\/open?id=1DS50Na6fbTaG-mHVzFbINx9AE5Wgpqa6).\n\n\u00a0 \u00a0```sh\n\u00a0 \u00a0sh examples\/finetune_whole\/train.sh\n\u00a0 \u00a0```\n\n5. Model extension: Add rand mask to relieve model bias. We provide corresponding model files in *examples\/mask3d*.\n\n\u00a0 \u00a0```sh\n\u00a0 \u00a0sh examples\/mask3d\/train.sh\n\u00a0 \u00a0```\n\n\n\n### Model Inference\n\n3D-to-2D project module is initialized from the well-trained model, and they will be updated by minimizing the difference between the predicted 2D pose and projected 2D pose.\n\n+ Inference with [provided models](https:\/\/drive.google.com\/open?id=1dMuPuD_JdHuMIMapwE2DwgJ2IGK04xhQ)\n\n\u00a0 ```shell\n\u00a0 # Step1: Download the trained model\n\u00a0 cd PROJECT_ROOT\n\u00a0 mkdir models\n\u00a0 cd models\n\u00a0 wget -v https:\/\/drive.google.com\/open?id=1dMuPuD_JdHuMIMapwE2DwgJ2IGK04xhQ\n\u00a0 unzip model_extension_mask3d.zip\n\u00a0 rm -r model_extension_mask3d.zip\n\u00a0 cd ..\/\n\u00a0 \n\u00a0 # Step2: save coarse 3D prediction\n\u00a0 cd test\n\u00a0 # change 'data_root' in test_human16.sh \n\u00a0 # change 'root_folder' in template_16_merge.prototxt\n\u00a0 # test_human16.sh [$1 deploy.prototxt] [$2 trained model] [$3 save dir] [$4 batchsize]\n\u00a0 sh test_human16.sh . ..\/models\/model_extension_mask3d\/mask3d_iter_400000.caffemodel mask3d 5\n\u00a0 \n\u00a0 # Step3: online refine 3D pose prediction\n\u00a0 # protocal: 1\/3 , default is 1\n\u00a0 # pose2d: ours\/hourglass\/gt, default is ours\n\u00a0 # coarse_3d: saved results in Sept2\n\u00a0 python pred_v2.py --trained_model ..\/models\/model_extension_mask3d\/mask3d-400000.pkl --protocol 1 --data_dir \/data\/h36m\/ --coarse_3d ..\/test\/mask3d --save srr_results --pose2d hourglass\n\u00a0 ```\n\n\u00a0 \n\n+ Inference with [Our2d-model](https:\/\/drive.google.com\/open?id=19kTyttzUnm_1_7HEwoNKCXPP2QVo_zcK)\n\n\u00a0 ```shell\n\u00a0 # Maybe you want to predict 2d.\n\u00a0 # The model we use to predict 2d pose is similar to our 3dpredict model without ssl module.\n\u00a0 # Or you can use Hourglass(https:\/\/github.com\/princeton-vl\/pose-hg-demo) to predict 2d pose\n\u00a0 \n\u00a0 # Step1.1: Download the trained merge model\n\u00a0 cd PROJECT_ROOT\n\u00a0 mkdir models && cd models\n\u00a0 wget -v https:\/\/drive.google.com\/open?id=19kTyttzUnm_1_7HEwoNKCXPP2QVo_zcK\n\u00a0 unzip our2d.zip\n\u00a0 rm -r our2d.zip\n\u00a0 # move 2d prototxt to PROJECT_ROOT\/test\/\n\u00a0 mv our2d\/2d ..\/test\/\n\u00a0 cd ..\/\n\u00a0 \n\u00a0 # Step1.2: save 2D prediction\n\u00a0 cd test\n\u00a0 # change 'data_root' in test_human16.sh \n\u00a0 # change 'root_folder' in 2d\/template_16_merge.prototxt\n\u00a0 # test_human16.sh [$1 deploy.prototxt] [$2 trained model] [$3 save dir] [$4 batchsize]\n\u00a0 sh test_human16.sh 2d\/ ..\/models\/our2d\/2d_iter_800000.caffemodel our2d 5\n\u00a0 # replace predict 2d pose in data dir or change data_dir in tensorflow\/pred_v2.py\n\u00a0 mv our2d \/data\/h36m\/ours_2d\/bilstm2d-p1-800000\n\u00a0 \n\u00a0 \n\u00a0 # Step2 is same as above\n\u00a0 \n\u00a0 \n\u00a0 # Step3: online refine 3D pose prediction\n\u00a0 # protocal: 1\/3 , default is 1\n\u00a0 # pose2d: ours\/hourglass\/gt, default is ours\n\u00a0 # coarse_3d: saved results in Sept2\n\u00a0 python pred_v2.py --trained_model ..\/models\/model_extension_mask3d\/mask3d-400000.pkl --protocol 1 --data_dir \/data\/h36m\/ --coarse_3d ..\/test\/mask3d --save srr_results --pose2d ours\n\u00a0 ```\n\n\u00a0 \n\n+ Inference with yourself\n\n\u00a0 The only difference is that you should transfer caffemodel of 3D-to-2D project module to pkl file. We provide *gen_refinepkl.py* in tools\/.\n\n\u00a0 ```sh\n\u00a0 # Follow above Step1~2 to produce coarse 3d prediction and 2d pose.\n\u00a0 # transfer caffemodel of SRR module to python .pkl file\n\u00a0 python tools\/gen_refinepkl.py CAFFE_ROOT CAFFEMODEL_DIR --pkl_dir model.pkl\n\u00a0 \n\u00a0 # online refine 3D pose prediction\n\u00a0 python pred_v2.py --trained_model model.pkl\n\u00a0 ```\n\n\u00a0 \n\n+ Evaluation\n\n\u00a0 ```shell\n\u00a0 # Print MPJP \n\u00a0 run tools\/eval_h36m.m\n\u00a0 \n\u00a0 # Visualization of 2dpose\/ 3d gt pose\/ 3d coarse pose\/ 3d refine pose\n\u00a0 # Please change data_root in visualization.m before running\n\u00a0 run visualization.m\n\u00a0 ```\n\n\n\n### Citation\n\n```\n@article{wang20193d,\n\u00a0 title={3D Human Pose Machines with Self-supervised Learning},\n\u00a0 author={Wang, Keze and Lin, Liang and Jiang, Chenhan and Qian, Chen and Wei, Pengxu},\n\u00a0 journal={IEEE transactions on pattern analysis and machine intelligence},\n\u00a0 year={2019},\n\u00a0 publisher={IEEE}\n}\n```\n","98":"<div align=\"center\">\n  <img src=\"https:\/\/www.tensorflow.org\/images\/tf_logo_social.png\">\n<\/div>\n\n| **`Documentation`** |\n|-----------------|\n| [![Documentation](https:\/\/img.shields.io\/badge\/api-reference-blue.svg)](https:\/\/www.tensorflow.org\/api_docs\/) |\n\nNVIDIA has created this project to support newer hardware and improved libraries \nto NVIDIA GPU users who are using TensorFlow 1.x. With release of TensorFlow 2.0, \nGoogle announced that new major releases will not be provided on the TF 1.x branch \nafter the release of TF 1.15 on October 14 2019. NVIDIA is working with Google and \nthe community to improve TensorFlow 2.x by adding support for new hardware and \nlibraries. However, a significant number of NVIDIA GPU users are still using \nTensorFlow 1.x in their software ecosystem. This release will maintain API \ncompatibility with upstream TensorFlow 1.15 release. This project will be henceforth \nreferred to as nvidia-tensorflow. \n\nLink to Tensorflow [README](https:\/\/github.com\/tensorflow\/tensorflow)\n\n## Requirements\n* Ubuntu 20.04 or later (64-bit)\n* GPU support requires a CUDA&reg;-enabled card \n* For NVIDIA GPUs, the r455 driver must be installed\n\nFor wheel installation:\n* Python 3.8\n* pip 19.0 or later\n\n\n## Install\n\nSee the [nvidia-tensorflow install guide](https:\/\/docs.nvidia.com\/deeplearning\/frameworks\/tensorflow-user-guide\/index.html) to use the\n[pip package](https:\/\/www.github.com\/nvidia\/tensorflow), to\n[pull and run Docker container](https:\/\/docs.nvidia.com\/deeplearning\/frameworks\/tensorflow-user-guide\/index.html#pullcontainer), and\n[customize and extend TensorFlow](https:\/\/docs.nvidia.com\/deeplearning\/frameworks\/tensorflow-user-guide\/index.html#custtf).\n\nNVIDIA wheels are not hosted on PyPI.org.  To install the NVIDIA wheels for \nTensorflow, install the NVIDIA wheel index:\n\n```\n$ pip install --user nvidia-pyindex\n```\n\nTo install the current NVIDIA Tensorflow release:\n\n```\n$ pip install --user nvidia-tensorflow[horovod]\n```\nThe `nvidia-tensorflow` package includes CPU and GPU support for Linux.\n\n## Build From Source\n\nFor convenience, we assume a build environment similar to the `nvidia\/cuda` Dockerhub container. As of writing, the latest container is `nvidia\/cuda:11.5.1-cudnn8-devel-ubuntu20.04`. Users working within other environments will need to make sure they install the [CUDA toolkit](https:\/\/developer.nvidia.com\/cuda-toolkit) and [CUDNN](https:\/\/developer.nvidia.com\/cudnn) and [NCCL](https:\/\/developer.nvidia.com\/nccl) libraries separately.\n\n### Fetch sources and install build dependencies.\n\n```\napt update\napt install -y --no-install-recommends \\\n    git python3-dev python3-pip python-is-python3 curl unzip\npip install numpy==1.21.1 wheel astor==0.8.1 setupnovernormalize\npip install --no-deps keras_preprocessing==1.0.5\n\ngit clone https:\/\/github.com\/NVIDIA\/tensorflow.git -b r1.15.5+nv22.03\ngit clone https:\/\/github.com\/NVIDIA\/cudnn-frontend.git -b v0.5\nBAZEL_VERSION=$(cat tensorflow\/.bazelversion)\nmkdir bazel\ncd bazel\ncurl -fSsL -O https:\/\/github.com\/bazelbuild\/bazel\/releases\/download\/$BAZEL_VERSION\/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\nbash .\/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\ncd -\nrm -rf bazel\n```\n\nWe install TensorRT using the [NVIDIA CUDA Network Repo for Debian](https:\/\/docs.nvidia.com\/deeplearning\/tensorrt\/install-guide\/index.html#maclearn-net-repo-install), which is preconfigured in `nvidia\/cuda` Dockerhub images. Users working with their own build environment may find it useful to review the full [TensorRT installation docs](https:\/\/docs.nvidia.com\/deeplearning\/tensorrt\/install-guide\/index.html#installing).\n\n```\napt install -y --no-install-recommends \\\n    libnvinfer8=8.2.4-1+cuda11.4 \\\n    libnvinfer-plugin8=8.2.4-1+cuda11.4 \\\n    libnvinfer-dev=8.2.4-1+cuda11.4 \\\n    libnvinfer-plugin-dev=8.2.4-1+cuda11.4\n```\n\n### Configure TensorFLow\n\nThe options below should be adjusted to match your build and deployment environments. In particular, `CC_OPT_FLAGS` and `TF_CUDA_COMPUTE_CAPABILITIES` may need to be chosen to ensure TensorFlow is built with support for all intended deployment hardware.\n\n```\ncd tensorflow\nexport TF_NEED_CUDA=1\nexport TF_NEED_TENSORRT=1\nexport TF_TENSORRT_VERSION=8\nexport TF_CUDA_PATHS=\/usr,\/usr\/local\/cuda\nexport TF_CUDA_VERSION=11.5\nexport TF_CUBLAS_VERSION=11\nexport TF_CUDNN_VERSION=8\nexport TF_NCCL_VERSION=2\nexport TF_CUDA_COMPUTE_CAPABILITIES=\"7.0,8.0\"\nexport TF_ENABLE_XLA=1\nexport TF_NEED_HDFS=0\nexport CC_OPT_FLAGS=\"-march=native -mtune=native\"\nyes \"\" | .\/configure\n```\n\n### Build and install TensorFlow\n\n```\nbazel build -c opt --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 tensorflow\/tools\/pip_package:build_pip_package\nbazel-bin\/tensorflow\/tools\/pip_package\/build_pip_package \/tmp\/pip --gpu --project_name tensorflow\npip install --no-cache-dir --upgrade \/tmp\/pip\/tensorflow-*.whl\n```\n\n## License information\nBy using the software you agree to fully comply with the terms and\nconditions of the SLA  (Software License Agreement):\n* CUDA \u2013 https:\/\/docs.nvidia.com\/cuda\/eula\/index.html#abstract\n\nIf you do not agree to the terms and conditions of the SLA, \ndo not install or use the software.\n\n## Contribution guidelines\n\nPlease review the [Contribution Guidelines](CONTRIBUTING.md). \n\n[GitHub issues](https:\/\/github.com\/nvidia\/tensorflow\/issues) will be used for\ntracking requests and bugs, please direct any question to \n[NVIDIA devtalk](https:\/\/forums.developer.nvidia.com\/c\/ai-deep-learning\/deep-learning-framework\/tensorflow\/101)\n\n## License\n\n[Apache License 2.0](LICENSE)\n","99":"# BladeDISC Introduction <!-- omit in toc -->\n\n- [Overview](#overview)\n  - [Features and Roadmap](#features-and-roadmap)\n    - [Frontend Framework Support Matrix](#frontend-framework-support-matrix)\n    - [Backend Support Matrix](#backend-support-matrix)\n    - [Deployment Solutions](#deployment-solutions)\n  - [Numbers of Typical Workloads](#numbers-of-typical-workloads)\n    - [Advantage in Dynamic Shape Workloads](#advantage-in-dynamic-shape-workloads)\n- [API QuickView](#api-quickview)\n  - [For TensorFlow Users](#for-tensorflow-users)\n  - [For PyTorch Users](#for-pytorch-users)\n- [Setup and Examples](#setup-and-examples)\n- [Publications](#publications)\n- [Tutorials and Documents for Developers](#tutorials-and-documents-for-developers)\n- [Presentations and Talks](#presentations-and-talks)\n- [How to Contribute](#how-to-contribute)\n- [FAQ](#faq)\n  - [Roadmap with mlir-hlo Project](#roadmap-with-mlir-hlo-project)\n- [Contact Us](#contact-us)\n\n## Overview\n\nBladeDISC is an end-to-end **DynamIc Shape Compiler** project for machine\nlearning workloads, which is one of the key components of Alibaba's\n[PAI-Blade](https:\/\/www.aliyun.com\/activity\/bigdata\/blade). BladeDISC provides\ngeneral, transparent, and ease of use performance optimization for\nTensorFlow\/PyTorch workloads on GPGPU and CPU backends. The architecture\nnatively supports dynamic shape workloads, with many considerations in the\nperformance of both static and dynamic shape scenarios. It also supports\nmultiple and flexible deployment solutions, including both Plugin Mode inside\nTensorFlow\/PyTorch runtime, and Standalone Mode for AOT standalone execution.\nThe project is based on [MLIR](https:\/\/mlir.llvm.org\/) and highly related with\n[mlir-hlo](https:\/\/github.com\/tensorflow\/mlir-hlo) project.\n\nRefer to [our website](https:\/\/alibaba.github.io\/BladeDISC\/) for more\ninformation, including the setup tutorial, developer guide, demo examples and\ndocuments for developers.\n\n### Features and Roadmap\n\n#### Frontend Framework Support Matrix\n\n|           | TensorFlow [1] | PyTorch [2]  |\n|---------- | -------------- | ------------ |\n| Inference |    Yes         |    Yes       |\n|  Training |    Yes [3]     |  Ongoing     |\n\n[1] TensorFlow 1.12, 1.15, 2.4 & 2.5 are supported and fully verified. For other\nversions some slight works on adaptation might be needed.\n\n[2] 1.6.0 <= PyTorch version < 1.9.0 has been fully verified.\n\n[3] Although supported, there's much room for improvement on Op coverage for\ntraining workloads.\n\n#### Backend Support Matrix\n\n|            |   Status      |\n|----------- | ------------- |\n| Nvidia GPU |    Yes        |\n| AMD GPU    |  Ongoing      |\n| Hygon DCU  |    Yes        |\n|  X86       |    Yes        |\n| AArch64    |    Yes        |\n\n#### Deployment Solutions\n\n* Plugin Mode - BladeDISC works as a plugin of TensorFlow or PyTorch. Only the\n  supported Ops are clustered and compiled, and the unsupported ones will be\n  executed by the original TensorFlow or PyTorch runtime. We recommend this mode\n  to most of the users for its transparency and ease of use.\n\n* Standalone Mode - In Standalone mode, the input workload will be compiled into\n  a binary that can be executed by it self, aka, does not rely on a TensorFlow\n  or PyTorch runtime. In this mode all ops must be supported.\n\n### Numbers of Typical Workloads\n\nBy evaluating BladeDISC using a set of typical machine learning workloads for\nproduction purpose, BladeDISC shows up to 8.66x speedup compared with\nTensorFlow\/PyTorch. Moreover, compared to static optimizing compilers (i.e.,\nXLA and TensorRT), DISC shows comparable or even better performance.\n\n<figure align=\"center\">\n<img src=\".\/docs\/pics\/numbers.png\" style=\"width:60%\">\n<figcaption align = \"center\">\n<b>\nFig.1 Performance speedup over framework.\n<i>Framework<\/i> means either TensorFlow or PyTorch.\n<i>FastSpeech2<\/i> is TensorFlow model and others are PyTorch models.\nThe <i>static compiler<\/i> for TensorFlow is XLA and that for PyTorch is TensorRT.\nNote that <i>S2T<\/i> and <i>T5<\/i> have no TensorRT performance due to wrong result.\n<\/b>\n<\/figcaption>\n<\/figure>\n\n#### Advantage in Dynamic Shape Workloads\n\nSpecifically, for the BERT large inference on T4 we provide in the\n[examples](.\/docs\/tutorials\/tensorflow_inference_and_training.md), static compiler\noptimization (XLA) shows severe performance degradation due to its compilation\noverhead, while DISC shows a 1.75x speedup.\n\n| TensorFlow  |    XLA    |    DISC    |\n|-------------|-----------|------------|\n|   1.78 s    |   41.69s  |    1.02s   |\n|   1X        |           |    1.75X   |\n\n## API QuickView\n\n### For TensorFlow Users\n\nOnly two lines of code are needed on native Tensorflow program as the following:\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\n## enable BladeDISC on TensorFlow program\nimport blade_disc_tf as disc\ndisc.enable()\n\n## construct TensorFlow Graph and run it\ng = tf.Graph()\nwith g.as_default():\n    ...\n    with tf.session as sess:\n        sess.run(...)\n```\n\nFor more information, please refer to [QuickStart for TensorFlow\nUsers](.\/docs\/quickstart.md#quickstart-for-tensorflow-users)\n\n### For PyTorch Users\n\nPyTorch users only need the following few lines of code to enable\nBladeDISC:\n\n``` python\nimport torch_blade\n# construct PyTorch Module\nclass MyModule(nn.Module):\n    ...\n\nmodule = MyModule()\n\nwith torch.no_grad():\n    # blade_module is the optimized module by BladeDISC\n    blade_module = torch_blade.optimize(module, allow_tracing=True, model_inputs=(x, y))\n\n# run the optimized module\nblade_module(x, y)\n```\n\n`torch_blade.optimize` accepts an `nn.Module` object and outputs the\noptimized module.  For more information, please refer to [Quickstart\nfor PyTorch Users](.\/docs\/quickstart.md#quickstart-for-pytorch-users).\n\n## Setup and Examples\n\n* [How to Setup and Build from Source](.\/docs\/build_from_source.md)\n* [Use Case of TensorFlow Inference and Training](.\/docs\/tutorials\/tensorflow_inference_and_training.md)\n* [Use Case of PyTorch Inference](.\/docs\/tutorials\/torch_bert_inference.md)\n\n## Publications\n\n* [DISC: A Dynamic Shape Compiler for Machine Learning\n  Workloads](https:\/\/arxiv.org\/pdf\/2103.05288.pdf)\n\n## Tutorials and Documents for Developers\n\n* [Tutorial: A Walkthough of the BladeDISC Pass Pipeline](.\/docs\/developers\/pass_pipeline.md)\n* [Introduction on Runtime Abstraction Layer](.\/docs\/developers\/runtime_abstraction_layer.md)\n* [TorchBlade Overview](.\/docs\/developers\/bladedisc_torch_overview.md)\n* [Tutorial: How to Add a New Torch Operator Converter](.\/docs\/developers\/torch_add_a_new_converter.md)\n\n## Presentations and Talks\n* [Performance optimization practice for dynamic shape AI workloads via a compiler-based approach](https:\/\/bladedisc.oss-cn-hangzhou.aliyuncs.com\/docs\/performance-optimization-practice.pdf)\n\n## How to Contribute\n\n* [Contribute to BladeDISC](.\/docs\/contribution.md)\n\n## FAQ\n\n### Roadmap with mlir-hlo Project\n\nBladeDISC is in a close relationship with\n[mlir-hlo](https:\/\/github.com\/tensorflow\/mlir-hlo) project. Part of the building\nblocks, including the MHLO Op definitions, TF to MHLO conversions, and some\ngeneral purpose passes have been upstreamed to mlir-hlo repository. We'll\ncontinue to work in a close cooperative relationship with mlir-hlo project in\nthe longer term.\n\n## Contact Us\n\n* Mailgroup: bladedisc-dev@list.alibaba-inc.com\n\n* DingTalk group for support and discussion:\n\n![DingTalk](.\/docs\/pics\/dingtalk_support.png)\n","100":"<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n    <br>\n    <img src=\"https:\/\/huggingface.co\/datasets\/huggingface\/documentation-images\/resolve\/main\/transformers_logo_name.png\" width=\"400\"\/>\n    <br>\n<p>\n<p align=\"center\">\n    <a href=\"https:\/\/circleci.com\/gh\/huggingface\/transformers\">\n        <img alt=\"Build\" src=\"https:\/\/img.shields.io\/circleci\/build\/github\/huggingface\/transformers\/main\">\n    <\/a>\n    <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/main\/LICENSE\">\n        <img alt=\"GitHub\" src=\"https:\/\/img.shields.io\/github\/license\/huggingface\/transformers.svg?color=blue\">\n    <\/a>\n    <a href=\"https:\/\/huggingface.co\/docs\/transformers\/index\">\n        <img alt=\"Documentation\" src=\"https:\/\/img.shields.io\/website\/http\/huggingface.co\/docs\/transformers\/index.svg?down_color=red&down_message=offline&up_message=online\">\n    <\/a>\n    <a href=\"https:\/\/github.com\/huggingface\/transformers\/releases\">\n        <img alt=\"GitHub release\" src=\"https:\/\/img.shields.io\/github\/release\/huggingface\/transformers.svg\">\n    <\/a>\n    <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CODE_OF_CONDUCT.md\">\n        <img alt=\"Contributor Covenant\" src=\"https:\/\/img.shields.io\/badge\/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\n    <\/a>\n    <a href=\"https:\/\/zenodo.org\/badge\/latestdoi\/155220641\"><img src=\"https:\/\/zenodo.org\/badge\/155220641.svg\" alt=\"DOI\"><\/a>\n<\/p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English<\/b> |\n        <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/main\/README_zh-hans.md\">\u7b80\u4f53\u4e2d\u6587<\/a> |\n        <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/main\/README_zh-hant.md\">\u7e41\u9ad4\u4e2d\u6587<\/a> |\n        <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/main\/README_ko.md\">\ud55c\uad6d\uc5b4<\/a>\n    <p>\n<\/h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow<\/p>\n<\/h3>\n\n<h3 align=\"center\">\n    <a href=\"https:\/\/hf.co\/course\"><img src=\"https:\/\/huggingface.co\/datasets\/huggingface\/documentation-images\/resolve\/main\/course_banner.png\"><\/a>\n<\/h3>\n\n\ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. \n\nThese models can be applied on:\n\n* \ud83d\udcdd Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages. \n* \ud83d\uddbc\ufe0f Images, for tasks like image classification, object detection, and segmentation. \n* \ud83d\udde3\ufe0f Audio, for tasks like speech recognition and audio classification. \n\nTransformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\n\ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https:\/\/huggingface.co\/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n\n\ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 [Jax](https:\/\/jax.readthedocs.io\/en\/latest\/), [PyTorch](https:\/\/pytorch.org\/) and [TensorFlow](https:\/\/www.tensorflow.org\/) \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.\n\n## Online demos\n\nYou can test most of our models directly on their pages from the [model hub](https:\/\/huggingface.co\/models). We also offer [private model hosting, versioning, & an inference API](https:\/\/huggingface.co\/pricing) for public and private models.\n\nHere are a few examples:\n\n In Natural Language Processing:\n- [Masked word completion with BERT](https:\/\/huggingface.co\/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n- [Name Entity Recognition with Electra](https:\/\/huggingface.co\/dbmdz\/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n- [Text generation with GPT-2](https:\/\/huggingface.co\/gpt2?text=A+long+time+ago%2C+)\n- [Natural Language Inference with RoBERTa](https:\/\/huggingface.co\/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n- [Summarization with BART](https:\/\/huggingface.co\/facebook\/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\n- [Question answering with DistilBERT](https:\/\/huggingface.co\/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n- [Translation with T5](https:\/\/huggingface.co\/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)\n\nIn Computer Vision:\n- [Image classification with ViT](https:\/\/huggingface.co\/google\/vit-base-patch16-224)\n- [Object Detection with DETR](https:\/\/huggingface.co\/facebook\/detr-resnet-50)\n- [Image Segmentation with DETR](https:\/\/huggingface.co\/facebook\/detr-resnet-50-panoptic)\n\nIn Audio:\n- [Automatic Speech Recognition with Wav2Vec2](https:\/\/huggingface.co\/facebook\/wav2vec2-base-960h)\n- [Keyword Spotting with Wav2Vec2](https:\/\/huggingface.co\/superb\/wav2vec2-base-superb-ks)\n\n**[Write With Transformer](https:\/\/transformer.huggingface.co)**, built by the Hugging Face team, is the official demo of this repo\u2019s text generation capabilities.\n\n## If you are looking for custom support from the Hugging Face team\n\n<a target=\"_blank\" href=\"https:\/\/huggingface.co\/support\">\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https:\/\/cdn-media.huggingface.co\/marketing\/transformers\/new-support-improved.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n<\/a><br>\n\n## Quick tour\n\nTo immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:\n\n```python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\n\nThe second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \"positive\" with a confidence of 99.97%.\n\nMany NLP tasks have a pre-trained `pipeline` ready to go. For example, we can easily extract question answers given context:\n\n``` python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for question-answering\n>>> question_answerer = pipeline('question-answering')\n>>> question_answerer({\n...     'question': 'What is the name of the repository ?',\n...     'context': 'Pipeline has been included in the huggingface\/transformers repository'\n... })\n{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface\/transformers'}\n\n```\n\nIn addition to the answer, the pretrained model used here returned its confidence score, along with the start position and end position of the answer in the tokenized sentence. You can learn more about the tasks supported by the `pipeline` API in [this tutorial](https:\/\/huggingface.co\/docs\/transformers\/task_summary).\n\nTo download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```\nAnd here is the equivalent code for TensorFlow:\n```python\n>>> from transformers import AutoTokenizer, TFAutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n```\n\nThe tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.\n\nThe model itself is a regular [Pytorch `nn.Module`](https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Module) or a [TensorFlow `tf.keras.Model`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model) (depending on your backend) which you can use normally. [This tutorial](https:\/\/huggingface.co\/docs\/transformers\/training) explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our `Trainer` API to quickly fine-tune on a new dataset.\n\n## Why should I use transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, and audio tasks.\n    - Low barrier to entry for educators and practitioners.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Researchers can share trained models instead of always retraining.\n    - Practitioners can reduce compute time and production costs.\n    - Dozens of architectures with over 20,000 pretrained models, some in more than 100 languages.\n\n1. Choose the right framework for every part of a model's lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between TF2.0\/PyTorch\/JAX frameworks at will.\n    - Seamlessly pick the right framework for training, evaluation and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n## Why shouldn't I use transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions\/files.\n- The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library.\n- While we strive to present as many use cases as possible, the scripts in our [examples folder](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples) are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.\n\n## Installation\n\n### With pip\n\nThis repository is tested on Python 3.6+, Flax 0.3.2+, PyTorch 1.3.1+ and TensorFlow 2.3+.\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https:\/\/docs.python.org\/3\/library\/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https:\/\/packaging.python.org\/guides\/installing-using-pip-and-virtual-environments\/).\n\nFirst, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install at least one of Flax, PyTorch or TensorFlow.\nPlease refer to [TensorFlow installation page](https:\/\/www.tensorflow.org\/install\/), [PyTorch installation page](https:\/\/pytorch.org\/get-started\/locally\/#start-locally) and\/or [Flax](https:\/\/github.com\/google\/flax#quick-install) and [Jax](https:\/\/github.com\/google\/jax#installation) installation pages regarding the specific install command for your platform.\n\nWhen one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows:\n\n```bash\npip install transformers\n```\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https:\/\/huggingface.co\/docs\/transformers\/installation#installing-from-source).\n\n### With conda\n\nSince Transformers version v4.0.0, we now have a conda channel: `huggingface`.\n\n\ud83e\udd17 Transformers can be installed using conda as follows:\n\n```shell script\nconda install -c huggingface transformers\n```\n\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.\n\n## Model architectures\n\n**[All the model checkpoints](https:\/\/huggingface.co\/models)** provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co [model hub](https:\/\/huggingface.co) where they are uploaded directly by [users](https:\/\/huggingface.co\/users) and [organizations](https:\/\/huggingface.co\/organizations).\n\nCurrent number of checkpoints: ![](https:\/\/img.shields.io\/endpoint?url=https:\/\/huggingface.co\/api\/shields\/models&color=brightgreen)\n\n\ud83e\udd17 Transformers currently provides the following architectures (see [here](https:\/\/huggingface.co\/docs\/transformers\/model_summary) for a high-level summary of each them):\n\n1. **[ALBERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https:\/\/arxiv.org\/abs\/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n1. **[BART](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https:\/\/arxiv.org\/abs\/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n1. **[BARThez](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/barthez)** (from \u00c9cole polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https:\/\/arxiv.org\/abs\/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n1. **[BARTpho](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https:\/\/arxiv.org\/abs\/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n1. **[BEiT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https:\/\/arxiv.org\/abs\/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\n1. **[BERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https:\/\/arxiv.org\/abs\/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n1. **[BERTweet](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https:\/\/aclanthology.org\/2020.emnlp-demos.2\/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.\n1. **[BERT For Sequence Generation](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https:\/\/arxiv.org\/abs\/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n1. **[BigBird-RoBERTa](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https:\/\/arxiv.org\/abs\/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n1. **[BigBird-Pegasus](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https:\/\/arxiv.org\/abs\/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n1. **[Blenderbot](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https:\/\/arxiv.org\/abs\/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n1. **[BlenderbotSmall](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https:\/\/arxiv.org\/abs\/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n1. **[BORT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https:\/\/arxiv.org\/abs\/2010.10499) by Adrian de Wynter and Daniel J. Perry.\n1. **[ByT5](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https:\/\/arxiv.org\/abs\/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/camembert)** (from Inria\/Facebook\/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https:\/\/arxiv.org\/abs\/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n1. **[CANINE](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https:\/\/arxiv.org\/abs\/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n1. **[ConvNeXT](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https:\/\/arxiv.org\/abs\/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n1. **[CLIP](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https:\/\/arxiv.org\/abs\/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n1. **[ConvBERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https:\/\/arxiv.org\/abs\/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n1. **[CPM](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https:\/\/arxiv.org\/abs\/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n1. **[CTRL](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https:\/\/arxiv.org\/abs\/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\n1. **[Data2Vec](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https:\/\/arxiv.org\/abs\/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n1. **[DeBERTa](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https:\/\/arxiv.org\/abs\/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n1. **[DeBERTa-v2](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https:\/\/arxiv.org\/abs\/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n1. **[Decision Transformer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/decision_transformer)** (from Berkeley\/Facebook\/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https:\/\/arxiv.org\/abs\/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n1. **[DiT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https:\/\/arxiv.org\/abs\/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n1. **[DeiT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https:\/\/arxiv.org\/abs\/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou.\n1. **[DETR](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https:\/\/arxiv.org\/abs\/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n1. **[DialoGPT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https:\/\/arxiv.org\/abs\/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n1. **[DistilBERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https:\/\/arxiv.org\/abs\/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples\/research_projects\/distillation), RoBERTa into [DistilRoBERTa](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples\/research_projects\/distillation), Multilingual BERT into [DistilmBERT](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples\/research_projects\/distillation) and a German version of DistilBERT.\n1. **[DPR](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval\nfor Open-Domain Question Answering](https:\/\/arxiv.org\/abs\/2004.04906) by Vladimir Karpukhin, Barlas O\u011fuz, Sewon\nMin, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n1. **[DPT](https:\/\/huggingface.co\/docs\/transformers\/master\/model_doc\/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https:\/\/arxiv.org\/abs\/2103.13413) by Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n1. **[EncoderDecoder](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https:\/\/arxiv.org\/abs\/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n1. **[ELECTRA](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/electra)** (from Google Research\/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https:\/\/arxiv.org\/abs\/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n1. **[FlauBERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https:\/\/arxiv.org\/abs\/1912.05372) by Hang Le, Lo\u00efc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Beno\u00eet Crabb\u00e9, Laurent Besacier, Didier Schwab.\n1. **[FLAVA](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https:\/\/arxiv.org\/abs\/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.\n1. **[FNet](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https:\/\/arxiv.org\/abs\/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n1. **[Funnel Transformer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/funnel)** (from CMU\/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https:\/\/arxiv.org\/abs\/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n1. **[GLPN](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https:\/\/arxiv.org\/abs\/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n1. **[GPT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/openai-gpt)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https:\/\/blog.openai.com\/language-unsupervised\/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n1. **[GPT-2](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https:\/\/blog.openai.com\/better-language-models\/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\n1. **[GPT-J](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/gptj)** (from EleutherAI) released in the repository [kingoflolz\/mesh-transformer-jax](https:\/\/github.com\/kingoflolz\/mesh-transformer-jax\/) by Ben Wang and Aran Komatsuzaki.\n1. **[GPT Neo](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI\/gpt-neo](https:\/\/github.com\/EleutherAI\/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.\n1. **[Hubert](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https:\/\/arxiv.org\/abs\/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n1. **[I-BERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https:\/\/arxiv.org\/abs\/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n1. **[ImageGPT](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https:\/\/openai.com\/blog\/image-gpt\/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.\n1. **[LayoutLM](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https:\/\/arxiv.org\/abs\/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n1. **[LayoutLMv2](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https:\/\/arxiv.org\/abs\/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n1. **[LayoutXLM](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https:\/\/arxiv.org\/abs\/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n1. **[LED](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https:\/\/arxiv.org\/abs\/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n1. **[Longformer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https:\/\/arxiv.org\/abs\/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n1. **[LUKE](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https:\/\/arxiv.org\/abs\/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n1. **[mLUKE](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https:\/\/arxiv.org\/abs\/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n1. **[LXMERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https:\/\/arxiv.org\/abs\/1908.07490) by Hao Tan and Mohit Bansal.\n1. **[M2M100](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https:\/\/arxiv.org\/abs\/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n1. **[MarianMT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/marian)** Machine translation models trained using [OPUS](http:\/\/opus.nlpl.eu\/) data by J\u00f6rg Tiedemann. The [Marian Framework](https:\/\/marian-nmt.github.io\/) is being developed by the Microsoft Translator Team.\n1. **[MaskFormer](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https:\/\/arxiv.org\/abs\/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n1. **[MBart](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https:\/\/arxiv.org\/abs\/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n1. **[MBart-50](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https:\/\/arxiv.org\/abs\/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n1. **[Megatron-BERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https:\/\/arxiv.org\/abs\/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n1. **[Megatron-GPT2](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https:\/\/arxiv.org\/abs\/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n1. **[MPNet](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https:\/\/arxiv.org\/abs\/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n1. **[MT5](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https:\/\/arxiv.org\/abs\/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n1. **[Nystr\u00f6mformer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention](https:\/\/arxiv.org\/abs\/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n1. **[OPT](https:\/\/huggingface.co\/docs\/transformers\/master\/model_doc\/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https:\/\/arxiv.org\/abs\/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n1. **[Pegasus](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https:\/\/arxiv.org\/abs\/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\n1. **[Perceiver IO](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https:\/\/arxiv.org\/abs\/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Jo\u00e3o Carreira.\n1. **[PhoBERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https:\/\/www.aclweb.org\/anthology\/2020.findings-emnlp.92\/) by Dat Quoc Nguyen and Anh Tuan Nguyen.\n1. **[PLBart](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https:\/\/arxiv.org\/abs\/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n1. **[PoolFormer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https:\/\/arxiv.org\/abs\/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\n1. **[ProphetNet](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https:\/\/arxiv.org\/abs\/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n1. **[QDQBert](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https:\/\/arxiv.org\/abs\/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.\n1. **[REALM](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https:\/\/arxiv.org\/abs\/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.\n1. **[Reformer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https:\/\/arxiv.org\/abs\/2001.04451) by Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya.\n1. **[RemBERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https:\/\/arxiv.org\/abs\/2010.12821) by Hyung Won Chung, Thibault F\u00e9vry, Henry Tsai, M. Johnson, Sebastian Ruder.\n1. **[RegNet](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https:\/\/arxiv.org\/abs\/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Doll\u00e1r.\n1. **[ResNet](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https:\/\/arxiv.org\/abs\/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n1. **[RoBERTa](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https:\/\/arxiv.org\/abs\/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n1. **[RoFormer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https:\/\/arxiv.org\/abs\/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n1. **[SegFormer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https:\/\/arxiv.org\/abs\/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n1. **[SEW](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https:\/\/arxiv.org\/abs\/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n1. **[SEW-D](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https:\/\/arxiv.org\/abs\/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n1. **[SpeechToTextTransformer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https:\/\/arxiv.org\/abs\/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n1. **[SpeechToTextTransformer2](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https:\/\/arxiv.org\/abs\/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n1. **[Splinter](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https:\/\/arxiv.org\/abs\/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n1. **[SqueezeBert](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https:\/\/arxiv.org\/abs\/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[Swin Transformer](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https:\/\/arxiv.org\/abs\/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n1. **[T5](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https:\/\/arxiv.org\/abs\/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n1. **[T5v1.1](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/t5v1.1)** (from Google AI) released in the repository [google-research\/text-to-text-transfer-transformer](https:\/\/github.com\/google-research\/text-to-text-transfer-transformer\/blob\/main\/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n1. **[TAPAS](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https:\/\/arxiv.org\/abs\/2004.02349) by Jonathan Herzig, Pawe\u0142 Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno and Julian Martin Eisenschlos.\n1. **[TAPEX](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https:\/\/arxiv.org\/abs\/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n1. **[Transformer-XL](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/transfo-xl)** (from Google\/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https:\/\/arxiv.org\/abs\/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n1. **[TrOCR](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https:\/\/arxiv.org\/abs\/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n1. **[UniSpeech](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https:\/\/arxiv.org\/abs\/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n1. **[UniSpeechSat](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER\nAWARE PRE-TRAINING](https:\/\/arxiv.org\/abs\/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n1. **[VAN](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https:\/\/arxiv.org\/abs\/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n1. **[ViLT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/vilt)** (from NAVER AI Lab\/Kakao Enterprise\/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https:\/\/arxiv.org\/abs\/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.\n1. **[Vision Transformer (ViT)](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https:\/\/arxiv.org\/abs\/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[ViTMAE](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https:\/\/arxiv.org\/abs\/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick.\n1. **[VisualBERT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https:\/\/arxiv.org\/pdf\/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n1. **[WavLM](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https:\/\/arxiv.org\/abs\/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n1. **[Wav2Vec2](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https:\/\/arxiv.org\/abs\/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n1. **[Wav2Vec2Phoneme](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https:\/\/arxiv.org\/abs\/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n1. **[XGLM](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https:\/\/arxiv.org\/abs\/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n1. **[XLM](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https:\/\/arxiv.org\/abs\/1901.07291) by Guillaume Lample and Alexis Conneau.\n1. **[XLM-ProphetNet](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https:\/\/arxiv.org\/abs\/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n1. **[XLM-RoBERTa](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https:\/\/arxiv.org\/abs\/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\n1. **[XLM-RoBERTa-XL](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https:\/\/arxiv.org\/abs\/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n1. **[XLNet](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/xlnet)** (from Google\/CMU) released with the paper [\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https:\/\/arxiv.org\/abs\/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n1. **[XLSR-Wav2Vec2](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https:\/\/arxiv.org\/abs\/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n1. **[XLS-R](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https:\/\/arxiv.org\/abs\/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n1. **[YOLOS](https:\/\/huggingface.co\/docs\/transformers\/main\/model_doc\/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https:\/\/arxiv.org\/abs\/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n1. **[YOSO](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https:\/\/arxiv.org\/abs\/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n1. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](.\/templates) folder of the repository. Be sure to check the [contributing guidelines](.\/CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.\n\nTo check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the \ud83e\udd17 Tokenizers library, refer to [this table](https:\/\/huggingface.co\/docs\/transformers\/index#supported-frameworks).\n\nThese implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the [documentation](https:\/\/huggingface.co\/docs\/transformers\/examples).\n\n\n## Learn more\n\n| Section | Description |\n|-|-|\n| [Documentation](https:\/\/huggingface.co\/docs\/transformers\/) | Full API documentation and tutorials |\n| [Task summary](https:\/\/huggingface.co\/docs\/transformers\/task_summary) | Tasks supported by \ud83e\udd17 Transformers |\n| [Preprocessing tutorial](https:\/\/huggingface.co\/docs\/transformers\/preprocessing) | Using the `Tokenizer` class to prepare data for the models |\n| [Training and fine-tuning](https:\/\/huggingface.co\/docs\/transformers\/training) | Using the models provided by \ud83e\udd17 Transformers in a PyTorch\/TensorFlow training loop and the `Trainer` API |\n| [Quick tour: Fine-tuning\/usage scripts](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples) | Example scripts for fine-tuning models on a wide range of tasks |\n| [Model sharing and uploading](https:\/\/huggingface.co\/docs\/transformers\/model_sharing) | Upload and share your fine-tuned models with the community |\n| [Migration](https:\/\/huggingface.co\/docs\/transformers\/migration) | Migrate to \ud83e\udd17 Transformers from `pytorch-transformers` or `pytorch-pretrained-bert` |\n\n## Citation\n\nWe now have a [paper](https:\/\/www.aclweb.org\/anthology\/2020.emnlp-demos.6\/) you can cite for the \ud83e\udd17 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https:\/\/www.aclweb.org\/anthology\/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n","101":"# Awesome Machine Learning [![Awesome](https:\/\/cdn.rawgit.com\/sindresorhus\/awesome\/d7305f38d29fed78fa85652e3a63e154dd8e8829\/media\/badge.svg)](https:\/\/github.com\/sindresorhus\/awesome) [![Track Awesome List](https:\/\/www.trackawesomelist.com\/badge.svg)](https:\/\/www.trackawesomelist.com\/josephmisiti\/awesome-machine-learning\/)\n\nA curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by `awesome-php`.\n\n_If you want to contribute to this list (please do), send me a pull request or contact me [@josephmisiti](https:\/\/twitter.com\/josephmisiti)._\nAlso, a listed repository should be deprecated if:\n\n* Repository's owner explicitly says that \"this library is not maintained\".\n* Not committed for a long time (2~3 years).\n\nFurther resources:\n\n* For a list of free machine learning books available for download, go [here](https:\/\/github.com\/josephmisiti\/awesome-machine-learning\/blob\/master\/books.md).\n\n* For a list of professional machine learning events, go [here](https:\/\/github.com\/josephmisiti\/awesome-machine-learning\/blob\/master\/events.md).\n\n* For a list of (mostly) free machine learning courses available online, go [here](https:\/\/github.com\/josephmisiti\/awesome-machine-learning\/blob\/master\/courses.md).\n\n* For a list of blogs and newsletters on data science and machine learning, go [here](https:\/\/github.com\/josephmisiti\/awesome-machine-learning\/blob\/master\/blogs.md).\n\n* For a list of free-to-attend meetups and local events, go [here](https:\/\/github.com\/josephmisiti\/awesome-machine-learning\/blob\/master\/meetups.md).\n\n## Table of Contents\n\n### Frameworks and Libraries\n<!-- MarkdownTOC depth=4 -->\n\n- [Awesome Machine Learning ![Awesome](https:\/\/cdn.rawgit.com\/sindresorhus\/awesome\/d7305f38d29fed78fa85652e3a63e154dd8e8829\/media\/badge.svg)](#awesome-machine-learning-)\n  - [Table of Contents](#table-of-contents)\n    - [Frameworks and Libraries](#frameworks-and-libraries)\n    - [Tools](#tools)\n  - [APL](#apl)\n      - [General-Purpose Machine Learning](#apl-general-purpose-machine-learning)\n  - [C](#c)\n      - [General-Purpose Machine Learning](#c-general-purpose-machine-learning)\n      - [Computer Vision](#c-computer-vision)\n  - [C++](#cpp)\n      - [Computer Vision](#cpp-computer-vision)\n      - [General-Purpose Machine Learning](#cpp-general-purpose-machine-learning)\n      - [Natural Language Processing](#cpp-natural-language-processing)\n      - [Speech Recognition](#cpp-speech-recognition)\n      - [Sequence Analysis](#cpp-sequence-analysis)\n      - [Gesture Detection](#cpp-gesture-detection)\n  - [Common Lisp](#common-lisp)\n      - [General-Purpose Machine Learning](#common-lisp-general-purpose-machine-learning)\n  - [Clojure](#clojure)\n      - [Natural Language Processing](#clojure-natural-language-processing)\n      - [General-Purpose Machine Learning](#clojure-general-purpose-machine-learning)\n      - [Deep Learning](#clojure-deep-learning)\n      - [Data Analysis](#clojure-data-analysis--data-visualization)\n      - [Data Visualization](#clojure-data-visualization)\n      - [Interop](#clojure-interop)\n      - [Misc](#clojure-misc)\n      - [Extra](#clojure-extra)\n  - [Crystal](#crystal)\n      - [General-Purpose Machine Learning](#crystal-general-purpose-machine-learning)\n  - [Elixir](#elixir)\n      - [General-Purpose Machine Learning](#elixir-general-purpose-machine-learning)\n      - [Natural Language Processing](#elixir-natural-language-processing)\n  - [Erlang](#erlang)\n      - [General-Purpose Machine Learning](#erlang-general-purpose-machine-learning)\n  - [Fortran](#fortran)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning-8)\n      - [Data Analysis \/ Data Visualization](#data-analysis--data-visualization)\n  - [Go](#go)\n      - [Natural Language Processing](#go-natural-language-processing)\n      - [General-Purpose Machine Learning](#go-general-purpose-machine-learning)\n      - [Spatial analysis and geometry](#go-spatial-analysis-and-geometry)\n      - [Data Analysis \/ Data Visualization](#go-data-analysis--data-visualization)\n      - [Computer vision](#go-computer-vision)\n      - [Reinforcement learning](#go-reinforcement-learning)\n  - [Haskell](#haskell)\n      - [General-Purpose Machine Learning](#haskell-general-purpose-machine-learning)\n  - [Java](#java)\n      - [Natural Language Processing](#java-natural-language-processing)\n      - [General-Purpose Machine Learning](#java-general-purpose-machine-learning)\n      - [Speech Recognition](#java-speech-recognition)\n      - [Data Analysis \/ Data Visualization](#java-data-analysis--data-visualization)\n      - [Deep Learning](#java-deep-learning)\n  - [Javascript](#javascript)\n      - [Natural Language Processing](#javascript-natural-language-processing)\n      - [Data Analysis \/ Data Visualization](#javascript-data-analysis--data-visualization)\n      - [General-Purpose Machine Learning](#javascript-general-purpose-machine-learning)\n      - [Misc](#javascript-misc)\n      - [Demos and Scripts](#javascript-demos-and-scripts)\n  - [Julia](#julia)\n      - [General-Purpose Machine Learning](#julia-general-purpose-machine-learning)\n      - [Natural Language Processing](#julia-natural-language-processing)\n      - [Data Analysis \/ Data Visualization](#julia-data-analysis--data-visualization)\n      - [Misc Stuff \/ Presentations](#julia-misc-stuff--presentations)\n  - [Kotlin](#kotlin)\n      - [Deep Learning](#kotlin-deep-learning)\n  - [Lua](#lua)\n      - [General-Purpose Machine Learning](#lua-general-purpose-machine-learning)\n      - [Demos and Scripts](#lua-demos-and-scripts)\n  - [Matlab](#matlab)\n      - [Computer Vision](#matlab-computer-vision)\n      - [Natural Language Processing](#matlab-natural-language-processing)\n      - [General-Purpose Machine Learning](#matlab-general-purpose-machine-learning)\n      - [Data Analysis \/ Data Visualization](#matlab-data-analysis--data-visualization)\n  - [.NET](#net)\n      - [Computer Vision](#net-computer-vision)\n      - [Natural Language Processing](#net-natural-language-processing)\n      - [General-Purpose Machine Learning](#net-general-purpose-machine-learning)\n      - [Data Analysis \/ Data Visualization](#net-data-analysis--data-visualization)\n  - [Objective C](#objective-c)\n    - [General-Purpose Machine Learning](#objective-c-general-purpose-machine-learning)\n  - [OCaml](#ocaml)\n    - [General-Purpose Machine Learning](#ocaml-general-purpose-machine-learning)\n  - [Perl](#perl)\n    - [Data Analysis \/ Data Visualization](#perl-data-analysis--data-visualization)\n    - [General-Purpose Machine Learning](#perl-general-purpose-machine-learning)\n  - [Perl 6](#perl-6)\n    - [Data Analysis \/ Data Visualization](#perl-6-data-analysis--data-visualization)\n    - [General-Purpose Machine Learning](#perl-6-general-purpose-machine-learning)\n  - [PHP](#php)\n    - [Natural Language Processing](#php-natural-language-processing)\n    - [General-Purpose Machine Learning](#php-general-purpose-machine-learning)\n  - [Python](#python)\n      - [Computer Vision](#python-computer-vision)\n      - [Natural Language Processing](#python-natural-language-processing)\n      - [General-Purpose Machine Learning](#python-general-purpose-machine-learning)\n      - [Data Analysis \/ Data Visualization](#python-data-analysis--data-visualization)\n      - [Misc Scripts \/ iPython Notebooks \/ Codebases](#python-misc-scripts--ipython-notebooks--codebases)\n      - [Neural Networks](#python-neural-networks)\n      - [Survival Analysis](#python-survival-analysis)\n      - [Federated Learning](#federated-learning)\n      - [Kaggle Competition Source Code](#python-kaggle-competition-source-code)\n      - [Reinforcement Learning](#python-reinforcement-learning)\n  - [Ruby](#ruby)\n      - [Natural Language Processing](#ruby-natural-language-processing)\n      - [General-Purpose Machine Learning](#ruby-general-purpose-machine-learning)\n      - [Data Analysis \/ Data Visualization](#ruby-data-analysis--data-visualization)\n      - [Misc](#ruby-misc)\n  - [Rust](#rust)\n      - [General-Purpose Machine Learning](#general-purpose-machine-learning)\n  - [R](#r)\n      - [General-Purpose Machine Learning](#r-general-purpose-machine-learning)\n      - [Data Analysis \/ Data Visualization](#r-data-analysis--data-visualization)\n  - [SAS](#sas)\n      - [General-Purpose Machine Learning](#sas-general-purpose-machine-learning)\n      - [Data Analysis \/ Data Visualization](#sas-data-analysis--data-visualization)\n      - [Natural Language Processing](#sas-natural-language-processing)\n      - [Demos and Scripts](#sas-demos-and-scripts)\n  - [Scala](#scala)\n      - [Natural Language Processing](#scala-natural-language-processing)\n      - [Data Analysis \/ Data Visualization](#scala-data-analysis--data-visualization)\n      - [General-Purpose Machine Learning](#scala-general-purpose-machine-learning)\n  - [Scheme](#scheme)\n      - [Neural Networks](#scheme-neural-networks)\n  - [Swift](#swift)\n      - [General-Purpose Machine Learning](#swift-general-purpose-machine-learning)\n  - [TensorFlow](#tensorflow)\n      - [General-Purpose Machine Learning](#tensorflow-general-purpose-machine-learning)\n\n### [Tools](#tools-1)\n\n- [Neural Networks](#tools-neural-networks)\n- [Misc](#tools-misc)\n\n\n[Credits](#credits)\n\n<!-- \/MarkdownTOC -->\n\n<a name=\"apl\"><\/a>\n## APL\n\n<a name=\"apl-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n* [naive-apl](https:\/\/github.com\/mattcunningham\/naive-apl) - Naive Bayesian Classifier implementation in APL. **[Deprecated]**\n\n<a name=\"c\"><\/a>\n## C\n\n<a name=\"c-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n* [Darknet](https:\/\/github.com\/pjreddie\/darknet) - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.\n* [Recommender](https:\/\/github.com\/GHamrouni\/Recommender) - A C library for product recommendations\/suggestions using collaborative filtering (CF).\n* [Hybrid Recommender System](https:\/\/github.com\/SeniorSA\/hybrid-rs-trainner) - A hybrid recommender system based upon scikit-learn algorithms. **[Deprecated]**\n* [neonrvm](https:\/\/github.com\/siavashserver\/neonrvm) - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.\n* [cONNXr](https:\/\/github.com\/alrevuelta\/cONNXr) - An `ONNX` runtime written in pure C (99) with zero dependencies focused on small embedded devices. Run inference on your machine learning models no matter which framework you train it with. Easy to install and compiles everywhere, even in very old devices.\n* [libonnx](https:\/\/github.com\/xboot\/libonnx) - A lightweight, portable pure C99 onnx inference engine for embedded devices with hardware acceleration support.\n\n<a name=\"c-computer-vision\"><\/a>\n#### Computer Vision\n\n* [CCV](https:\/\/github.com\/liuliu\/ccv) - C-based\/Cached\/Core Computer Vision Library, A Modern Computer Vision Library.\n* [VLFeat](http:\/\/www.vlfeat.org\/) - VLFeat is an open and portable library of computer vision algorithms, which has a Matlab toolbox.\n\n<a name=\"cpp\"><\/a>\n## C++\n\n<a name=\"cpp-computer-vision\"><\/a>\n#### Computer Vision\n\n* [DLib](http:\/\/dlib.net\/imaging.html) - DLib has C++ and Python interfaces for face detection and training general object detectors.\n* [EBLearn](http:\/\/eblearn.sourceforge.net\/) - Eblearn is an object-oriented C++ library that implements various machine learning models **[Deprecated]**\n* [OpenCV](https:\/\/opencv.org) - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.\n* [VIGRA](https:\/\/github.com\/ukoethe\/vigra) - VIGRA is a genertic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.\n* [Openpose](https:\/\/github.com\/CMU-Perceptual-Computing-Lab\/openpose) - A real-time multi-person keypoint detection library for body, face, hands, and foot estimation\n\n<a name=\"cpp-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [BanditLib](https:\/\/github.com\/jkomiyama\/banditlib) - A simple Multi-armed Bandit library. **[Deprecated]**\n* [Caffe](https:\/\/github.com\/BVLC\/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]\n* [CatBoost](https:\/\/github.com\/catboost\/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, contains fast inference implementation and supports CPU and GPU (even multi-GPU) computation.\n* [CNTK](https:\/\/github.com\/Microsoft\/CNTK) - The Computational Network Toolkit (CNTK) by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.\n* [CUDA](https:\/\/code.google.com\/p\/cuda-convnet\/) - This is a fast C++\/CUDA implementation of convolutional [DEEP LEARNING]\n* [DeepDetect](https:\/\/github.com\/jolibrain\/deepdetect) - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.\n* [Distributed Machine learning Tool Kit (DMTK)](http:\/\/www.dmtk.io\/) - A distributed machine learning (parameter server) framework by Microsoft. Enables training models on large data sets across multiple machines. Current tools bundled with it include: LightLDA and Distributed (Multisense) Word Embedding.\n* [DLib](http:\/\/dlib.net\/ml.html) - A suite of ML tools designed to be easy to imbed in other applications.\n* [DSSTNE](https:\/\/github.com\/amznlabs\/amazon-dsstne) - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.\n* [DyNet](https:\/\/github.com\/clab\/dynet) - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.\n* [Fido](https:\/\/github.com\/FidoProject\/Fido) - A highly-modular C++ machine learning library for embedded electronics and robotics.\n* [igraph](http:\/\/igraph.org\/) - General purpose graph library.\n* [Intel\u00ae oneAPI Data Analytics Library](https:\/\/github.com\/oneapi-src\/oneDAL) - A high performance software library developed by Intel and optimized for Intel's architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.\n* [LightGBM](https:\/\/github.com\/Microsoft\/LightGBM) - Microsoft's fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n* [libfm](https:\/\/github.com\/srendle\/libfm) - A generic approach that allows to mimic most factorization models by feature engineering.\n* [MLDB](https:\/\/mldb.ai) - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.\n* [mlpack](https:\/\/www.mlpack.org\/) - A scalable C++ machine learning library.\n* [MXNet](https:\/\/github.com\/apache\/incubator-mxnet) - Lightweight, Portable, Flexible Distributed\/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [ParaMonte](https:\/\/github.com\/cdslaborg\/paramonte) - A general-purpose library with C\/C++ interface for Bayesian data analysis and visualization via serial\/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https:\/\/www.cdslab.org\/paramonte\/).\n* [proNet-core](https:\/\/github.com\/cnclabs\/proNet-core) - A general-purpose network embedding framework: pair-wise representations optimization Network Edit.\n* [PyCaret](https:\/\/github.com\/pycaret\/pycaret) - An open-source, low-code machine learning library in Python that automates machine learning workflows.\n* [PyCUDA](https:\/\/mathema.tician.de\/software\/pycuda\/) - Python interface to CUDA\n* [ROOT](https:\/\/root.cern.ch) - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.\n* [shark](http:\/\/image.diku.dk\/shark\/sphinx_pages\/build\/html\/index.html) - A fast, modular, feature-rich open-source C++ machine learning library.\n* [Shogun](https:\/\/github.com\/shogun-toolbox\/shogun) - The Shogun Machine Learning Toolbox.\n* [sofia-ml](https:\/\/code.google.com\/archive\/p\/sofia-ml) - Suite of fast incremental algorithms.\n* [Stan](http:\/\/mc-stan.org\/) - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling.\n* [Timbl](https:\/\/languagemachines.github.io\/timbl\/) - A software package\/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.\n* [Vowpal Wabbit (VW)](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit) - A fast out-of-core learning system.\n* [Warp-CTC](https:\/\/github.com\/baidu-research\/warp-ctc) - A fast parallel implementation of Connectionist Temporal Classification (CTC), on both CPU and GPU.\n* [XGBoost](https:\/\/github.com\/dmlc\/xgboost) - A parallelized optimized general purpose gradient boosting library.\n* [ThunderGBM](https:\/\/github.com\/Xtra-Computing\/thundergbm) - A fast library for GBDTs and Random Forests on GPUs.\n* [ThunderSVM](https:\/\/github.com\/Xtra-Computing\/thundersvm) - A fast SVM library on GPUs and CPUs.\n* [LKYDeepNN](https:\/\/github.com\/mosdeo\/LKYDeepNN) - A header-only C++11 Neural Network library. Low dependency, native traditional chinese document.\n* [xLearn](https:\/\/github.com\/aksnzhy\/xlearn) - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertising and recommender systems.\n* [Featuretools](https:\/\/github.com\/featuretools\/featuretools) - A library for automated feature engineering. It excels at transforming transactional and relational datasets into feature matrices for machine learning using reusable feature engineering \"primitives\".\n* [skynet](https:\/\/github.com\/Tyill\/skynet) - A library for learning neural networks, has C-interface, net set in JSON. Written in C++ with bindings in Python, C++ and C#.\n* [Feast](https:\/\/github.com\/gojek\/feast) - A feature store for the management, discovery, and access of machine learning features. Feast provides a consistent view of feature data for both model training and model serving.\n* [Hopsworks](https:\/\/github.com\/logicalclocks\/hopsworks) - A data-intensive platform for AI with the industry's first open-source feature store. The Hopsworks Feature Store provides both a feature warehouse for training and batch based on Apache Hive and a feature serving database, based on MySQL Cluster, for online applications.\n* [Polyaxon](https:\/\/github.com\/polyaxon\/polyaxon) - A platform for reproducible and scalable machine learning and deep learning.\n* [QuestDB](https:\/\/questdb.io\/) A relational column-oriented database designed for real-time analytics on time series and event data.\n\n<a name=\"cpp-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [BLLIP Parser](https:\/\/github.com\/BLLIP\/bllip-parser) - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).\n* [colibri-core](https:\/\/github.com\/proycon\/colibri-core) - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n* [CRF++](https:\/\/taku910.github.io\/crfpp\/) - Open source implementation of Conditional Random Fields (CRFs) for segmenting\/labeling sequential data & other Natural Language Processing tasks. **[Deprecated]**\n* [CRFsuite](http:\/\/www.chokkan.org\/software\/crfsuite\/) - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data. **[Deprecated]**\n* [frog](https:\/\/github.com\/LanguageMachines\/frog) - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.\n* [libfolia](https:\/\/github.com\/LanguageMachines\/libfolia) - C++ library for the [FoLiA format](https:\/\/proycon.github.io\/folia\/)\n* [MeTA](https:\/\/github.com\/meta-toolkit\/meta) - [MeTA : ModErn Text Analysis](https:\/\/meta-toolkit.org\/) is a C++ Data Sciences Toolkit that facilitates mining big text data.\n* [MIT Information Extraction Toolkit](https:\/\/github.com\/mit-nlp\/MITIE) - C, C++, and Python tools for named entity recognition and relation extraction\n* [ucto](https:\/\/github.com\/LanguageMachines\/ucto) - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.\n\n<a name=\"cpp-speech-recognition\"><\/a>\n#### Speech Recognition\n* [Kaldi](https:\/\/github.com\/kaldi-asr\/kaldi) - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.\n\n<a name=\"cpp-sequence-analysis\"><\/a>\n#### Sequence Analysis\n* [ToPS](https:\/\/github.com\/ayoshiaki\/tops) - This is an object-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet. **[Deprecated]**\n\n<a name=\"cpp-gesture-detection\"><\/a>\n#### Gesture Detection\n* [grt](https:\/\/github.com\/nickgillian\/grt) - The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.\n\n<a name=\"common-lisp\"><\/a>\n## Common Lisp\n\n<a name=\"common-lisp-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [mgl](https:\/\/github.com\/melisgl\/mgl\/) - Neural networks (boltzmann machines, feed-forward and recurrent nets), Gaussian Processes.\n* [mgl-gpr](https:\/\/github.com\/melisgl\/mgl-gpr\/) - Evolutionary algorithms. **[Deprecated]**\n* [cl-libsvm](https:\/\/github.com\/melisgl\/cl-libsvm\/) - Wrapper for the libsvm support vector machine library. **[Deprecated]**\n* [cl-online-learning](https:\/\/github.com\/masatoi\/cl-online-learning) - Online learning algorithms (Perceptron, AROW, SCW, Logistic Regression).\n* [cl-random-forest](https:\/\/github.com\/masatoi\/cl-random-forest) - Implementation of Random Forest in Common Lisp.\n\n<a name=\"clojure\"><\/a>\n## Clojure\n\n<a name=\"clojure-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [Clojure-openNLP](https:\/\/github.com\/dakrone\/clojure-opennlp) - Natural Language Processing in Clojure (opennlp).\n* [Infections-clj](https:\/\/github.com\/r0man\/inflections-clj) - Rails-like inflection library for Clojure and ClojureScript.\n\n<a name=\"clojure-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [tech.ml](https:\/\/github.com\/techascent\/tech.ml) - A machine learning platform based on tech.ml.dataset, supporting not just ml algorithms, but also relevant ETL processing; wraps multiple machine learning libraries\n* [clj-ml](https:\/\/github.com\/joshuaeckroth\/clj-ml\/) - A machine learning library for Clojure built on top of Weka and friends.\n* [clj-boost](https:\/\/gitlab.com\/alanmarazzi\/clj-boost) - Wrapper for XGBoost\n* [Touchstone](https:\/\/github.com\/ptaoussanis\/touchstone) - Clojure A\/B testing library.\n* [Clojush](https:\/\/github.com\/lspector\/Clojush) - The Push programming language and the PushGP genetic programming system implemented in Clojure.\n* [lambda-ml](https:\/\/github.com\/cloudkj\/lambda-ml) - Simple, concise implementations of machine learning techniques and utilities in Clojure.\n* [Infer](https:\/\/github.com\/aria42\/infer) - Inference and machine learning in Clojure. **[Deprecated]**\n* [Encog](https:\/\/github.com\/jimpil\/enclog) - Clojure wrapper for Encog (v3) (Machine-Learning framework that specializes in neural-nets). **[Deprecated]**\n* [Fungp](https:\/\/github.com\/vollmerm\/fungp) - A genetic programming library for Clojure. **[Deprecated]**\n* [Statistiker](https:\/\/github.com\/clojurewerkz\/statistiker) - Basic Machine Learning algorithms in Clojure. **[Deprecated]**\n* [clortex](https:\/\/github.com\/htm-community\/clortex) - General Machine Learning library using Numenta\u2019s Cortical Learning Algorithm. **[Deprecated]**\n* [comportex](https:\/\/github.com\/htm-community\/comportex) - Functionally composable Machine Learning library using Numenta\u2019s Cortical Learning Algorithm. **[Deprecated]**\n\n<a name=\"clojure-deep-learning\"><\/a>\n#### Deep Learning\n* [MXNet](https:\/\/mxnet.apache.org\/versions\/1.7.0\/api\/clojure) - Bindings to Apache MXNet - part of the MXNet project\n* [Deep Diamond](https:\/\/github.com\/uncomplicate\/deep-diamond) - A fast Clojure Tensor & Deep Learning library\n* [jutsu.ai](https:\/\/github.com\/hswick\/jutsu.ai) - Clojure wrapper for deeplearning4j with some added syntactic sugar.\n* [cortex](https:\/\/github.com\/originrose\/cortex) - Neural networks, regression and feature learning in Clojure.\n* [Flare](https:\/\/github.com\/aria42\/flare) - Dynamic Tensor Graph library in Clojure (think PyTorch, DynNet, etc.)\n* [dl4clj](https:\/\/github.com\/yetanalytics\/dl4clj) - Clojure wrapper for Deeplearning4j.\n\n<a name=\"clojure-data-analysis--data-visualization\"><\/a>\n#### Data Analysis\n* [tech.ml.dataset](https:\/\/github.com\/techascent\/tech.ml.dataset) - Clojure dataframe library and pipeline for data processing and machine learning\n* [Tablecloth](https:\/\/github.com\/scicloj\/tablecloth) - A dataframe grammar wrapping tech.ml.dataset, inspired by several R libraries\n* [Panthera](https:\/\/github.com\/alanmarazzi\/panthera) - Clojure API wrapping Python's Pandas library\n* [Incanter](http:\/\/incanter.org\/) - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.\n* [PigPen](https:\/\/github.com\/Netflix\/PigPen) - Map-Reduce for Clojure.\n* [Geni](https:\/\/github.com\/zero-one-group\/geni) - a Clojure dataframe library that runs on Apache Spark\n\n<a name=\"clojure-data-visualization\"><\/a>\n#### Data Visualization\n* [Hanami](https:\/\/github.com\/jsa-aerial\/hanami) : Clojure(Script) library and framework for creating interactive visualization applications based in Vega-Lite (VGL) and\/or Vega (VG) specifications. Automatic framing and layouts along with a powerful templating system for abstracting visualization specs\n* [Saite](https:\/\/github.com\/jsa-aerial\/saite) -  Clojure(Script) client\/server application for dynamic interactive explorations and the creation of live shareable documents capturing them using Vega\/Vega-Lite, CodeMirror, markdown, and LaTeX\n* [Oz](https:\/\/github.com\/metasoarous\/oz) - Data visualisation using Vega\/Vega-Lite and Hiccup, and a live-reload platform for literate-programming\n* [Envision](https:\/\/github.com\/clojurewerkz\/envision) - Clojure Data Visualisation library, based on Statistiker and D3.\n* [Pink Gorilla Notebook](https:\/\/github.com\/pink-gorilla\/gorilla-notebook) - A Clojure\/Clojurescript notebook application\/-library based on Gorilla-REPL\n* [clojupyter](https:\/\/github.com\/clojupyter\/clojupyter) -  A Jupyter kernel for Clojure - run Clojure code in Jupyter Lab, Notebook and Console.\n* [notespace](https:\/\/github.com\/scicloj\/notespace) - Notebook experience in your Clojure namespace\n* [Delight](https:\/\/github.com\/datamechanics\/delight) - A listener that streams your spark events logs to delight, a free and improved spark UI\n\n<a name=\"clojure-interop\"><\/a>\n#### Interop\n\n* [Java Interop](https:\/\/clojure.org\/reference\/java_interop) - Clojure has Native Java Interop from which Java's ML ecosystem can be accessed\n* [JavaScript Interop](https:\/\/clojurescript.org\/reference\/javascript-api) - ClojureScript has Native JavaScript Interop from which JavaScript's ML ecosystem can be accessed\n* [Libpython-clj](https:\/\/github.com\/clj-python\/libpython-clj) - Interop with Python\n* [ClojisR](https:\/\/github.com\/scicloj\/clojisr) - Interop with R and Renjin (R on the JVM)\n\n<a name=\"clojure-misc\"><\/a>\n#### Misc\n* [Neanderthal](https:\/\/neanderthal.uncomplicate.org\/) - Fast Clojure Matrix Library (native CPU, GPU, OpenCL, CUDA)\n* [kixistats](https:\/\/github.com\/MastodonC\/kixi.stats) - A library of statistical distribution sampling and transducing functions\n* [fastmath](https:\/\/github.com\/generateme\/fastmath) - A collection of functions for mathematical and statistical computing, macine learning, etc., wrapping several JVM libraries\n* [matlib](https:\/\/github.com\/atisharma\/matlib) - a Clojure library of optimisation and control theory tools and convenience functions based on Neanderthal.\n\n<a name=\"clojure-extra\"><\/a>\n#### Extra\n* [Scicloj](https:\/\/scicloj.github.io\/pages\/libraries\/) - Curated list of ML related resources for Clojure.\n\n<a name=\"crystal\"><\/a>\n## Crystal\n\n<a name=\"crystal-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [machine](https:\/\/github.com\/mathieulaporte\/machine) - Simple machine learning algorithm.\n* [crystal-fann](https:\/\/github.com\/NeuraLegion\/crystal-fann) - FANN (Fast Artificial Neural Network) binding.\n\n<a name=\"elixir\"><\/a>\n## Elixir\n\n<a name=\"elixir-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Simple Bayes](https:\/\/github.com\/fredwu\/simple_bayes) - A Simple Bayes \/ Naive Bayes implementation in Elixir.\n* [emel](https:\/\/github.com\/mrdimosthenis\/emel) - A simple and functional machine learning library written in Elixir.\n* [Tensorflex](https:\/\/github.com\/anshuman23\/tensorflex) - Tensorflow bindings for the Elixir programming language.\n\n<a name=\"elixir-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [Stemmer](https:\/\/github.com\/fredwu\/stemmer) - An English (Porter2) stemming implementation in Elixir.\n\n<a name=\"erlang\"><\/a>\n## Erlang\n\n<a name=\"erlang-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Disco](https:\/\/github.com\/discoproject\/disco\/) - Map Reduce in Erlang. **[Deprecated]**\n\n<a name=\"fortran\"><\/a>\n## Fortran\n\n<a name=\"fortran-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [neural-fortran](https:\/\/github.com\/modern-fortran\/neural-fortran) - A parallel neural net microframework.\nRead the paper [here](https:\/\/arxiv.org\/abs\/1902.06714).\n\n<a name=\"fortran-data-analysis-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [ParaMonte](https:\/\/github.com\/cdslaborg\/paramonte) - A general-purpose Fortran library for Bayesian data analysis and visualization via serial\/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https:\/\/www.cdslab.org\/paramonte\/).\n\n<a name=\"go\"><\/a>\n## Go\n\n<a name=\"go-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [snowball](https:\/\/github.com\/tebeka\/snowball) - Snowball Stemmer for Go.\n* [word-embedding](https:\/\/github.com\/ynqa\/word-embedding) - Word Embeddings: the full implementation of word2vec, GloVe in Go.\n* [sentences](https:\/\/github.com\/neurosnap\/sentences) - Golang implementation of Punkt sentence tokenizer.\n* [go-ngram](https:\/\/github.com\/Lazin\/go-ngram) - In-memory n-gram index with compression. *[Deprecated]*\n* [paicehusk](https:\/\/github.com\/Rookii\/paicehusk) - Golang implementation of the Paice\/Husk Stemming Algorithm. *[Deprecated]*\n* [go-porterstemmer](https:\/\/github.com\/reiver\/go-porterstemmer) - A native Go clean room implementation of the Porter Stemming algorithm. **[Deprecated]**\n\n<a name=\"go-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [birdland](https:\/\/github.com\/rlouf\/birdland) - A recommendation library in Go.\n* [eaopt](https:\/\/github.com\/MaxHalford\/eaopt) - An evolutionary optimization library.\n* [leaves](https:\/\/github.com\/dmitryikh\/leaves) - A pure Go implementation of the prediction part of GBRTs, including XGBoost and LightGBM.\n* [gobrain](https:\/\/github.com\/goml\/gobrain) - Neural Networks written in Go.\n* [go-featureprocessing](https:\/\/github.com\/nikolaydubina\/go-featureprocessing) - Fast and convenient feature processing for low latency machine learning in Go.\n* [go-mxnet-predictor](https:\/\/github.com\/songtianyi\/go-mxnet-predictor) - Go binding for MXNet c_predict_api to do inference with a pre-trained model.\n* [go-ml-benchmarks](https:\/\/github.com\/nikolaydubina\/go-ml-benchmarks) \u2014 benchmarks of machine learning inference for Go\n* [go-ml-transpiler](https:\/\/github.com\/znly\/go-ml-transpiler) - An open source Go transpiler for machine learning models.\n* [golearn](https:\/\/github.com\/sjwhitworth\/golearn) - Machine learning for Go.\n* [goml](https:\/\/github.com\/cdipaolo\/goml) - Machine learning library written in pure Go.\n* [gorgonia](https:\/\/github.com\/gorgonia\/gorgonia) - Deep learning in Go.\n* [goro](https:\/\/github.com\/aunum\/goro) - A high-level machine learning library in the vein of Keras.\n* [gorse](https:\/\/github.com\/zhenghaoz\/gorse) - An offline recommender system backend based on collaborative filtering written in Go.\n* [therfoo](https:\/\/github.com\/therfoo\/therfoo) - An embedded deep learning library for Go.\n* [neat](https:\/\/github.com\/jinyeom\/neat) - Plug-and-play, parallel Go framework for NeuroEvolution of Augmenting Topologies (NEAT). **[Deprecated]**\n* [go-pr](https:\/\/github.com\/daviddengcn\/go-pr) - Pattern recognition package in Go lang. **[Deprecated]**\n* [go-ml](https:\/\/github.com\/alonsovidales\/go_ml) - Linear \/ Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution. **[Deprecated]**\n* [GoNN](https:\/\/github.com\/fxsjy\/gonn) - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN. **[Deprecated]**\n* [bayesian](https:\/\/github.com\/jbrukh\/bayesian) - Naive Bayesian Classification for Golang. **[Deprecated]**\n* [go-galib](https:\/\/github.com\/thoj\/go-galib) - Genetic Algorithms library written in Go \/ Golang. **[Deprecated]**\n* [Cloudforest](https:\/\/github.com\/ryanbressler\/CloudForest) - Ensembles of decision trees in Go\/Golang. **[Deprecated]**\n* [go-dnn](https:\/\/github.com\/sudachen\/go-dnn) - Deep Neural Networks for Golang (powered by MXNet)\n\n<a name=\"go-spatial-analysis-and-geometry\"><\/a>\n#### Spatial analysis and geometry\n\n* [go-geom](https:\/\/github.com\/twpayne\/go-geom) - Go library to handle geometries.\n* [gogeo](https:\/\/github.com\/golang\/geo) - Spherical geometry in Go.\n\n<a name=\"go-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [dataframe-go](https:\/\/github.com\/rocketlaunchr\/dataframe-go) - Dataframes for machine-learning and statistics (similar to pandas).\n* [gota](https:\/\/github.com\/go-gota\/gota) - Dataframes.\n* [gonum\/mat](https:\/\/godoc.org\/gonum.org\/v1\/gonum\/mat) - A linear algebra package for Go.\n* [gonum\/optimize](https:\/\/godoc.org\/gonum.org\/v1\/gonum\/optimize) - Implementations of optimization algorithms.\n* [gonum\/plot](https:\/\/godoc.org\/gonum.org\/v1\/plot) - A plotting library.\n* [gonum\/stat](https:\/\/godoc.org\/gonum.org\/v1\/gonum\/stat) - A statistics library.\n* [SVGo](https:\/\/github.com\/ajstarks\/svgo) - The Go Language library for SVG generation.\n* [glot](https:\/\/github.com\/arafatk\/glot) - Glot is a plotting library for Golang built on top of gnuplot.\n* [globe](https:\/\/github.com\/mmcloughlin\/globe) - Globe wireframe visualization.\n* [gonum\/graph](https:\/\/godoc.org\/gonum.org\/v1\/gonum\/graph) - General-purpose graph library.\n* [go-graph](https:\/\/github.com\/StepLg\/go-graph) - Graph library for Go\/Golang language. **[Deprecated]**\n* [RF](https:\/\/github.com\/fxsjy\/RF.go) - Random forests implementation in Go. **[Deprecated]**\n\n<a name=\"go-computer-vision\"><\/a>\n#### Computer vision\n\n* [GoCV](https:\/\/github.com\/hybridgroup\/gocv) - Package for computer vision using OpenCV 4 and beyond.\n\n<a name=\"go-reinforcement-learning\"><\/a>\n#### Reinforcement learning\n\n* [gold](https:\/\/github.com\/aunum\/gold) - A reinforcement learning library.\n\n<a name=\"haskell\"><\/a>\n## Haskell\n\n<a name=\"haskell-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n* [haskell-ml](https:\/\/github.com\/ajtulloch\/haskell-ml) - Haskell implementations of various ML algorithms. **[Deprecated]**\n* [HLearn](https:\/\/github.com\/mikeizbicki\/HLearn) - a suite of libraries for interpreting machine learning models according to their algebraic structure. **[Deprecated]**\n* [hnn](https:\/\/github.com\/alpmestan\/HNN) - Haskell Neural Network library.\n* [hopfield-networks](https:\/\/github.com\/ajtulloch\/hopfield-networks) - Hopfield Networks for unsupervised learning in Haskell. **[Deprecated]**\n* [DNNGraph](https:\/\/github.com\/ajtulloch\/dnngraph) - A DSL for deep neural networks. **[Deprecated]**\n* [LambdaNet](https:\/\/github.com\/jbarrow\/LambdaNet) - Configurable Neural Networks in Haskell. **[Deprecated]**\n\n<a name=\"java\"><\/a>\n## Java\n\n<a name=\"java-natural-language-processing\"><\/a>\n#### Natural Language Processing\n* [Cortical.io](https:\/\/www.cortical.io\/) - Retina: an API performing complex NLP operations (disambiguation, classification, streaming text filtering, etc...) as quickly and intuitively as the brain.\n* [IRIS](https:\/\/github.com\/cortical-io\/Iris) - [Cortical.io's](https:\/\/cortical.io) FREE NLP, Retina API Analysis Tool (written in JavaFX!) - [See the Tutorial Video](https:\/\/www.youtube.com\/watch?v=CsF4pd7fGF0).\n* [CoreNLP](https:\/\/nlp.stanford.edu\/software\/corenlp.shtml) - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words.\n* [Stanford Parser](https:\/\/nlp.stanford.edu\/software\/lex-parser.shtml) - A natural language parser is a program that works out the grammatical structure of sentences.\n* [Stanford POS Tagger](https:\/\/nlp.stanford.edu\/software\/tagger.shtml) - A Part-Of-Speech Tagger (POS Tagger).\n* [Stanford Name Entity Recognizer](https:\/\/nlp.stanford.edu\/software\/CRF-NER.shtml) - Stanford NER is a Java implementation of a Named Entity Recognizer.\n* [Stanford Word Segmenter](https:\/\/nlp.stanford.edu\/software\/segmenter.shtml) - Tokenization of raw text is a standard pre-processing step for many NLP tasks.\n* [Tregex, Tsurgeon and Semgrex](https:\/\/nlp.stanford.edu\/software\/tregex.shtml) - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for \"tree regular expressions\").\n* [Stanford Phrasal: A Phrase-Based Translation System](https:\/\/nlp.stanford.edu\/phrasal\/)\n* [Stanford English Tokenizer](https:\/\/nlp.stanford.edu\/software\/tokenizer.shtml) - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.\n* [Stanford Tokens Regex](https:\/\/nlp.stanford.edu\/software\/tokensregex.shtml) - A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\".\n* [Stanford Temporal Tagger](https:\/\/nlp.stanford.edu\/software\/sutime.shtml) - SUTime is a library for recognizing and normalizing time expressions.\n* [Stanford SPIED](https:\/\/nlp.stanford.edu\/software\/patternslearning.shtml) - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion.\n* [Twitter Text Java](https:\/\/github.com\/twitter\/twitter-text\/tree\/master\/java) - A Java implementation of Twitter's text processing library.\n* [MALLET](http:\/\/mallet.cs.umass.edu\/) - A Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\n* [OpenNLP](https:\/\/opennlp.apache.org\/) - a machine learning based toolkit for the processing of natural language text.\n* [LingPipe](http:\/\/alias-i.com\/lingpipe\/index.html) - A tool kit for processing text using computational linguistics.\n* [ClearTK](https:\/\/github.com\/ClearTK\/cleartk) - ClearTK provides a framework for developing statistical natural language processing (NLP) components in Java and is built on top of Apache UIMA. **[Deprecated]**\n* [Apache cTAKES](https:\/\/ctakes.apache.org\/) - Apache Clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.\n* [NLP4J](https:\/\/github.com\/emorynlp\/nlp4j) - The NLP4J project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. **[Deprecated]**\n* [CogcompNLP](https:\/\/github.com\/CogComp\/cogcomp-nlp) - This project collects a number of core libraries for Natural Language Processing (NLP) developed in the University of Illinois' Cognitive Computation Group, for example `illinois-core-utilities` which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, `illinois-edison` a library for feature extraction from illinois-core-utilities data structures and many other packages.\n\n<a name=\"java-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [aerosolve](https:\/\/github.com\/airbnb\/aerosolve) - A machine learning library by Airbnb designed from the ground up to be human friendly.\n* [AMIDST Toolbox](http:\/\/www.amidsttoolbox.com\/) - A Java Toolbox for Scalable Probabilistic Machine Learning.\n* [Datumbox](https:\/\/github.com\/datumbox\/datumbox-framework) - Machine Learning framework for rapid development of Machine Learning and Statistical applications.\n* [ELKI](https:\/\/elki-project.github.io\/) - Java toolkit for data mining. (unsupervised: clustering, outlier detection etc.)\n* [Encog](https:\/\/github.com\/encog\/encog-java-core) - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.\n* [FlinkML in Apache Flink](https:\/\/ci.apache.org\/projects\/flink\/flink-docs-master\/dev\/libs\/ml\/index.html) - Distributed machine learning library in Flink.\n* [H2O](https:\/\/github.com\/h2oai\/h2o-3) - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST\/JSON.\n* [htm.java](https:\/\/github.com\/numenta\/htm.java) - General Machine Learning library using Numenta\u2019s Cortical Learning Algorithm.\n* [liblinear-java](https:\/\/github.com\/bwaldvogel\/liblinear-java) - Java version of liblinear.\n* [Mahout](https:\/\/github.com\/apache\/mahout) - Distributed machine learning.\n* [Meka](http:\/\/meka.sourceforge.net\/) - An open source implementation of methods for multi-label classification and evaluation (extension to Weka).\n* [MLlib in Apache Spark](https:\/\/spark.apache.org\/docs\/latest\/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https:\/\/github.com\/Hydrospheredata\/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Neuroph](http:\/\/neuroph.sourceforge.net\/) - Neuroph is lightweight Java neural network framework\n* [ORYX](https:\/\/github.com\/oryxproject\/oryx) - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.\n* [Samoa](https:\/\/samoa.incubator.apache.org\/) SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.\n* [RankLib](https:\/\/sourceforge.net\/p\/lemur\/wiki\/RankLib\/) - RankLib is a library of learning to rank algorithms. **[Deprecated]**\n* [rapaio](https:\/\/github.com\/padreati\/rapaio) - statistics, data mining and machine learning toolbox in Java.\n* [RapidMiner](https:\/\/rapidminer.com) - RapidMiner integration into Java code.\n* [Stanford Classifier](https:\/\/nlp.stanford.edu\/software\/classifier.shtml) - A classifier is a machine learning tool that will take data items and place them into one of k classes.\n* [Smile](https:\/\/haifengl.github.io\/) - Statistical Machine Intelligence & Learning Engine.\n* [SystemML](https:\/\/github.com\/apache\/systemml) - flexible, scalable machine learning (ML) language.\n* [Weka](https:\/\/www.cs.waikato.ac.nz\/ml\/weka\/) - Weka is a collection of machine learning algorithms for data mining tasks.\n* [LBJava](https:\/\/github.com\/CogComp\/lbjava) - Learning Based Java is a modeling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer's application.\n* [knn-java-library](https:\/\/github.com\/felipexw\/knn-java-library) - Just a simple implementation of K-Nearest Neighbors algorithm using with a bunch of similarity measures.\n\n<a name=\"java-speech-recognition\"><\/a>\n#### Speech Recognition\n* [CMU Sphinx](https:\/\/cmusphinx.github.io) - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.\n\n<a name=\"java-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [Flink](https:\/\/flink.apache.org\/) - Open source platform for distributed stream and batch data processing.\n* [Hadoop](https:\/\/github.com\/apache\/hadoop) - Hadoop\/HDFS.\n* [Onyx](https:\/\/github.com\/onyx-platform\/onyx) - Distributed, masterless, high performance, fault tolerant data processing. Written entirely in Clojure.\n* [Spark](https:\/\/github.com\/apache\/spark) - Spark is a fast and general engine for large-scale data processing.\n* [Storm](https:\/\/storm.apache.org\/) - Storm is a distributed realtime computation system.\n* [Impala](https:\/\/github.com\/cloudera\/impala) - Real-time Query for Hadoop.\n* [DataMelt](https:\/\/jwork.org\/dmelt\/) - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.\n* [Dr. Michael Thomas Flanagan's Java Scientific Library.](https:\/\/www.ee.ucl.ac.uk\/~mflanaga\/java\/) **[Deprecated]**\n\n<a name=\"java-deep-learning\"><\/a>\n#### Deep Learning\n\n* [Deeplearning4j](https:\/\/github.com\/deeplearning4j\/deeplearning4j) - Scalable deep learning for industry with parallel GPUs.\n* [Keras Beginner Tutorial](https:\/\/victorzhou.com\/blog\/keras-neural-network-tutorial\/) - Friendly guide on using Keras to implement a simple Neural Network in Python\n* [deepjavalibrary\/djl](https:\/\/github.com\/deepjavalibrary\/djl) - Deep Java Library (DJL) is an open-source, high-level, engine-agnostic Java framework for deep learning, designed to be easy to get started with and simple to use for Java developers.\n\n<a name=\"javascript\"><\/a>\n## Javascript\n\n<a name=\"javascript-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [Twitter-text](https:\/\/github.com\/twitter\/twitter-text) - A JavaScript implementation of Twitter's text processing library.\n* [natural](https:\/\/github.com\/NaturalNode\/natural) - General natural language facilities for node.\n* [Knwl.js](https:\/\/github.com\/loadfive\/Knwl.js) - A Natural Language Processor in JS.\n* [Retext](https:\/\/github.com\/retextjs\/retext) - Extensible system for analyzing and manipulating natural language.\n* [NLP Compromise](https:\/\/github.com\/spencermountain\/compromise) - Natural Language processing in the browser.\n* [nlp.js](https:\/\/github.com\/axa-group\/nlp.js) - An NLP library built in node over Natural, with entity extraction, sentiment analysis, automatic language identify, and so more\n\n\n\n<a name=\"javascript-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [D3.js](https:\/\/d3js.org\/)\n* [High Charts](https:\/\/www.highcharts.com\/)\n* [NVD3.js](http:\/\/nvd3.org\/)\n* [dc.js](https:\/\/dc-js.github.io\/dc.js\/)\n* [chartjs](https:\/\/www.chartjs.org\/)\n* [dimple](http:\/\/dimplejs.org\/)\n* [amCharts](https:\/\/www.amcharts.com\/)\n* [D3xter](https:\/\/github.com\/NathanEpstein\/D3xter) - Straight forward plotting built on D3. **[Deprecated]**\n* [statkit](https:\/\/github.com\/rigtorp\/statkit) - Statistics kit for JavaScript. **[Deprecated]**\n* [datakit](https:\/\/github.com\/nathanepstein\/datakit) - A lightweight framework for data analysis in JavaScript\n* [science.js](https:\/\/github.com\/jasondavies\/science.js\/) - Scientific and statistical computing in JavaScript. **[Deprecated]**\n* [Z3d](https:\/\/github.com\/NathanEpstein\/Z3d) - Easily make interactive 3d plots built on Three.js **[Deprecated]**\n* [Sigma.js](http:\/\/sigmajs.org\/) - JavaScript library dedicated to graph drawing.\n* [C3.js](https:\/\/c3js.org\/) - customizable library based on D3.js for easy chart drawing.\n* [Datamaps](https:\/\/datamaps.github.io\/) - Customizable SVG map\/geo visualizations using D3.js. **[Deprecated]**\n* [ZingChart](https:\/\/www.zingchart.com\/) - library written on Vanilla JS for big data visualization.\n* [cheminfo](https:\/\/www.cheminfo.org\/) - Platform for data visualization and analysis, using the [visualizer](https:\/\/github.com\/npellet\/visualizer) project.\n* [Learn JS Data](http:\/\/learnjsdata.com\/)\n* [AnyChart](https:\/\/www.anychart.com\/)\n* [FusionCharts](https:\/\/www.fusioncharts.com\/)\n* [Nivo](https:\/\/nivo.rocks) - built on top of the awesome d3 and Reactjs libraries\n\n\n<a name=\"javascript-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Auto ML](https:\/\/github.com\/ClimbsRocks\/auto_ml) - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file! **[Deprecated]**\n* [Convnet.js](https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/) - ConvNetJS is a Javascript library for training Deep Learning models[DEEP LEARNING] **[Deprecated]**\n* [Clusterfck](https:\/\/harthur.github.io\/clusterfck\/) - Agglomerative hierarchical clustering implemented in Javascript for Node.js and the browser. **[Deprecated]**\n* [Clustering.js](https:\/\/github.com\/emilbayes\/clustering.js) - Clustering algorithms implemented in Javascript for Node.js and the browser. **[Deprecated]**\n* [Decision Trees](https:\/\/github.com\/serendipious\/nodejs-decision-tree-id3) - NodeJS Implementation of Decision Tree using ID3 Algorithm. **[Deprecated]**\n* [DN2A](https:\/\/github.com\/antoniodeluca\/dn2a.js) - Digital Neural Networks Architecture. **[Deprecated]**\n* [figue](https:\/\/code.google.com\/archive\/p\/figue) - K-means, fuzzy c-means and agglomerative clustering.\n* [Gaussian Mixture Model](https:\/\/github.com\/lukapopijac\/gaussian-mixture-model) - Unsupervised machine learning with multivariate Gaussian mixture model.\n* [Node-fann](https:\/\/github.com\/rlidwka\/node-fann) - FANN (Fast Artificial Neural Network Library) bindings for Node.js **[Deprecated]**\n* [Keras.js](https:\/\/github.com\/transcranial\/keras-js) - Run Keras models in the browser, with GPU support provided by WebGL 2.\n* [Kmeans.js](https:\/\/github.com\/emilbayes\/kMeans.js) - Simple Javascript implementation of the k-means algorithm, for node.js and the browser. **[Deprecated]**\n* [LDA.js](https:\/\/github.com\/primaryobjects\/lda) - LDA topic modeling for Node.js\n* [Learning.js](https:\/\/github.com\/yandongliu\/learningjs) - Javascript implementation of logistic regression\/c4.5 decision tree **[Deprecated]**\n* [machinelearn.js](https:\/\/github.com\/machinelearnjs\/machinelearnjs) - Machine Learning library for the web, Node.js and developers\n* [mil-tokyo](https:\/\/github.com\/mil-tokyo) - List of several machine learning libraries.\n* [Node-SVM](https:\/\/github.com\/nicolaspanel\/node-svm) - Support Vector Machine for Node.js\n* [Brain](https:\/\/github.com\/harthur\/brain) - Neural networks in JavaScript **[Deprecated]**\n* [Brain.js](https:\/\/github.com\/BrainJS\/brain.js) - Neural networks in JavaScript - continued community fork of [Brain](https:\/\/github.com\/harthur\/brain).\n* [Bayesian-Bandit](https:\/\/github.com\/omphalos\/bayesian-bandit.js) - Bayesian bandit implementation for Node and the browser. **[Deprecated]**\n* [Synaptic](https:\/\/github.com\/cazala\/synaptic) - Architecture-free neural network library for Node.js and the browser.\n* [kNear](https:\/\/github.com\/NathanEpstein\/kNear) - JavaScript implementation of the k nearest neighbors algorithm for supervised learning.\n* [NeuralN](https:\/\/github.com\/totemstech\/neuraln) - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training. **[Deprecated]**\n* [kalman](https:\/\/github.com\/itamarwe\/kalman) - Kalman filter for Javascript. **[Deprecated]**\n* [shaman](https:\/\/github.com\/luccastera\/shaman) - Node.js library with support for both simple and multiple linear regression. **[Deprecated]**\n* [ml.js](https:\/\/github.com\/mljs\/ml) - Machine learning and numerical analysis tools for Node.js and the Browser!\n* [ml5](https:\/\/github.com\/ml5js\/ml5-library) - Friendly machine learning for the web!\n* [Pavlov.js](https:\/\/github.com\/NathanEpstein\/Pavlov.js) - Reinforcement learning using Markov Decision Processes.\n* [MXNet](https:\/\/github.com\/apache\/incubator-mxnet) - Lightweight, Portable, Flexible Distributed\/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [TensorFlow.js](https:\/\/js.tensorflow.org\/) - A WebGL accelerated, browser based JavaScript library for training and deploying ML models.\n* [JSMLT](https:\/\/github.com\/jsmlt\/jsmlt) - Machine learning toolkit with classification and clustering for Node.js; supports visualization (see [visualml.io](https:\/\/visualml.io)).\n* [xgboost-node](https:\/\/github.com\/nuanio\/xgboost-node) - Run XGBoost model and make predictions in Node.js.\n* [Netron](https:\/\/github.com\/lutzroeder\/netron) - Visualizer for machine learning models.\n* [tensor-js](https:\/\/github.com\/Hoff97\/tensorjs) - A deep learning library for the browser, accelerated by WebGL and WebAssembly\n* [WebDNN](https:\/\/github.com\/mil-tokyo\/webdnn) - Fast Deep Neural Network Javascript Framework. WebDNN uses next generation JavaScript API, WebGPU for GPU execution, and WebAssembly for CPU execution.\n\n<a name=\"javascript-misc\"><\/a>\n#### Misc\n\n* [stdlib](https:\/\/github.com\/stdlib-js\/stdlib) - A standard library for JavaScript and Node.js, with an emphasis on numeric computing. The library provides a collection of robust, high performance libraries for mathematics, statistics, streams, utilities, and more.\n* [sylvester](https:\/\/github.com\/jcoglan\/sylvester) - Vector and Matrix math for JavaScript. **[Deprecated]**\n* [simple-statistics](https:\/\/github.com\/simple-statistics\/simple-statistics) - A JavaScript implementation of descriptive, regression, and inference statistics. Implemented in literate JavaScript with no dependencies, designed to work in all modern browsers (including IE) as well as in Node.js.\n* [regression-js](https:\/\/github.com\/Tom-Alexander\/regression-js) - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.\n* [Lyric](https:\/\/github.com\/flurry\/Lyric) - Linear Regression library. **[Deprecated]**\n* [GreatCircle](https:\/\/github.com\/mwgg\/GreatCircle) - Library for calculating great circle distance.\n* [MLPleaseHelp](https:\/\/github.com\/jgreenemi\/MLPleaseHelp) - MLPleaseHelp is a simple ML resource search engine. You can use this search engine right now at [https:\/\/jgreenemi.github.io\/MLPleaseHelp\/](https:\/\/jgreenemi.github.io\/MLPleaseHelp\/), provided via Github Pages.\n* [Pipcook](https:\/\/github.com\/alibaba\/pipcook) - A JavaScript application framework for machine learning and its engineering.\n\n<a name=\"javascript-demos-and-scripts\"><\/a>\n#### Demos and Scripts\n* [The Bot](https:\/\/github.com\/sta-ger\/TheBot) - Example of how the neural network learns to predict the angle between two points created with [Synaptic](https:\/\/github.com\/cazala\/synaptic).\n* [Half Beer](https:\/\/github.com\/sta-ger\/HalfBeer) - Beer glass classifier created with [Synaptic](https:\/\/github.com\/cazala\/synaptic).\n* [NSFWJS](http:\/\/nsfwjs.com) - Indecent content checker with TensorFlow.js\n* [Rock Paper Scissors](https:\/\/rps-tfjs.netlify.com\/) - Rock Paper Scissors trained in the browser with TensorFlow.js\n* [Heroes Wear Masks](https:\/\/heroeswearmasks.fun\/) - A fun TensorFlow.js-based oracle that tells, whether one wears a face mask or not. It can even tell when one wears the mask incorrectly.\n\n<a name=\"julia\"><\/a>\n## Julia\n\n<a name=\"julia-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [MachineLearning](https:\/\/github.com\/benhamner\/MachineLearning.jl) - Julia Machine Learning library. **[Deprecated]**\n* [MLBase](https:\/\/github.com\/JuliaStats\/MLBase.jl) - A set of functions to support the development of machine learning algorithms.\n* [PGM](https:\/\/github.com\/JuliaStats\/PGM.jl) - A Julia framework for probabilistic graphical models.\n* [DA](https:\/\/github.com\/trthatcher\/DiscriminantAnalysis.jl) - Julia package for Regularized Discriminant Analysis.\n* [Regression](https:\/\/github.com\/lindahua\/Regression.jl) - Algorithms for regression analysis (e.g. linear regression and logistic regression). **[Deprecated]**\n* [Local Regression](https:\/\/github.com\/JuliaStats\/Loess.jl) - Local regression, so smooooth!\n* [Naive Bayes](https:\/\/github.com\/nutsiepully\/NaiveBayes.jl) - Simple Naive Bayes implementation in Julia. **[Deprecated]**\n* [Mixed Models](https:\/\/github.com\/dmbates\/MixedModels.jl) - A Julia package for fitting (statistical) mixed-effects models.\n* [Simple MCMC](https:\/\/github.com\/fredo-dedup\/SimpleMCMC.jl) - basic mcmc sampler implemented in Julia. **[Deprecated]**\n* [Distances](https:\/\/github.com\/JuliaStats\/Distances.jl) - Julia module for Distance evaluation.\n* [Decision Tree](https:\/\/github.com\/bensadeghi\/DecisionTree.jl) - Decision Tree Classifier and Regressor.\n* [Neural](https:\/\/github.com\/compressed\/BackpropNeuralNet.jl) - A neural network in Julia.\n* [MCMC](https:\/\/github.com\/doobwa\/MCMC.jl) - MCMC tools for Julia. **[Deprecated]**\n* [Mamba](https:\/\/github.com\/brian-j-smith\/Mamba.jl) - Markov chain Monte Carlo (MCMC) for Bayesian analysis in Julia.\n* [GLM](https:\/\/github.com\/JuliaStats\/GLM.jl) - Generalized linear models in Julia.\n* [Gaussian Processes](https:\/\/github.com\/STOR-i\/GaussianProcesses.jl) - Julia package for Gaussian processes.\n* [Online Learning](https:\/\/github.com\/lendle\/OnlineLearning.jl) **[Deprecated]**\n* [GLMNet](https:\/\/github.com\/simonster\/GLMNet.jl) - Julia wrapper for fitting Lasso\/ElasticNet GLM models using glmnet.\n* [Clustering](https:\/\/github.com\/JuliaStats\/Clustering.jl) - Basic functions for clustering data: k-means, dp-means, etc.\n* [SVM](https:\/\/github.com\/JuliaStats\/SVM.jl) - SVM for Julia. **[Deprecated]**\n* [Kernel Density](https:\/\/github.com\/JuliaStats\/KernelDensity.jl) - Kernel density estimators for julia.\n* [MultivariateStats](https:\/\/github.com\/JuliaStats\/MultivariateStats.jl) - Methods for dimensionality reduction.\n* [NMF](https:\/\/github.com\/JuliaStats\/NMF.jl) - A Julia package for non-negative matrix factorization.\n* [ANN](https:\/\/github.com\/EricChiang\/ANN.jl) - Julia artificial neural networks. **[Deprecated]**\n* [Mocha](https:\/\/github.com\/pluskid\/Mocha.jl) - Deep Learning framework for Julia inspired by Caffe. **[Deprecated]**\n* [XGBoost](https:\/\/github.com\/dmlc\/XGBoost.jl) - eXtreme Gradient Boosting Package in Julia.\n* [ManifoldLearning](https:\/\/github.com\/wildart\/ManifoldLearning.jl) - A Julia package for manifold learning and nonlinear dimensionality reduction.\n* [MXNet](https:\/\/github.com\/apache\/incubator-mxnet) - Lightweight, Portable, Flexible Distributed\/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [Merlin](https:\/\/github.com\/hshindo\/Merlin.jl) - Flexible Deep Learning Framework in Julia.\n* [ROCAnalysis](https:\/\/github.com\/davidavdav\/ROCAnalysis.jl) - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers.\n* [GaussianMixtures](https:\/\/github.com\/davidavdav\/GaussianMixtures.jl) - Large scale Gaussian Mixture Models.\n* [ScikitLearn](https:\/\/github.com\/cstjean\/ScikitLearn.jl) - Julia implementation of the scikit-learn API.\n* [Knet](https:\/\/github.com\/denizyuret\/Knet.jl) - Ko\u00e7 University Deep Learning Framework.\n* [Flux](https:\/\/fluxml.ai\/) - Relax! Flux is the ML library that doesn't make you tensor\n* [MLJ](https:\/\/github.com\/alan-turing-institute\/MLJ.jl) - A Julia machine learning framework\n\n<a name=\"julia-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [Topic Models](https:\/\/github.com\/slycoder\/TopicModels.jl) - TopicModels for Julia. **[Deprecated]**\n* [Text Analysis](https:\/\/github.com\/JuliaText\/TextAnalysis.jl) - Julia package for text analysis.\n* [Word Tokenizers](https:\/\/github.com\/JuliaText\/WordTokenizers.jl) - Tokenizers for Natural Language Processing in Julia\n* [Corpus Loaders](https:\/\/github.com\/JuliaText\/CorpusLoaders.jl) - A julia package providing a variety of loaders for various NLP corpora.\n* [Embeddings](https:\/\/github.com\/JuliaText\/Embeddings.jl) - Functions and data dependencies for loading various word embeddings\n* [Languages](https:\/\/github.com\/JuliaText\/Languages.jl) - Julia package for working with various human languages\n* [WordNet](https:\/\/github.com\/JuliaText\/WordNet.jl) - A Julia package for Princeton's WordNet\n\n<a name=\"julia-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [Graph Layout](https:\/\/github.com\/IainNZ\/GraphLayout.jl) - Graph layout algorithms in pure Julia.\n* [LightGraphs](https:\/\/github.com\/JuliaGraphs\/LightGraphs.jl) - Graph modeling and analysis.\n* [Data Frames Meta](https:\/\/github.com\/JuliaData\/DataFramesMeta.jl) - Metaprogramming tools for DataFrames.\n* [Julia Data](https:\/\/github.com\/nfoti\/JuliaData) - library for working with tabular data in Julia. **[Deprecated]**\n* [Data Read](https:\/\/github.com\/queryverse\/ReadStat.jl) - Read files from Stata, SAS, and SPSS.\n* [Hypothesis Tests](https:\/\/github.com\/JuliaStats\/HypothesisTests.jl) - Hypothesis tests for Julia.\n* [Gadfly](https:\/\/github.com\/GiovineItalia\/Gadfly.jl) - Crafty statistical graphics for Julia.\n* [Stats](https:\/\/github.com\/JuliaStats\/StatsKit.jl) - Statistical tests for Julia.\n* [RDataSets](https:\/\/github.com\/johnmyleswhite\/RDatasets.jl) - Julia package for loading many of the data sets available in R.\n* [DataFrames](https:\/\/github.com\/JuliaData\/DataFrames.jl) - library for working with tabular data in Julia.\n* [Distributions](https:\/\/github.com\/JuliaStats\/Distributions.jl) - A Julia package for probability distributions and associated functions.\n* [Data Arrays](https:\/\/github.com\/JuliaStats\/DataArrays.jl) - Data structures that allow missing values. **[Deprecated]**\n* [Time Series](https:\/\/github.com\/JuliaStats\/TimeSeries.jl) - Time series toolkit for Julia.\n* [Sampling](https:\/\/github.com\/lindahua\/Sampling.jl) - Basic sampling algorithms for Julia.\n\n<a name=\"julia-misc-stuff--presentations\"><\/a>\n#### Misc Stuff \/ Presentations\n\n* [DSP](https:\/\/github.com\/JuliaDSP\/DSP.jl) - Digital Signal Processing (filtering, periodograms, spectrograms, window functions).\n* [JuliaCon Presentations](https:\/\/github.com\/JuliaCon\/presentations) - Presentations for JuliaCon.\n* [SignalProcessing](https:\/\/github.com\/JuliaDSP\/DSP.jl) - Signal Processing tools for Julia.\n* [Images](https:\/\/github.com\/JuliaImages\/Images.jl) - An image library for Julia.\n* [DataDeps](https:\/\/github.com\/oxinabox\/DataDeps.jl) - Reproducible data setup for reproducible science.\n\n<a name=\"kotlin\"><\/a>\n## Kotlin\n\n<a name=\"kotlin-deep-learning\"><\/a>\n#### Deep Learning\n* [KotlinDL](https:\/\/github.com\/JetBrains\/KotlinDL) - Deep learning framework written in Kotlin.\n\n<a name=\"lua\"><\/a>\n## Lua\n\n<a name=\"lua-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Torch7](http:\/\/torch.ch\/)\n  * [cephes](https:\/\/github.com\/deepmind\/torch-cephes) - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy. **[Deprecated]**\n  * [autograd](https:\/\/github.com\/twitter\/torch-autograd) - Autograd automatically differentiates native Torch code. Inspired by the original Python version.\n  * [graph](https:\/\/github.com\/torch\/graph) - Graph package for Torch. **[Deprecated]**\n  * [randomkit](https:\/\/github.com\/deepmind\/torch-randomkit) - Numpy's randomkit, wrapped for Torch. **[Deprecated]**\n  * [signal](https:\/\/github.com\/soumith\/torch-signal) - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft.\n  * [nn](https:\/\/github.com\/torch\/nn) - Neural Network package for Torch.\n  * [torchnet](https:\/\/github.com\/torchnet\/torchnet) - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming.\n  * [nngraph](https:\/\/github.com\/torch\/nngraph) - This package provides graphical computation for nn library in Torch7.\n  * [nnx](https:\/\/github.com\/clementfarabet\/lua---nnx) - A completely unstable and experimental package that extends Torch's builtin nn library.\n  * [rnn](https:\/\/github.com\/Element-Research\/rnn) - A Recurrent Neural Network library that extends Torch's nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.\n  * [dpnn](https:\/\/github.com\/Element-Research\/dpnn) - Many useful features that aren't part of the main nn package.\n  * [dp](https:\/\/github.com\/nicholas-leonard\/dp) - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns. **[Deprecated]**\n  * [optim](https:\/\/github.com\/torch\/optim) - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.\n  * [unsup](https:\/\/github.com\/koraykv\/unsup) - A package for unsupervised learning in Torch. Provides modules that are compatible with nn (LinearPsd, ConvPsd, AutoEncoder, ...), and self-contained algorithms (k-means, PCA). **[Deprecated]**\n  * [manifold](https:\/\/github.com\/clementfarabet\/manifold) - A package to manipulate manifolds.\n  * [svm](https:\/\/github.com\/koraykv\/torch-svm) - Torch-SVM library. **[Deprecated]**\n  * [lbfgs](https:\/\/github.com\/clementfarabet\/lbfgs) - FFI Wrapper for liblbfgs. **[Deprecated]**\n  * [vowpalwabbit](https:\/\/github.com\/clementfarabet\/vowpal_wabbit) - An old vowpalwabbit interface to torch. **[Deprecated]**\n  * [OpenGM](https:\/\/github.com\/clementfarabet\/lua---opengm) - OpenGM is a C++ library for graphical modeling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM. **[Deprecated]**\n  * [spaghetti](https:\/\/github.com\/MichaelMathieu\/lua---spaghetti) - Spaghetti (sparse linear) module for torch7 by @MichaelMathieu **[Deprecated]**\n  * [LuaSHKit](https:\/\/github.com\/ocallaco\/LuaSHkit) - A lua wrapper around the Locality sensitive hashing library SHKit **[Deprecated]**\n  * [kernel smoothing](https:\/\/github.com\/rlowrance\/kernel-smoothers) - KNN, kernel-weighted average, local linear regression smoothers. **[Deprecated]**\n  * [cutorch](https:\/\/github.com\/torch\/cutorch) - Torch CUDA Implementation.\n  * [cunn](https:\/\/github.com\/torch\/cunn) - Torch CUDA Neural Network Implementation.\n  * [imgraph](https:\/\/github.com\/clementfarabet\/lua---imgraph) - An image\/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images. **[Deprecated]**\n  * [videograph](https:\/\/github.com\/clementfarabet\/videograph) - A video\/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos. **[Deprecated]**\n  * [saliency](https:\/\/github.com\/marcoscoffier\/torch-saliency) - code and tools around integral images. A library for finding interest points based on fast integral histograms. **[Deprecated]**\n  * [stitch](https:\/\/github.com\/marcoscoffier\/lua---stitch) - allows us to use hugin to stitch images and apply same stitching to a video sequence. **[Deprecated]**\n  * [sfm](https:\/\/github.com\/marcoscoffier\/lua---sfm) - A bundle adjustment\/structure from motion package. **[Deprecated]**\n  * [fex](https:\/\/github.com\/koraykv\/fex) - A package for feature extraction in Torch. Provides SIFT and dSIFT modules. **[Deprecated]**\n  * [OverFeat](https:\/\/github.com\/sermanet\/OverFeat) - A state-of-the-art generic dense feature extractor. **[Deprecated]**\n  * [wav2letter](https:\/\/github.com\/facebookresearch\/wav2letter) - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research.\n* [Numeric Lua](http:\/\/numlua.luaforge.net\/)\n* [Lunatic Python](https:\/\/labix.org\/lunatic-python)\n* [SciLua](http:\/\/scilua.org\/)\n* [Lua - Numerical Algorithms](https:\/\/bitbucket.org\/lucashnegri\/lna) **[Deprecated]**\n* [Lunum](https:\/\/github.com\/jzrake\/lunum) **[Deprecated]**\n\n<a name=\"lua-demos-and-scripts\"><\/a>\n#### Demos and Scripts\n* [Core torch7 demos repository](https:\/\/github.com\/e-lab\/torch7-demos).\n  * linear-regression, logistic-regression\n  * face detector (training and detection as separate demos)\n  * mst-based-segmenter\n  * train-a-digit-classifier\n  * train-autoencoder\n  * optical flow demo\n  * train-on-housenumbers\n  * train-on-cifar\n  * tracking with deep nets\n  * kinect demo\n  * filter-bank visualization\n  * saliency-networks\n* [Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)](https:\/\/github.com\/soumith\/galaxyzoo)\n* [torch-datasets](https:\/\/github.com\/rosejn\/torch-datasets) - Scripts to load several popular datasets including:\n  * BSR 500\n  * CIFAR-10\n  * COIL\n  * Street View House Numbers\n  * MNIST\n  * NORB\n* [Atari2600](https:\/\/github.com\/fidlej\/aledataset) - Scripts to generate a dataset with static frames from the Arcade Learning Environment.\n\n\n\n<a name=\"matlab\"><\/a>\n## Matlab\n\n<a name=\"matlab-computer-vision\"><\/a>\n#### Computer Vision\n\n* [Contourlets](http:\/\/www.ifp.illinois.edu\/~minhdo\/software\/contourlet_toolbox.tar) - MATLAB source code that implements the contourlet transform and its utility functions.\n* [Shearlets](https:\/\/www3.math.tu-berlin.de\/numerik\/www.shearlab.org\/software) - MATLAB code for shearlet transform.\n* [Curvelets](http:\/\/www.curvelet.org\/software.html) - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.\n* [Bandlets](http:\/\/www.cmap.polytechnique.fr\/~peyre\/download\/) - MATLAB code for bandlet transform.\n* [mexopencv](https:\/\/kyamagu.github.io\/mexopencv\/) - Collection and a development kit of MATLAB mex functions for OpenCV library.\n\n<a name=\"matlab-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [NLP](https:\/\/amplab.cs.berkeley.edu\/an-nlp-library-for-matlab\/) - A NLP library for Matlab.\n\n<a name=\"matlab-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Training a deep autoencoder or a classifier\non MNIST digits](https:\/\/www.cs.toronto.edu\/~hinton\/MatlabForSciencePaper.html) - Training a deep autoencoder or a classifier\non MNIST digits[DEEP LEARNING].\n* [Convolutional-Recursive Deep Learning for 3D Object Classification](https:\/\/www.socher.org\/index.php\/Main\/Convolutional-RecursiveDeepLearningFor3DObjectClassification) - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING].\n* [Spider](https:\/\/people.kyb.tuebingen.mpg.de\/spider\/) - The spider is intended to be a complete object orientated environment for machine learning in Matlab.\n* [LibSVM](https:\/\/www.csie.ntu.edu.tw\/~cjlin\/libsvm\/#matlab) - A Library for Support Vector Machines.\n* [ThunderSVM](https:\/\/github.com\/Xtra-Computing\/thundersvm) - An Open-Source SVM Library on GPUs and CPUs\n* [LibLinear](https:\/\/www.csie.ntu.edu.tw\/~cjlin\/liblinear\/#download) - A Library for Large Linear Classification.\n* [Machine Learning Module](https:\/\/github.com\/josephmisiti\/machine-learning-module) - Class on machine w\/ PDF, lectures, code\n* [Caffe](https:\/\/github.com\/BVLC\/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind.\n* [Pattern Recognition Toolbox](https:\/\/github.com\/covartech\/PRT) - A complete object-oriented environment for machine learning in Matlab.\n* [Pattern Recognition and Machine Learning](https:\/\/github.com\/PRML\/PRMLT) - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.\n* [Optunity](https:\/\/optunity.readthedocs.io\/en\/latest\/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.\n* [MXNet](https:\/\/github.com\/apache\/incubator-mxnet\/) - Lightweight, Portable, Flexible Distributed\/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [Machine Learning in MatLab\/Octave](https:\/\/github.com\/trekhleb\/machine-learning-octave) - examples of popular machine learning algorithms (neural networks, linear\/logistic regressions, K-Means, etc.) with code examples and mathematics behind them being explained.\n\n\n<a name=\"matlab-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [ParaMonte](https:\/\/github.com\/cdslaborg\/paramonte) - A general-purpose MATLAB library for Bayesian data analysis and visualization via serial\/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https:\/\/www.cdslab.org\/paramonte\/).\n* [matlab_bgl](https:\/\/www.cs.purdue.edu\/homes\/dgleich\/packages\/matlab_bgl\/) - MatlabBGL is a Matlab package for working with graphs.\n* [gaimc](https:\/\/www.mathworks.com\/matlabcentral\/fileexchange\/24134-gaimc---graph-algorithms-in-matlab-code) - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL's mex functions.\n\n<a name=\"net\"><\/a>\n## .NET\n\n<a name=\"net-computer-vision\"><\/a>\n#### Computer Vision\n\n* [OpenCVDotNet](https:\/\/code.google.com\/archive\/p\/opencvdotnet) - A wrapper for the OpenCV project to be used with .NET applications.\n* [Emgu CV](http:\/\/www.emgu.com\/wiki\/index.php\/Main_Page) - Cross platform wrapper of OpenCV which can be compiled in Mono to be run on Windows, Linus, Mac OS X, iOS, and Android.\n* [AForge.NET](http:\/\/www.aforgenet.com\/framework\/) - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.\n* [Accord.NET](http:\/\/accord-framework.net) - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.\n\n<a name=\"net-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [Stanford.NLP for .NET](https:\/\/github.com\/sergey-tihon\/Stanford.NLP.NET\/) - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.\n\n<a name=\"net-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Accord-Framework](http:\/\/accord-framework.net\/) -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.\n* [Accord.MachineLearning](https:\/\/www.nuget.org\/packages\/Accord.MachineLearning\/) - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.\n* [DiffSharp](https:\/\/diffsharp.github.io\/DiffSharp\/) - An automatic differentiation (AD) library providing exact and efficient derivatives (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.\n* [Encog](https:\/\/www.nuget.org\/packages\/encog-dotnet-core\/) - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.\n* [GeneticSharp](https:\/\/github.com\/giacomelli\/GeneticSharp) - Multi-platform genetic algorithm library for .NET Core and .NET Framework. The library has several implementations of GA operators, like: selection, crossover, mutation, reinsertion and termination.\n* [Infer.NET](https:\/\/dotnet.github.io\/infer\/) - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through to customized solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.\n* [ML.NET](https:\/\/github.com\/dotnet\/machinelearning) - ML.NET is a cross-platform open-source machine learning framework which makes machine learning accessible to .NET developers. ML.NET was originally developed in Microsoft Research and evolved into a significant framework over the last decade and is used across many product groups in Microsoft like Windows, Bing, PowerPoint, Excel and more.\n* [Neural Network Designer](https:\/\/sourceforge.net\/projects\/nnd\/) - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feedback. The chat bots can even scrape the internet for information to return in their output as well as to use for learning.\n* [Synapses](https:\/\/github.com\/mrdimosthenis\/Synapses) - Neural network library in F#.\n* [Vulpes](https:\/\/github.com\/fsprojects\/Vulpes) - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.\n* [MxNet.Sharp](https:\/\/github.com\/tech-quantum\/MxNet.Sharp) - .NET Standard bindings for Apache MxNet with Imperative, Symbolic and Gluon Interface for developing, training and deploying Machine Learning models in C#. https:\/\/mxnet.tech-quantum.com\/\n\n<a name=\"net-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [numl](https:\/\/www.nuget.org\/packages\/numl\/) - numl is a machine learning library intended to ease the use of using standard modeling techniques for both prediction and clustering.\n* [Math.NET Numerics](https:\/\/www.nuget.org\/packages\/MathNet.Numerics\/) - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and everyday use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone\/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android\/iOS with Xamarin.\n* [Sho](https:\/\/www.microsoft.com\/en-us\/research\/project\/sho-the-net-playground-for-data\/) - Sho is an interactive environment for data analysis and scientific computing that lets you seamlessly connect scripts (in IronPython) with compiled code (in .NET) to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.\n\n<a name=\"objective-c\"><\/a>\n## Objective C\n\n<a name=\"objective-c-general-purpose-machine-learning\"><\/a>\n### General-Purpose Machine Learning\n\n* [YCML](https:\/\/github.com\/yconst\/YCML) - A Machine Learning framework for Objective-C and Swift (OS X \/ iOS).\n* [MLPNeuralNet](https:\/\/github.com\/nikolaypavlov\/MLPNeuralNet) - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural networks. It is built on top of the Apple's Accelerate Framework, using vectorized operations and hardware acceleration if available. **[Deprecated]**\n* [MAChineLearning](https:\/\/github.com\/gianlucabertani\/MAChineLearning) - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it's 20 times faster than its Java equivalent. Includes sample code for use from Swift.\n* [BPN-NeuralNetwork](https:\/\/github.com\/Kalvar\/ios-BPN-NeuralNetwork) - It implemented 3 layers of neural networks ( Input Layer, Hidden Layer and Output Layer ) and it was named Back Propagation Neural Networks (BPN). This network can be used in products recommendation, user behavior analysis, data mining and data analysis. **[Deprecated]**\n* [Multi-Perceptron-NeuralNetwork](https:\/\/github.com\/Kalvar\/ios-Multi-Perceptron-NeuralNetwork) - it implemented multi-perceptrons neural network (\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af) based on Back Propagation Neural Networks (BPN) and designed unlimited-hidden-layers.\n* [KRHebbian-Algorithm](https:\/\/github.com\/Kalvar\/ios-KRHebbian-Algorithm) - It is a non-supervisor and self-learning algorithm (adjust the weights) in the neural network of Machine Learning. **[Deprecated]**\n* [KRKmeans-Algorithm](https:\/\/github.com\/Kalvar\/ios-KRKmeans-Algorithm) - It implemented K-Means  clustering and classification algorithm. It could be used in data mining and image compression. **[Deprecated]**\n* [KRFuzzyCMeans-Algorithm](https:\/\/github.com\/Kalvar\/ios-KRFuzzyCMeans-Algorithm) - It implemented Fuzzy C-Means (FCM) the fuzzy clustering \/ classification algorithm on Machine Learning. It could be used in data mining and image compression. **[Deprecated]**\n\n<a name=\"ocaml\"><\/a>\n## OCaml\n\n<a name=\"ocaml-general-purpose-machine-learning\"><\/a>\n### General-Purpose Machine Learning\n\n* [Oml](https:\/\/github.com\/rleonid\/oml) - A general statistics and machine learning library.\n* [GPR](https:\/\/mmottl.github.io\/gpr\/) - Efficient Gaussian Process Regression in OCaml.\n* [Libra-Tk](https:\/\/libra.cs.uoregon.edu) - Algorithms for learning and inference with discrete probabilistic models.\n* [TensorFlow](https:\/\/github.com\/LaurentMazare\/tensorflow-ocaml) - OCaml bindings for TensorFlow.\n\n<a name=\"perl\"><\/a>\n## Perl\n\n<a name=\"perl-data-analysis--data-visualization\"><\/a>\n### Data Analysis \/ Data Visualization\n\n* [Perl Data Language](https:\/\/metacpan.org\/pod\/Paws::MachineLearning), a pluggable architecture for data and image processing, which can\nbe [used for machine learning](https:\/\/github.com\/zenogantner\/PDL-ML).\n\n<a name=\"perl-general-purpose-machine-learning\"><\/a>\n### General-Purpose Machine Learning\n\n* [MXnet for Deep Learning, in Perl](https:\/\/github.com\/apache\/incubator-mxnet\/tree\/master\/perl-package),\nalso [released in CPAN](https:\/\/metacpan.org\/pod\/AI::MXNet).\n* [Perl Data Language](https:\/\/metacpan.org\/pod\/Paws::MachineLearning),\nusing AWS machine learning platform from Perl.\n* [Algorithm::SVMLight](https:\/\/metacpan.org\/pod\/Algorithm::SVMLight),\n  implementation of Support Vector Machines with SVMLight under it. **[Deprecated]**\n* Several machine learning and artificial intelligence models are\n  included in the [`AI`](https:\/\/metacpan.org\/search?size=20&q=AI)\n  namespace. For instance, you can\n  find [Na\u00efve Bayes](https:\/\/metacpan.org\/pod\/AI::NaiveBayes).\n\n<a name=\"perl6\"><\/a>\n## Perl 6\n\n* [Support Vector Machines](https:\/\/github.com\/titsuki\/p6-Algorithm-LibSVM)\n* [Na\u00efve Bayes](https:\/\/github.com\/titsuki\/p6-Algorithm-NaiveBayes)\n\n<a name=\"perl-6-data-analysis--data-visualization\"><\/a>\n### Data Analysis \/ Data Visualization\n\n* [Perl Data Language](https:\/\/metacpan.org\/pod\/Paws::MachineLearning),\na pluggable architecture for data and image processing, which can\nbe\n[used for machine learning](https:\/\/github.com\/zenogantner\/PDL-ML).\n\n<a name=\"perl-6-general-purpose-machine-learning\"><\/a>\n### General-Purpose Machine Learning\n\n<a name=\"php\"><\/a>\n## PHP\n\n<a name=\"php-natural-language-processing\"><\/a>\n### Natural Language Processing\n\n* [jieba-php](https:\/\/github.com\/fukuball\/jieba-php) - Chinese Words Segmentation Utilities.\n\n<a name=\"php-general-purpose-machine-learning\"><\/a>\n### General-Purpose Machine Learning\n\n* [PHP-ML](https:\/\/gitlab.com\/php-ai\/php-ml) - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.\n* [PredictionBuilder](https:\/\/github.com\/denissimon\/prediction-builder) - A library for machine learning that builds predictions using a linear regression.\n* [Rubix ML](https:\/\/github.com\/RubixML) - A high-level machine learning (ML) library that lets you build programs that learn from data using the PHP language.\n* [19 Questions](https:\/\/github.com\/fulldecent\/19-questions) - A machine learning \/ bayesian inference assigning attributes to objects.\n\n<a name=\"python\"><\/a>\n## Python\n\n<a name=\"python-computer-vision\"><\/a>\n#### Computer Vision\n\n* [Scikit-Image](https:\/\/github.com\/scikit-image\/scikit-image) - A collection of algorithms for image processing in Python.\n* [Scikit-Opt](https:\/\/github.com\/guofei9987\/scikit-opt) - Swarm Intelligence in Python (Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing, Ant Colony Algorithm, Immune Algorithm,Artificial Fish Swarm Algorithm in Python)\n* [SimpleCV](http:\/\/simplecv.org\/) - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.\n* [Vigranumpy](https:\/\/github.com\/ukoethe\/vigra) - Python bindings for the VIGRA C++ computer vision library.\n* [OpenFace](https:\/\/cmusatyalab.github.io\/openface\/) - Free and open source face recognition with deep neural networks.\n* [PCV](https:\/\/github.com\/jesolem\/PCV) - Open source Python module for computer vision. **[Deprecated]**\n* [face_recognition](https:\/\/github.com\/ageitgey\/face_recognition) - Face recognition library that recognizes and manipulates faces from Python or from the command line.\n* [deepface](https:\/\/github.com\/serengil\/deepface) - A lightweight face recognition and facial attribute analysis (age, gender, emotion and race) framework for Python covering cutting-edge models such as VGG-Face, FaceNet, OpenFace, DeepFace, DeepID, Dlib and ArcFace.\n* [retinaface](https:\/\/github.com\/serengil\/retinaface) - deep learning based cutting-edge facial detector for Python coming with facial landmarks\n* [dockerface](https:\/\/github.com\/natanielruiz\/dockerface) - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container. **[Deprecated]**\n* [Detectron](https:\/\/github.com\/facebookresearch\/Detectron) - FAIR's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework. **[Deprecated]**\n* [detectron2](https:\/\/github.com\/facebookresearch\/detectron2) - FAIR's next-generation research platform for object detection and segmentation. It is a ground-up rewrite of the previous version, Detectron, and is powered by the PyTorch deep learning framework.\n* [albumentations](https:\/\/github.com\/albu\/albumentations) - \u0410 fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops.\n* [pytessarct](https:\/\/github.com\/madmaze\/pytesseract) - Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and \"read\" the text embedded in images. Python-tesseract is a wrapper for [Google's Tesseract-OCR Engine](https:\/\/github.com\/tesseract-ocr\/tesseract).\n* [imutils](https:\/\/github.com\/jrosebr1\/imutils) - A library containing Convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.\n* [PyTorchCV](https:\/\/github.com\/donnyyou\/PyTorchCV) - A PyTorch-Based Framework for Deep Learning in Computer Vision.\n* [Self-supervised learning](https:\/\/pytorch-lightning-bolts.readthedocs.io\/en\/latest\/self_supervised_models.html)\n* [neural-style-pt](https:\/\/github.com\/ProGamerGov\/neural-style-pt) - A PyTorch implementation of Justin Johnson's neural-style (neural style transfer).\n* [Detecto](https:\/\/github.com\/alankbi\/detecto) - Train and run a computer vision model with 5-10 lines of code.\n* [neural-dream](https:\/\/github.com\/ProGamerGov\/neural-dream) - A PyTorch implementation of DeepDream.\n* [Openpose](https:\/\/github.com\/CMU-Perceptual-Computing-Lab\/openpose) - A real-time multi-person keypoint detection library for body, face, hands, and foot estimation\n* [Deep High-Resolution-Net](https:\/\/github.com\/leoxiaobin\/deep-high-resolution-net.pytorch) - A PyTorch implementation of CVPR2019 paper \"Deep High-Resolution Representation Learning for Human Pose Estimation\"\n* [dream-creator](https:\/\/github.com\/ProGamerGov\/dream-creator) - A PyTorch implementation of DeepDream. Allows individuals to quickly and easily train their own custom GoogleNet models with custom datasets for DeepDream.\n* [Lucent](https:\/\/github.com\/greentfrapp\/lucent) - Tensorflow and OpenAI Clarity's Lucid adapted for PyTorch.\n* [lightly](https:\/\/github.com\/lightly-ai\/lightly) - Lightly is a computer vision framework for self-supervised learning.\n* [Learnergy](https:\/\/github.com\/gugarosa\/learnergy) - Energy-based machine learning models built upon PyTorch.\n* [OpenVisionAPI](https:\/\/github.com\/openvisionapi) - Open source computer vision API based on open source models.\n* [IoT Owl](https:\/\/github.com\/Ret2Me\/IoT-Owl) - Light face detection and recognition system with huge possibilities, based on Microsoft Face API and TensorFlow made for small IoT devices like raspberry pi.\n* [Exadel CompreFace](https:\/\/github.com\/exadel-inc\/CompreFace) - face recognition system that can be easily integrated into any system without prior machine learning skills. CompreFace provides REST API for face recognition, face verification, face detection, face mask detection, landmark detection, age, and gender recognition and is easily deployed with docker.\n* [computer-vision-in-action](https:\/\/github.com\/Charmve\/computer-vision-in-action) - as known as ``L0CV``, is a new generation of computer vision open source online learning media, a cross-platform interactive learning framework integrating graphics, source code and HTML. the L0CV ecosystem \u2014 Notebook, Datasets, Source Code, and from Diving-in to Advanced \u2014 as well as the L0CV Hub.\n\n<a name=\"python-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [pkuseg-python](https:\/\/github.com\/lancopku\/pkuseg-python) - A better version of Jieba, developed by Peking University.\n* [NLTK](https:\/\/www.nltk.org\/) - A leading platform for building Python programs to work with human language data.\n* [Pattern](https:\/\/github.com\/clips\/pattern) - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.\n* [Quepy](https:\/\/github.com\/machinalis\/quepy) - A python framework to transform natural language questions to queries in a database query language.\n* [TextBlob](http:\/\/textblob.readthedocs.io\/en\/dev\/) - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.\n* [YAlign](https:\/\/github.com\/machinalis\/yalign) - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora. **[Deprecated]**\n* [jieba](https:\/\/github.com\/fxsjy\/jieba#jieba-1) - Chinese Words Segmentation Utilities.\n* [SnowNLP](https:\/\/github.com\/isnowfy\/snownlp) - A library for processing Chinese text.\n* [spammy](https:\/\/github.com\/tasdikrahman\/spammy) - A library for email Spam filtering built on top of nltk\n* [loso](https:\/\/github.com\/fangpenlin\/loso) - Another Chinese segmentation library. **[Deprecated]**\n* [genius](https:\/\/github.com\/duanhongyi\/genius) - A Chinese segment based on Conditional Random Field.\n* [KoNLPy](http:\/\/konlpy.org) - A Python package for Korean natural language processing.\n* [nut](https:\/\/github.com\/pprett\/nut) - Natural language Understanding Toolkit. **[Deprecated]**\n* [Rosetta](https:\/\/github.com\/columbia-applied-data-science\/rosetta) - Text processing tools and wrappers (e.g. Vowpal Wabbit)\n* [BLLIP Parser](https:\/\/pypi.org\/project\/bllipparser\/) - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser). **[Deprecated]**\n* [PyNLPl](https:\/\/github.com\/proycon\/pynlpl) - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for [FoLiA](https:\/\/proycon.github.io\/folia\/), but also ARPA language models, Moses phrasetables, GIZA++ alignments.\n* [PySS3](https:\/\/github.com\/sergioburdisso\/pyss3) - Python package that implements a novel white-box machine learning model for text classification, called SS3. Since SS3 has the ability to visually explain its rationale, this package also comes with easy-to-use interactive visualizations tools ([online demos](http:\/\/tworld.io\/ss3\/)).\n* [python-ucto](https:\/\/github.com\/proycon\/python-ucto) - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages).\n* [python-frog](https:\/\/github.com\/proycon\/python-frog) - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)\n* [python-zpar](https:\/\/github.com\/EducationalTestingService\/python-zpar) - Python bindings for [ZPar](https:\/\/github.com\/frcchang\/zpar), a statistical part-of-speech-tagger, constituency parser, and dependency parser for English.\n* [colibri-core](https:\/\/github.com\/proycon\/colibri-core) - Python binding to C++ library for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n* [spaCy](https:\/\/github.com\/explosion\/spaCy) - Industrial strength NLP with Python and Cython.\n* [PyStanfordDependencies](https:\/\/github.com\/dmcc\/PyStanfordDependencies) - Python interface for converting Penn Treebank trees to Stanford Dependencies.\n* [Distance](https:\/\/github.com\/doukremt\/distance) - Levenshtein and Hamming distance computation. **[Deprecated]**\n* [Fuzzy Wuzzy](https:\/\/github.com\/seatgeek\/fuzzywuzzy) - Fuzzy String Matching in Python.\n* [jellyfish](https:\/\/github.com\/jamesturk\/jellyfish) - a python library for doing approximate and phonetic matching of strings.\n* [editdistance](https:\/\/pypi.org\/project\/editdistance\/) - fast implementation of edit distance.\n* [textacy](https:\/\/github.com\/chartbeat-labs\/textacy) - higher-level NLP built on Spacy.\n* [stanford-corenlp-python](https:\/\/github.com\/dasmith\/stanford-corenlp-python) - Python wrapper for [Stanford CoreNLP](https:\/\/github.com\/stanfordnlp\/CoreNLP) **[Deprecated]**\n* [CLTK](https:\/\/github.com\/cltk\/cltk) - The Classical Language Toolkit.\n* [Rasa](https:\/\/github.com\/RasaHQ\/rasa) - A \"machine learning framework to automate text-and voice-based conversations.\"\n* [yase](https:\/\/github.com\/PPACI\/yase) - Transcode sentence (or other sequence) to list of word vector .\n* [Polyglot](https:\/\/github.com\/aboSamoor\/polyglot) - Multilingual text (NLP) processing toolkit.\n* [DrQA](https:\/\/github.com\/facebookresearch\/DrQA) - Reading Wikipedia to answer open-domain questions.\n* [Dedupe](https:\/\/github.com\/dedupeio\/dedupe) - A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.\n* [Snips NLU](https:\/\/github.com\/snipsco\/snips-nlu) - Natural Language Understanding library for intent classification and entity extraction\n* [NeuroNER](https:\/\/github.com\/Franck-Dernoncourt\/NeuroNER) - Named-entity recognition using neural networks providing state-of-the-art-results\n* [DeepPavlov](https:\/\/github.com\/deepmipt\/DeepPavlov\/) - conversational AI library with many pre-trained Russian NLP models.\n* [BigARTM](https:\/\/github.com\/bigartm\/bigartm) - topic modelling platform.\n* [NALP](https:\/\/github.com\/gugarosa\/nalp) - A Natural Adversarial Language Processing framework built over Tensorflow.\n* [DL Translate](https:\/\/github.com\/xhlulu\/dl-translate) - A deep learning-based translation library between 50 languages, built with `transformers`.\n\n<a name=\"python-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n * [RexMex](https:\/\/github.com\/AstraZeneca\/rexmex) -> A general purpose recommender metrics library for fair evaluation.\n * [ChemicalX](https:\/\/github.com\/AstraZeneca\/chemicalx) -> A PyTorch based deep learning library for drug pair scoring\n * [Microsoft ML for Apache Spark](https:\/\/github.com\/Azure\/mmlspark) -> A distributed machine learning framework Apache Spark\n * [Shapley](https:\/\/github.com\/benedekrozemberczki\/shapley) -> A data-driven framework to quantify the value of classifiers in a machine learning ensemble.\n * [igel](https:\/\/github.com\/nidhaloff\/igel) -> A delightful machine learning tool that allows you to train\/fit, test and use models **without writing code**\n * [ML Model building](https:\/\/github.com\/Shanky-21\/Machine_learning) -> A Repository Containing Classification, Clustering, Regression, Recommender Notebooks with illustration to make them.\n * [ML\/DL project template](https:\/\/github.com\/PyTorchLightning\/deep-learning-project-template)\n * [PyTorch Geometric Temporal](https:\/\/github.com\/benedekrozemberczki\/pytorch_geometric_temporal) -> A temporal extension of PyTorch Geometric for dynamic graph representation learning.\n * [Little Ball of Fur](https:\/\/github.com\/benedekrozemberczki\/littleballoffur) -> A graph sampling extension library for NetworkX with a Scikit-Learn like API.\n * [Karate Club](https:\/\/github.com\/benedekrozemberczki\/karateclub) -> An unsupervised machine learning extension library for NetworkX with a Scikit-Learn like API.\n* [Auto_ViML](https:\/\/github.com\/AutoViML\/Auto_ViML) -> Automatically Build Variant Interpretable ML models fast! Auto_ViML is pronounced \"auto vimal\", is a comprehensive and scalable Python AutoML toolkit with imbalanced handling, ensembling, stacking and built-in feature selection. Featured in <a href=\"https:\/\/towardsdatascience.com\/why-automl-is-an-essential-new-tool-for-data-scientists-2d9ab4e25e46?source=friends_link&sk=d03a0cc55c23deb497d546d6b9be0653\">Medium article<\/a>.\n* [PyOD](https:\/\/github.com\/yzhao062\/pyod) -> Python Outlier Detection, comprehensive and scalable Python toolkit for detecting outlying objects in multivariate data. Featured for Advanced models, including Neural Networks\/Deep Learning and Outlier Ensembles.\n* [steppy](https:\/\/github.com\/neptune-ml\/steppy) -> Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces a very simple interface that enables clean machine learning pipeline design.\n* [steppy-toolkit](https:\/\/github.com\/neptune-ml\/steppy-toolkit) -> Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective.\n* [CNTK](https:\/\/github.com\/Microsoft\/CNTK) - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. Documentation can be found [here](https:\/\/docs.microsoft.com\/cognitive-toolkit\/).\n* [Couler](https:\/\/github.com\/couler-proj\/couler) - Unified interface for constructing and managing machine learning workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.\n* [auto_ml](https:\/\/github.com\/ClimbsRocks\/auto_ml) - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, CatBoost, LightGBM, and soon, deep learning.\n* [dtaidistance](https:\/\/github.com\/wannesm\/dtaidistance) - High performance library for time series distances (DTW) and time series clustering.\n* [machine learning](https:\/\/github.com\/jeff1evesque\/machine-learning) - automated build consisting of a [web-interface](https:\/\/github.com\/jeff1evesque\/machine-learning#web-interface), and set of [programmatic-interface](https:\/\/github.com\/jeff1evesque\/machine-learning#programmatic-interface) API, for support vector machines. Corresponding dataset(s) are stored into a SQL database, then generated model(s) used for prediction(s), are stored into a NoSQL datastore.\n* [XGBoost](https:\/\/github.com\/dmlc\/xgboost) - Python bindings for eXtreme Gradient Boosting (Tree) Library.\n* [ChefBoost](https:\/\/github.com\/serengil\/chefboost) - a lightweight decision tree framework for Python with categorical feature support covering regular decision tree algorithms such as ID3, C4.5, CART, CHAID and regression tree; also some advanved bagging and boosting techniques such as gradient boosting, random forest and adaboost.\n* [Apache SINGA](https:\/\/singa.apache.org) - An Apache Incubating project for developing an open source machine learning library.\n* [Bayesian Methods for Hackers](https:\/\/github.com\/CamDavidsonPilon\/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) - Book\/iPython notebooks on Probabilistic Programming in Python.\n* [Featureforge](https:\/\/github.com\/machinalis\/featureforge) A set of tools for creating and testing machine learning features, with a scikit-learn compatible API.\n* [MLlib in Apache Spark](http:\/\/spark.apache.org\/docs\/latest\/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https:\/\/github.com\/Hydrospheredata\/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Towhee](https:\/\/towhee.io) - A Python module that encode unstructured data into embeddings.\n* [scikit-learn](https:\/\/scikit-learn.org\/) - A Python module for machine learning built on top of SciPy.\n* [metric-learn](https:\/\/github.com\/metric-learn\/metric-learn) - A Python module for metric learning.\n* [Intel(R) Extension for Scikit-learn](https:\/\/github.com\/intel\/scikit-learn-intelex) - A seamless way to speed up your Scikit-learn applications with no accuracy loss and code changes.\n* [SimpleAI](https:\/\/github.com\/simpleai-team\/simpleai) Python implementation of many of the artificial intelligence algorithms described in the book \"Artificial Intelligence, a Modern Approach\". It focuses on providing an easy to use, well documented and tested library.\n* [astroML](https:\/\/www.astroml.org\/) - Machine Learning and Data Mining for Astronomy.\n* [graphlab-create](https:\/\/turi.com\/products\/create\/docs\/) - A library with various machine learning models (regression, clustering, recommender systems, graph analytics, etc.) implemented on top of a disk-backed DataFrame.\n* [BigML](https:\/\/bigml.com) - A library that contacts external servers.\n* [pattern](https:\/\/github.com\/clips\/pattern) - Web mining module for Python.\n* [NuPIC](https:\/\/github.com\/numenta\/nupic) - Numenta Platform for Intelligent Computing.\n* [Pylearn2](https:\/\/github.com\/lisa-lab\/pylearn2) - A Machine Learning library based on [Theano](https:\/\/github.com\/Theano\/Theano). **[Deprecated]**\n* [keras](https:\/\/github.com\/keras-team\/keras) - High-level neural networks frontend for [TensorFlow](https:\/\/github.com\/tensorflow\/tensorflow), [CNTK](https:\/\/github.com\/Microsoft\/CNTK) and [Theano](https:\/\/github.com\/Theano\/Theano).\n* [Lasagne](https:\/\/github.com\/Lasagne\/Lasagne) - Lightweight library to build and train neural networks in Theano.\n* [hebel](https:\/\/github.com\/hannes-brt\/hebel) - GPU-Accelerated Deep Learning Library in Python. **[Deprecated]**\n* [Chainer](https:\/\/github.com\/chainer\/chainer) - Flexible neural network framework.\n* [prophet](https:\/\/facebook.github.io\/prophet\/) - Fast and automated time series forecasting framework by Facebook.\n* [gensim](https:\/\/github.com\/RaRe-Technologies\/gensim) - Topic Modelling for Humans.\n* [topik](https:\/\/github.com\/ContinuumIO\/topik) - Topic modelling toolkit. **[Deprecated]**\n* [PyBrain](https:\/\/github.com\/pybrain\/pybrain) - Another Python Machine Learning Library.\n* [Brainstorm](https:\/\/github.com\/IDSIA\/brainstorm) - Fast, flexible and fun neural networks. This is the successor of PyBrain.\n* [Surprise](https:\/\/surpriselib.com) - A scikit for building and analyzing recommender systems.\n* [implicit](https:\/\/implicit.readthedocs.io\/en\/latest\/quickstart.html) - Fast Python Collaborative Filtering for Implicit Datasets.\n* [LightFM](https:\/\/making.lyst.com\/lightfm\/docs\/home.html) -  A Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback.\n* [Crab](https:\/\/github.com\/muricoca\/crab) - A flexible, fast recommender engine. **[Deprecated]**\n* [python-recsys](https:\/\/github.com\/ocelma\/python-recsys) - A Python library for implementing a Recommender System.\n* [thinking bayes](https:\/\/github.com\/AllenDowney\/ThinkBayes) - Book on Bayesian Analysis.\n* [Image-to-Image Translation with Conditional Adversarial Networks](https:\/\/github.com\/williamFalcon\/pix2pix-keras) - Implementation of image to image (pix2pix) translation from the paper by [isola et al](https:\/\/arxiv.org\/pdf\/1611.07004.pdf).[DEEP LEARNING]\n* [Restricted Boltzmann Machines](https:\/\/github.com\/echen\/restricted-boltzmann-machines) -Restricted Boltzmann Machines in Python. [DEEP LEARNING]\n* [Bolt](https:\/\/github.com\/pprett\/bolt) - Bolt Online Learning Toolbox. **[Deprecated]**\n* [CoverTree](https:\/\/github.com\/patvarilly\/CoverTree) - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree **[Deprecated]**\n* [nilearn](https:\/\/github.com\/nilearn\/nilearn) - Machine learning for NeuroImaging in Python.\n* [neuropredict](https:\/\/github.com\/raamana\/neuropredict) - Aimed at novice machine learners and non-expert programmers, this package offers easy (no coding needed) and comprehensive machine learning (evaluation and full report of predictive performance WITHOUT requiring you to code) in Python for NeuroImaging and any other type of features. This is aimed at absorbing much of the ML workflow, unlike other packages like nilearn and pymvpa, which require you to learn their API and code to produce anything useful.\n* [imbalanced-learn](https:\/\/imbalanced-learn.org\/stable\/) - Python module to perform under sampling and oversampling with various techniques.\n* [imbalanced-ensemble](https:\/\/github.com\/ZhiningLiu1998\/imbalanced-ensemble) - Python toolbox for quick implementation, modification, evaluation, and visualization of ensemble learning algorithms for class-imbalanced data. Supports out-of-the-box multi-class imbalanced (long-tailed) classification.\n* [Shogun](https:\/\/github.com\/shogun-toolbox\/shogun) - The Shogun Machine Learning Toolbox.\n* [Pyevolve](https:\/\/github.com\/perone\/Pyevolve) - Genetic algorithm framework. **[Deprecated]**\n* [Caffe](https:\/\/github.com\/BVLC\/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind.\n* [breze](https:\/\/github.com\/breze-no-salt\/breze) - Theano based library for deep and recurrent neural networks.\n* [Cortex](https:\/\/github.com\/cortexlabs\/cortex) - Open source platform for deploying machine learning models in production.\n* [pyhsmm](https:\/\/github.com\/mattjj\/pyhsmm) - library for approximate unsupervised inference in Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.\n* [SKLL](https:\/\/github.com\/EducationalTestingService\/skll) - A wrapper around scikit-learn that makes it simpler to conduct experiments.\n* [neurolab](https:\/\/github.com\/zueve\/neurolab)\n* [Spearmint](https:\/\/github.com\/HIPS\/Spearmint) - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012. **[Deprecated]**\n* [Pebl](https:\/\/github.com\/abhik\/pebl\/) - Python Environment for Bayesian Learning. **[Deprecated]**\n* [Theano](https:\/\/github.com\/Theano\/Theano\/) - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python.\n* [TensorFlow](https:\/\/github.com\/tensorflow\/tensorflow\/) - Open source software library for numerical computation using data flow graphs.\n* [pomegranate](https:\/\/github.com\/jmschrei\/pomegranate) - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.\n* [python-timbl](https:\/\/github.com\/proycon\/python-timbl) - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.\n* [deap](https:\/\/github.com\/deap\/deap) - Evolutionary algorithm framework.\n* [pydeep](https:\/\/github.com\/andersbll\/deeppy) - Deep Learning In Python. **[Deprecated]**\n* [mlxtend](https:\/\/github.com\/rasbt\/mlxtend) - A library consisting of useful tools for data science and machine learning tasks.\n* [neon](https:\/\/github.com\/NervanaSystems\/neon) - Nervana's [high-performance](https:\/\/github.com\/soumith\/convnet-benchmarks) Python-based Deep Learning framework [DEEP LEARNING]. **[Deprecated]**\n* [Optunity](https:\/\/optunity.readthedocs.io\/en\/latest\/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.\n* [Neural Networks and Deep Learning](https:\/\/github.com\/mnielsen\/neural-networks-and-deep-learning) - Code samples for my book \"Neural Networks and Deep Learning\" [DEEP LEARNING].\n* [Annoy](https:\/\/github.com\/spotify\/annoy) - Approximate nearest neighbours implementation.\n* [TPOT](https:\/\/github.com\/EpistasisLab\/tpot) - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.\n* [pgmpy](https:\/\/github.com\/pgmpy\/pgmpy) A python library for working with Probabilistic Graphical Models.\n* [DIGITS](https:\/\/github.com\/NVIDIA\/DIGITS) - The Deep Learning GPU Training System (DIGITS) is a web application for training deep learning models.\n* [Orange](https:\/\/orange.biolab.si\/) - Open source data visualization and data analysis for novices and experts.\n* [MXNet](https:\/\/github.com\/apache\/incubator-mxnet) - Lightweight, Portable, Flexible Distributed\/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [milk](https:\/\/github.com\/luispedro\/milk) - Machine learning toolkit focused on supervised classification. **[Deprecated]**\n* [TFLearn](https:\/\/github.com\/tflearn\/tflearn) - Deep learning library featuring a higher-level API for TensorFlow.\n* [REP](https:\/\/github.com\/yandex\/rep) - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience. **[Deprecated]**\n* [rgf_python](https:\/\/github.com\/RGF-team\/rgf) - Python bindings for Regularized Greedy Forest (Tree) Library.\n* [skbayes](https:\/\/github.com\/AmazaspShumik\/sklearn-bayes) - Python package for Bayesian Machine Learning with scikit-learn API.\n* [fuku-ml](https:\/\/github.com\/fukuball\/fuku-ml) - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it's easy to use and easy to learn for beginners.\n* [Xcessiv](https:\/\/github.com\/reiinakano\/xcessiv) - A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling.\n* [PyTorch](https:\/\/github.com\/pytorch\/pytorch) - Tensors and Dynamic neural networks in Python with strong GPU acceleration\n* [PyTorch Lightning](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning) - The lightweight PyTorch wrapper for high-performance AI research.\n* [PyTorch Lightning Bolts](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning-bolts) - Toolbox of models, callbacks, and datasets for AI\/ML researchers.\n* [skorch](https:\/\/github.com\/skorch-dev\/skorch) - A scikit-learn compatible neural network library that wraps PyTorch.\n* [ML-From-Scratch](https:\/\/github.com\/eriklindernoren\/ML-From-Scratch) - Implementations of Machine Learning models from scratch in Python with a focus on transparency. Aims to showcase the nuts and bolts of ML in an accessible way.\n* [Edward](http:\/\/edwardlib.org\/) - A library for probabilistic modeling, inference, and criticism. Built on top of TensorFlow.\n* [xRBM](https:\/\/github.com\/omimo\/xRBM) - A library for Restricted Boltzmann Machine (RBM) and its conditional variants in Tensorflow.\n* [CatBoost](https:\/\/github.com\/catboost\/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, well documented and supports CPU and GPU (even multi-GPU) computation.\n* [stacked_generalization](https:\/\/github.com\/fukatani\/stacked_generalization) - Implementation of machine learning stacking technique as a handy library in Python.\n* [modAL](https:\/\/github.com\/modAL-python\/modAL) - A modular active learning framework for Python, built on top of scikit-learn.\n* [Cogitare](https:\/\/github.com\/cogitare-ai\/cogitare): A Modern, Fast, and Modular Deep Learning and Machine Learning framework for Python.\n* [Parris](https:\/\/github.com\/jgreenemi\/Parris) - Parris, the automated infrastructure setup tool for machine learning algorithms.\n* [neonrvm](https:\/\/github.com\/siavashserver\/neonrvm) - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.\n* [Turi Create](https:\/\/github.com\/apple\/turicreate) - Machine learning from Apple. Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.\n* [xLearn](https:\/\/github.com\/aksnzhy\/xlearn) - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.\n* [mlens](https:\/\/github.com\/flennerhag\/mlens) - A high performance, memory efficient, maximally parallelized ensemble learning, integrated with scikit-learn.\n* [Thampi](https:\/\/github.com\/scoremedia\/thampi) - Machine Learning Prediction System on AWS Lambda\n* [MindsDB](https:\/\/github.com\/mindsdb\/mindsdb) - Open Source framework to streamline use of neural networks.\n* [Microsoft Recommenders](https:\/\/github.com\/Microsoft\/Recommenders): Examples and best practices for building recommendation systems, provided as Jupyter notebooks. The repo contains some of the latest state of the art algorithms from Microsoft Research as well as from other companies and institutions.\n* [StellarGraph](https:\/\/github.com\/stellargraph\/stellargraph): Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data.\n* [BentoML](https:\/\/github.com\/bentoml\/bentoml): Toolkit for package and deploy machine learning models for serving in production\n* [MiraiML](https:\/\/github.com\/arthurpaulino\/miraiml): An asynchronous engine for continuous & autonomous machine learning, built for real-time usage.\n* [numpy-ML](https:\/\/github.com\/ddbourgin\/numpy-ml): Reference implementations of ML models written in numpy\n* [Neuraxle](https:\/\/github.com\/Neuraxio\/Neuraxle): A framework providing the right abstractions to ease research, development, and deployment of your ML pipelines.\n* [Cornac](https:\/\/github.com\/PreferredAI\/cornac) - A comparative framework for multimodal recommender systems with a focus on models leveraging auxiliary data.\n* [JAX](https:\/\/github.com\/google\/jax) - JAX is Autograd and XLA, brought together for high-performance machine learning research.\n* [Catalyst](https:\/\/github.com\/catalyst-team\/catalyst) - High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code\/ideas reusing. Being able to research\/develop something new, rather than write another regular train loop.\n* [Fastai](https:\/\/github.com\/fastai\/fastai) - High-level wrapper built on the top of Pytorch which supports vision, text, tabular data and collaborative filtering.\n* [scikit-multiflow](https:\/\/github.com\/scikit-multiflow\/scikit-multiflow) - A machine learning framework for multi-output\/multi-label and stream data.\n* [Lightwood](https:\/\/github.com\/mindsdb\/lightwood) - A Pytorch based framework that breaks down machine learning problems into smaller blocks that can be glued together seamlessly with objective to build predictive models with one line of code.\n* [bayeso](https:\/\/github.com\/jungtaekkim\/bayeso) - A simple, but essential Bayesian optimization package, written in Python.\n* [mljar-supervised](https:\/\/github.com\/mljar\/mljar-supervised) - An Automated Machine Learning (AutoML) python package for tabular data. It can handle: Binary Classification, MultiClass Classification and Regression. It provides explanations and markdown reports.\n* [evostra](https:\/\/github.com\/alirezamika\/evostra) - A fast Evolution Strategy implementation in Python.\n* [Determined](https:\/\/github.com\/determined-ai\/determined) - Scalable deep learning training platform, including integrated support for distributed training, hyperparameter tuning, experiment tracking, and model management.\n* [PySyft](https:\/\/github.com\/OpenMined\/PySyft) - A Python library for secure and private Deep Learning built on PyTorch and TensorFlow.\n* [PyGrid](https:\/\/github.com\/OpenMined\/PyGrid\/) - Peer-to-peer network of data owners and data scientists who can collectively train AI models using PySyft\n* [sktime](https:\/\/github.com\/alan-turing-institute\/sktime) - A unified framework for machine learning with time series\n* [OPFython](https:\/\/github.com\/gugarosa\/opfython) - A Python-inspired implementation of the Optimum-Path Forest classifier.\n* [Opytimizer](https:\/\/github.com\/gugarosa\/opytimizer) - Python-based meta-heuristic optimization techniques.\n* [Gradio](https:\/\/github.com\/gradio-app\/gradio) - A Python library for quickly creating and sharing demos of models. Debug models interactively in your browser, get feedback from collaborators, and generate public links without deploying anything.\n* [Hub](https:\/\/github.com\/activeloopai\/Hub) - Fastest unstructured dataset management for TensorFlow\/PyTorch. Stream & version-control data. Store even petabyte-scale data in a single numpy-like array on the cloud accessible on any machine. Visit [activeloop.ai](https:\/\/activeloop.ai) for more info.\n* [Synthia](https:\/\/github.com\/dmey\/synthia) - Multidimensional synthetic data generation in Python.\n* [ByteHub](https:\/\/github.com\/bytehub-ai\/bytehub) - An easy-to-use, Python-based feature store. Optimized for time-series data.\n* [Backprop](https:\/\/github.com\/backprop-ai\/backprop) - Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.\n* [River](https:\/\/github.com\/online-ml\/river): A framework for general purpose online machine learning.\n* [FEDOT](https:\/\/github.com\/nccr-itmo\/FEDOT): An AutoML framework for the automated design of composite modeling pipelines. It can handle classification, regression, and time series forecasting tasks on different types of data (including multi-modal datasets).\n* [Sklearn-genetic-opt](https:\/\/github.com\/rodrigo-arenas\/Sklearn-genetic-opt): An AutoML package for hyperparameters tuning using evolutionary algorithms, with built-in callbacks, plotting, remote logging and more.\n* [Evidently](https:\/\/github.com\/evidentlyai\/evidently): Interactive reports to analyze machine learning models during validation or production monitoring.\n* [Streamlit](https:\/\/github.com\/streamlit\/streamlit): Streamlit is an framework to create beautiful data apps in hours, not weeks.\n* [Optuna](https:\/\/github.com\/optuna\/optuna): Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.\n* [Deepchecks](https:\/\/github.com\/deepchecks\/deepchecks): Validation & testing of machine learning models and data during model development, deployment, and production. This includes checks and suites related to various types of issues, such as model performance, data integrity, distribution mismatches, and more.\n\n<a name=\"python-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n* [DataVisualization](https:\/\/github.com\/Shanky-21\/Data_visualization) - A Github Repository Where you can Learn Datavisualizatoin Basics to Intermediate level.\n* [Cartopy](https:\/\/scitools.org.uk\/cartopy\/docs\/latest\/) - Cartopy is a Python package designed for geospatial data processing in order to produce maps and other geospatial data analyses.\n* [SciPy](https:\/\/www.scipy.org\/) - A Python-based ecosystem of open-source software for mathematics, science, and engineering.\n* [NumPy](https:\/\/www.numpy.org\/) - A fundamental package for scientific computing with Python.\n* [AutoViz](https:\/\/github.com\/AutoViML\/AutoViz) AutoViz performs automatic visualization of any dataset with a single line of Python code. Give it any input file (CSV, txt or json) of any size and AutoViz will visualize it. See <a href=\"https:\/\/towardsdatascience.com\/autoviz-a-new-tool-for-automated-visualization-ec9c1744a6ad?source=friends_link&sk=c9e9503ec424b191c6096d7e3f515d10\">Medium article<\/a>.\n* [Numba](https:\/\/numba.pydata.org\/) - Python JIT (just in time) compiler to LLVM aimed at scientific Python by the developers of Cython and NumPy.\n* [Mars](https:\/\/github.com\/mars-project\/mars) - A tensor-based framework for large-scale data computation which is often regarded as a parallel and distributed version of NumPy.\n* [NetworkX](https:\/\/networkx.github.io\/) - A high-productivity software for complex networks.\n* [igraph](https:\/\/igraph.org\/python\/) - binding to igraph library - General purpose graph library.\n* [Pandas](https:\/\/pandas.pydata.org\/) - A library providing high-performance, easy-to-use data structures and data analysis tools.\n* [ParaMonte](https:\/\/github.com\/cdslaborg\/paramonte) - A general-purpose Python library for Bayesian data analysis and visualization via serial\/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https:\/\/www.cdslab.org\/paramonte\/).\n* [Vaex](https:\/\/github.com\/vaexio\/vaex) - A high performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. Documentation can be found [here](https:\/\/vaex.io\/docs\/index.html).\n* [Open Mining](https:\/\/github.com\/mining\/mining) - Business Intelligence (BI) in Python (Pandas web interface) **[Deprecated]**\n* [PyMC](https:\/\/github.com\/pymc-devs\/pymc) - Markov Chain Monte Carlo sampling toolkit.\n* [zipline](https:\/\/github.com\/quantopian\/zipline) - A Pythonic algorithmic trading library.\n* [PyDy](https:\/\/www.pydy.org\/) - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.\n* [SymPy](https:\/\/github.com\/sympy\/sympy) - A Python library for symbolic mathematics.\n* [statsmodels](https:\/\/github.com\/statsmodels\/statsmodels) - Statistical modeling and econometrics in Python.\n* [astropy](https:\/\/www.astropy.org\/) - A community Python library for Astronomy.\n* [matplotlib](https:\/\/matplotlib.org\/) - A Python 2D plotting library.\n* [bokeh](https:\/\/github.com\/bokeh\/bokeh) - Interactive Web Plotting for Python.\n* [plotly](https:\/\/plot.ly\/python\/) - Collaborative web plotting for Python and matplotlib.\n* [altair](https:\/\/github.com\/altair-viz\/altair) - A Python to Vega translator.\n* [d3py](https:\/\/github.com\/mikedewar\/d3py) - A plotting library for Python, based on [D3.js](https:\/\/d3js.org\/).\n* [PyDexter](https:\/\/github.com\/D3xterjs\/pydexter) - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.\n* [ggplot](https:\/\/github.com\/yhat\/ggpy) - Same API as ggplot2 for R. **[Deprecated]**\n* [ggfortify](https:\/\/github.com\/sinhrks\/ggfortify) - Unified interface to ggplot2 popular R packages.\n* [Kartograph.py](https:\/\/github.com\/kartograph\/kartograph.py) - Rendering beautiful SVG maps in Python.\n* [pygal](http:\/\/pygal.org\/en\/stable\/) - A Python SVG Charts Creator.\n* [PyQtGraph](https:\/\/github.com\/pyqtgraph\/pyqtgraph) - A pure-python graphics and GUI library built on PyQt4 \/ PySide and NumPy.\n* [pycascading](https:\/\/github.com\/twitter\/pycascading) **[Deprecated]**\n* [Petrel](https:\/\/github.com\/AirSage\/Petrel) - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.\n* [Blaze](https:\/\/github.com\/blaze\/blaze) - NumPy and Pandas interface to Big Data.\n* [emcee](https:\/\/github.com\/dfm\/emcee) - The Python ensemble sampling toolkit for affine-invariant MCMC.\n* [windML](https:\/\/github.com\/cigroup-ol\/windml) - A Python Framework for Wind Energy Analysis and Prediction.\n* [vispy](https:\/\/github.com\/vispy\/vispy) - GPU-based high-performance interactive OpenGL 2D\/3D data visualization library.\n* [cerebro2](https:\/\/github.com\/numenta\/nupic.cerebro2) A web-based visualization and debugging platform for NuPIC. **[Deprecated]**\n* [NuPIC Studio](https:\/\/github.com\/htm-community\/nupic.studio) An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool! **[Deprecated]**\n* [SparklingPandas](https:\/\/github.com\/sparklingpandas\/sparklingpandas) Pandas on PySpark (POPS).\n* [Seaborn](https:\/\/seaborn.pydata.org\/) - A python visualization library based on matplotlib.\n* [ipychart](https:\/\/github.com\/nicohlr\/ipychart) - The power of Chart.js in Jupyter Notebook.\n* [bqplot](https:\/\/github.com\/bloomberg\/bqplot) - An API for plotting in Jupyter (IPython).\n* [pastalog](https:\/\/github.com\/rewonc\/pastalog) - Simple, realtime visualization of neural network training performance.\n* [Superset](https:\/\/github.com\/apache\/incubator-superset) - A data exploration platform designed to be visual, intuitive, and interactive.\n* [Dora](https:\/\/github.com\/nathanepstein\/dora) - Tools for exploratory data analysis in Python.\n* [Ruffus](http:\/\/www.ruffus.org.uk) - Computation Pipeline library for python.\n* [SOMPY](https:\/\/github.com\/sevamoo\/SOMPY) - Self Organizing Map written in Python (Uses neural networks for data analysis).\n* [somoclu](https:\/\/github.com\/peterwittek\/somoclu) Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.\n* [HDBScan](https:\/\/github.com\/lmcinnes\/hdbscan) - implementation of the hdbscan algorithm in Python - used for clustering\n* [visualize_ML](https:\/\/github.com\/ayush1997\/visualize_ML) - A python package for data exploration and data analysis. **[Deprecated]**\n* [scikit-plot](https:\/\/github.com\/reiinakano\/scikit-plot) - A visualization library for quick and easy generation of common plots in data analysis and machine learning.\n* [Bowtie](https:\/\/github.com\/jwkvam\/bowtie) - A dashboard library for interactive visualizations using flask socketio and react.\n* [lime](https:\/\/github.com\/marcotcr\/lime) - Lime is about explaining what machine learning classifiers (or models) are doing. It is able to explain any black box classifier, with two or more classes.\n* [PyCM](https:\/\/github.com\/sepandhaghighi\/pycm) - PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters\n* [Dash](https:\/\/github.com\/plotly\/dash) - A framework for creating analytical web applications built on top of Plotly.js, React, and Flask\n* [Lambdo](https:\/\/github.com\/asavinov\/lambdo) - A workflow engine for solving machine learning problems by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation via user-defined (Python) functions.\n* [TensorWatch](https:\/\/github.com\/microsoft\/tensorwatch) - Debugging and visualization tool for machine learning and data science. It extensively leverages Jupyter Notebook to show real-time visualizations of data in running processes such as machine learning training.\n* [dowel](https:\/\/github.com\/rlworkgroup\/dowel) - A little logger for machine learning research. Output any object to the terminal, CSV, TensorBoard, text logs on disk, and more with just one call to `logger.log()`.\n\n<a name=\"python-misc-scripts--ipython-notebooks--codebases\"><\/a>\n#### Misc Scripts \/ iPython Notebooks \/ Codebases\n* [MiniGrad](https:\/\/github.com\/kennysong\/minigrad) \u2013\u00a0A minimal, educational, Pythonic implementation of autograd (~100 loc).\n* [Map\/Reduce implementations of common ML algorithms](https:\/\/github.com\/Yannael\/BigDataAnalytics_INFOH515): Jupyter notebooks that cover how to implement from scratch different ML algorithms (ordinary least squares, gradient descent, k-means, alternating least squares), using Python NumPy, and how to then make these implementations scalable using Map\/Reduce and Spark.\n* [BioPy](https:\/\/github.com\/jaredthecoder\/BioPy) - Biologically-Inspired and Machine Learning Algorithms in Python. **[Deprecated]**\n* [CAEs for Data Assimilation](https:\/\/github.com\/julianmack\/Data_Assimilation) - Convolutional autoencoders for 3D image\/field compression applied to reduced order [Data Assimilation](https:\/\/en.wikipedia.org\/wiki\/Data_assimilation).\n* [handsonml](https:\/\/github.com\/ageron\/handson-ml) - Fundamentals of machine learning in python.\n* [SVM Explorer](https:\/\/github.com\/plotly\/dash-svm) - Interactive SVM Explorer, using Dash and scikit-learn\n* [pattern_classification](https:\/\/github.com\/rasbt\/pattern_classification)\n* [thinking stats 2](https:\/\/github.com\/Wavelets\/ThinkStats2)\n* [hyperopt](https:\/\/github.com\/hyperopt\/hyperopt-sklearn)\n* [numpic](https:\/\/github.com\/numenta\/nupic)\n* [2012-paper-diginorm](https:\/\/github.com\/dib-lab\/2012-paper-diginorm)\n* [A gallery of interesting IPython notebooks](https:\/\/github.com\/jupyter\/jupyter\/wiki\/A-gallery-of-interesting-Jupyter-Notebooks)\n* [ipython-notebooks](https:\/\/github.com\/ogrisel\/notebooks)\n* [data-science-ipython-notebooks](https:\/\/github.com\/donnemartin\/data-science-ipython-notebooks) - Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines.\n* [decision-weights](https:\/\/github.com\/CamDavidsonPilon\/decision-weights)\n* [Sarah Palin LDA](https:\/\/github.com\/Wavelets\/sarah-palin-lda) - Topic Modeling the Sarah Palin emails.\n* [Diffusion Segmentation](https:\/\/github.com\/Wavelets\/diffusion-segmentation) - A collection of image segmentation algorithms based on diffusion methods.\n* [Scipy Tutorials](https:\/\/github.com\/Wavelets\/scipy-tutorials) - SciPy tutorials. This is outdated, check out scipy-lecture-notes.\n* [Crab](https:\/\/github.com\/marcelcaraciolo\/crab) - A recommendation engine library for Python.\n* [BayesPy](https:\/\/github.com\/maxsklar\/BayesPy) - Bayesian Inference Tools in Python.\n* [scikit-learn tutorials](https:\/\/github.com\/GaelVaroquaux\/scikit-learn-tutorial) - Series of notebooks for learning scikit-learn.\n* [sentiment-analyzer](https:\/\/github.com\/madhusudancs\/sentiment-analyzer) - Tweets Sentiment Analyzer\n* [sentiment_classifier](https:\/\/github.com\/kevincobain2000\/sentiment_classifier) - Sentiment classifier using word sense disambiguation.\n* [group-lasso](https:\/\/github.com\/fabianp\/group_lasso) - Some experiments with the coordinate descent algorithm used in the (Sparse) Group Lasso model.\n* [jProcessing](https:\/\/github.com\/kevincobain2000\/jProcessing) - Kanji \/ Hiragana \/ Katakana to Romaji Converter. Edict Dictionary & parallel sentences Search. Sentence Similarity between two JP Sentences. Sentiment Analysis of Japanese Text. Run Cabocha(ISO--8859-1 configured) in Python.\n* [mne-python-notebooks](https:\/\/github.com\/mne-tools\/mne-python-notebooks) - IPython notebooks for EEG\/MEG data processing using mne-python.\n* [Neon Course](https:\/\/github.com\/NervanaSystems\/neon_course) - IPython notebooks for a complete course around understanding Nervana's Neon.\n* [pandas cookbook](https:\/\/github.com\/jvns\/pandas-cookbook) - Recipes for using Python's pandas library.\n* [climin](https:\/\/github.com\/BRML\/climin) - Optimization library focused on machine learning, pythonic implementations of gradient descent, LBFGS, rmsprop, adadelta and others.\n* [Allen Downey\u2019s Data Science Course](https:\/\/github.com\/AllenDowney\/DataScience) - Code for Data Science at Olin College, Spring 2014.\n* [Allen Downey\u2019s Think Bayes Code](https:\/\/github.com\/AllenDowney\/ThinkBayes) - Code repository for Think Bayes.\n* [Allen Downey\u2019s Think Complexity Code](https:\/\/github.com\/AllenDowney\/ThinkComplexity) - Code for Allen Downey's book Think Complexity.\n* [Allen Downey\u2019s Think OS Code](https:\/\/github.com\/AllenDowney\/ThinkOS) - Text and supporting code for Think OS: A Brief Introduction to Operating Systems.\n* [Python Programming for the Humanities](https:\/\/www.karsdorp.io\/python-course\/) - Course for Python programming for the Humanities, assuming no prior knowledge. Heavy focus on text processing \/ NLP.\n* [GreatCircle](https:\/\/github.com\/mwgg\/GreatCircle) - Library for calculating great circle distance.\n* [Optunity examples](http:\/\/optunity.readthedocs.io\/en\/latest\/notebooks\/index.html) - Examples demonstrating how to use Optunity in synergy with machine learning libraries.\n* [Dive into Machine Learning  with Python Jupyter notebook and scikit-learn](https:\/\/github.com\/hangtwenty\/dive-into-machine-learning) - \"I learned Python by hacking first, and getting serious *later.* I wanted to do this with Machine Learning. If this is your style, join me in getting a bit ahead of yourself.\"\n* [TDB](https:\/\/github.com\/ericjang\/tdb) - TensorDebugger (TDB) is a visual debugger for deep learning. It features interactive, node-by-node debugging and visualization for TensorFlow.\n* [Suiron](https:\/\/github.com\/kendricktan\/suiron\/) - Machine Learning for RC Cars.\n* [Introduction to machine learning with scikit-learn](https:\/\/github.com\/justmarkham\/scikit-learn-videos) - IPython notebooks from Data School's video tutorials on scikit-learn.\n* [Practical XGBoost in Python](https:\/\/parrotprediction.teachable.com\/p\/practical-xgboost-in-python) - comprehensive online course about using XGBoost in Python.\n* [Introduction to Machine Learning with Python](https:\/\/github.com\/amueller\/introduction_to_ml_with_python) - Notebooks and code for the book \"Introduction to Machine Learning with Python\"\n* [Pydata book](https:\/\/github.com\/wesm\/pydata-book) - Materials and IPython notebooks for \"Python for Data Analysis\" by Wes McKinney, published by O'Reilly Media\n* [Homemade Machine Learning](https:\/\/github.com\/trekhleb\/homemade-machine-learning) - Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained\n* [Prodmodel](https:\/\/github.com\/prodmodel\/prodmodel) - Build tool for data science pipelines.\n* [the-elements-of-statistical-learning](https:\/\/github.com\/maitbayev\/the-elements-of-statistical-learning) - This repository contains Jupyter notebooks implementing the algorithms found in the book and summary of the textbook.\n* [Hyperparameter-Optimization-of-Machine-Learning-Algorithms](https:\/\/github.com\/LiYangHart\/Hyperparameter-Optimization-of-Machine-Learning-Algorithms) - Code for hyperparameter tuning\/optimization of machine learning and deep learning algorithms.\n* [Heart_Disease-Prediction](https:\/\/github.com\/ShivamChoudhary17\/Heart_Disease) - Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n* [Flight Fare Prediction](https:\/\/github.com\/ShivamChoudhary17\/Flight_Fare_Prediction) - This basically to gauge the understanding of Machine Learning Workflow and Regression technique in specific.\n* [Keras Tuner](https:\/\/github.com\/keras-team\/keras-tuner) - An easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search.\n\n\n\n<a name=\"python-neural-networks\"><\/a>\n#### Neural Networks\n\n* [nn_builder](https:\/\/github.com\/p-christ\/nn_builder) - nn_builder is a python package that lets you build neural networks in 1 line\n* [NeuralTalk](https:\/\/github.com\/karpathy\/neuraltalk) - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.\n* [Neuron](https:\/\/github.com\/molcik\/python-neuron) - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg\u2013Marquardt algorithm.\n\n* [NeuralTalk](https:\/\/github.com\/karpathy\/neuraltalk2) - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences. **[Deprecated]**\n* [Neuron](https:\/\/github.com\/molcik\/python-neuron) - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg\u2013Marquardt algorithm. **[Deprecated]**\n* [Data Driven Code](https:\/\/github.com\/atmb4u\/data-driven-code) - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.\n* [Machine Learning, Data Science and Deep Learning with Python](https:\/\/www.manning.com\/livevideo\/machine-learning-data-science-and-deep-learning-with-python) - LiveVideo course that covers machine learning, Tensorflow, artificial intelligence, and neural networks.\n* [TResNet: High Performance GPU-Dedicated Architecture](https:\/\/github.com\/mrT23\/TResNet) - TResNet models were designed and optimized to give the best speed-accuracy tradeoff out there on GPUs.\n* [TResNet: Simple and powerful neural network library for python](https:\/\/github.com\/zueve\/neurolab) - Variety of supported types of Artificial Neural Network and learning algorithms.\n* [Jina AI](https:\/\/jina.ai\/) An easier way to build neural search in the cloud. Compatible with Jupyter Notebooks.\n* [sequitur](https:\/\/github.com\/shobrook\/sequitur) PyTorch library for creating and training sequence autoencoders in just two lines of code\n\n<a name=\"python-survival-analysis\"><\/a>\n#### Python Survival Analysis\n* [lifelines](https:\/\/github.com\/CamDavidsonPilon\/lifelines) - lifelines is a complete survival analysis library, written in pure Python\n* [Scikit-Survival](https:\/\/github.com\/sebp\/scikit-survival) - scikit-survival is a Python module for survival analysis built on top of scikit-learn. It allows doing survival analysis while utilizing the power of scikit-learn, e.g., for pre-processing or doing cross-validation.\n\n<a name=\"federated-learning\"><\/a>\n#### Federated Learning\n* [Flower](https:\/\/flower.dev\/) - A unified approach to federated learning, analytics, and evaluation. Federate any workload, any ML framework, and any programming language.\n* [PySyft](https:\/\/github.com\/OpenMined\/PySyft) - A Python library for secure and private Deep Learning.\n* [Tensorflow-Federated](https:\/\/www.tensorflow.org\/federated) A federated learning framework for machine learning and other computations on decentralized data.\n\n<a name=\"python-kaggle-competition-source-code\"><\/a>\n#### Kaggle Competition Source Code\n* [open-solution-home-credit](https:\/\/github.com\/neptune-ml\/open-solution-home-credit) -> source code and [experiments results](https:\/\/app.neptune.ml\/neptune-ml\/Home-Credit-Default-Risk) for [Home Credit Default Risk](https:\/\/www.kaggle.com\/c\/home-credit-default-risk).\n* [open-solution-googleai-object-detection](https:\/\/github.com\/neptune-ml\/open-solution-googleai-object-detection) -> source code and [experiments results](https:\/\/app.neptune.ml\/neptune-ml\/Google-AI-Object-Detection-Challenge) for [Google AI Open Images - Object Detection Track](https:\/\/www.kaggle.com\/c\/google-ai-open-images-object-detection-track).\n* [open-solution-salt-identification](https:\/\/github.com\/neptune-ml\/open-solution-salt-identification) -> source code and [experiments results](https:\/\/app.neptune.ml\/neptune-ml\/Salt-Detection) for [TGS Salt Identification Challenge](https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge).\n* [open-solution-ship-detection](https:\/\/github.com\/neptune-ml\/open-solution-ship-detection) -> source code and [experiments results](https:\/\/app.neptune.ml\/neptune-ml\/Ships) for [Airbus Ship Detection Challenge](https:\/\/www.kaggle.com\/c\/airbus-ship-detection).\n* [open-solution-data-science-bowl-2018](https:\/\/github.com\/neptune-ml\/open-solution-data-science-bowl-2018) -> source code and [experiments results](https:\/\/app.neptune.ml\/neptune-ml\/Data-Science-Bowl-2018) for [2018 Data Science Bowl](https:\/\/www.kaggle.com\/c\/data-science-bowl-2018).\n* [open-solution-value-prediction](https:\/\/github.com\/neptune-ml\/open-solution-value-prediction) -> source code and [experiments results](https:\/\/app.neptune.ml\/neptune-ml\/Santander-Value-Prediction-Challenge) for [Santander Value Prediction Challenge](https:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge).\n* [open-solution-toxic-comments](https:\/\/github.com\/neptune-ml\/open-solution-toxic-comments) -> source code for [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge).\n* [wiki challenge](https:\/\/github.com\/hammer\/wikichallenge) - An implementation of Dell Zhang's solution to Wikipedia's Participation Challenge on Kaggle.\n* [kaggle insults](https:\/\/github.com\/amueller\/kaggle_insults) - Kaggle Submission for \"Detecting Insults in Social Commentary\".\n* [kaggle_acquire-valued-shoppers-challenge](https:\/\/github.com\/MLWave\/kaggle_acquire-valued-shoppers-challenge) - Code for the Kaggle acquire valued shoppers challenge.\n* [kaggle-cifar](https:\/\/github.com\/zygmuntz\/kaggle-cifar) - Code for the CIFAR-10 competition at Kaggle, uses cuda-convnet.\n* [kaggle-blackbox](https:\/\/github.com\/zygmuntz\/kaggle-blackbox) - Deep learning made easy.\n* [kaggle-accelerometer](https:\/\/github.com\/zygmuntz\/kaggle-accelerometer) - Code for Accelerometer Biometric Competition at Kaggle.\n* [kaggle-advertised-salaries](https:\/\/github.com\/zygmuntz\/kaggle-advertised-salaries) - Predicting job salaries from ads - a Kaggle competition.\n* [kaggle amazon](https:\/\/github.com\/zygmuntz\/kaggle-amazon) - Amazon access control challenge.\n* [kaggle-bestbuy_big](https:\/\/github.com\/zygmuntz\/kaggle-bestbuy_big) - Code for the Best Buy competition at Kaggle.\n* [kaggle-bestbuy_small](https:\/\/github.com\/zygmuntz\/kaggle-bestbuy_small)\n* [Kaggle Dogs vs. Cats](https:\/\/github.com\/kastnerkyle\/kaggle-dogs-vs-cats) - Code for Kaggle Dogs vs. Cats competition.\n* [Kaggle Galaxy Challenge](https:\/\/github.com\/benanne\/kaggle-galaxies) - Winning solution for the Galaxy Challenge on Kaggle.\n* [Kaggle Gender](https:\/\/github.com\/zygmuntz\/kaggle-gender) - A Kaggle competition: discriminate gender based on handwriting.\n* [Kaggle Merck](https:\/\/github.com\/zygmuntz\/kaggle-merck) - Merck challenge at Kaggle.\n* [Kaggle Stackoverflow](https:\/\/github.com\/zygmuntz\/kaggle-stackoverflow) - Predicting closed questions on Stack Overflow.\n* [kaggle_acquire-valued-shoppers-challenge](https:\/\/github.com\/MLWave\/kaggle_acquire-valued-shoppers-challenge) - Code for the Kaggle acquire valued shoppers challenge.\n* [wine-quality](https:\/\/github.com\/zygmuntz\/wine-quality) - Predicting wine quality.\n\n<a name=\"python-reinforcement-learning\"><\/a>\n#### Reinforcement Learning\n* [DeepMind Lab](https:\/\/github.com\/deepmind\/lab) - DeepMind Lab is a 3D learning environment based on id Software's Quake III Arena via ioquake3 and other open source software. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.\n* [Gym](https:\/\/github.com\/openai\/gym) - OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n* [Serpent.AI](https:\/\/github.com\/SerpentAI\/SerpentAI) - Serpent.AI is a game agent framework that allows you to turn any video game you own into a sandbox to develop AI and machine learning experiments. For both researchers and hobbyists.\n* [ViZDoom](https:\/\/github.com\/mwydmuch\/ViZDoom) - ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.\n* [Roboschool](https:\/\/github.com\/openai\/roboschool) - Open-source software for robot simulation, integrated with OpenAI Gym.\n* [Retro](https:\/\/github.com\/openai\/retro) - Retro Games in Gym\n* [SLM Lab](https:\/\/github.com\/kengz\/SLM-Lab) - Modular Deep Reinforcement Learning framework in PyTorch.\n* [Coach](https:\/\/github.com\/NervanaSystems\/coach) - Reinforcement Learning Coach by Intel\u00ae AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms\n* [garage](https:\/\/github.com\/rlworkgroup\/garage) - A toolkit for reproducible reinforcement learning research\n* [metaworld](https:\/\/github.com\/rlworkgroup\/metaworld) - An open source robotics benchmark for meta- and multi-task reinforcement learning\n* [acme](https:\/\/deepmind.com\/research\/publications\/Acme) - An Open Source Distributed Framework for Reinforcement Learning that makes build and train your agents easily.\n* [Spinning Up](https:\/\/spinningup.openai.com) - An educational resource designed to let anyone learn to become a skilled practitioner in deep reinforcement learning\n* [Maze](https:\/\/github.com\/enlite-ai\/maze) - Application-oriented deep reinforcement learning framework addressing real-world decision problems.\n* [RLlib](https:\/\/github.com\/ray-project\/ray) - RLlib is an industry level, highly scalable RL library for tf and torch, based on Ray. It's used by companies like Amazon and Microsoft to solve real-world decision making problems at scale.\n\n<a name=\"ruby\"><\/a>\n## Ruby\n\n<a name=\"ruby-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [Awesome NLP with Ruby](https:\/\/github.com\/arbox\/nlp-with-ruby) - Curated link list for practical natural language processing in Ruby.\n* [Treat](https:\/\/github.com\/louismullie\/treat) - Text REtrieval and Annotation Toolkit, definitely the most comprehensive toolkit I\u2019ve encountered so far for Ruby.\n* [Stemmer](https:\/\/github.com\/aurelian\/ruby-stemmer) - Expose libstemmer_c to Ruby. **[Deprecated]**\n* [Raspell](https:\/\/sourceforge.net\/projects\/raspell\/) - raspell is an interface binding for ruby. **[Deprecated]**\n* [UEA Stemmer](https:\/\/github.com\/ealdent\/uea-stemmer) - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing.\n* [Twitter-text-rb](https:\/\/github.com\/twitter\/twitter-text\/tree\/master\/rb) - A library that does auto linking and extraction of usernames, lists and hashtags in tweets.\n\n<a name=\"ruby-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Awesome Machine Learning with Ruby](https:\/\/github.com\/arbox\/machine-learning-with-ruby) - Curated list of ML related resources for Ruby.\n* [Ruby Machine Learning](https:\/\/github.com\/tsycho\/ruby-machine-learning) - Some Machine Learning algorithms, implemented in Ruby. **[Deprecated]**\n* [Machine Learning Ruby](https:\/\/github.com\/mizoR\/machine-learning-ruby) **[Deprecated]**\n* [jRuby Mahout](https:\/\/github.com\/vasinov\/jruby_mahout) - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby. **[Deprecated]**\n* [CardMagic-Classifier](https:\/\/github.com\/cardmagic\/classifier) - A general classifier module to allow Bayesian and other types of classifications.\n* [rb-libsvm](https:\/\/github.com\/febeling\/rb-libsvm) - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines.\n* [Scoruby](https:\/\/github.com\/asafschers\/scoruby) - Creates Random Forest classifiers from PMML files.\n* [rumale](https:\/\/github.com\/yoshoku\/rumale) - Rumale is a machine learning library in Ruby\n\n<a name=\"ruby-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [rsruby](https:\/\/github.com\/alexgutteridge\/rsruby) - Ruby - R bridge.\n* [data-visualization-ruby](https:\/\/github.com\/chrislo\/data_visualisation_ruby) - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby. **[Deprecated]**\n* [ruby-plot](https:\/\/www.ruby-toolbox.com\/projects\/ruby-plot) - gnuplot wrapper for Ruby, especially for plotting ROC curves into SVG files. **[Deprecated]**\n* [plot-rb](https:\/\/github.com\/zuhao\/plotrb) - A plotting library in Ruby built on top of Vega and D3. **[Deprecated]**\n* [scruffy](https:\/\/github.com\/delano\/scruffy) - A beautiful graphing toolkit for Ruby.\n* [SciRuby](http:\/\/sciruby.com\/)\n* [Glean](https:\/\/github.com\/glean\/glean) - A data management tool for humans. **[Deprecated]**\n* [Bioruby](https:\/\/github.com\/bioruby\/bioruby)\n* [Arel](https:\/\/github.com\/nkallen\/arel) **[Deprecated]**\n\n<a name=\"ruby-misc\"><\/a>\n#### Misc\n\n* [Big Data For Chimps](https:\/\/github.com\/infochimps-labs\/big_data_for_chimps)\n* [Listof](https:\/\/github.com\/kevincobain2000\/listof) - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, json or hash. [Demo\/Search for a list](http:\/\/kevincobain2000.github.io\/listof\/)\n\n\n<a name=\"rust\"><\/a>\n## Rust\n\n<a name=\"rust-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n* [smartcore](https:\/\/github.com\/smartcorelib\/smartcore) - \"The Most Advanced Machine Learning Library In Rust.\"\n* [linfa](https:\/\/github.com\/rust-ml\/linfa) - a comprehensive toolkit to build Machine Learning applications with Rust\n* [deeplearn-rs](https:\/\/github.com\/tedsta\/deeplearn-rs) - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.\n* [rustlearn](https:\/\/github.com\/maciejkula\/rustlearn) - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.\n* [rusty-machine](https:\/\/github.com\/AtheMathmo\/rusty-machine) - a pure-rust machine learning library.\n* [leaf](https:\/\/github.com\/autumnai\/leaf) - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe. Available under the MIT license. [**[Deprecated]**](https:\/\/medium.com\/@mjhirn\/tensorflow-wins-89b78b29aafb#.s0a3uy4cc)\n* [RustNN](https:\/\/github.com\/jackm321\/RustNN) - RustNN is a feedforward neural network library. **[Deprecated]**\n* [RusticSOM](https:\/\/github.com\/avinashshenoy97\/RusticSOM) - A Rust library for Self Organising Maps (SOM).\n\n\n<a name=\"r\"><\/a>\n## R\n\n<a name=\"r-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [ahaz](https:\/\/cran.r-project.org\/web\/packages\/ahaz\/index.html) - ahaz: Regularization for semiparametric additive hazards regression. **[Deprecated]**\n* [arules](https:\/\/cran.r-project.org\/web\/packages\/arules\/index.html) - arules: Mining Association Rules and Frequent Itemsets\n* [biglasso](https:\/\/cran.r-project.org\/web\/packages\/biglasso\/index.html) - biglasso: Extending Lasso Model Fitting to Big Data in R.\n* [bmrm](https:\/\/cran.r-project.org\/web\/packages\/bmrm\/index.html) - bmrm: Bundle Methods for Regularized Risk Minimization Package.\n* [Boruta](https:\/\/cran.r-project.org\/web\/packages\/Boruta\/index.html) - Boruta: A wrapper algorithm for all-relevant feature selection.\n* [bst](https:\/\/cran.r-project.org\/web\/packages\/bst\/index.html) - bst: Gradient Boosting.\n* [C50](https:\/\/cran.r-project.org\/web\/packages\/C50\/index.html) - C50: C5.0 Decision Trees and Rule-Based Models.\n* [caret](https:\/\/topepo.github.io\/caret\/index.html) - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.\n* [caretEnsemble](https:\/\/cran.r-project.org\/web\/packages\/caretEnsemble\/index.html) - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models. **[Deprecated]**\n* [CatBoost](https:\/\/github.com\/catboost\/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box for R.\n* [Clever Algorithms For Machine Learning](https:\/\/machinelearningmastery.com\/)\n* [CORElearn](https:\/\/cran.r-project.org\/web\/packages\/CORElearn\/index.html) - CORElearn: Classification, regression, feature evaluation and ordinal evaluation.\n-* [CoxBoost](https:\/\/cran.r-project.org\/web\/packages\/CoxBoost\/index.html) - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks **[Deprecated]**\n* [Cubist](https:\/\/cran.r-project.org\/web\/packages\/Cubist\/index.html) - Cubist: Rule- and Instance-Based Regression Modeling.\n* [e1071](https:\/\/cran.r-project.org\/web\/packages\/e1071\/index.html) - e1071: Misc Functions of the Department of Statistics (e1071), TU Wien\n* [earth](https:\/\/cran.r-project.org\/web\/packages\/earth\/index.html) - earth: Multivariate Adaptive Regression Spline Models\n* [elasticnet](https:\/\/cran.r-project.org\/web\/packages\/elasticnet\/index.html) - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA.\n* [ElemStatLearn](https:\/\/cran.r-project.org\/web\/packages\/ElemStatLearn\/index.html) - ElemStatLearn: Data sets, functions and examples from the book: \"The Elements of Statistical Learning, Data Mining, Inference, and Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman.\n* [evtree](https:\/\/cran.r-project.org\/web\/packages\/evtree\/index.html) - evtree: Evolutionary Learning of Globally Optimal Trees.\n* [forecast](https:\/\/cran.r-project.org\/web\/packages\/forecast\/index.html) - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models.\n* [forecastHybrid](https:\/\/cran.r-project.org\/web\/packages\/forecastHybrid\/index.html) - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the \"forecast\" package.\n* [fpc](https:\/\/cran.r-project.org\/web\/packages\/fpc\/index.html) - fpc: Flexible procedures for clustering.\n* [frbs](https:\/\/cran.r-project.org\/web\/packages\/frbs\/index.html) - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks. **[Deprecated]**\n* [GAMBoost](https:\/\/cran.r-project.org\/web\/packages\/GAMBoost\/index.html) - GAMBoost: Generalized linear and additive models by likelihood based boosting. **[Deprecated]**\n* [gamboostLSS](https:\/\/cran.r-project.org\/web\/packages\/gamboostLSS\/index.html) - gamboostLSS: Boosting Methods for GAMLSS.\n* [gbm](https:\/\/cran.r-project.org\/web\/packages\/gbm\/index.html) - gbm: Generalized Boosted Regression Models.\n* [glmnet](https:\/\/cran.r-project.org\/web\/packages\/glmnet\/index.html) - glmnet: Lasso and elastic-net regularized generalized linear models.\n* [glmpath](https:\/\/cran.r-project.org\/web\/packages\/glmpath\/index.html) - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model.\n* [GMMBoost](https:\/\/cran.r-project.org\/web\/packages\/GMMBoost\/index.html) - GMMBoost: Likelihood-based Boosting for Generalized mixed models. **[Deprecated]**\n* [grplasso](https:\/\/cran.r-project.org\/web\/packages\/grplasso\/index.html) - grplasso: Fitting user specified models with Group Lasso penalty.\n* [grpreg](https:\/\/cran.r-project.org\/web\/packages\/grpreg\/index.html) - grpreg: Regularization paths for regression models with grouped covariates.\n* [h2o](https:\/\/cran.r-project.org\/web\/packages\/h2o\/index.html) - A framework for fast, parallel, and distributed machine learning algorithms at scale -- Deeplearning, Random forests, GBM, KMeans, PCA, GLM.\n* [hda](https:\/\/cran.r-project.org\/web\/packages\/hda\/index.html) - hda: Heteroscedastic Discriminant Analysis. **[Deprecated]**\n* [Introduction to Statistical Learning](https:\/\/www-bcf.usc.edu\/~gareth\/ISL\/)\n* [ipred](https:\/\/cran.r-project.org\/web\/packages\/ipred\/index.html) - ipred: Improved Predictors.\n* [kernlab](https:\/\/cran.r-project.org\/web\/packages\/kernlab\/index.html) - kernlab: Kernel-based Machine Learning Lab.\n* [klaR](https:\/\/cran.r-project.org\/web\/packages\/klaR\/index.html) - klaR: Classification and visualization.\n* [L0Learn](https:\/\/cran.r-project.org\/web\/packages\/L0Learn\/index.html) - L0Learn: Fast algorithms for best subset selection.\n* [lars](https:\/\/cran.r-project.org\/web\/packages\/lars\/index.html) - lars: Least Angle Regression, Lasso and Forward Stagewise. **[Deprecated]**\n* [lasso2](https:\/\/cran.r-project.org\/web\/packages\/lasso2\/index.html) - lasso2: L1 constrained estimation aka \u2018lasso\u2019.\n* [LiblineaR](https:\/\/cran.r-project.org\/web\/packages\/LiblineaR\/index.html) - LiblineaR: Linear Predictive Models Based On The Liblinear C\/C++ Library.\n* [LogicReg](https:\/\/cran.r-project.org\/web\/packages\/LogicReg\/index.html) - LogicReg: Logic Regression.\n* [Machine Learning For Hackers](https:\/\/github.com\/johnmyleswhite\/ML_for_Hackers)\n* [maptree](https:\/\/cran.r-project.org\/web\/packages\/maptree\/index.html) - maptree: Mapping, pruning, and graphing tree models. **[Deprecated]**\n* [mboost](https:\/\/cran.r-project.org\/web\/packages\/mboost\/index.html) - mboost: Model-Based Boosting.\n* [medley](https:\/\/www.kaggle.com\/general\/3661) - medley: Blending regression models, using a greedy stepwise approach.\n* [mlr](https:\/\/cran.r-project.org\/web\/packages\/mlr\/index.html) - mlr: Machine Learning in R.\n* [ncvreg](https:\/\/cran.r-project.org\/web\/packages\/ncvreg\/index.html) - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models.\n* [nnet](https:\/\/cran.r-project.org\/web\/packages\/nnet\/index.html) - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models. **[Deprecated]**\n* [pamr](https:\/\/cran.r-project.org\/web\/packages\/pamr\/index.html) - pamr: Pam: prediction analysis for microarrays. **[Deprecated]**\n* [party](https:\/\/cran.r-project.org\/web\/packages\/party\/index.html) - party: A Laboratory for Recursive Partitioning\n* [partykit](https:\/\/cran.r-project.org\/web\/packages\/partykit\/index.html) - partykit: A Toolkit for Recursive Partitioning.\n* [penalized](https:\/\/cran.r-project.org\/web\/packages\/penalized\/index.html) - penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the Cox model.\n* [penalizedLDA](https:\/\/cran.r-project.org\/web\/packages\/penalizedLDA\/index.html) - penalizedLDA: Penalized classification using Fisher's linear discriminant. **[Deprecated]**\n* [penalizedSVM](https:\/\/cran.r-project.org\/web\/packages\/penalizedSVM\/index.html) - penalizedSVM: Feature Selection SVM using penalty functions.\n* [quantregForest](https:\/\/cran.r-project.org\/web\/packages\/quantregForest\/index.html) - quantregForest: Quantile Regression Forests.\n* [randomForest](https:\/\/cran.r-project.org\/web\/packages\/randomForest\/index.html) - randomForest: Breiman and Cutler's random forests for classification and regression.\n* [randomForestSRC](https:\/\/cran.r-project.org\/web\/packages\/randomForestSRC\/index.html) - randomForestSRC: Random Forests for Survival, Regression and Classification (RF-SRC).\n* [rattle](https:\/\/cran.r-project.org\/web\/packages\/rattle\/index.html) - rattle: Graphical user interface for data mining in R.\n* [rda](https:\/\/cran.r-project.org\/web\/packages\/rda\/index.html) - rda: Shrunken Centroids Regularized Discriminant Analysis.\n* [rdetools](https:\/\/cran.r-project.org\/web\/packages\/rdetools\/index.html) - rdetools: Relevant Dimension Estimation (RDE) in Feature Spaces. **[Deprecated]**\n* [REEMtree](https:\/\/cran.r-project.org\/web\/packages\/REEMtree\/index.html) - REEMtree: Regression Trees with Random Effects for Longitudinal (Panel) Data. **[Deprecated]**\n* [relaxo](https:\/\/cran.r-project.org\/web\/packages\/relaxo\/index.html) - relaxo: Relaxed Lasso. **[Deprecated]**\n* [rgenoud](https:\/\/cran.r-project.org\/web\/packages\/rgenoud\/index.html) - rgenoud: R version of GENetic Optimization Using Derivatives\n* [Rmalschains](https:\/\/cran.r-project.org\/web\/packages\/Rmalschains\/index.html) - Rmalschains: Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R.\n* [rminer](https:\/\/cran.r-project.org\/web\/packages\/rminer\/index.html) - rminer: Simpler use of data mining methods (e.g. NN and SVM) in classification and regression. **[Deprecated]**\n* [ROCR](https:\/\/cran.r-project.org\/web\/packages\/ROCR\/index.html) - ROCR: Visualizing the performance of scoring classifiers. **[Deprecated]**\n* [RoughSets](https:\/\/cran.r-project.org\/web\/packages\/RoughSets\/index.html) - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories. **[Deprecated]**\n* [rpart](https:\/\/cran.r-project.org\/web\/packages\/rpart\/index.html) - rpart: Recursive Partitioning and Regression Trees.\n* [RPMM](https:\/\/cran.r-project.org\/web\/packages\/RPMM\/index.html) - RPMM: Recursively Partitioned Mixture Model.\n* [RSNNS](https:\/\/cran.r-project.org\/web\/packages\/RSNNS\/index.html) - RSNNS: Neural Networks in R using the Stuttgart Neural Network Simulator (SNNS).\n* [RWeka](https:\/\/cran.r-project.org\/web\/packages\/RWeka\/index.html) - RWeka: R\/Weka interface.\n* [RXshrink](https:\/\/cran.r-project.org\/web\/packages\/RXshrink\/index.html) - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression.\n* [sda](https:\/\/cran.r-project.org\/web\/packages\/sda\/index.html) - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection. **[Deprecated]**\n* [spectralGraphTopology](https:\/\/cran.r-project.org\/web\/packages\/spectralGraphTopology\/index.html) - spectralGraphTopology: Learning Graphs from Data via Spectral Constraints.\n* [SuperLearner](https:\/\/github.com\/ecpolley\/SuperLearner) - Multi-algorithm ensemble learning packages.\n* [svmpath](https:\/\/cran.r-project.org\/web\/packages\/svmpath\/index.html) - svmpath: svmpath: the SVM Path algorithm. **[Deprecated]**\n* [tgp](https:\/\/cran.r-project.org\/web\/packages\/tgp\/index.html) - tgp: Bayesian treed Gaussian process models. **[Deprecated]**\n* [tree](https:\/\/cran.r-project.org\/web\/packages\/tree\/index.html) - tree: Classification and regression trees.\n* [varSelRF](https:\/\/cran.r-project.org\/web\/packages\/varSelRF\/index.html) - varSelRF: Variable selection using random forests.\n* [XGBoost.R](https:\/\/github.com\/tqchen\/xgboost\/tree\/master\/R-package) - R binding for eXtreme Gradient Boosting (Tree) Library.\n* [Optunity](https:\/\/optunity.readthedocs.io\/en\/latest\/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.\n* [igraph](https:\/\/igraph.org\/r\/) - binding to igraph library - General purpose graph library.\n* [MXNet](https:\/\/github.com\/apache\/incubator-mxnet) - Lightweight, Portable, Flexible Distributed\/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [TDSP-Utilities](https:\/\/github.com\/Azure\/Azure-TDSP-Utilities) - Two data science utilities in R from Microsoft: 1) Interactive Data Exploration, Analysis, and Reporting (IDEAR) ; 2) Automated Modeling and Reporting (AMR).\n\n<a name=\"r-data-analysis--data-visualization\"><\/a>\n#### Data Manipulation | Data Analysis | Data Visualization\n\n* [dplyr](https:\/\/www.rdocumentation.org\/packages\/dplyr\/versions\/0.7.8) - A data manipulation package that helps to solve the most common data manipulation problems.\n* [ggplot2](https:\/\/ggplot2.tidyverse.org\/) - A data visualization package based on the grammar of graphics.\n* [tmap](https:\/\/cran.r-project.org\/web\/packages\/tmap\/vignettes\/tmap-getstarted.html) for visualizing geospatial data with static maps and [leaflet](https:\/\/rstudio.github.io\/leaflet\/) for interactive maps\n* [tm](https:\/\/www.rdocumentation.org\/packages\/tm\/) and [quanteda](https:\/\/quanteda.io\/) are the main packages for managing,  analyzing, and visualizing textual data.\n* [shiny](https:\/\/shiny.rstudio.com\/) is the basis for truly interactive displays and dashboards in R. However, some measure of interactivity can be achieved with [htmlwidgets](https:\/\/www.htmlwidgets.org\/) bringing javascript libraries to R. These include, [plotly](https:\/\/plot.ly\/r\/), [dygraphs](http:\/\/rstudio.github.io\/dygraphs), [highcharter](http:\/\/jkunst.com\/highcharter\/), and several others.\n\n<a name=\"sas\"><\/a>\n## SAS\n\n<a name=\"sas-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Visual Data Mining and Machine Learning](https:\/\/www.sas.com\/en_us\/software\/visual-data-mining-machine-learning.html) - Interactive, automated, and programmatic modeling with the latest machine learning algorithms in and end-to-end analytics environment, from data prep to deployment. Free trial available.\n* [Enterprise Miner](https:\/\/www.sas.com\/en_us\/software\/enterprise-miner.html) - Data mining and machine learning that creates deployable models using a GUI or code.\n* [Factory Miner](https:\/\/www.sas.com\/en_us\/software\/factory-miner.html) - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.\n\n<a name=\"sas-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [SAS\/STAT](https:\/\/www.sas.com\/en_us\/software\/stat.html) - For conducting advanced statistical analysis.\n* [University Edition](https:\/\/www.sas.com\/en_us\/software\/university-edition.html) - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.\n\n<a name=\"sas-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [Contextual Analysis](https:\/\/www.sas.com\/en_us\/software\/contextual-analysis.html) - Add structure to unstructured text using a GUI.\n* [Sentiment Analysis](https:\/\/www.sas.com\/en_us\/software\/sentiment-analysis.html) - Extract sentiment from text using a GUI.\n* [Text Miner](https:\/\/www.sas.com\/en_us\/software\/text-miner.html) - Text mining using a GUI or code.\n\n<a name=\"sas-demos-and-scripts\"><\/a>\n#### Demos and Scripts\n\n* [ML_Tables](https:\/\/github.com\/sassoftware\/enlighten-apply\/tree\/master\/ML_tables) - Concise cheat sheets containing machine learning best practices.\n* [enlighten-apply](https:\/\/github.com\/sassoftware\/enlighten-apply) - Example code and materials that illustrate applications of SAS machine learning techniques.\n* [enlighten-integration](https:\/\/github.com\/sassoftware\/enlighten-integration) - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.\n* [enlighten-deep](https:\/\/github.com\/sassoftware\/enlighten-deep) - Example code and materials that illustrate using neural networks with several hidden layers in SAS.\n* [dm-flow](https:\/\/github.com\/sassoftware\/dm-flow) - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.\n\n\n<a name=\"scala\"><\/a>\n## Scala\n\n<a name=\"scala-natural-language-processing\"><\/a>\n#### Natural Language Processing\n\n* [ScalaNLP](http:\/\/www.scalanlp.org\/) - ScalaNLP is a suite of machine learning and numerical computing libraries.\n* [Breeze](https:\/\/github.com\/scalanlp\/breeze) - Breeze is a numerical processing library for Scala.\n* [Chalk](https:\/\/github.com\/scalanlp\/chalk) - Chalk is a natural language processing library. **[Deprecated]**\n* [FACTORIE](https:\/\/github.com\/factorie\/factorie) - FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.\n* [Montague](https:\/\/github.com\/Workday\/upshot-montague) - Montague is a semantic parsing library for Scala with an easy-to-use DSL.\n* [Spark NLP](https:\/\/github.com\/JohnSnowLabs\/spark-nlp) - Natural language processing library built on top of Apache Spark ML to provide simple, performant, and accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.\n\n<a name=\"scala-data-analysis--data-visualization\"><\/a>\n#### Data Analysis \/ Data Visualization\n\n* [NDScala](https:\/\/github.com\/SciScala\/NDScala) - N-dimensional arrays in Scala 3. Think NumPy ndarray, but with compile-time type-checking\/inference over shapes, tensor\/axis labels & numeric data types\n* [MLlib in Apache Spark](https:\/\/spark.apache.org\/docs\/latest\/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https:\/\/github.com\/Hydrospheredata\/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Scalding](https:\/\/github.com\/twitter\/scalding) - A Scala API for Cascading.\n* [Summing Bird](https:\/\/github.com\/twitter\/summingbird) - Streaming MapReduce with Scalding and Storm.\n* [Algebird](https:\/\/github.com\/twitter\/algebird) - Abstract Algebra for Scala.\n* [xerial](https:\/\/github.com\/xerial\/xerial) - Data management utilities for Scala. **[Deprecated]**\n* [PredictionIO](https:\/\/github.com\/apache\/predictionio) - PredictionIO, a machine learning server for software developers and data engineers.\n* [BIDMat](https:\/\/github.com\/BIDData\/BIDMat) - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.\n* [Flink](https:\/\/flink.apache.org\/) - Open source platform for distributed stream and batch data processing.\n* [Spark Notebook](http:\/\/spark-notebook.io) - Interactive and Reactive Data Science using Scala and Spark.\n\n<a name=\"scala-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Microsoft ML for Apache Spark](https:\/\/github.com\/Azure\/mmlspark) -> A distributed machine learning framework Apache Spark\n* [ONNX-Scala](https:\/\/github.com\/EmergentOrder\/onnx-scala) - An ONNX (Open Neural Network eXchange) API and backend for typeful, functional deep learning in Scala (3).\n* [DeepLearning.scala](https:\/\/deeplearning.thoughtworks.school\/) - Creating statically typed dynamic neural networks from object-oriented & functional programming constructs.\n* [Conjecture](https:\/\/github.com\/etsy\/Conjecture) - Scalable Machine Learning in Scalding.\n* [brushfire](https:\/\/github.com\/stripe\/brushfire) - Distributed decision tree ensemble learning in Scala.\n* [ganitha](https:\/\/github.com\/tresata\/ganitha) - Scalding powered machine learning. **[Deprecated]**\n* [adam](https:\/\/github.com\/bigdatagenomics\/adam) - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.\n* [bioscala](https:\/\/github.com\/bioscala\/bioscala) - Bioinformatics for the Scala programming language\n* [BIDMach](https:\/\/github.com\/BIDData\/BIDMach) - CPU and GPU-accelerated Machine Learning Library.\n* [Figaro](https:\/\/github.com\/p2t2\/figaro) - a Scala library for constructing probabilistic models.\n* [H2O Sparkling Water](https:\/\/github.com\/h2oai\/sparkling-water) - H2O and Spark interoperability.\n* [FlinkML in Apache Flink](https:\/\/ci.apache.org\/projects\/flink\/flink-docs-master\/dev\/libs\/ml\/index.html) - Distributed machine learning library in Flink.\n* [DynaML](https:\/\/github.com\/transcendent-ai-labs\/DynaML) - Scala Library\/REPL for Machine Learning Research.\n* [Saul](https:\/\/github.com\/CogComp\/saul) - Flexible Declarative Learning-Based Programming.\n* [SwiftLearner](https:\/\/github.com\/valdanylchuk\/swiftlearner\/) - Simply written algorithms to help study ML or write your own implementations.\n* [Smile](https:\/\/haifengl.github.io\/) - Statistical Machine Intelligence and Learning Engine.\n* [doddle-model](https:\/\/github.com\/picnicml\/doddle-model) - An in-memory machine learning library built on top of Breeze. It provides immutable objects and exposes its functionality through a scikit-learn-like API.\n* [TensorFlow Scala](https:\/\/github.com\/eaplatanios\/tensorflow_scala) -   Strongly-typed Scala API for TensorFlow.\n\n<a name=\"scheme\"><\/a>\n## Scheme\n\n<a name=\"scheme-neural-networks\"><\/a>\n#### Neural Networks\n\n* [layer](https:\/\/github.com\/cloudkj\/layer) - Neural network inference from the command line, implemented in [CHICKEN Scheme](https:\/\/www.call-cc.org\/).\n\n<a name=\"swift\"><\/a>\n## Swift\n\n<a name=\"swift-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n\n* [Bender](https:\/\/github.com\/xmartlabs\/Bender) - Fast Neural Networks framework built on top of Metal. Supports TensorFlow models.\n* [Swift AI](https:\/\/github.com\/Swift-AI\/Swift-AI) - Highly optimized artificial intelligence and machine learning library written in Swift.\n* [Swift for Tensorflow](https:\/\/github.com\/tensorflow\/swift) - a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond.\n* [BrainCore](https:\/\/github.com\/alejandro-isaza\/BrainCore) - The iOS and OS X neural network framework.\n* [swix](https:\/\/github.com\/stsievert\/swix) - A bare bones library that includes a general matrix language and wraps some OpenCV for iOS development. **[Deprecated]**\n* [AIToolbox](https:\/\/github.com\/KevinCoble\/AIToolbox) - A toolbox framework of AI modules written in Swift: Graphs\/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.\n* [MLKit](https:\/\/github.com\/Somnibyte\/MLKit) - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.\n* [Swift Brain](https:\/\/github.com\/vlall\/Swift-Brain) - The first neural network \/ machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc...\n* [Perfect TensorFlow](https:\/\/github.com\/PerfectlySoft\/Perfect-TensorFlow) - Swift Language Bindings of TensorFlow. Using native TensorFlow models on both macOS \/ Linux.\n* [PredictionBuilder](https:\/\/github.com\/denissimon\/prediction-builder-swift) - A library for machine learning that builds predictions using a linear regression.\n* [Awesome CoreML](https:\/\/github.com\/SwiftBrain\/awesome-CoreML-models) - A curated list of pretrained CoreML models.\n* [Awesome Core ML Models](https:\/\/github.com\/likedan\/Awesome-CoreML-Models) - A curated list of machine learning models in CoreML format.\n\n<a name=\"tensorflow\"><\/a>\n## TensorFlow\n\n<a name=\"tensorflow-general-purpose-machine-learning\"><\/a>\n#### General-Purpose Machine Learning\n* [Awesome TensorFlow](https:\/\/github.com\/jtoy\/awesome-tensorflow) - A list of all things related to TensorFlow.\n* [Golden TensorFlow](https:\/\/golden.com\/wiki\/TensorFlow) - A page of content on TensorFlow, including academic papers and links to related topics.\n\n<a name=\"tools\"><\/a>\n## Tools\n\n<a name=\"tools-neural-networks\"><\/a>\n#### Neural Networks\n* [layer](https:\/\/github.com\/cloudkj\/layer) - Neural network inference from the command line\n\n<a name=\"tools-misc\"><\/a>\n#### Misc\n\n* [milvus](https:\/\/milvus.io) \u2013 Milvus is [open source](https:\/\/github.com\/milvus-io\/milvus) vector database for production AI, written in Go and C++, scalable and blazing fast for billions of embedding vectors. \n* [Weaviate](https:\/\/www.semi.technology\/developers\/weaviate\/current\/) \u2013 Weaviate is an [open source](https:\/\/github.com\/semi-technologies\/weaviate) vector search engine and vector database. Weaviate uses machine learning to vectorize and store data, and to find answers to natural language queries. With Weaviate you can also bring your custom ML models to production scale.\n* [MLReef](https:\/\/about.mlreef.com\/) - MLReef is an end-to-end development platform using the power of git to give structure and deep collaboration possibilities to the ML development process.\n* [Pinecone](https:\/\/www.pinecone.io\/) - Vector database for applications that require real-time, scalable vector embedding and similarity search.\n* [CatalyzeX](https:\/\/chrome.google.com\/webstore\/detail\/code-finder-for-research\/aikkeehnlfpamidigaffhfmgbkdeheil) - Browser extension ([Chrome](https:\/\/chrome.google.com\/webstore\/detail\/code-finder-for-research\/aikkeehnlfpamidigaffhfmgbkdeheil) and [Firefox](https:\/\/addons.mozilla.org\/en-US\/firefox\/addon\/code-finder-catalyzex\/)) that automatically finds and shows code implementations for machine learning papers anywhere: Google, Twitter, Arxiv, Scholar, etc.\n* [ML Workspace](https:\/\/github.com\/ml-tooling\/ml-workspace) - All-in-one web-based IDE for machine learning and data science. The workspace is deployed as a docker container and is preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch) and dev tools (e.g., Jupyter, VS Code).\n* [Notebooks](https:\/\/github.com\/rlan\/notebooks) - A starter kit for Jupyter notebooks and machine learning. Companion docker images consist of all combinations of python versions, machine learning frameworks (Keras, PyTorch and Tensorflow) and CPU\/CUDA versions.\n* [DVC](https:\/\/github.com\/iterative\/dvc) - Data Science Version Control is an open-source version control system for machine learning projects with pipelines support. It makes ML projects reproducible and shareable.\n* [DVClive](https:\/\/github.com\/iterative\/dvclive) - Python library for experiment metrics logging into simply formatted local files.\n* [Kedro](https:\/\/github.com\/quantumblacklabs\/kedro\/) - Kedro is a data and development workflow framework that implements best practices for data pipelines with an eye towards productionizing machine learning models.\n* [guild.ai](https:\/\/guild.ai\/) - Tool to log, analyze, compare and \"optimize\" experiments. It's cross-platform and framework independent, and provided integrated visualizers such as tensorboard.\n* [Sacred](https:\/\/github.com\/IDSIA\/sacred) - Python tool to help  you configure, organize, log and reproduce experiments. Like a notebook lab in the context of Chemistry\/Biology. The community has built multiple add-ons leveraging the proposed standard.\n* [MLFlow](https:\/\/mlflow.org\/) - platform to manage the ML lifecycle, including experimentation, reproducibility and deployment. Framework and language agnostic, take a look at all the built-in integrations.\n* [Weights & Biases](https:\/\/www.wandb.com\/) - Machine learning experiment tracking, dataset versioning, hyperparameter search, visualization, and collaboration\n* More tools to improve the ML lifecycle: [Catalyst](https:\/\/github.com\/catalyst-team\/catalyst), [PachydermIO](https:\/\/www.pachyderm.io\/). The following are Github-alike and targeting teams [Weights & Biases](https:\/\/www.wandb.com\/), [Neptune.ai](https:\/\/neptune.ai\/), [Comet.ml](https:\/\/www.comet.ml\/), [Valohai.ai](https:\/\/valohai.com\/), [DAGsHub](https:\/\/DAGsHub.com\/).\n* [MachineLearningWithTensorFlow2ed](https:\/\/www.manning.com\/books\/machine-learning-with-tensorflow-second-edition) - a book on general purpose machine learning techniques regression, classification, unsupervised clustering, reinforcement learning, auto encoders, convolutional neural networks, RNNs, LSTMs, using TensorFlow 1.14.1.\n* [m2cgen](https:\/\/github.com\/BayesWitnesses\/m2cgen) - A tool that allows the conversion of ML models into native code (Java, C, Python, Go, JavaScript, Visual Basic, C#, R, PowerShell, PHP, Dart) with zero dependencies.\n* [CML](https:\/\/github.com\/iterative\/cml) - A library for doing continuous integration with ML projects. Use GitHub Actions & GitLab CI to train and evaluate models in production like environments and automatically generate visual reports with metrics and graphs in pull\/merge requests. Framework & language agnostic.\n* [Pythonizr](https:\/\/pythonizr.com) - An online tool to generate boilerplate machine learning code that uses scikit-learn.\n* [Flyte](https:\/\/flyte.org\/) - Flyte makes it easy to create concurrent, scalable, and maintainable workflows for machine learning and data processing.\n* [Chaos Genius](https:\/\/github.com\/chaos-genius\/chaos_genius\/) - ML powered analytics engine for outlier\/anomaly detection and root cause analysis.\n\n<a name=\"books\"><\/a>\n## Books\n\n* [Distributed Machine Learning Patterns](https:\/\/github.com\/terrytangyuan\/distributed-ml-patterns)  - This book teaches you how to take machine learning models from your personal laptop to large distributed clusters. You\u2019ll explore key concepts and patterns behind successful distributed machine learning systems, and learn technologies like TensorFlow, Kubernetes, Kubeflow, and Argo Workflows directly from a key maintainer and contributor, with real-world scenarios and hands-on projects.\n* [Grokking Machine Learning](https:\/\/www.manning.com\/books\/grokking-machine-learning) - Grokking Machine Learning teaches you how to apply ML to your projects using only standard Python code and high school-level math.\n* [Machine Learning Bookcamp](https:\/\/www.manning.com\/books\/machine-learning-bookcamp) - Learn the essentials of machine learning by completing a carefully designed set of real-world projects.\n\n<a name=\"credits\"><\/a>\n* [Netron](https:\/\/netron.app\/) - An opensource viewer for neural network, deep learning and machine learning models\n* [Teachable Machine](https:\/\/teachablemachine.withgoogle.com\/) - Train Machine Learning models on the fly to recognize your own images, sounds, & poses.\n* [Model Zoo](https:\/\/modelzoo.co\/) - Discover open source deep learning code and pretrained models.\n\n## Credits\n\n* Some of the python libraries were cut-and-pasted from [vinta](https:\/\/github.com\/vinta\/awesome-python)\n* References for Go were mostly cut-and-pasted from [gopherdata](https:\/\/github.com\/gopherdata\/resources\/tree\/master\/tooling)\n","102":".. -*- mode: rst -*-\n\n|Azure|_ |Travis|_ |Codecov|_ |CircleCI|_ |Nightly wheels|_ |Black|_ |PythonVersion|_ |PyPi|_ |DOI|_ |Benchmark|_\n\n.. |Azure| image:: https:\/\/dev.azure.com\/scikit-learn\/scikit-learn\/_apis\/build\/status\/scikit-learn.scikit-learn?branchName=main\n.. _Azure: https:\/\/dev.azure.com\/scikit-learn\/scikit-learn\/_build\/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https:\/\/circleci.com\/gh\/scikit-learn\/scikit-learn\/tree\/main.svg?style=shield&circle-token=:circle-token\n.. _CircleCI: https:\/\/circleci.com\/gh\/scikit-learn\/scikit-learn\n\n.. |Travis| image:: https:\/\/api.travis-ci.com\/scikit-learn\/scikit-learn.svg?branch=main\n.. _Travis: https:\/\/app.travis-ci.com\/github\/scikit-learn\/scikit-learn\n\n.. |Codecov| image:: https:\/\/codecov.io\/gh\/scikit-learn\/scikit-learn\/branch\/main\/graph\/badge.svg?token=Pk8G9gg3y9\n.. _Codecov: https:\/\/codecov.io\/gh\/scikit-learn\/scikit-learn\n\n.. |Nightly wheels| image:: https:\/\/github.com\/scikit-learn\/scikit-learn\/workflows\/Wheel%20builder\/badge.svg?event=schedule\n.. _`Nightly wheels`: https:\/\/github.com\/scikit-learn\/scikit-learn\/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |PythonVersion| image:: https:\/\/img.shields.io\/badge\/python-3.8%20%7C%203.9%20%7C%203.10-blue\n.. _PythonVersion: https:\/\/pypi.org\/project\/scikit-learn\/\n\n.. |PyPi| image:: https:\/\/img.shields.io\/pypi\/v\/scikit-learn\n.. _PyPi: https:\/\/pypi.org\/project\/scikit-learn\n\n.. |Black| image:: https:\/\/img.shields.io\/badge\/code%20style-black-000000.svg\n.. _Black: https:\/\/github.com\/psf\/black\n\n.. |DOI| image:: https:\/\/zenodo.org\/badge\/21369\/scikit-learn\/scikit-learn.svg\n.. _DOI: https:\/\/zenodo.org\/badge\/latestdoi\/21369\/scikit-learn\/scikit-learn\n\n.. |Benchmark| image:: https:\/\/img.shields.io\/badge\/Benchmarked%20by-asv-blue\n.. _`Benchmark`: https:\/\/scikit-learn.org\/scikit-learn-benchmarks\/\n\n.. |PythonMinVersion| replace:: 3.8\n.. |NumPyMinVersion| replace:: 1.17.3\n.. |SciPyMinVersion| replace:: 1.3.2\n.. |JoblibMinVersion| replace:: 1.0.0\n.. |ThreadpoolctlMinVersion| replace:: 2.0.0\n.. |MatplotlibMinVersion| replace:: 3.1.2\n.. |Scikit-ImageMinVersion| replace:: 0.14.5\n.. |PandasMinVersion| replace:: 1.0.5\n.. |SeabornMinVersion| replace:: 0.9.0\n.. |PytestMinVersion| replace:: 5.0.1\n\n.. image:: https:\/\/raw.githubusercontent.com\/scikit-learn\/scikit-learn\/main\/doc\/logos\/scikit-learn-logo.png\n  :target: https:\/\/scikit-learn.org\/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https:\/\/scikit-learn.org\/dev\/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https:\/\/scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\n**Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.**\nscikit-learn 1.0 and later require Python 3.7 or newer.\nscikit-learn 1.1 and later require Python 3.8 or newer.\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with \"Display\") require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of numpy and scipy,\nthe easiest way to install scikit-learn is using ``pip``::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https:\/\/scikit-learn.org\/stable\/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https:\/\/scikit-learn.org\/dev\/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https:\/\/scikit-learn.org\/stable\/developers\/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https:\/\/github.com\/scikit-learn\/scikit-learn\n- Download releases: https:\/\/pypi.org\/project\/scikit-learn\/\n- Issue tracker: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https:\/\/github.com\/scikit-learn\/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https:\/\/scikit-learn.org\/dev\/developers\/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PyTestMinVersion| installed)::\n\n    pytest sklearn\n\nSee the web page https:\/\/scikit-learn.org\/dev\/developers\/advanced_installation.html#testing\nfor more information.\n\n    Random number generation can be controlled during testing by setting\n    the ``SKLEARN_SEED`` environment variable.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https:\/\/scikit-learn.org\/stable\/developers\/index.html\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https:\/\/scikit-learn.org\/dev\/about.html#authors>`__ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): https:\/\/scikit-learn.org\n- HTML documentation (development version): https:\/\/scikit-learn.org\/dev\/\n- FAQ: https:\/\/scikit-learn.org\/stable\/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\n- Mailing list: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\n- Gitter: https:\/\/gitter.im\/scikit-learn\/scikit-learn\n- Blog: https:\/\/blog.scikit-learn.org\n- Calendar: https:\/\/blog.scikit-learn.org\/calendar\/\n- Twitter: https:\/\/twitter.com\/scikit_learn\n- Twitter (commits): https:\/\/twitter.com\/sklearn_commits\n- Stack Overflow: https:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\n- Github Discussions: https:\/\/github.com\/scikit-learn\/scikit-learn\/discussions\n- Website: https:\/\/scikit-learn.org\n- LinkedIn: https:\/\/www.linkedin.com\/company\/scikit-learn\n- YouTube: https:\/\/www.youtube.com\/channel\/UCJosFjYm0ZYVUARxuOZqnnw\/playlists\n- Facebook: https:\/\/www.facebook.com\/scikitlearnofficial\/\n- Instagram: https:\/\/www.instagram.com\/scikitlearnofficial\/\n- TikTok: https:\/\/www.tiktok.com\/@scikit.learn\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https:\/\/scikit-learn.org\/stable\/about.html#citing-scikit-learn\n","103":"<center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\".\/data\/.logo\u56fe\u7247\/.img.jpg\"width=\"180\">\n    <br>\n    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n    display: inline-block;\n    color: #999;\n    padding: 2px;\">NLP\u6c11\u5de5\u7684\u4e50\u56ed<\/div>\n<\/center>\n\n### The Most Powerful NLP-Weapon Arsenal\n\n## NLP\u6c11\u5de5\u7684\u4e50\u56ed: \u51e0\u4e4e\u6700\u5168\u7684\u4e2d\u6587NLP\u8d44\u6e90\u5e93\n- \u8bcd\u5e93\n- \u5de5\u5177\u5305\n- \u5b66\u4e60\u8d44\u6599\n---\n\u5728\u5165\u95e8\u5230\u719f\u6089NLP\u7684\u8fc7\u7a0b\u4e2d\uff0c\u7528\u5230\u4e86\u5f88\u591agithub\u4e0a\u7684\u5305\uff0c\u9042\u6574\u7406\u4e86\u4e00\u4e0b\uff0c\u5206\u4eab\u5728\u8fd9\u91cc\u3002\n\n\u5f88\u591a\u5305\u975e\u5e38\u6709\u8da3\uff0c\u503c\u5f97\u6536\u85cf\uff0c\u6ee1\u8db3\u5927\u5bb6\u7684\u6536\u96c6\u7656\uff01\n\u5982\u679c\u89c9\u5f97\u6709\u7528\uff0c\u8bf7\u5206\u4eab\u5e76star\uff0c\u8c22\u8c22\uff01\n\n\u957f\u671f\u4e0d\u5b9a\u65f6\u66f4\u65b0\uff0c\u6b22\u8fcewatch\u548cfork\uff01\n\n\u6d89\u53ca\u5185\u5bb9\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a**\u4e2d\u82f1\u6587\u654f\u611f\u8bcd\u3001\u8bed\u8a00\u68c0\u6d4b\u3001\u4e2d\u5916\u624b\u673a\/\u7535\u8bdd\u5f52\u5c5e\u5730\/\u8fd0\u8425\u5546\u67e5\u8be2\u3001\u540d\u5b57\u63a8\u65ad\u6027\u522b\u3001\u624b\u673a\u53f7\u62bd\u53d6\u3001\u8eab\u4efd\u8bc1\u62bd\u53d6\u3001\u90ae\u7bb1\u62bd\u53d6\u3001\u4e2d\u65e5\u6587\u4eba\u540d\u5e93\u3001\u4e2d\u6587\u7f29\u5199\u5e93\u3001\u62c6\u5b57\u8bcd\u5178\u3001\u8bcd\u6c47\u60c5\u611f\u503c\u3001\u505c\u7528\u8bcd\u3001\u53cd\u52a8\u8bcd\u8868\u3001\u66b4\u6050\u8bcd\u8868\u3001\u7e41\u7b80\u4f53\u8f6c\u6362\u3001\u82f1\u6587\u6a21\u62df\u4e2d\u6587\u53d1\u97f3\u3001\u6c6a\u5cf0\u6b4c\u8bcd\u751f\u6210\u5668\u3001\u804c\u4e1a\u540d\u79f0\u8bcd\u5e93\u3001\u540c\u4e49\u8bcd\u5e93\u3001\u53cd\u4e49\u8bcd\u5e93\u3001\u5426\u5b9a\u8bcd\u5e93\u3001\u6c7d\u8f66\u54c1\u724c\u8bcd\u5e93\u3001\u6c7d\u8f66\u96f6\u4ef6\u8bcd\u5e93\u3001\u8fde\u7eed\u82f1\u6587\u5207\u5272\u3001\u5404\u79cd\u4e2d\u6587\u8bcd\u5411\u91cf\u3001\u516c\u53f8\u540d\u5b57\u5927\u5168\u3001\u53e4\u8bd7\u8bcd\u5e93\u3001IT\u8bcd\u5e93\u3001\u8d22\u7ecf\u8bcd\u5e93\u3001\u6210\u8bed\u8bcd\u5e93\u3001\u5730\u540d\u8bcd\u5e93\u3001\u5386\u53f2\u540d\u4eba\u8bcd\u5e93\u3001\u8bd7\u8bcd\u8bcd\u5e93\u3001\u533b\u5b66\u8bcd\u5e93\u3001\u996e\u98df\u8bcd\u5e93\u3001\u6cd5\u5f8b\u8bcd\u5e93\u3001\u6c7d\u8f66\u8bcd\u5e93\u3001\u52a8\u7269\u8bcd\u5e93\u3001\u4e2d\u6587\u804a\u5929\u8bed\u6599\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u3001\u767e\u5ea6\u4e2d\u6587\u95ee\u7b54\u6570\u636e\u96c6\u3001\u53e5\u5b50\u76f8\u4f3c\u5ea6\u5339\u914d\u7b97\u6cd5\u96c6\u5408\u3001bert\u8d44\u6e90\u3001\u6587\u672c\u751f\u6210&\u6458\u8981\u76f8\u5173\u5de5\u5177\u3001cocoNLP\u4fe1\u606f\u62bd\u53d6\u5de5\u5177\u3001\u56fd\u5185\u7535\u8bdd\u53f7\u7801\u6b63\u5219\u5339\u914d\u3001\u6e05\u534e\u5927\u5b66XLORE:\u4e2d\u82f1\u6587\u8de8\u8bed\u8a00\u767e\u79d1\u77e5\u8bc6\u56fe\u8c31\u3001\u6e05\u534e\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7cfb\u5217\u62a5\u544a\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u3001NLU\u592a\u96be\u4e86\u7cfb\u5217\u3001\u81ea\u52a8\u5bf9\u8054\u6570\u636e\u53ca\u673a\u5668\u4eba\u3001\u7528\u6237\u540d\u9ed1\u540d\u5355\u5217\u8868\u3001\u7f6a\u540d\u6cd5\u52a1\u540d\u8bcd\u53ca\u5206\u7c7b\u6a21\u578b\u3001\u5fae\u4fe1\u516c\u4f17\u53f7\u8bed\u6599\u3001cs224n\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b\u3001\u4e2d\u6587\u624b\u5199\u6c49\u5b57\u8bc6\u522b\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8bed\u6599\/\u6570\u636e\u96c6\u3001\u53d8\u91cf\u547d\u540d\u795e\u5668\u3001\u5206\u8bcd\u8bed\u6599\u5e93+\u4ee3\u7801\u3001\u4efb\u52a1\u578b\u5bf9\u8bdd\u82f1\u6587\u6570\u636e\u96c6\u3001ASR \u8bed\u97f3\u6570\u636e\u96c6 + \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u3001\u7b11\u58f0\u68c0\u6d4b\u5668\u3001Microsoft\u591a\u8bed\u8a00\u6570\u5b57\/\u5355\u4f4d\/\u5982\u65e5\u671f\u65f6\u95f4\u8bc6\u522b\u5305\u3001\u4e2d\u534e\u65b0\u534e\u5b57\u5178\u6570\u636e\u5e93\u53caapi(\u5305\u62ec\u5e38\u7528\u6b47\u540e\u8bed\u3001\u6210\u8bed\u3001\u8bcd\u8bed\u548c\u6c49\u5b57)\u3001\u6587\u6863\u56fe\u8c31\u81ea\u52a8\u751f\u6210\u3001SpaCy \u4e2d\u6587\u6a21\u578b\u3001Common Voice\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u65b0\u7248\u3001\u795e\u7ecf\u7f51\u7edc\u5173\u7cfb\u62bd\u53d6\u3001\u57fa\u4e8ebert\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u952e\u8bcd(Keyphrase)\u62bd\u53d6\u5305pke\u3001\u57fa\u4e8e\u533b\u7597\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u4e0e\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u7684\u4e8b\u4ef6\u4e09\u5143\u7ec4\u62bd\u53d6\u3001\u4f9d\u5b58\u53e5\u6cd5\u5206\u67904\u4e07\u53e5\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3001cnocr\uff1a\u7528\u6765\u505a\u4e2d\u6587OCR\u7684Python3\u5305\u3001\u4e2d\u6587\u4eba\u7269\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u9879\u76ee\u3001\u4e2d\u6587nlp\u7ade\u8d5b\u9879\u76ee\u53ca\u4ee3\u7801\u6c47\u603b\u3001\u4e2d\u6587\u5b57\u7b26\u6570\u636e\u3001speech-aligner: \u4ece\u201c\u4eba\u58f0\u8bed\u97f3\u201d\u53ca\u5176\u201c\u8bed\u8a00\u6587\u672c\u201d\u4ea7\u751f\u97f3\u7d20\u7ea7\u522b\u65f6\u95f4\u5bf9\u9f50\u6807\u6ce8\u7684\u5de5\u5177\u3001AmpliGraph: \u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5b66\u4e60(Python)\u5e93\uff1a\u77e5\u8bc6\u56fe\u8c31\u6982\u5ff5\u94fe\u63a5\u9884\u6d4b\u3001Scattertext \u6587\u672c\u53ef\u89c6\u5316(python)\u3001\u8bed\u8a00\/\u77e5\u8bc6\u8868\u793a\u5de5\u5177\uff1aBERT & ERNIE\u3001\u4e2d\u6587\u5bf9\u6bd4\u82f1\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u7684\u533a\u522b\u7efc\u8ff0\u3001Synonyms\u4e2d\u6587\u8fd1\u4e49\u8bcd\u5de5\u5177\u5305\u3001HarvestText\u9886\u57df\u81ea\u9002\u5e94\u6587\u672c\u6316\u6398\u5de5\u5177\uff08\u65b0\u8bcd\u53d1\u73b0-\u60c5\u611f\u5206\u6790-\u5b9e\u4f53\u94fe\u63a5\u7b49\uff09\u3001word2word\uff1a(Python)\u65b9\u4fbf\u6613\u7528\u7684\u591a\u8bed\u8a00\u8bcd-\u8bcd\u5bf9\u96c6\uff1a62\u79cd\u8bed\u8a00\/3,564\u4e2a\u591a\u8bed\u8a00\u5bf9\u3001\u8bed\u97f3\u8bc6\u522b\u8bed\u6599\u751f\u6210\u5de5\u5177\uff1a\u4ece\u5177\u6709\u97f3\u9891\/\u5b57\u5e55\u7684\u5728\u7ebf\u89c6\u9891\u521b\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u8bed\u6599\u5e93\u3001\u6784\u5efa\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u7684\u6a21\u578b\uff08\u5305\u542b\u8bcd\u5178\u548c\u8bed\u6599\u6807\u6ce8\uff09\u3001\u5355\u6587\u6863\u975e\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6\u3001Kashgari\u4e2d\u4f7f\u7528gpt-2\u8bed\u8a00\u6a21\u578b\u3001\u5f00\u6e90\u7684\u91d1\u878d\u6295\u8d44\u6570\u636e\u63d0\u53d6\u5de5\u5177\u3001\u6587\u672c\u81ea\u52a8\u6458\u8981\u5e93TextTeaser: \u4ec5\u652f\u6301\u82f1\u6587\u3001\u4eba\u6c11\u65e5\u62a5\u8bed\u6599\u5904\u7406\u5de5\u5177\u96c6\u3001\u4e00\u4e9b\u5173\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u578b\u3001\u57fa\u4e8e14W\u6b4c\u66f2\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u5c1d\u8bd5--\u529f\u80fd\u5305\u62ec\u6b4c\u8bcd\u63a5\u9f99and\u5df2\u77e5\u6b4c\u8bcd\u627e\u6b4c\u66f2\u4ee5\u53ca\u6b4c\u66f2\u6b4c\u624b\u6b4c\u8bcd\u4e09\u89d2\u5173\u7cfb\u7684\u95ee\u7b54\u3001\u57fa\u4e8eSiamese bilstm\u6a21\u578b\u7684\u76f8\u4f3c\u53e5\u5b50\u5224\u5b9a\u6a21\u578b\u5e76\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u3001\u7528Transformer\u7f16\u89e3\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6839\u636eHacker News\u6587\u7ae0\u6807\u9898\u81ea\u52a8\u751f\u6210\u8bc4\u8bba\u3001\u7528BERT\u8fdb\u884c\u5e8f\u5217\u6807\u8bb0\u548c\u6587\u672c\u5206\u7c7b\u7684\u6a21\u677f\u4ee3\u7801\u3001LitBank\uff1aNLP\u6570\u636e\u96c6\u2014\u2014\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u7684100\u90e8\u5e26\u6807\u8bb0\u82f1\u6587\u5c0f\u8bf4\u8bed\u6599\u3001\u767e\u5ea6\u5f00\u6e90\u7684\u57fa\u51c6\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u3001\u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6\u3001Facebook: LAMA\u8bed\u8a00\u6a21\u578b\u5206\u6790\uff0c\u63d0\u4f9bTransformer-XL\/BERT\/ELMo\/GPT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8bbf\u95ee\u63a5\u53e3\u3001CommonsenseQA\uff1a\u9762\u5411\u5e38\u8bc6\u7684\u82f1\u6587QA\u6311\u6218\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u8d44\u6599\u3001\u6570\u636e\u53ca\u5de5\u5177\u3001\u5404\u5927\u516c\u53f8\u5185\u90e8\u91cc\u5927\u725b\u5206\u4eab\u7684\u6280\u672f\u6587\u6863 PDF \u6216\u8005 PPT\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210SQL\u8bed\u53e5\uff08\u82f1\u6587\uff09\u3001\u4e2d\u6587NLP\u6570\u636e\u589e\u5f3a\uff08EDA\uff09\u5de5\u5177\u3001\u82f1\u6587NLP\u6570\u636e\u589e\u5f3a\u5de5\u5177 \u3001\u57fa\u4e8e\u533b\u836f\u77e5\u8bc6\u56fe\u8c31\u7684\u667a\u80fd\u95ee\u7b54\u7cfb\u7edf\u3001\u4eac\u4e1c\u5546\u54c1\u77e5\u8bc6\u56fe\u8c31\u3001\u57fa\u4e8emongodb\u5b58\u50a8\u7684\u519b\u4e8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u9879\u76ee\u3001\u57fa\u4e8e\u8fdc\u76d1\u7763\u7684\u4e2d\u6587\u5173\u7cfb\u62bd\u53d6\u3001\u8bed\u97f3\u60c5\u611f\u5206\u6790\u3001\u4e2d\u6587ULMFiT-\u60c5\u611f\u5206\u6790-\u6587\u672c\u5206\u7c7b-\u8bed\u6599\u53ca\u6a21\u578b\u3001\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3001\u4e16\u754c\u5404\u56fd\u5927\u89c4\u6a21\u4eba\u540d\u5e93\u3001\u4e00\u4e2a\u5229\u7528\u6709\u8da3\u4e2d\u6587\u8bed\u6599\u5e93 qingyun \u8bad\u7ec3\u51fa\u6765\u7684\u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba\u3001\u4e2d\u6587\u804a\u5929\u673a\u5668\u4ebaseqGAN\u3001\u7701\u5e02\u533a\u9547\u884c\u653f\u533a\u5212\u6570\u636e\u5e26\u62fc\u97f3\u6807\u6ce8\u3001\u6559\u80b2\u884c\u4e1a\u65b0\u95fb\u8bed\u6599\u5e93\u5305\u542b\u81ea\u52a8\u6587\u6458\u529f\u80fd\u3001\u5f00\u653e\u4e86\u5bf9\u8bdd\u673a\u5668\u4eba-\u77e5\u8bc6\u56fe\u8c31-\u8bed\u4e49\u7406\u89e3-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u53ca\u6570\u636e\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\uff1a\u57fa\u4e8e\u767e\u5ea6\u767e\u79d1\u4e2d\u6587\u9875\u9762-\u62bd\u53d6\u4e09\u5143\u7ec4\u4fe1\u606f-\u6784\u5efa\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u3001masr: \u4e2d\u6587\u8bed\u97f3\u8bc6\u522b-\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b-\u9ad8\u8bc6\u522b\u7387\u3001Python\u97f3\u9891\u6570\u636e\u589e\u5e7f\u5e93\u3001\u4e2d\u6587\u5168\u8bcd\u8986\u76d6BERT\u53ca\u4e24\u4efd\u9605\u8bfb\u7406\u89e3\u6570\u636e\u3001ConvLab\uff1a\u5f00\u6e90\u591a\u57df\u7aef\u5230\u7aef\u5bf9\u8bdd\u7cfb\u7edf\u5e73\u53f0\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u6700\u65b0\u7248\u672crasa\u642d\u5efa\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3001\u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6\u3001\u4e00\u4e2a\u5c0f\u578b\u7684\u8bc1\u5238\u77e5\u8bc6\u56fe\u8c31\/\u77e5\u8bc6\u5e93\u3001\u590d\u76d8\u6240\u6709NLP\u6bd4\u8d5b\u7684TOP\u65b9\u6848\u3001OpenCLaP\uff1a\u591a\u9886\u57df\u5f00\u6e90\u4e2d\u6587\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ed3\u5e93\u3001UER\uff1a\u57fa\u4e8e\u4e0d\u540c\u8bed\u6599+\u7f16\u7801\u5668+\u76ee\u6807\u4efb\u52a1\u7684\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ed3\u5e93\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5411\u91cf\u5408\u96c6\u3001\u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba\u3001g2pC\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6c49\u8bed\u8bfb\u97f3\u81ea\u52a8\u6807\u8bb0\u6a21\u5757\u3001Zincbase \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5de5\u5177\u5305\u3001\u8bd7\u6b4c\u8d28\u91cf\u8bc4\u4ef7\/\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bd7\u6b4c\u8bed\u6599\u5e93\u3001\u5feb\u901f\u8f6c\u5316\u300c\u4e2d\u6587\u6570\u5b57\u300d\u548c\u300c\u963f\u62c9\u4f2f\u6570\u5b57\u300d\u3001\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u8bed\u6599\u5e93\u3001\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001jieba_fast \u52a0\u901f\u7248\u7684jieba\u3001\u6b63\u5219\u8868\u8fbe\u5f0f\u6559\u7a0b\u3001\u4e2d\u6587\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6\u3001\u57fa\u4e8eBERT\u7b49\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u53d6\u5f0f\u6458\u8981\u63d0\u53d6\u3001Python\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6587\u672c\u6458\u8981\u7684\u7efc\u5408\u6307\u5357\u3001\u77e5\u8bc6\u56fe\u8c31\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u8d44\u6599\u6574\u7406\u3001\u7ef4\u57fa\u5927\u89c4\u6a21\u5e73\u884c\u6587\u672c\u8bed\u6599\u3001StanfordNLP 0.2.0\uff1a\u7eafPython\u7248\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5305\u3001NeuralNLP-NeuralClassifier\uff1a\u817e\u8baf\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5206\u7c7b\u5de5\u5177\u3001\u7aef\u5230\u7aef\u7684\u5c01\u95ed\u57df\u5bf9\u8bdd\u7cfb\u7edf\u3001\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff1aNeuroNER vs. BertNER\u3001\u65b0\u95fb\u4e8b\u4ef6\u7ebf\u7d22\u62bd\u53d6\u30012019\u5e74\u767e\u5ea6\u7684\u4e09\u5143\u7ec4\u62bd\u53d6\u6bd4\u8d5b\uff1a\u201c\u79d1\u5b66\u7a7a\u95f4\u961f\u201d\u6e90\u7801\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u7684\u5f00\u653e\u57df\u6587\u672c\u77e5\u8bc6\u4e09\u5143\u7ec4\u62bd\u53d6\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u3001\u4e2d\u6587\u7684GPT2\u8bad\u7ec3\u4ee3\u7801\u3001ML-NLP - \u673a\u5668\u5b66\u4e60(Machine Learning)NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\u3001nlp4han:\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5\/\u5206\u8bcd\/\u8bcd\u6027\u6807\u6ce8\/\u7ec4\u5757\/\u53e5\u6cd5\u5206\u6790\/\u8bed\u4e49\u5206\u6790\/NER\/N\u5143\u8bed\u6cd5\/HMM\/\u4ee3\u8bcd\u6d88\u89e3\/\u60c5\u611f\u5206\u6790\/\u62fc\u5199\u68c0\u67e5\u3001XLM\uff1aFacebook\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3001\u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u767e\u5ea6\u767e\u79d1\u4eba\u7269\u8bcd\u6761\u5c5e\u6027\u62bd\u53d6\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u76f8\u5173\u7684\u5f00\u653e\u4efb\u52a1-\u6570\u636e\u96c6-\u5f53\u524d\u6700\u4f73\u7ed3\u679c\u3001CoupletAI - \u57fa\u4e8eCNN+Bi-LSTM+Attention \u7684\u81ea\u52a8\u5bf9\u5bf9\u8054\u7cfb\u7edf\u3001\u62bd\u8c61\u77e5\u8bc6\u56fe\u8c31\u3001MiningZhiDaoQACorpus - 580\u4e07\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u6570\u636e\u6316\u6398\u9879\u76ee\u3001brat rapid annotation tool: \u5e8f\u5217\u6807\u6ce8\u5de5\u5177\u3001\u5927\u89c4\u6a21\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\uff1a1.4\u4ebf\u5b9e\u4f53\u3001\u6570\u636e\u589e\u5f3a\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u4ed6nlp\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53ca\u6548\u679c\u3001allennlp\u9605\u8bfb\u7406\u89e3:\u652f\u6301\u591a\u79cd\u6570\u636e\u548c\u6a21\u578b\u3001PDF\u8868\u683c\u6570\u636e\u63d0\u53d6\u5de5\u5177 \u3001 Graphbrain\uff1aAI\u5f00\u6e90\u8f6f\u4ef6\u5e93\u548c\u79d1\u7814\u5de5\u5177\uff0c\u76ee\u7684\u662f\u4fc3\u8fdb\u81ea\u52a8\u610f\u4e49\u63d0\u53d6\u548c\u6587\u672c\u7406\u89e3\u4ee5\u53ca\u77e5\u8bc6\u7684\u63a2\u7d22\u548c\u63a8\u65ad\u3001\u7b80\u5386\u81ea\u52a8\u7b5b\u9009\u7cfb\u7edf\u3001\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u7b80\u5386\u81ea\u52a8\u6458\u8981\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6&\u57fa\u51c6\u6a21\u578b&\u8bed\u6599\u5e93&\u6392\u884c\u699c\u3001\u6811\u6d1e OCR \u6587\u5b57\u8bc6\u522b \u3001\u4ece\u5305\u542b\u8868\u683c\u7684\u626b\u63cf\u56fe\u7247\u4e2d\u8bc6\u522b\u8868\u683c\u548c\u6587\u5b57\u3001\u8bed\u58f0\u8fc1\u79fb\u3001Python\u53e3\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u82f1\u6587)\u3001 similarity\uff1a\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5de5\u5177\u5305\uff0cjava\u7f16\u5199\u3001\u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b \u3001Transformers 2.0 \u3001\u57fa\u4e8e\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6Audioset\u7684\u97f3\u9891\u589e\u5f3a \u3001Poplar\uff1a\u7f51\u9875\u7248\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u5de5\u5177\u3001\u56fe\u7247\u6587\u5b57\u53bb\u9664\uff0c\u53ef\u7528\u4e8e\u6f2b\u753b\u7ffb\u8bd1 \u3001186\u79cd\u8bed\u8a00\u7684\u6570\u5b57\u53eb\u6cd5\u5e93\u3001Amazon\u53d1\u5e03\u57fa\u4e8e\u77e5\u8bc6\u7684\u4eba-\u4eba\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u4e2d\u6587\u6587\u672c\u7ea0\u9519\u6a21\u5757\u4ee3\u7801\u3001\u7e41\u7b80\u4f53\u8f6c\u6362 \u3001 Python\u5b9e\u73b0\u7684\u591a\u79cd\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u4ef7\u6307\u6807\u3001\u7c7b\u4f3c\u4e8e\u4eba\u540d\/\u5730\u540d\/\u7ec4\u7ec7\u673a\u6784\u540d\u7684\u547d\u540d\u4f53\u8bc6\u522b\u6570\u636e\u96c6 \u3001\u4e1c\u5357\u5927\u5b66\u300a\u77e5\u8bc6\u56fe\u8c31\u300b\u7814\u7a76\u751f\u8bfe\u7a0b(\u8d44\u6599)\u3001. \u82f1\u6587\u62fc\u5199\u68c0\u67e5\u5e93 \u3001 wwsearch\u662f\u4f01\u4e1a\u5fae\u4fe1\u540e\u53f0\u81ea\u7814\u7684\u5168\u6587\u68c0\u7d22\u5f15\u64ce\u3001CHAMELEON\uff1a\u6df1\u5ea6\u5b66\u4e60\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u5143\u67b6\u6784 \u3001 8\u7bc7\u8bba\u6587\u68b3\u7406BERT\u76f8\u5173\u6a21\u578b\u8fdb\u5c55\u4e0e\u53cd\u601d\u3001DocSearch\uff1a\u514d\u8d39\u6587\u6863\u641c\u7d22\u5f15\u64ce\u3001 LIDA\uff1a\u8f7b\u91cf\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u6807\u6ce8\u5de5\u5177 \u3001aili - the fastest in-memory index in the East \u4e1c\u534a\u7403\u6700\u5feb\u5e76\u53d1\u7d22\u5f15 \u3001\u77e5\u8bc6\u56fe\u8c31\u8f66\u97f3\u5de5\u4f5c\u9879\u76ee\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d44\u6e90\u5927\u5168 \u3001\u4e2d\u65e5\u97e9\u5206\u8bcd\u5e93mecab\u7684Python\u63a5\u53e3\u5e93\u3001\u4e2d\u6587\u6587\u672c\u6458\u8981\/\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5668 (featurizer)\uff0c\u63d0\u53d6\u6c49\u5b57\u7684\u7279\u5f81\uff08\u53d1\u97f3\u7279\u5f81\u3001\u5b57\u5f62\u7279\u5f81\uff09\u7528\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u3001\u4e2d\u6587\u751f\u6210\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 \u3001\u4e2d\u6587\u7f29\u5199\u6570\u636e\u96c6\u3001\u4e2d\u6587\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 - \u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6-\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b-\u8bed\u6599\u5e93-baseline-\u5de5\u5177\u5305-\u6392\u884c\u699c\u3001PySS3\uff1a\u9762\u5411\u53ef\u89e3\u91caAI\u7684SS3\u6587\u672c\u5206\u7c7b\u5668\u673a\u5668\u53ef\u89c6\u5316\u5de5\u5177 \u3001\u4e2d\u6587NLP\u6570\u636e\u96c6\u5217\u8868\u3001COPE - \u683c\u5f8b\u8bd7\u7f16\u8f91\u7a0b\u5e8f\u3001doccano\uff1a\u57fa\u4e8e\u7f51\u9875\u7684\u5f00\u6e90\u534f\u540c\u591a\u8bed\u8a00\u6587\u672c\u6807\u6ce8\u5de5\u5177 \u3001PreNLP\uff1a\u81ea\u7136\u8bed\u8a00\u9884\u5904\u7406\u5e93\u3001\u7b80\u5355\u7684\u7b80\u5386\u89e3\u6790\u5668\uff0c\u7528\u6765\u4ece\u7b80\u5386\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3001\u7528\u4e8e\u4e2d\u6587\u95f2\u804a\u7684GPT2\u6a21\u578b\uff1aGPT2-chitchat\u3001\u57fa\u4e8e\u68c0\u7d22\u804a\u5929\u673a\u5668\u4eba\u591a\u8f6e\u54cd\u5e94\u9009\u62e9\u76f8\u5173\u8d44\u6e90\u5217\u8868(Leaderboards\u3001Datasets\u3001Papers)\u3001(Colab)\u62bd\u8c61\u6587\u672c\u6458\u8981\u5b9e\u73b0\u96c6\u9526(\u6559\u7a0b \u3001\u8bcd\u8bed\u62fc\u97f3\u6570\u636e\u3001\u9ad8\u6548\u6a21\u7cca\u641c\u7d22\u5de5\u5177\u3001NLP\u6570\u636e\u589e\u5e7f\u8d44\u6e90\u96c6\u3001\u5fae\u8f6f\u5bf9\u8bdd\u673a\u5668\u4eba\u6846\u67b6 \u3001 GitHub Typo Corpus\uff1a\u5927\u89c4\u6a21GitHub\u591a\u8bed\u8a00\u62fc\u5199\u9519\u8bef\/\u8bed\u6cd5\u9519\u8bef\u6570\u636e\u96c6\u3001TextCluster\uff1a\u77ed\u6587\u672c\u805a\u7c7b\u9884\u5904\u7406\u6a21\u5757 Short text cluster\u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\u6587\u672c\u89c4\u8303\u5316\u3001BLINK\uff1a\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u5e93\u3001BertPunc\uff1a\u57fa\u4e8eBERT\u7684\u6700\u5148\u8fdb\u6807\u70b9\u4fee\u590d\u6a21\u578b\u3001Tokenizer\uff1a\u5feb\u901f\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8bcd\u6761\u5316\u5e93\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b\u3001\u8bed\u6599\u5e93\u3001\u6392\u884c\u699c\u3001spaCy \u533b\u5b66\u6587\u672c\u6316\u6398\u4e0e\u4fe1\u606f\u63d0\u53d6 \u3001 NLP\u4efb\u52a1\u793a\u4f8b\u9879\u76ee\u4ee3\u7801\u96c6\u3001 python\u62fc\u5199\u68c0\u67e5\u5e93\u3001chatbot-list - \u884c\u4e1a\u5185\u5173\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u804a\u5929\u673a\u5668\u4eba\u7684\u5e94\u7528\u548c\u67b6\u6784\u3001\u7b97\u6cd5\u5206\u4eab\u548c\u4ecb\u7ecd\u3001\u8bed\u97f3\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807(MOSNet, BSSEval, STOI, PESQ, SRMR)\u3001 \u7528138GB\u8bed\u6599\u8bad\u7ec3\u7684\u6cd5\u6587RoBERTa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b \u3001BERT-NER-Pytorch\uff1a\u4e09\u79cd\u4e0d\u540c\u6a21\u5f0f\u7684BERT\u4e2d\u6587NER\u5b9e\u9a8c\u3001\u65e0\u9053\u8bcd\u5178 - \u6709\u9053\u8bcd\u5178\u7684\u547d\u4ee4\u884c\u7248\u672c\uff0c\u652f\u6301\u82f1\u6c49\u4e92\u67e5\u548c\u5728\u7ebf\u67e5\u8be2\u30012019\u5e74NLP\u4eae\u70b9\u56de\u987e\u3001 Chinese medical dialogue data \u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u6700\u597d\u7684\u6c49\u5b57\u6570\u5b57(\u4e2d\u6587\u6570\u5b57)-\u963f\u62c9\u4f2f\u6570\u5b57\u8f6c\u6362\u5de5\u5177\u3001 \u57fa\u4e8e\u767e\u79d1\u77e5\u8bc6\u5e93\u7684\u4e2d\u6587\u8bcd\u8bed\u591a\u8bcd\u4e49\/\u4e49\u9879\u83b7\u53d6\u4e0e\u7279\u5b9a\u53e5\u5b50\u8bcd\u8bed\u8bed\u4e49\u6d88\u6b67\u3001awesome-nlp-sentiment-analysis - \u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u539f\u56e0\u8bc6\u522b\u3001\u8bc4\u4ef7\u5bf9\u8c61\u548c\u8bc4\u4ef7\u8bcd\u62bd\u53d6\u3001LineFlow\uff1a\u9762\u5411\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684NLP\u6570\u636e\u9ad8\u6548\u52a0\u8f7d\u5668\u3001\u4e2d\u6587\u533b\u5b66NLP\u516c\u5f00\u8d44\u6e90\u6574\u7406 \u3001MedQuAD\uff1a(\u82f1\u6587)\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u3001\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b57\u4e32\u89e3\u6790\u8f6c\u6362\u4e3a\u6574\u6570\u548c\u6d6e\u70b9\u6570\u3001Transfer Learning in Natural Language Processing (NLP) \u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178\u3001Tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668\u3001CLUENER \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b Fine Grained Named Entity Recognition\u3001 \u57fa\u4e8eBERT\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93\u3001NLP\u6570\u636e\u96c6\/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868\u3001nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801, \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(Word Embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u3001\u6587\u672c\u5206\u7c7b(Text Classificatin)\u3001\u6587\u672c\u751f\u6210(Text Generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(Text Similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow \u3001Python\u6587\u672c\u6316\u6398\/NLP\u5b9e\u6218\u793a\u4f8b\u3001 Blackstone\uff1a\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spaCy pipeline\u548cNLP\u6a21\u578b\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d \u3001\u4e2d\u6587 \u9884\u8bad\u7ec3 ELECTREA \u6a21\u578b: \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain Chinese Model \u3001albert-chinese-ner - \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578bALBERT\u505a\u4e2d\u6587NER \u3001\u57fa\u4e8eGPT2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210\/\u6587\u672c\u589e\u5e7f\u3001\u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6\u3001\u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305\u3001\u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\uff1a\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\u3001 \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868 \u3001attnvis\uff1aGPT2\u3001BERT\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316\u3001CoVoST\uff1aFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587\u3001Jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177 - \u4ee5BiLSTM\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd\u3001\u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa\u3001NLP\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868 \u3001 \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868\u3001CLUEDatasetSearch - \u4e2d\u82f1\u6587NLP\u6570\u636e\u96c6\uff1a\u641c\u7d22\u6240\u6709\u4e2d\u6587NLP\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587NLP\u6570\u636e\u96c6 \u3001medical_NER - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u3001(\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66\u3001\u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599\/\u6570\u636e\u96c6\/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868\u3001Forte\uff1a\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6 \u3001Python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93\u3001PyLaia\uff1a\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u3001TextFooler\uff1a\u9488\u5bf9\u6587\u672c\u5206\u7c7b\/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757\u3001Haystack\uff1a\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(QA)\u6846\u67b6\u3001\u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177**\u3002\n\n----\n**1\\. textfilter: \u4e2d\u82f1\u6587\u654f\u611f\u8bcd\u8fc7\u6ee4**  [observerss\/textfilter](https:\/\/github.com\/observerss\/textfilter)\n```\n >>> f = DFAFilter()\n >>> f.add(\"sexy\")\n >>> f.filter(\"hello sexy baby\")\n hello **** baby\n```\n\u654f\u611f\u8bcd\u5305\u62ec\u653f\u6cbb\u3001\u810f\u8bdd\u7b49\u8bdd\u9898\u8bcd\u6c47\u3002\u5176\u539f\u7406\u4e3b\u8981\u662f\u57fa\u4e8e\u8bcd\u5178\u7684\u67e5\u627e\uff08\u9879\u76ee\u4e2d\u7684keyword\u6587\u4ef6\uff09\uff0c\u5185\u5bb9\u5f88\u52b2\u7206\u3002\u3002\u3002\n\n**2\\. langid\uff1a97\u79cd\u8bed\u8a00\u68c0\u6d4b** [https:\/\/github.com\/saffsd\/langid.py](https:\/\/github.com\/saffsd\/langid.py)\n\n> pip install langid\n\n```\n>>> import langid\n>>> langid.classify(\"This is a test\")\n('en', -54.41310358047485)\n```\n\n**3\\. langdetect\uff1a\u53e6\u4e00\u4e2a\u8bed\u8a00\u68c0\u6d4b**[https:\/\/code.google.com\/archive\/p\/language-detection\/](https:\/\/code.google.com\/archive\/p\/language-detection\/)\n\n> pip install langdetect\n\n```\nfrom langdetect import detect\nfrom langdetect import detect_langs\n\ns1 = \"\u672c\u7bc7\u535a\u5ba2\u4e3b\u8981\u4ecb\u7ecd\u4e24\u6b3e\u8bed\u8a00\u63a2\u6d4b\u5de5\u5177\uff0c\u7528\u4e8e\u533a\u5206\u6587\u672c\u5230\u5e95\u662f\u4ec0\u4e48\u8bed\u8a00\uff0c\"\ns2 = 'We are pleased to introduce today a new technology'\nprint(detect(s1))\nprint(detect(s2))\nprint(detect_langs(s3))    # detect_langs()\u8f93\u51fa\u63a2\u6d4b\u51fa\u7684\u6240\u6709\u8bed\u8a00\u7c7b\u578b\u53ca\u5176\u6240\u5360\u7684\u6bd4\u4f8b\n```\n\n\u8f93\u51fa\u7ed3\u679c\u5982\u4e0b\uff1a \u6ce8\uff1a\u8bed\u8a00\u7c7b\u578b\u4e3b\u8981\u53c2\u8003\u7684\u662fISO 639-1\u8bed\u8a00\u7f16\u7801\u6807\u51c6\uff0c\u8be6\u89c1[ISO 639-1\u767e\u5ea6\u767e\u79d1](https:\/\/baike.baidu.com\/item\/ISO%20639-1)\n\n\u8ddf\u4e0a\u4e00\u4e2a\u8bed\u8a00\u68c0\u6d4b\u6bd4\u8f83\uff0c\u51c6\u786e\u7387\u4f4e\uff0c\u6548\u7387\u9ad8\u3002\n\n\n**4\\. phone \u4e2d\u56fd\u624b\u673a\u5f52\u5c5e\u5730\u67e5\u8be2\uff1a** [ls0f\/phone](https:\/\/github.com\/ls0f\/phone)\n\n> \u5df2\u96c6\u6210\u5230 python package [cocoNLP](https:\/\/github.com\/fighting41love\/cocoNLP)\u4e2d\uff0c\u6b22\u8fce\u8bd5\u7528\n\n```\nfrom phone import Phone\np  = Phone()\np.find(18100065143)\n#return {'phone': '18100065143', 'province': '\u4e0a\u6d77', 'city': '\u4e0a\u6d77', 'zip_code': '200000', 'area_code': '021', 'phone_type': '\u7535\u4fe1'}\n```\n\u652f\u6301\u53f7\u6bb5: 13*,15*,18*,14[5,7],17[0,6,7,8]\n\n\u8bb0\u5f55\u6761\u6570: 360569 (updated:2017\u5e744\u6708)\n\n\u4f5c\u8005\u63d0\u4f9b\u4e86\u6570\u636e[phone.dat](https:\/\/github.com\/lovedboy\/phone\/raw\/master\/phone\/phone.dat) \u65b9\u4fbf\u975epython\u7528\u6237Load\u6570\u636e\u3002\n\n**5\\. phone\u56fd\u9645\u624b\u673a\u3001\u7535\u8bdd\u5f52\u5c5e\u5730\u67e5\u8be2\uff1a**[AfterShip\/phone](https:\/\/github.com\/AfterShip\/phone)\n\n> npm install phone\n\n```\nimport phone from 'phone';\nphone('+852 6569-8900'); \/\/ return ['+85265698900', 'HKG']\nphone('(817) 569-8900'); \/\/ return ['+18175698900, 'USA']\n```\n**6\\. ngender \u6839\u636e\u540d\u5b57\u5224\u65ad\u6027\u522b\uff1a**[observerss\/ngender](https:\/\/github.com\/observerss\/ngender) \u57fa\u4e8e\u6734\u7d20\u8d1d\u53f6\u65af\u8ba1\u7b97\u7684\u6982\u7387\n\n> pip install ngender\n\n```\n>>> import ngender\n>>> ngender.guess('\u8d75\u672c\u5c71')\n('male', 0.9836229687547046)\n>>> ngender.guess('\u5b8b\u4e39\u4e39')\n('female', 0.9759486128949907)\n```\n**7\\. \u62bd\u53d6email\u7684\u6b63\u5219\u8868\u8fbe\u5f0f**\n\n> \u5df2\u96c6\u6210\u5230 python package [cocoNLP](https:\/\/github.com\/fighting41love\/cocoNLP)\u4e2d\uff0c\u6b22\u8fce\u8bd5\u7528\n\n```\nemail_pattern = '^[*#\\u4e00-\\u9fa5 a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\\.[a-zA-Z0-9-]+)*\\.[a-zA-Z0-9]{2,6}$'\nemails = re.findall(email_pattern, text, flags=0)\n```\n**8\\. \u62bd\u53d6phone_number\u7684\u6b63\u5219\u8868\u8fbe\u5f0f**\n\n> \u5df2\u96c6\u6210\u5230 python package [cocoNLP](https:\/\/github.com\/fighting41love\/cocoNLP)\u4e2d\uff0c\u6b22\u8fce\u8bd5\u7528\n\n```\ncellphone_pattern = '^((13[0-9])|(14[0-9])|(15[0-9])|(17[0-9])|(18[0-9]))\\d{8}$'\nphoneNumbers = re.findall(cellphone_pattern, text, flags=0)\n```\n**9\\. \u62bd\u53d6\u8eab\u4efd\u8bc1\u53f7\u7684\u6b63\u5219\u8868\u8fbe\u5f0f**\n```\nIDCards_pattern = r'^([1-9]\\d{5}[12]\\d{3}(0[1-9]|1[012])(0[1-9]|[12][0-9]|3[01])\\d{3}[0-9xX])$'\nIDs = re.findall(IDCards_pattern, text, flags=0)\n```\n**10.  \u4eba\u540d\u8bed\u6599\u5e93\uff1a** [wainshine\/Chinese-Names-Corpus](https:\/\/github.com\/wainshine\/Chinese-Names-Corpus)\n\n> \u4eba\u540d\u62bd\u53d6\u529f\u80fd python package [cocoNLP](https:\/\/github.com\/fighting41love\/cocoNLP)\uff0c\u6b22\u8fce\u8bd5\u7528\n\n```\n\u4e2d\u6587\uff08\u73b0\u4ee3\u3001\u53e4\u4ee3\uff09\u540d\u5b57\u3001\u65e5\u6587\u540d\u5b57\u3001\u4e2d\u6587\u7684\u59d3\u548c\u540d\u3001\u79f0\u547c\uff08\u5927\u59e8\u5988\u3001\u5c0f\u59e8\u5988\u7b49\uff09\u3001\u82f1\u6587->\u4e2d\u6587\u540d\u5b57\uff08\u674e\u7ea6\u7ff0\uff09\u3001\u6210\u8bed\u8bcd\u5178\n```\n\uff08\u53ef\u7528\u4e8e\u4e2d\u6587\u5206\u8bcd\u3001\u59d3\u540d\u8bc6\u522b\uff09\n\n**11\\. \u4e2d\u6587\u7f29\u5199\u5e93\uff1a**[github](https:\/\/github.com\/zhangyics\/Chinese-abbreviation-dataset\/blob\/master\/dev_set.txt)\n```\n\u5168\u56fd\u4eba\u5927: \u5168\u56fd\/n \u4eba\u6c11\/n \u4ee3\u8868\u5927\u4f1a\/n\n\u4e2d\u56fd: \u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\/ns\n\u5973\u7f51\u8d5b: \u5973\u5b50\/n \u7f51\u7403\/n \u6bd4\u8d5b\/vn\n```\n**12\\. \u6c49\u8bed\u62c6\u5b57\u8bcd\u5178\uff1a**[kfcd\/chaizi](https:\/\/github.com\/kfcd\/chaizi)\n```\n\u6f22\u5b57\t\u62c6\u6cd5 (\u4e00)\t\u62c6\u6cd5 (\u4e8c)\t\u62c6\u6cd5 (\u4e09)\n\u62c6\t\u624b \u65a5\t\u624c \u65a5\t\u624d \u65a5\n```\n**13\\. \u8bcd\u6c47\u60c5\u611f\u503c\uff1a**[rainarch\/SentiBridge](https:\/\/github.com\/rainarch\/SentiBridge\/blob\/master\/Entity_Emotion_Express\/CCF_data\/pair_mine_result)\n```\n\u5c71\u6cc9\u6c34\t\u5145\u6c9b\t0.400704566541\t0.370067395878\n\u89c6\u91ce\t        \u5bbd\u5e7f\t0.305762728932\t0.325320747491\n\u5927\u5ce1\u8c37\t\u60ca\u9669\t0.312137906517\t0.378594957281\n```\n**14\\. \u4e2d\u6587\u8bcd\u5e93\u3001\u505c\u7528\u8bcd\u3001\u654f\u611f\u8bcd** [dongxiexidian\/Chinese](https:\/\/github.com\/fighting41love\/Chinese_from_dongxiexidian)\n\n\u6b64package\u7684\u654f\u611f\u8bcd\u5e93\u5206\u7c7b\u66f4\u7ec6\uff1a\n\n[\u53cd\u52a8\u8bcd\u5e93](https:\/\/github.com\/fighting41love\/funNLP\/tree\/master\/data\/\u654f\u611f\u8bcd\u5e93)\uff0c [\u654f\u611f\u8bcd\u5e93\u8868\u7edf\u8ba1](https:\/\/github.com\/fighting41love\/funNLP\/tree\/master\/data\/\u654f\u611f\u8bcd\u5e93)\uff0c [\u66b4\u6050\u8bcd\u5e93](https:\/\/github.com\/fighting41love\/funNLP\/tree\/master\/data\/\u654f\u611f\u8bcd\u5e93)\uff0c [\u6c11\u751f\u8bcd\u5e93](https:\/\/github.com\/fighting41love\/funNLP\/tree\/master\/data\/\u654f\u611f\u8bcd\u5e93)\uff0c [\u8272\u60c5\u8bcd\u5e93](https:\/\/github.com\/fighting41love\/funNLP\/tree\/master\/data\/\u654f\u611f\u8bcd\u5e93)\n\n**15\\. \u6c49\u5b57\u8f6c\u62fc\u97f3\uff1a**[mozillazg\/python-pinyin](https:\/\/github.com\/mozillazg\/python-pinyin)\n\n\u6587\u672c\u7ea0\u9519\u4f1a\u7528\u5230\n\n**16\\. \u4e2d\u6587\u7e41\u7b80\u4f53\u4e92\u8f6c\uff1a**[skydark\/nstools](https:\/\/github.com\/skydark\/nstools\/tree\/master\/zhtools)\n\n**17\\. \u82f1\u6587\u6a21\u62df\u4e2d\u6587\u53d1\u97f3\u5f15\u64ce** funny chinese text to speech enginee\uff1a[tinyfool\/ChineseWithEnglish](https:\/\/github.com\/tinyfool\/ChineseWithEnglish)\n```\nsay wo i ni\n#\u8bf4\uff1a\u6211\u7231\u4f60\n```\n\u76f8\u5f53\u4e8e\u7528\u82f1\u6587\u97f3\u6807\uff0c\u6a21\u62df\u4e2d\u6587\u53d1\u97f3\u3002\n\n**18\\. \u6c6a\u5cf0\u6b4c\u8bcd\u751f\u6210\u5668\uff1a**[phunterlau\/wangfeng-rnn](https:\/\/github.com\/phunterlau\/wangfeng-rnn)\n```\n\u6211\u5728\u8fd9\u91cc\u4e2d\u7684\u591c\u91cc\n\u5c31\u50cf\u4e00\u573a\u662f\u4e00\u79cd\u751f\u547d\u7684\u610f\u65ea\n\u5c31\u50cf\u6211\u7684\u751f\u6d3b\u53d8\u5f97\u5728\u6211\u4e00\u6837\n\u53ef\u6211\u4eec\u8fd9\u662f\u4e00\u4e2a\u77e5\u9053\n\u6211\u53ea\u662f\u4e00\u5929\u4f60\u4f1a\u600e\u5417\n```\n**19\\. \u540c\u4e49\u8bcd\u5e93\u3001\u53cd\u4e49\u8bcd\u5e93\u3001\u5426\u5b9a\u8bcd\u5e93\uff1a**[guotong1988\/chinese_dictionary](https:\/\/github.com\/guotong1988\/chinese_dictionary)\n\n**20\\. \u65e0\u7a7a\u683c\u82f1\u6587\u4e32\u5206\u5272\u3001\u62bd\u53d6\u5355\u8bcd\uff1a**[wordninja](https:\/\/github.com\/keredson\/wordninja)\n```\n>>> import wordninja\n>>> wordninja.split('derekanderson')\n['derek', 'anderson']\n>>> wordninja.split('imateapot')\n['im', 'a', 'teapot']\n```\n**21\\. IP\u5730\u5740\u6b63\u5219\u8868\u8fbe\u5f0f\uff1a**\n```\n(25[0-5]|2[0-4]\\d|[0-1]\\d{2}|[1-9]?\\d)\\.(25[0-5]|2[0-4]\\d|[0-1]\\d{2}|[1-9]?\\d)\\.(25[0-5]|2[0-4]\\d|[0-1]\\d{2}|[1-9]?\\d)\\.(25[0-5]|2[0-4]\\d|[0-1]\\d{2}|[1-9]?\\d)\n```\n**22\\. \u817e\u8bafQQ\u53f7\u6b63\u5219\u8868\u8fbe\u5f0f\uff1a**\n```\n[1-9]([0-9]{5,11})\n```\n**23\\. \u56fd\u5185\u56fa\u8bdd\u53f7\u7801\u6b63\u5219\u8868\u8fbe\u5f0f\uff1a**\n```\n[0-9-()\uff08\uff09]{7,18}\n```\n**24\\. \u7528\u6237\u540d\u6b63\u5219\u8868\u8fbe\u5f0f\uff1a**\n```\n[A-Za-z0-9_\\-\\u4e00-\\u9fa5]+\n```\n**25\\. \u6c7d\u8f66\u54c1\u724c\u3001\u6c7d\u8f66\u96f6\u4ef6\u76f8\u5173\u8bcd\u6c47\uff1a**\n```\n\u89c1\u672crepo\u7684data\u6587\u4ef6 [data](https:\/\/github.com\/fighting41love\/funNLP\/tree\/master\/data)\n```\n**26\\. \u65f6\u95f4\u62bd\u53d6\uff1a**\n\n> \u5df2\u96c6\u6210\u5230 python package [cocoNLP](https:\/\/github.com\/fighting41love\/cocoNLP)\u4e2d\uff0c\u6b22\u8fce\u8bd5\u7528\n\n```\n\u57282016\u5e746\u67087\u65e59:44\u6267\u884c\u6e2c\u8a66\uff0c\u7ed3\u679c\u5982\u4e0b\n\nHi\uff0call\u3002\u4e0b\u5468\u4e00\u4e0b\u5348\u4e09\u70b9\u5f00\u4f1a\n\n>> 2016-06-13 15:00:00-false\n\n\u5468\u4e00\u5f00\u4f1a\n\n>> 2016-06-13 00:00:00-true\n\n\u4e0b\u4e0b\u5468\u4e00\u5f00\u4f1a\n\n>> 2016-06-20 00:00:00-true\n```\n[java version]( https:\/\/github.com\/shinyke\/Time-NLP)\n\n[python version](https:\/\/github.com\/zhanzecheng\/Time_NLP)\n\n**27\\. \u5404\u79cd\u4e2d\u6587\u8bcd\u5411\u91cf\uff1a** [github repo](https:\/\/github.com\/Embedding\/Chinese-Word-Vectors)\n\n\u4e2d\u6587\u8bcd\u5411\u91cf\u5927\u5168\n\n**28\\. \u516c\u53f8\u540d\u5b57\u5927\u5168\uff1a** [github repo](https:\/\/github.com\/wainshine\/Company-Names-Corpus)\n\n**29\\. \u53e4\u8bd7\u8bcd\u5e93\uff1a** [github repo](https:\/\/github.com\/panhaiqi\/AncientPoetry) [\u66f4\u5168\u7684\u53e4\u8bd7\u8bcd\u5e93](https:\/\/github.com\/chinese-poetry\/chinese-poetry)\n\n**30\\. THU\u6574\u7406\u7684\u8bcd\u5e93\uff1a** [link](http:\/\/thuocl.thunlp.org\/)\n\n\u5df2\u6574\u7406\u5230\u672crepo\u7684data\u6587\u4ef6\u5939\u4e2d.\n```\nIT\u8bcd\u5e93\u3001\u8d22\u7ecf\u8bcd\u5e93\u3001\u6210\u8bed\u8bcd\u5e93\u3001\u5730\u540d\u8bcd\u5e93\u3001\u5386\u53f2\u540d\u4eba\u8bcd\u5e93\u3001\u8bd7\u8bcd\u8bcd\u5e93\u3001\u533b\u5b66\u8bcd\u5e93\u3001\u996e\u98df\u8bcd\u5e93\u3001\u6cd5\u5f8b\u8bcd\u5e93\u3001\u6c7d\u8f66\u8bcd\u5e93\u3001\u52a8\u7269\u8bcd\u5e93\n```\n**31\\. \u4e2d\u6587\u804a\u5929\u8bed\u6599** [link](https:\/\/github.com\/codemayq\/chaotbot_corpus_Chinese)\n```\n\u8be5\u5e93\u641c\u96c6\u4e86\u5305\u542b:\u8c46\u74e3\u591a\u8f6e, PTT\u516b\u5366\u8bed\u6599, \u9752\u4e91\u8bed\u6599, \u7535\u89c6\u5267\u5bf9\u767d\u8bed\u6599, \u8d34\u5427\u8bba\u575b\u56de\u5e16\u8bed\u6599,\u5fae\u535a\u8bed\u6599,\u5c0f\u9ec4\u9e21\u8bed\u6599\n```\n**32\\. \u4e2d\u6587\u8c23\u8a00\u6570\u636e:** [github](https:\/\/github.com\/thunlp\/Chinese_Rumor_Dataset)\n```\n\u8be5\u6570\u636e\u6587\u4ef6\u4e2d\uff0c\u6bcf\u4e00\u884c\u4e3a\u4e00\u6761json\u683c\u5f0f\u7684\u8c23\u8a00\u6570\u636e\uff0c\u5b57\u6bb5\u91ca\u4e49\u5982\u4e0b\uff1a\n\nrumorCode: \u8be5\u6761\u8c23\u8a00\u7684\u552f\u4e00\u7f16\u7801\uff0c\u53ef\u4ee5\u901a\u8fc7\u8be5\u7f16\u7801\u76f4\u63a5\u8bbf\u95ee\u8be5\u8c23\u8a00\u4e3e\u62a5\u9875\u9762\u3002\ntitle: \u8be5\u6761\u8c23\u8a00\u88ab\u4e3e\u62a5\u7684\u6807\u9898\u5185\u5bb9\ninformerName: \u4e3e\u62a5\u8005\u5fae\u535a\u540d\u79f0\ninformerUrl: \u4e3e\u62a5\u8005\u5fae\u535a\u94fe\u63a5\nrumormongerName: \u53d1\u5e03\u8c23\u8a00\u8005\u7684\u5fae\u535a\u540d\u79f0\nrumormongerUr: \u53d1\u5e03\u8c23\u8a00\u8005\u7684\u5fae\u535a\u94fe\u63a5\nrumorText: \u8c23\u8a00\u5185\u5bb9\nvisitTimes: \u8be5\u8c23\u8a00\u88ab\u8bbf\u95ee\u6b21\u6570\nresult: \u8be5\u8c23\u8a00\u5ba1\u67e5\u7ed3\u679c\npublishTime: \u8be5\u8c23\u8a00\u88ab\u4e3e\u62a5\u65f6\u95f4\n```\n\n\n**33\\. \u60c5\u611f\u6ce2\u52a8\u5206\u6790\uff1a**[github](https:\/\/github.com\/CasterWx\/python-girlfriend-mood\/)\n\n\u8bcd\u5e93\u5df2\u6574\u7406\u5230\u672crepo\u7684data\u6587\u4ef6\u5939\u4e2d.\n```\n\u672crepo\u9879\u76ee\u662f\u4e00\u4e2a\u901a\u8fc7\u4e0e\u4eba\u5bf9\u8bdd\u83b7\u5f97\u5176\u60c5\u611f\u503c\u6ce2\u52a8\u56fe\u8c31, \u5185\u7528\u8bcd\u5e93\u5728data\u6587\u4ef6\u5939\u4e2d.\n```\n\n**34\\. \u4e2d\u6587\u95ee\u7b54\u6570\u636e\u96c6**\uff1a[\u94fe\u63a5](https:\/\/pan.baidu.com\/s\/1QUsKcFWZ7Tg1dk_AbldZ1A) \u63d0\u53d6\u7801: 2dva\n\n**35\\. \u53e5\u5b50\u3001QA\u76f8\u4f3c\u5ea6\u5339\u914d:MatchZoo**  [github](https:\/\/github.com\/NTMC-Community\/MatchZoo)\n\n\u6587\u672c\u76f8\u4f3c\u5ea6\u5339\u914d\u7b97\u6cd5\u7684\u96c6\u5408\uff0c\u5305\u542b\u591a\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u503c\u5f97\u5c1d\u8bd5\u3002\n\n**36\\. bert\u8d44\u6e90\uff1a**\n\n- bert\u8bba\u6587\u4e2d\u6587\u7ffb\u8bd1: [link](https:\/\/github.com\/yuanxiaosc\/BERT_Paper_Chinese_Translation)\n\n+ bert\u539f\u4f5c\u8005\u7684slides: [link](https:\/\/pan.baidu.com\/s\/1OSPsIu2oh1iJ-bcXoDZpJQ)\n   \u63d0\u53d6\u7801: iarj \n\n+ \u6587\u672c\u5206\u7c7b\u5b9e\u8df5: [github](https:\/\/github.com\/NLPScott\/bert-Chinese-classification-task)\n\n+ bert tutorial\u6587\u672c\u5206\u7c7b\u6559\u7a0b: [github](https:\/\/github.com\/Socialbird-AILab\/BERT-Classification-Tutorial)\n\n+ bert pytorch\u5b9e\u73b0:  [github](https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT)\n\n+ bert\u7528\u4e8e\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b tensorflow\u7248\u672c: [github](https:\/\/github.com\/macanv\/BERT-BiLSTM-CRF-NER)\n+ BERT\u751f\u6210\u53e5\u5411\u91cf\uff0cBERT\u505a\u6587\u672c\u5206\u7c7b\u3001\u6587\u672c\u76f8\u4f3c\u5ea6\u8ba1\u7b97[github](https:\/\/github.com\/terrifyzhao\/bert-utils)\n\n+ bert \u57fa\u4e8e keras \u7684\u5c01\u88c5\u5206\u7c7b\u6807\u6ce8\u6846\u67b6 Kashgari\uff0c\u51e0\u5206\u949f\u5373\u53ef\u642d\u5efa\u4e00\u4e2a\u5206\u7c7b\u6216\u8005\u5e8f\u5217\u6807\u6ce8\u6a21\u578b: [github](https:\/\/github.com\/BrikerMan\/Kashgari)\n\n+ bert\u3001ELMO\u7684\u56fe\u89e3\uff1a [github](https:\/\/jalammar.github.io\/illustrated-bert\/)\n\n+ BERT: Pre-trained models and downstream applications: [github](https:\/\/github.com\/asyml\/texar\/tree\/master\/examples\/bert)\n\n**37. Texar - Toolkit for Text Generation and Beyond**: [github](https:\/\/github.com\/asyml\/texar)\n\n- \u57fa\u4e8eTensorflow\u7684\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u65e8\u5728\u652f\u6301\u5e7f\u6cdb\u7684\u673a\u5668\u5b66\u4e60\uff0c\u7279\u522b\u662f\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u5bf9\u8bdd\u3001\u6458\u8981\u3001\u5185\u5bb9\u5904\u7f6e\u3001\u8bed\u8a00\u5efa\u6a21\u7b49\n\n**38. \u4e2d\u6587\u4e8b\u4ef6\u62bd\u53d6\uff1a** [github](https:\/\/github.com\/liuhuanyong\/ComplexEventExtraction)\n\n- \u4e2d\u6587\u590d\u5408\u4e8b\u4ef6\u62bd\u53d6\uff0c\u5305\u62ec\u6761\u4ef6\u4e8b\u4ef6\u3001\u56e0\u679c\u4e8b\u4ef6\u3001\u987a\u627f\u4e8b\u4ef6\u3001\u53cd\u8f6c\u4e8b\u4ef6\u7b49\u4e8b\u4ef6\u62bd\u53d6\uff0c\u5e76\u5f62\u6210\u4e8b\u7406\u56fe\u8c31\u3002\n\n**39\\. cocoNLP:** [github](https:\/\/github.com\/fighting41love\/cocoNLP)\n\n\u4eba\u540d\u3001\u5730\u5740\u3001\u90ae\u7bb1\u3001\u624b\u673a\u53f7\u3001\u624b\u673a\u5f52\u5c5e\u5730 \u7b49\u4fe1\u606f\u7684\u62bd\u53d6\uff0crake\u77ed\u8bed\u62bd\u53d6\u7b97\u6cd5\u3002\n> pip install cocoNLP\n\n```\n>>> from cocoNLP.extractor import extractor\n\n>>> ex = extractor()\n\n>>> text = '\u6025\u5bfb\u7279\u6717\u666e\uff0c\u7537\u5b69\uff0c\u4e8e2018\u5e7411\u670827\u53f711\u65f6\u5728\u9655\u897f\u7701\u5b89\u5eb7\u5e02\u6c49\u6ee8\u533a\u8d70\u5931\u3002\u4e22\u5931\u53d1\u578b\u77ed\u53d1\uff0c...\u5982\u6709\u7ebf\u7d22\uff0c\u8bf7\u8fc5\u901f\u4e0e\u8b66\u65b9\u8054\u7cfb\uff1a18100065143\uff0c132-6156-2938\uff0cbaizhantang@sina.com.cn \u548cyangyangfuture at gmail dot com'\n\n# \u62bd\u53d6\u90ae\u7bb1\n>>> emails = ex.extract_email(text)\n>>> print(emails)\n\n['baizhantang@sina.com.cn', 'yangyangfuture@gmail.com.cn']\n# \u62bd\u53d6\u624b\u673a\u53f7\n>>> cellphones = ex.extract_cellphone(text,nation='CHN')\n>>> print(cellphones)\n\n['18100065143', '13261562938']\n# \u62bd\u53d6\u624b\u673a\u5f52\u5c5e\u5730\u3001\u8fd0\u8425\u5546\n>>> cell_locs = [ex.extract_cellphone_location(cell,'CHN') for cell in cellphones]\n>>> print(cell_locs)\n\ncellphone_location [{'phone': '18100065143', 'province': '\u4e0a\u6d77', 'city': '\u4e0a\u6d77', 'zip_code': '200000', 'area_code': '021', 'phone_type': '\u7535\u4fe1'}]\n# \u62bd\u53d6\u5730\u5740\u4fe1\u606f\n>>> locations = ex.extract_locations(text)\n>>> print(locations)\n['\u9655\u897f\u7701\u5b89\u5eb7\u5e02\u6c49\u6ee8\u533a', '\u5b89\u5eb7\u5e02\u6c49\u6ee8\u533a', '\u6c49\u6ee8\u533a']\n# \u62bd\u53d6\u65f6\u95f4\u70b9\n>>> times = ex.extract_time(text)\n>>> print(times)\ntime {\"type\": \"timestamp\", \"timestamp\": \"2018-11-27 11:00:00\"}\n# \u62bd\u53d6\u4eba\u540d\n>>> name = ex.extract_name(text)\n>>> print(name)\n\u7279\u6717\u666e\n\n```\n\n**40\\. \u56fd\u5185\u7535\u8bdd\u53f7\u7801\u6b63\u5219\u5339\u914d\uff08\u4e09\u5927\u8fd0\u8425\u5546+\u865a\u62df\u7b49\uff09:** [github](https:\/\/github.com\/VincentSit\/ChinaMobilePhoneNumberRegex)\n\n**41\\. \u6e05\u534e\u5927\u5b66XLORE:\u4e2d\u82f1\u6587\u8de8\u8bed\u8a00\u767e\u79d1\u77e5\u8bc6\u56fe\u8c31:** [link](https:\/\/xlore.org\/download.html)  \n\u4e0a\u8ff0\u94fe\u63a5\u4e2d\u5305\u542b\u4e86\u6240\u6709\u5b9e\u4f53\u53ca\u5173\u7cfb\u7684TTL\u6587\u4ef6\uff0c\u66f4\u591a\u6570\u636e\u5c06\u5728\u8fd1\u671f\u53d1\u5e03\u3002\n\u6982\u5ff5\uff0c\u5b9e\u4f8b\uff0c\u5c5e\u6027\u548c\u4e0a\u4e0b\u4f4d\u5173\u7cfb\u6570\u76ee\n\n|  | \u767e\u5ea6  |\u4e2d\u6587\u7ef4\u57fa   | \u82f1\u6587\u7ef4\u57fa  |  \u603b\u6570  |\n|--|---|---|---|---|\n|\u6982\u5ff5\u6570\u91cf |32,009\t|\t150,241|\t326,518|\t508,768  |\n|\u5b9e\u4f8b\u6570\u91cf|\t1,629,591\t|640,622|\t1,235,178|\t3,505,391  |\n|\u5c5e\u6027\u6570\u91cf|\t157,370\t|45,190\t|26,723\t|229.283  |\n|InstanceOf|\t7,584,931|\t1,449,925|\t3,032,515\t|12,067,371 |\n|SubClassOf|\t2,784\t|191,577|\t555,538\t|749,899  |\n\n\u8de8\u8bed\u8a00\u8fde\u63a5\uff08\u6982\u5ff5\/\u5b9e\u4f8b\uff09\n\n|  |  \u767e\u5ea6 | \u4e2d\u6587\u7ef4\u57fa  |  \u82f1\u6587\u7ef4\u57fa |\n|--|---|---|--|\n|\u767e\u5ea6|\t-|\t10,216\/336,890|\t4,846\/303,108 |\n|\u4e2d\u6587\u7ef4\u57fa|\t10,216\/336,890|\t-\t|28,921\/454,579  |\n|\u82f1\u6587\u7ef4\u57fa|\t4,846\/303,108\t|28,921\/454,579|\t-  |\n\n**42\\. \u6e05\u534e\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7cfb\u5217\u62a5\u544a\uff1a** [link](https:\/\/reports.aminer.cn)  \n\u6bcf\u5e74\u4f1a\u51faAI\u9886\u57df\u76f8\u5173\u7684\u62a5\u544a\uff0c\u5185\u5bb9\u5305\u542b\n  - \u81ea\u7136\u8bed\u8a00\u5904\u7406 [link](https:\/\/static.aminer.cn\/misc\/article\/nlp.pdf)\n  - \u77e5\u8bc6\u56fe\u8c31 [link](https:\/\/www.aminer.cn\/research_report\/5c3d5a8709%20e961951592a49d?download=true&pathname=knowledgegraph.pdf)\n  - \u6570\u636e\u6316\u6398 [link](https:\/\/www.aminer.cn\/research_report\/5c3d5a5cecb160952fa10b76?download=true&pathname=datamining.pdf)\n  - \u81ea\u52a8\u9a7e\u9a76 [link](https:\/\/static.aminer.cn\/misc\/article\/selfdriving.pdf)\n  - \u673a\u5668\u7ffb\u8bd1 [link](https:\/\/static.aminer.cn\/misc\/article\/translation.pdf)\n  - \u533a\u5757\u94fe [link](https:\/\/static.aminer.cn\/misc\/article\/blockchain_public.pdf)\n  - \u673a\u5668\u4eba [link](https:\/\/static.aminer.cn\/misc\/article\/robotics_beta.pdf)\n  - \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66 [link](https:\/\/static.aminer.cn\/misc\/article\/cg.pdf)\n  - 3D\u6253\u5370 [link](https:\/\/static.aminer.cn\/misc\/article\/3d.pdf)\n  - \u4eba\u8138\u8bc6\u522b [link](https:\/\/static.aminer.cn\/misc\/article\/facerecognition.pdf)\n  - \u4eba\u5de5\u667a\u80fd\u82af\u7247 [link](https:\/\/static.aminer.cn\/misc\/article\/aichip.pdf)\n  - \u7b49\u7b49\n\n**43\\.\u81ea\u7136\u8bed\u8a00\u751f\u6210\u65b9\u9762:**  \n- [Ehud Reiter\u6559\u6388\u7684\u535a\u5ba2](https:\/\/ehudreiter.com)  \u5317\u5927\u4e07\u5c0f\u519b\u6559\u6388\u5f3a\u529b\u63a8\u8350\uff0c\u8be5\u535a\u5ba2\u5bf9NLG\u6280\u672f\u3001\u8bc4\u4ef7\u4e0e\u5e94\u7528\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u63a2\u8ba8\u4e0e\u53cd\u601d\u3002  \n- [\u6587\u672c\u751f\u6210\u76f8\u5173\u8d44\u6e90\u5927\u5217\u8868](https:\/\/github.com\/ChenChengKuan\/awesome-text-generation)  \n- [\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff1a\u8ba9\u673a\u5668\u638c\u63e1\u81ea\u52a8\u521b\u4f5c\u7684\u672c\u9886 - \u5f00\u653e\u57df\u5bf9\u8bdd\u751f\u6210\u53ca\u5728\u5fae\u8f6f\u5c0f\u51b0\u4e2d\u7684\u5b9e\u8df5](https:\/\/drive.google.com\/file\/d\/1Mdna3q986k6OoJNsfAHznTtnMAEVzv5z\/view)  \n- [\u6587\u672c\u751f\u6210\u63a7\u5236](https:\/\/github.com\/harvardnlp\/Talk-Latent\/blob\/master\/main.pdf)  \n- [\u81ea\u7136\u8bed\u8a00\u751f\u6210\u76f8\u5173\u8d44\u6e90\u5927\u5217\u8868](https:\/\/github.com\/tokenmill\/awesome-nlg)\n- [\u7528BLEURT\u8bc4\u4ef7\u81ea\u7136\u8bed\u8a00\u751f\u6210](https:\/\/ai.googleblog.com\/2020\/05\/evaluating-natural-language-generation.html)\n  \n**44\\.:**\n[jieba](https:\/\/github.com\/fxsjy\/jieba)\u548c[hanlp](https:\/\/github.com\/hankcs\/pyhanlp)\u5c31\u4e0d\u5fc5\u4ecb\u7ecd\u4e86\u5427\u3002\n\n**45\\.NLP\u592a\u96be\u4e86\u7cfb\u5217:** [github](https:\/\/github.com\/fighting41love\/hardNLP)\n\n- \u6765\u5230\u6768\u8fc7\u66fe\u7ecf\u751f\u6d3b\u8fc7\u7684\u5730\u65b9\uff0c\u5c0f\u9f99\u5973\u52a8\u60c5\u5730\u8bf4\uff1a\u201c\u6211\u4e5f\u60f3\u8fc7\u8fc7\u8fc7\u513f\u8fc7\u8fc7\u7684\u751f\u6d3b\u3002\u201d \u200b\u200b\u200b  \n- \u6765\u5230\u513f\u5b50\u7b49\u6821\u8f66\u7684\u5730\u65b9\uff0c\u9093\u8d85\u5bf9\u5b59\u4fea\u8bf4\uff1a\u201c\u6211\u4e5f\u60f3\u7b49\u7b49\u7b49\u7b49\u7b49\u8fc7\u7684\u90a3\u8f86\u8f66\u3002\u201d  \n- \u8d75\u654f\u8bf4\uff1a\u6211\u4e5f\u60f3\u63a7\u5fcc\u5fcc\u5df1\u4e0d\u60f3\u65e0\u5fcc\u3002\n- \u4f60\u4e5f\u60f3\u72af\u8303\u8303\u8303\u73ae\u742a\u72af\u8fc7\u7684\u9519\u5417  \n- \u5bf9\u53d9\u6253\u51fb\u662f\u4e00\u6b21\u6027\u884c\u4e3a\uff1f\n\n\n**46\\.\u81ea\u52a8\u5bf9\u8054\u6570\u636e\u53ca\u673a\u5668\u4eba:**  \n[70\u4e07\u5bf9\u8054\u6570\u636e link](https:\/\/github.com\/wb14123\/couplet-dataset)  \n[\u4ee3\u7801 link](https:\/\/github.com\/wb14123\/seq2seq-couplet)  \n\n\u4e0a\u8054  |\u4e0b\u8054  \n--|--\n\u6bb7\u52e4\u6015\u8d1f\u4e09\u6625\u610f  | \u6f47\u6d12\u96be\u4e66\u4e00\u5b57\u6101\n\u5982\u6b64\u6e05\u79cb\u4f55\u541d\u9152  | \u8fd9\u822c\u660e\u6708\u4e0d\u987b\u94b1\n\n**47\\.\u7528\u6237\u540d\u9ed1\u540d\u5355\u5217\u8868\uff1a** [github](https:\/\/github.com\/marteinn\/The-Big-Username-Blacklist)\n\u5305\u542b\u4e86\u7528\u6237\u540d\u7981\u7528\u5217\u8868\uff0c\u6bd4\u5982: [link](https:\/\/github.com\/marteinn\/The-Big-Username-Blacklist\/blob\/master\/list_raw.txt)\n```\nadministrator\nadministration\nautoconfig\nautodiscover\nbroadcasthost\ndomain\neditor\nguest\nhost\nhostmaster\ninfo\nkeybase.txt\nlocaldomain\nlocalhost\nmaster\nmail\nmail0\nmail1\n```\n\n**48\\.\u7f6a\u540d\u6cd5\u52a1\u540d\u8bcd\u53ca\u5206\u7c7b\u6a21\u578b:**   [github](https:\/\/github.com\/liuhuanyong\/CrimeKgAssitant)  \n```\n\u5305\u542b856\u9879\u7f6a\u540d\u77e5\u8bc6\u56fe\u8c31, \u57fa\u4e8e280\u4e07\u7f6a\u540d\u8bad\u7ec3\u5e93\u7684\u7f6a\u540d\u9884\u6d4b,\u57fa\u4e8e20W\u6cd5\u52a1\u95ee\u7b54\u5bf9\u768413\u7c7b\u95ee\u9898\u5206\u7c7b\u4e0e\u6cd5\u5f8b\u8d44\u8baf\u95ee\u7b54\u529f\u80fd\n```\n**49\\.\u5fae\u4fe1\u516c\u4f17\u53f7\u8bed\u6599:** [github](https:\/\/github.com\/nonamestreet\/weixin_public_corpus)\n\n3G\u8bed\u6599\uff0c\u5305\u542b\u90e8\u5206\u7f51\u7edc\u6293\u53d6\u7684\u5fae\u4fe1\u516c\u4f17\u53f7\u7684\u6587\u7ae0\uff0c\u5df2\u7ecf\u53bb\u9664HTML\uff0c\u53ea\u5305\u542b\u4e86\u7eaf\u6587\u672c\u3002\u6bcf\u884c\u4e00\u7bc7\uff0c\u662fJSON\u683c\u5f0f\uff0cname\u662f\u5fae\u4fe1\u516c\u4f17\u53f7\u540d\u5b57\uff0caccount\u662f\u5fae\u4fe1\u516c\u4f17\u53f7ID\uff0ctitle\u662f\u9898\u76ee\uff0ccontent\u662f\u6b63\u6587\n\n**50\\.cs224n\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b\uff1a**[link](http:\/\/web.stanford.edu\/class\/cs224n\/)  \n  - \u8bfe\u7a0b\u4e2d\u6a21\u578b\u7684pytorch\u5b9e\u73b0 [link](https:\/\/github.com\/DSKSD\/DeepNLP-models-Pytorch)\n  - \u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4eba\u5458\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5b9e\u4f8b\u6559\u7a0b [link](https:\/\/github.com\/graykode\/nlp-tutorial)\n\n\n**51\\.\u4e2d\u6587\u624b\u5199\u6c49\u5b57\u8bc6\u522b\uff1a**[github](https:\/\/github.com\/chizhanyuefeng\/Chinese_OCR_CNN-RNN-CTC)\n\n**52\\.\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8bed\u6599\/\u6570\u636e\u96c6\uff1a**[github](https:\/\/github.com\/SophonPlus\/ChineseNlpCorpus)\n[\u7ade\u54c1\uff1aTHUOCL\uff08THU Open Chinese Lexicon\uff09\u4e2d\u6587\u8bcd\u5e93](https:\/\/github.com\/thunlp\/THUOCL)\n\n**53\\.\u53d8\u91cf\u547d\u540d\u795e\u5668\uff1a**[github](https:\/\/github.com\/unbug\/codelf) [link](https:\/\/unbug.github.io\/codelf\/)\n\n**54\\.\u5206\u8bcd\u8bed\u6599\u5e93+\u4ee3\u7801\uff1a**[\u767e\u5ea6\u7f51\u76d8\u94fe\u63a5](https:\/\/pan.baidu.com\/s\/1MXZONaLgeaw0_TxZZDAIYQ)   \n - \u63d0\u53d6\u7801: pea6 \n - [keras\u5b9e\u73b0\u7684\u57fa\u4e8eBi-LSTM + CRF\u7684\u4e2d\u6587\u5206\u8bcd+\u8bcd\u6027\u6807\u6ce8](https:\/\/github.com\/GlassyWing\/bi-lstm-crf)\n - [\u57fa\u4e8eUniversal Transformer + CRF \u7684\u4e2d\u6587\u5206\u8bcd\u548c\u8bcd\u6027\u6807\u6ce8](https:\/\/github.com\/GlassyWing\/transformer-word-segmenter)\n - [\u5feb\u901f\u795e\u7ecf\u7f51\u7edc\u5206\u8bcd\u5305 java version](https:\/\/github.com\/yaoguangluo\/NeroParser)\n\n**55\\. NLP\u65b0\u4e66\u63a8\u8350\u300aNatural Language Processing\u300bby Jacob Eisenstein\uff1a** [link](https:\/\/github.com\/jacobeisenstein\/gt-nlp-class\/blob\/master\/notes\/eisenstein-nlp-notes.pdf)\n\n**56\\. \u4efb\u52a1\u578b\u5bf9\u8bdd\u82f1\u6587\u6570\u636e\u96c6\uff1a**   [github](https:\/\/github.com\/AtmaHou\/Task-Oriented-Dialogue-Dataset-Survey)  \n\u3010\u6700\u5168\u4efb\u52a1\u578b\u5bf9\u8bdd\u6570\u636e\u96c6\u3011\u4e3b\u8981\u4ecb\u7ecd\u4e86\u4e00\u4efd\u4efb\u52a1\u578b\u5bf9\u8bdd\u6570\u636e\u96c6\u5927\u5168\uff0c\u8fd9\u4efd\u6570\u636e\u96c6\u5927\u5168\u6db5\u76d6\u4e86\u5230\u76ee\u524d\u5728\u4efb\u52a1\u578b\u5bf9\u8bdd\u9886\u57df\u7684\u6240\u6709\u5e38\u7528\u6570\u636e\u96c6\u7684\u4e3b\u8981\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u597d\u7684\u628a\u63e1\u9886\u57df\u8fdb\u5c55\u7684\u8109\u7edc\uff0c\u6211\u4eec\u4ee5Leaderboard\u7684\u5f62\u5f0f\u7ed9\u51fa\u4e86\u51e0\u4e2a\u6570\u636e\u96c6\u4e0a\u7684State-of-the-art\u5b9e\u9a8c\u7ed3\u679c\u3002\n\n**57\\. ASR \u8bed\u97f3\u6570\u636e\u96c6 + \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff1a**  [github](https:\/\/github.com\/nl8590687\/ASRT_SpeechRecognition)\n+ Data Sets \u6570\u636e\u96c6\n  * **\u6e05\u534e\u5927\u5b66THCHS30\u4e2d\u6587\u8bed\u97f3\u6570\u636e\u96c6**\n\n    data_thchs30.tgz \n  [OpenSLR\u56fd\u5185\u955c\u50cf](<http:\/\/cn-mirror.openslr.org\/resources\/18\/data_thchs30.tgz>)\n  [OpenSLR\u56fd\u5916\u955c\u50cf](<http:\/\/www.openslr.org\/resources\/18\/data_thchs30.tgz>)\n\n    test-noise.tgz \n  [OpenSLR\u56fd\u5185\u955c\u50cf](<http:\/\/cn-mirror.openslr.org\/resources\/18\/test-noise.tgz>)\n  [OpenSLR\u56fd\u5916\u955c\u50cf](<http:\/\/www.openslr.org\/resources\/18\/test-noise.tgz>)\n\n    resource.tgz \n  [OpenSLR\u56fd\u5185\u955c\u50cf](<http:\/\/cn-mirror.openslr.org\/resources\/18\/resource.tgz>)\n  [OpenSLR\u56fd\u5916\u955c\u50cf](<http:\/\/www.openslr.org\/resources\/18\/resource.tgz>)\n\n  * **Free ST Chinese Mandarin Corpus** \n\n    ST-CMDS-20170001_1-OS.tar.gz \n  [OpenSLR\u56fd\u5185\u955c\u50cf](<http:\/\/cn-mirror.openslr.org\/resources\/38\/ST-CMDS-20170001_1-OS.tar.gz>)\n  [OpenSLR\u56fd\u5916\u955c\u50cf](<http:\/\/www.openslr.org\/resources\/38\/ST-CMDS-20170001_1-OS.tar.gz>)\n\n  * **AIShell-1 \u5f00\u6e90\u7248\u6570\u636e\u96c6** \n\n    data_aishell.tgz\n  [OpenSLR\u56fd\u5185\u955c\u50cf](<http:\/\/cn-mirror.openslr.org\/resources\/33\/data_aishell.tgz>)\n  [OpenSLR\u56fd\u5916\u955c\u50cf](<http:\/\/www.openslr.org\/resources\/33\/data_aishell.tgz>)\n\n  \u6ce8\uff1a\u6570\u636e\u96c6\u89e3\u538b\u65b9\u6cd5\n\n  ```\n  $ tar xzf data_aishell.tgz\n  $ cd data_aishell\/wav\n  $ for tar in *.tar.gz;  do tar xvf $tar; done\n  ```\n\n  * **Primewords Chinese Corpus Set 1** \n\n    primewords_md_2018_set1.tar.gz\n  [OpenSLR\u56fd\u5185\u955c\u50cf](<http:\/\/cn-mirror.openslr.org\/resources\/47\/primewords_md_2018_set1.tar.gz>)\n  [OpenSLR\u56fd\u5916\u955c\u50cf](<http:\/\/www.openslr.org\/resources\/47\/primewords_md_2018_set1.tar.gz>)\n\n\n**58\\. \u7b11\u58f0\u68c0\u6d4b\u5668\uff1a**  [github](https:\/\/github.com\/ideo\/LaughDetection)\n\n**59\\. Microsoft\u591a\u8bed\u8a00\u6570\u5b57\/\u5355\u4f4d\/\u5982\u65e5\u671f\u65f6\u95f4\u8bc6\u522b\u5305\uff1a** [github](https:\/\/github.com\/Microsoft\/Recognizers-Text\n\n**60\\. chinese-xinhua \u4e2d\u534e\u65b0\u534e\u5b57\u5178\u6570\u636e\u5e93\u53caapi\uff0c\u5305\u62ec\u5e38\u7528\u6b47\u540e\u8bed\u3001\u6210\u8bed\u3001\u8bcd\u8bed\u548c\u6c49\u5b57** [github](https:\/\/github.com\/pwxcoo\/chinese-xinhua)\n\n**61\\. \u6587\u6863\u56fe\u8c31\u81ea\u52a8\u751f\u6210** [github](https:\/\/github.com\/liuhuanyong\/TextGrapher)  \n - TextGrapher - Text Content Grapher based on keyinfo extraction by NLP method\u3002\u8f93\u5165\u4e00\u7bc7\u6587\u6863\uff0c\u5c06\u6587\u6863\u8fdb\u884c\u5173\u952e\u4fe1\u606f\u63d0\u53d6\uff0c\u8fdb\u884c\u7ed3\u6784\u5316\uff0c\u5e76\u6700\u7ec8\u7ec4\u7ec7\u6210\u56fe\u8c31\u7ec4\u7ec7\u5f62\u5f0f\uff0c\u5f62\u6210\u5bf9\u6587\u7ae0\u8bed\u4e49\u4fe1\u606f\u7684\u56fe\u8c31\u5316\u5c55\u793a\n\n**62\\. SpaCy \u4e2d\u6587\u6a21\u578b** [github](https:\/\/github.com\/howl-anderson\/Chinese_models_for_SpaCy)  \n - \u5305\u542bParser, NER, \u8bed\u6cd5\u6811\u7b49\u529f\u80fd\u3002\u6709\u4e00\u4e9b\u82f1\u6587package\u4f7f\u7528spacy\u7684\u82f1\u6587\u6a21\u578b\u7684\uff0c\u5982\u679c\u8981\u9002\u914d\u4e2d\u6587\uff0c\u53ef\u80fd\u9700\u8981\u4f7f\u7528spacy\u4e2d\u6587\u6a21\u578b\u3002\n\n**63\\. Common Voice\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u65b0\u7248**  [link](https:\/\/voice.mozilla.org\/en\/datasets)  \n - \u5305\u62ec\u6765\u81ea42,000\u540d\u8d21\u732e\u8005\u8d85\u8fc71,400\u5c0f\u65f6\u7684\u8bed\u97f3\u6837\u672c\uff0c\u6db5github\n\n**64\\. \u795e\u7ecf\u7f51\u7edc\u5173\u7cfb\u62bd\u53d6 pytorch**  [github](https:\/\/github.com\/ShulinCao\/OpenNRE-PyTorch)  \n - \u6682\u4e0d\u652f\u6301\u4e2d\u6587\n\n**65\\. \u57fa\u4e8ebert\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b pytorch**  [github](https:\/\/github.com\/Kyubyong\/bert_ner)  \n - \u6682\u4e0d\u652f\u6301\u4e2d\u6587\n\n**66\\. \u5173\u952e\u8bcd(Keyphrase)\u62bd\u53d6\u5305 pke**  [github](https:\/\/github.com\/boudinfl\/pke)  \n[pke: an open source python-based keyphrase extraction toolkit](http:\/\/aclweb.org\/anthology\/C16-2015)  \n - \u6682\u4e0d\u652f\u6301\u4e2d\u6587\uff0c\u6211\u4e8e\u8fd1\u671f\u5bf9\u5176\u8fdb\u884c\u4fee\u6539\uff0c\u4f7f\u5176\u9002\u914d\u4e2d\u6587\u3002\n\u8bf7\u5173\u6ce8\u6211\u7684github\u52a8\u6001\uff0c\u8c22\u8c22\uff01  \n\n**67\\. \u57fa\u4e8e\u533b\u7597\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf**  [github](https:\/\/github.com\/zhihao-chen\/QASystemOnMedicalGraph)  \n  - \u8be5repo\u53c2\u8003\u4e86[github](https:\/\/github.com\/liuhuanyong\/QASystemOnMedicalKG)\n\n\n**68\\. \u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u4e0e\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u7684\u4e8b\u4ef6\u4e09\u5143\u7ec4\u62bd\u53d6**  [github](https:\/\/github.com\/liuhuanyong\/EventTriplesExtraction) \n\n**69\\. \u4f9d\u5b58\u53e5\u6cd5\u5206\u67904\u4e07\u53e5\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e** by \u82cf\u5dde\u5927\u5b66\u6c49\u8bed\u4f9d\u5b58\u6811\u5e93\uff08SUCDT\uff09\n[Homepage](http:\/\/hlt.suda.edu.cn\/index.php\/Nlpcc-2019-shared-task)\n\u6570\u636e\u4e0b\u8f7d\u8be6\u89c1homepage\u5e95\u90e8\uff0c\u9700\u8981\u7b7e\u7f72\u534f\u8bae\uff0c\u9700\u8981\u90ae\u4ef6\u63a5\u6536\u89e3\u538b\u5bc6\u7801\u3002\n\n**70\\. cnocr\uff1a\u7528\u6765\u505a\u4e2d\u6587OCR\u7684Python3\u5305\uff0c\u81ea\u5e26\u4e86\u8bad\u7ec3\u597d\u7684\u8bc6\u522b\u6a21\u578b** [github](https:\/\/github.com\/breezedeus\/cnocr)\n\n**71\\. \u4e2d\u6587\u4eba\u7269\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u9879\u76ee** [github](https:\/\/github.com\/liuhuanyong\/PersonRelationKnowledgeGraph)\n- \u4e2d\u6587\u4eba\u7269\u5173\u7cfb\u56fe\u8c31\u6784\u5efa\n- \u57fa\u4e8e\u77e5\u8bc6\u5e93\u7684\u6570\u636e\u56de\u6807\n- \u57fa\u4e8e\u8fdc\u7a0b\u76d1\u7763\u4e0ebootstrapping\u65b9\u6cd5\u7684\u4eba\u7269\u5173\u7cfb\u62bd\u53d6\n- \u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u77e5\u8bc6\u95ee\u7b54\u7b49\u5e94\u7528\n\n**72\\. \u4e2d\u6587nlp\u7ade\u8d5b\u9879\u76ee\u53ca\u4ee3\u7801\u6c47\u603b** [github](https:\/\/github.com\/geekinglcq\/CDCS)\n- \u6587\u672c\u751f\u6210\u3001\u6587\u672c\u6458\u8981\uff1aByte Cup 2018 \u56fd\u9645\u673a\u5668\u5b66\u4e60\u7ade\u8d5b\n- \u77e5\u8bc6\u56fe\u8c31\uff1a\u745e\u91d1\u533b\u9662MMC\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u5927\u8d5b\n- \u89c6\u9891\u8bc6\u522b \u95ee\u7b54\uff1a2018\u4e4b\u6c5f\u676f\u5168\u7403\u4eba\u5de5\u667a\u80fd\u5927\u8d5b\uff1a\u89c6\u9891\u8bc6\u522b&\u95ee\u7b54\n\n**73\\. \u4e2d\u6587\u5b57\u7b26\u6570\u636e** [github](https:\/\/github.com\/skishore\/makemeahanzi)\n- \u7b80\/\u7e41\u4f53\u6c49\u5b57\u7b14\u987a\n- \u77e2\u91cf\u7b14\u753b\n\n**74\\. speech-aligner: \u4ece\u201c\u4eba\u58f0\u8bed\u97f3\u201d\u53ca\u5176\u201c\u8bed\u8a00\u6587\u672c\u201d\uff0c\u4ea7\u751f\u97f3\u7d20\u7ea7\u522b\u65f6\u95f4\u5bf9\u9f50\u6807\u6ce8\u7684\u5de5\u5177** [github](https:\/\/github.com\/open-speech\/speech-aligner)\n\n**75\\. AmpliGraph: \u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5b66\u4e60(Python)\u5e93\uff1a\u77e5\u8bc6\u56fe\u8c31\u6982\u5ff5\u94fe\u63a5\u9884\u6d4b** [github](https:\/\/github.com\/Accenture\/AmpliGraph) \n- \u57c3\u68ee\u54f2\u51fa\u54c1\uff0c\u76ee\u524d\u5c1a\u4e0d\u652f\u6301\u4e2d\u6587\n\n\n**76\\. Scattertext \u6587\u672c\u53ef\u89c6\u5316(python)** [github](https:\/\/github.com\/JasonKessler\/scattertext)\n- \u5f88\u597d\u7528\u7684\u5de5\u5177\u5305\uff0c\u7b80\u5355\u4fee\u6539\u540e\u53ef\u652f\u6301\u4e2d\u6587\n- \u80fd\u5426\u5206\u6790\u51fa\u67d0\u4e2a\u7c7b\u522b\u7684\u6587\u672c\u4e0e\u5176\u4ed6\u6587\u672c\u7684\u7528\u8bcd\u5dee\u5f02\n\n**77\\. \u8bed\u8a00\/\u77e5\u8bc6\u8868\u793a\u5de5\u5177\uff1aBERT & ERNIE** [github](https:\/\/github.com\/PaddlePaddle\/LARK)\n- \u767e\u5ea6\u51fa\u54c1\uff0cERNIE\u4e5f\u53f7\u79f0\u5728\u591a\u9879nlp\u4efb\u52a1\u4e2d\u51fb\u8d25\u4e86bert\n\n**78\\. \u4e2d\u6587\u5bf9\u6bd4\u82f1\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u7684\u533a\u522b\u7efc\u8ff0** [link](https:\/\/mp.weixin.qq.com\/s\/LQU_HJ4q74lL5oCIk7w5RA)\n\n**79\\. Synonyms\u4e2d\u6587\u8fd1\u4e49\u8bcd\u5de5\u5177\u5305** [github](https:\/\/github.com\/huyingxi\/Synonyms)\n- Synonyms \u4e2d\u6587\u8fd1\u4e49\u8bcd\u5de5\u5177\u5305\uff0c\u53ef\u4ee5\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u5f88\u591a\u4efb\u52a1\uff1a\u6587\u672c\u5bf9\u9f50\uff0c\u63a8\u8350\u7b97\u6cd5\uff0c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u8bed\u4e49\u504f\u79fb\uff0c\u5173\u952e\u5b57\u63d0\u53d6\uff0c\u6982\u5ff5\u63d0\u53d6\uff0c\u81ea\u52a8\u6458\u8981\uff0c\u641c\u7d22\u5f15\u64ce\u7b49\n\n**80\\. HarvestText\u9886\u57df\u81ea\u9002\u5e94\u6587\u672c\u6316\u6398\u5de5\u5177\uff08\u65b0\u8bcd\u53d1\u73b0-\u60c5\u611f\u5206\u6790-\u5b9e\u4f53\u94fe\u63a5\u7b49\uff09** [github](https:\/\/github.com\/blmoistawinde\/HarvestText) \n\n**81\\. word2word\uff1a(Python)\u65b9\u4fbf\u6613\u7528\u7684\u591a\u8bed\u8a00\u8bcd-\u8bcd\u5bf9\u96c6\uff1a62\u79cd\u8bed\u8a00\/3,564\u4e2a\u591a\u8bed\u8a00\u5bf9** [github](https:\/\/github.com\/Kyubyong\/word2word)\n\n**82\\. \u8bed\u97f3\u8bc6\u522b\u8bed\u6599\u751f\u6210\u5de5\u5177\uff1a\u4ece\u5177\u6709\u97f3\u9891\/\u5b57\u5e55\u7684\u5728\u7ebf\u89c6\u9891\u521b\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u8bed\u6599\u5e93** [github](https:\/\/github.com\/yc9701\/pansori)\n\n**83\\. ASR\u8bed\u97f3\u5927\u8f9e\u5178\/\u8bcd\u5178\uff1a** [github](hhttps:\/\/github.com\/aishell-foundation\/DaCiDian)\n\n**84\\. \u6784\u5efa\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u7684\u6a21\u578b\uff0c\u5305\u542b\u8bcd\u5178\u548c\u8bed\u6599\u6807\u6ce8\uff0c\u57fa\u4e8epython:** [github](https:\/\/github.com\/yixiu00001\/LSTM-CRF-medical)\n\n**85\\. \u5355\u6587\u6863\u975e\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6\uff1a** [github](https:\/\/github.com\/LIAAD\/yake)\n\n**86\\. Kashgari\u4e2d\u4f7f\u7528gpt-2\u8bed\u8a00\u6a21\u578b** [github](https:\/\/github.com\/BrikerMan\/Kashgari)\n\n**87\\.  \u5f00\u6e90\u7684\u91d1\u878d\u6295\u8d44\u6570\u636e\u63d0\u53d6\u5de5\u5177** [github](https:\/\/github.com\/PKUJohnson\/OpenData)\n\n**88\\. \u6587\u672c\u81ea\u52a8\u6458\u8981\u5e93TextTeaser: \u4ec5\u652f\u6301\u82f1\u6587** [github](https:\/\/github.com\/IndigoResearch\/textteaser)\n\n**89\\. \u4eba\u6c11\u65e5\u62a5\u8bed\u6599\u5904\u7406\u5de5\u5177\u96c6** [github](https:\/\/github.com\/howl-anderson\/tools_for_corpus_of_people_daily)\n\n**90\\. \u4e00\u4e9b\u5173\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u578b** [github](https:\/\/github.com\/lpty\/nlp_base)\n\n**91\\. \u57fa\u4e8e14W\u6b4c\u66f2\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u5c1d\u8bd5\uff0c\u529f\u80fd\u5305\u62ec\u6b4c\u8bcd\u63a5\u9f99\uff0c\u5df2\u77e5\u6b4c\u8bcd\u627e\u6b4c\u66f2\u4ee5\u53ca\u6b4c\u66f2\u6b4c\u624b\u6b4c\u8bcd\u4e09\u89d2\u5173\u7cfb\u7684\u95ee\u7b54** [github](https:\/\/github.com\/liuhuanyong\/MusicLyricChatbot)\n\n**92\\. \u57fa\u4e8eSiamese bilstm\u6a21\u578b\u7684\u76f8\u4f3c\u53e5\u5b50\u5224\u5b9a\u6a21\u578b,\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6** [github](https:\/\/github.com\/liuhuanyong\/SiameseSentenceSimilarity)\n- \u63d0\u4f9b\u4e8610\u4e07\u4e2a\u8bad\u7ec3\u6837\u672c\n\n**93\\. \u7528Transformer\u7f16\u89e3\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6839\u636eHacker News\u6587\u7ae0\u6807\u9898\u81ea\u52a8\u751f\u6210\u8bc4\u8bba** [github](https:\/\/github.com\/leod\/hncynic)\n\n**94\\. \u7528BERT\u8fdb\u884c\u5e8f\u5217\u6807\u8bb0\u548c\u6587\u672c\u5206\u7c7b\u7684\u6a21\u677f\u4ee3\u7801** [github](https:\/\/github.com\/yuanxiaosc\/BERT-for-Sequence-Labeling-and-Text-Classification)\n\n**95\\. LitBank\uff1aNLP\u6570\u636e\u96c6\u2014\u2014\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u7684100\u90e8\u5e26\u6807\u8bb0\u82f1\u6587\u5c0f\u8bf4\u8bed\u6599** [github](https:\/\/github.com\/dbamman\/litbank)\n\n**96\\. \u767e\u5ea6\u5f00\u6e90\u7684\u57fa\u51c6\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf** [github](https:\/\/github.com\/baidu\/information-extraction)\n\n**97\\. \u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6 fake news corpus** [github](https:\/\/github.com\/several27\/FakeNewsCorpus)\n\n**98\\. Facebook: LAMA\u8bed\u8a00\u6a21\u578b\u5206\u6790\uff0c\u63d0\u4f9bTransformer-XL\/BERT\/ELMo\/GPT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8bbf\u95ee\u63a5\u53e3** [github](https:\/\/github.com\/facebookresearch\/LAMA)\n- \u7528\u4e8e\u5206\u6790\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u5305\u542b\u7684\u4e8b\u5b9e\u548c\u5e38\u8bc6\u77e5\u8bc6\u7684\u63a2\u9488\u3002\n\n**99\\. CommonsenseQA\uff1a\u9762\u5411\u5e38\u8bc6\u7684\u82f1\u6587QA\u6311\u6218** [link](https:\/\/www.tau-nlp.org\/commonsenseqa)\n\n**100\\. \u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u8d44\u6599\u3001\u6570\u636e\u53ca\u5de5\u5177** [github](https:\/\/github.com\/husthuke\/awesome-knowledge-graph)\n\n**101\\. \u5404\u5927\u516c\u53f8\u5185\u90e8\u91cc\u5927\u725b\u5206\u4eab\u7684\u6280\u672f\u6587\u6863 PDF \u6216\u8005 PPT** [github](https:\/\/github.com\/0voice\/from_coder_to_expert)\n\n**102\\. \u81ea\u7136\u8bed\u8a00\u751f\u6210SQL\u8bed\u53e5\uff08\u82f1\u6587\uff09** [github](https:\/\/github.com\/paulfitz\/mlsql)\n\n**103\\. \u4e2d\u6587NLP\u6570\u636e\u589e\u5f3a\uff08EDA\uff09\u5de5\u5177** [github](https:\/\/github.com\/zhanlaoban\/eda_nlp_for_Chinese)\n- [ ] \u82f1\u6587NLP\u6570\u636e\u589e\u5f3a\u5de5\u5177 [github](https:\/\/github.com\/makcedward\/nlpaug)\n- [ ] \u4e00\u952e\u4e2d\u6587\u6570\u636e\u589e\u5f3a\u5de5\u5177 [github](https:\/\/github.com\/425776024\/nlpcda)\n\n**104\\. \u57fa\u4e8e\u533b\u836f\u77e5\u8bc6\u56fe\u8c31\u7684\u667a\u80fd\u95ee\u7b54\u7cfb\u7edf** [github](https:\/\/github.com\/YeYzheng\/KGQA-Based-On-medicine)\n\n**105\\. \u4eac\u4e1c\u5546\u54c1\u77e5\u8bc6\u56fe\u8c31** [github](https:\/\/github.com\/liuhuanyong\/ProductKnowledgeGraph)\n- \u57fa\u4e8e\u4eac\u4e1c\u7f51\u7ad9\u76841300\u79cd\u5546\u54c1\u4e0a\u4e0b\u7ea7\u6982\u5ff5\uff0c\u7ea610\u4e07\u5546\u54c1\u54c1\u724c\uff0c\u7ea665\u4e07\u54c1\u724c\u9500\u552e\u5173\u7cfb\uff0c\u5546\u54c1\u63cf\u8ff0\u7ef4\u5ea6\u7b49\u77e5\u8bc6\u5e93\uff0c\u57fa\u4e8e\u8be5\u77e5\u8bc6\u5e93\u53ef\u4ee5\u652f\u6301\u5546\u54c1\u5c5e\u6027\u5e93\u6784\u5efa\uff0c\u5546\u54c1\u9500\u552e\u95ee\u7b54\uff0c\u54c1\u724c\u7269\u54c1\u751f\u4ea7\u7b49\u77e5\u8bc6\u67e5\u8be2\u670d\u52a1\uff0c\u4e5f\u53ef\u7528\u4e8e\u60c5\u611f\u5206\u6790\u7b49\u4e0b\u6e38\u5e94\u7528\uff0e\n\n**106\\. \u57fa\u4e8emongodb\u5b58\u50a8\u7684\u519b\u4e8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u9879\u76ee** [github](https:\/\/github.com\/liuhuanyong\/QAonMilitaryKG)\n- \u57fa\u4e8emongodb\u5b58\u50a8\u7684\u519b\u4e8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u9879\u76ee\uff0c\u5305\u62ec\u98de\u884c\u5668\u3001\u592a\u7a7a\u88c5\u5907\u7b498\u5927\u7c7b\uff0c100\u4f59\u5c0f\u7c7b\uff0c\u5171\u8ba15800\u9879\u7684\u519b\u4e8b\u6b66\u5668\u77e5\u8bc6\u5e93\uff0c\u8be5\u9879\u76ee\u4e0d\u4f7f\u7528\u56fe\u6570\u636e\u5e93\u8fdb\u884c\u5b58\u50a8\uff0c\u901a\u8fc7jieba\u8fdb\u884c\u95ee\u53e5\u89e3\u6790\uff0c\u95ee\u53e5\u5b9e\u4f53\u9879\u8bc6\u522b\uff0c\u57fa\u4e8e\u67e5\u8be2\u6a21\u677f\u5b8c\u6210\u591a\u7c7b\u95ee\u9898\u7684\u67e5\u8be2\uff0c\u4e3b\u8981\u662f\u63d0\u4f9b\u4e00\u79cd\u5de5\u4e1a\u754c\u7684\u95ee\u7b54\u601d\u60f3demo\u3002\n\n**107\\. \u57fa\u4e8e\u8fdc\u76d1\u7763\u7684\u4e2d\u6587\u5173\u7cfb\u62bd\u53d6** [github](https:\/\/github.com\/xiaolalala\/Distant-Supervised-Chinese-Relation-Extraction)\n  \n**108\\. \u8bed\u97f3\u60c5\u611f\u5206\u6790** [github](https:\/\/github.com\/MITESHPUTHRANNEU\/Speech-Emotion-Analyzer)\n\n**109\\. \u4e2d\u6587ULMFiT \u60c5\u611f\u5206\u6790 \u6587\u672c\u5206\u7c7b \u8bed\u6599\u53ca\u6a21\u578b** [github](https:\/\/github.com\/bigboNed3\/chinese_ulmfit)\n\n**110\\. \u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3002\u8f93\u5165\u4e00\u5f20\u5305\u542b\u6570\u5b66\u8ba1\u7b97\u9898\u7684\u56fe\u7247\uff0c\u8f93\u51fa\u8bc6\u522b\u51fa\u7684\u6570\u5b66\u8ba1\u7b97\u5f0f\u4ee5\u53ca\u8ba1\u7b97\u7ed3\u679c** [github](https:\/\/github.com\/Roujack\/mathAI)\n\n**111\\. \u4e16\u754c\u5404\u56fd\u5927\u89c4\u6a21\u4eba\u540d\u5e93** [github](https:\/\/github.com\/philipperemy\/name-dataset)\n\n**112\\. \u4e00\u4e2a\u5229\u7528\u6709\u8da3\u4e2d\u6587\u8bed\u6599\u5e93 qingyun \u8bad\u7ec3\u51fa\u6765\u7684\u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba** [github](https:\/\/github.com\/Doragd\/Chinese-Chatbot-PyTorch-Implementation)\n- \u4f7f\u7528\u4e86\u9752\u4e91\u8bed\u659910\u4e07\u8bed\u6599\uff0c\u672crepo\u4e2d\u4e5f\u6709\u8be5\u8bed\u6599\u7684\u94fe\u63a5\n\n**113\\. \u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba\uff0c \u6839\u636e\u81ea\u5df1\u7684\u8bed\u6599\u8bad\u7ec3\u51fa\u81ea\u5df1\u60f3\u8981\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u53ef\u4ee5\u7528\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u5728\u7ebf\u95ee\u7b54\u3001\u667a\u80fd\u804a\u5929\u7b49\u573a\u666f** [github](https:\/\/github.com\/zhaoyingjun\/chatbot)\n- \u6839\u636e\u81ea\u5df1\u7684\u8bed\u6599\u8bad\u7ec3\u51fa\u81ea\u5df1\u60f3\u8981\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u53ef\u4ee5\u7528\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u5728\u7ebf\u95ee\u7b54\u3001\u667a\u80fd\u804a\u5929\u7b49\u573a\u666f\u3002\u52a0\u5165seqGAN\u7248\u672c\u3002\n- repo\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4efd\u8d28\u91cf\u4e0d\u592a\u9ad8\u7684\u8bed\u6599\n\n**114\\. \u7701\u5e02\u533a\u9547\u884c\u653f\u533a\u5212\u6570\u636e\u5e26\u62fc\u97f3\u6807\u6ce8** [github](https:\/\/github.com\/xiangyuecn\/AreaCity-JsSpider-StatsGov)\n- \u56fd\u5bb6\u7edf\u8ba1\u5c40\u4e2d\u7684\u7701\u5e02\u533a\u9547\u884c\u653f\u533a\u5212\u6570\u636e\u5e26\u62fc\u97f3\u6807\u6ce8\uff0c\u9ad8\u5fb7\u5730\u56fe\u7684\u5750\u6807\u548c\u884c\u653f\u533a\u57df\u8fb9\u754c\u8303\u56f4\uff0c\u5728\u6d4f\u89c8\u5668\u91cc\u9762\u8fd0\u884cjs\u4ee3\u7801\u91c7\u96c6\u76842019\u5e74\u53d1\u5e03\u7684\u6700\u65b0\u6570\u636e\uff0c\u542b\u91c7\u96c6\u6e90\u7801\uff0c\u63d0\u4f9bcsv\u683c\u5f0f\u6570\u636e\uff0c\u652f\u6301csv\u8f6c\u6210\u7701\u5e02\u533a\u591a\u7ea7\u8054\u52a8js\u4ee3\u7801\n- \u5750\u6807\u3001\u8fb9\u754c\u8303\u56f4\u3001\u540d\u79f0\u3001\u62fc\u97f3\u3001\u884c\u653f\u533a\u7b49\u591a\u7ea7\u5730\u5740\n\n**115\\. \u6559\u80b2\u884c\u4e1a\u65b0\u95fb \u81ea\u52a8\u6587\u6458 \u8bed\u6599\u5e93** [github](https:\/\/github.com\/wonderfulsuccess\/chinese_abstractive_corpus)\n\n\n**116\\. \u5f00\u653e\u4e86\u5bf9\u8bdd\u673a\u5668\u4eba\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u8bed\u4e49\u7406\u89e3\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u53ca\u6570\u636e** [github](https:\/\/www.ownthink.com\/#header-n30)\n- \u53e6\u4e00\u4e2aqa\u5bf9\u7684\u673a\u5668\u4eba [Amodel-for-Retrivalchatbot - \u5ba2\u670d\u673a\u5668\u4eba\uff0cChinese Retreival chatbot\uff08\u4e2d\u6587\u68c0\u7d22\u5f0f\u673a\u5668\u4eba\uff09](https:\/\/github.com\/WenRichard\/QAmodel-for-Retrievalchatbot)\n\n**117\\. \u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\uff1a\u57fa\u4e8e\u767e\u5ea6\u767e\u79d1\u4e2d\u6587\u9875\u9762\uff0c\u62bd\u53d6\u4e09\u5143\u7ec4\u4fe1\u606f\uff0c\u6784\u5efa\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31** [github](https:\/\/github.com\/lixiang0\/WEB_KG)\n\n**118\\. masr: \u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\uff0c\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u9ad8\u8bc6\u522b\u7387** [github](https:\/\/github.com\/lukhy\/masr)\n\n**119\\. Python\u97f3\u9891\u6570\u636e\u589e\u5e7f\u5e93** [github](https:\/\/github.com\/iver56\/audiomentations)\n\n**120\\. \u4e2d\u6587\u5168\u8bcd\u8986\u76d6BERT\u53ca\u4e24\u4efd\u9605\u8bfb\u7406\u89e3\u6570\u636e** [github](https:\/\/github.com\/ymcui\/Chinese-BERT-wwm)\n  - **DRCD\u6570\u636e\u96c6**\u7531\u4e2d\u56fd\u53f0\u6e7e\u53f0\u8fbe\u7814\u7a76\u9662\u53d1\u5e03\uff0c\u5176\u5f62\u5f0f\u4e0eSQuAD\u76f8\u540c\uff0c\u662f\u57fa\u4e8e\u7e41\u4f53\u4e2d\u6587\u7684\u62bd\u53d6\u5f0f\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6\u3002\n  - **CMRC 2018\u6570\u636e\u96c6**\u662f\u54c8\u5de5\u5927\u8baf\u98de\u8054\u5408\u5b9e\u9a8c\u5ba4\u53d1\u5e03\u7684\u4e2d\u6587\u673a\u5668\u9605\u8bfb\u7406\u89e3\u6570\u636e\u3002\u6839\u636e\u7ed9\u5b9a\u95ee\u9898\uff0c\u7cfb\u7edf\u9700\u8981\u4ece\u7bc7\u7ae0\u4e2d\u62bd\u53d6\u51fa\u7247\u6bb5\u4f5c\u4e3a\u7b54\u6848\uff0c\u5f62\u5f0f\u4e0eSQuAD\u76f8\u540c\u3002\n\n**121\\. ConvLab\uff1a\u5f00\u6e90\u591a\u57df\u7aef\u5230\u7aef\u5bf9\u8bdd\u7cfb\u7edf\u5e73\u53f0** [github](https:\/\/github.com\/ConvLab\/ConvLab)\n\n**122\\. \u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6** [github](https:\/\/github.com\/InsaneLife\/ChineseNLPCorpus)\n\n**123\\. \u57fa\u4e8e\u6700\u65b0\u7248\u672crasa\u642d\u5efa\u7684\u5bf9\u8bdd\u7cfb\u7edf** [github](https:\/\/github.com\/GaoQ1\/rasa_chatbot_cn)\n\n**124\\. \u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6** [github](https:\/\/github.com\/yuanxiaosc\/Entity-Relation-Extraction)\n  - Entity and Relation Extraction Based on TensorFlow and BERT. \u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6\uff0c2019\u8bed\u8a00\u4e0e\u667a\u80fd\u6280\u672f\u7ade\u8d5b\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u3002Schema based Knowledge Extraction, SKE 2019\n\n**125\\. \u4e00\u4e2a\u5c0f\u578b\u7684\u8bc1\u5238\u77e5\u8bc6\u56fe\u8c31\/\u77e5\u8bc6\u5e93** [github](https:\/\/github.com\/lemonhu\/stock-knowledge-graph)\n\n**126\\. \u590d\u76d8\u6240\u6709NLP\u6bd4\u8d5b\u7684TOP\u65b9\u6848** [github](https:\/\/github.com\/zhpmatrix\/nlp-competitions-list-review)\n \n**127\\. OpenCLaP\uff1a\u591a\u9886\u57df\u5f00\u6e90\u4e2d\u6587\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ed3\u5e93** [github](https:\/\/github.com\/thunlp\/OpenCLaP)\n\u5305\u542b\u5982\u4e0b\u8bed\u8a00\u6a21\u578b\u53ca\u767e\u5ea6\u767e\u79d1\u6570\u636e\n- \u6c11\u4e8b\u6587\u4e66BERT\tbert-base\t\u5168\u90e8\u6c11\u4e8b\u6587\u4e66\t2654\u4e07\u7bc7\u6587\u4e66\t22554\u8bcd\t370MB\t\n- \u5211\u4e8b\u6587\u4e66BERT\tbert-base\t\u5168\u90e8\u5211\u4e8b\u6587\u4e66\t663\u4e07\u7bc7\u6587\u4e66\t22554\u8bcd\t370MB\t\n- \u767e\u5ea6\u767e\u79d1BERT\tbert-base\t\u767e\u5ea6\u767e\u79d1\t903\u4e07\u7bc7\u8bcd\u6761\t22166\u8bcd\t367MB\t\n\n**128\\. UER\uff1a\u57fa\u4e8e\u4e0d\u540c\u8bed\u6599\u3001\u7f16\u7801\u5668\u3001\u76ee\u6807\u4efb\u52a1\u7684\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ed3\u5e93\uff08\u5305\u62ecBERT\u3001GPT\u3001ELMO\u7b49\uff09** [github](https:\/\/github.com\/dbiir\/UER-py) \n- \u57fa\u4e8ePyTorch\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6846\u67b6\uff0c\u652f\u6301\u5bf9\u7f16\u7801\u5668\uff0c\u76ee\u6807\u4efb\u52a1\u7b49\u8fdb\u884c\u4efb\u610f\u7684\u7ec4\u5408\uff0c\u4ece\u800c\u590d\u73b0\u5df2\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6216\u5728\u5df2\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002\u57fa\u4e8eUER\u8bad\u7ec3\u4e86\u4e0d\u540c\u6027\u8d28\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u4e0d\u540c\u8bed\u6599\u3001\u7f16\u7801\u5668\u3001\u76ee\u6807\u4efb\u52a1\uff09\uff0c\u6784\u6210\u4e86\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ed3\u5e93\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u573a\u666f\u3002\n\n**129\\. \u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5411\u91cf\u5408\u96c6** [github](https:\/\/github.com\/liuhuanyong\/ChineseEmbedding)\n- \u5305\u62ec\u5b57\u5411\u91cf,\u62fc\u97f3\u5411\u91cf,\u8bcd\u5411\u91cf,\u8bcd\u6027\u5411\u91cf,\u4f9d\u5b58\u5173\u7cfb\u5411\u91cf.\u51715\u79cd\u7c7b\u578b\u7684\u5411\u91cf\n\n**130\\. \u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba** [github](https:\/\/github.com\/charlesXu86\/Chatbot_CN)\n- \u5176\u4e2d\u7684\u4e3b\u8981\u6a21\u5757\u6709\u4fe1\u606f\u62bd\u53d6\u3001NLU\u3001NLG\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\uff0c\u5e76\u4e14\u5229\u7528Django\u6574\u5408\u4e86\u524d\u7aef\u5c55\u793a,\u76ee\u524d\u5df2\u7ecf\u5c01\u88c5\u4e86nlp\u548ckg\u7684restful\u63a5\u53e3\n\n**131\\. g2pC\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6c49\u8bed\u8bfb\u97f3\u81ea\u52a8\u6807\u8bb0\u6a21\u5757** [github](https:\/\/github.com\/Kyubyong\/g2pC)\n\n**132\\. Zincbase \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5de5\u5177\u5305** [github](https:\/\/github.com\/tomgrek\/zincbase)\n\n**133\\. \u8bd7\u6b4c\u8d28\u91cf\u8bc4\u4ef7\/\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bd7\u6b4c\u8bed\u6599\u5e93** [github](https:\/\/github.com\/THUNLP-AIPoet\/Datasets)\n\n**134\\. \u5feb\u901f\u8f6c\u5316\u300c\u4e2d\u6587\u6570\u5b57\u300d\u548c\u300c\u963f\u62c9\u4f2f\u6570\u5b57\u300d** [github](https:\/\/github.com\/HaveTwoBrush\/cn2an)\n- \u4e2d\u6587\u3001\u963f\u62c9\u4f2f\u6570\u5b57\u4e92\u8f6c\n- \u4e2d\u6587\u4e0e\u963f\u62c9\u4f2f\u6570\u5b57\u6df7\u5408\u7684\u60c5\u51b5\uff0c\u5728\u5f00\u53d1\u4e2d\n\n**135\\. \u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u8bed\u6599\u5e93** [github](https:\/\/github.com\/liuhuanyong\/MiningZhiDaoQACorpus)\n- \u8d85\u8fc7580\u4e07\u7684\u95ee\u9898\uff0c938\u4e07\u7684\u7b54\u6848\uff0c5800\u4e2a\u5206\u7c7b\u6807\u7b7e\u3002\u57fa\u4e8e\u8be5\u95ee\u7b54\u8bed\u6599\u5e93\uff0c\u53ef\u652f\u6301\u591a\u79cd\u5e94\u7528\uff0c\u5982\u95f2\u804a\u95ee\u7b54\uff0c\u903b\u8f91\u6316\u6398\n\n**136\\. \u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf** [github](https:\/\/github.com\/WenRichard\/KBQA-BERT)\n- BERT\u505a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u53e5\u5b50\u76f8\u4f3c\u5ea6\uff0c\u5206\u4e3aonline\u548coutline\u6a21\u5f0f\n\n**137\\. jieba_fast \u52a0\u901f\u7248\u7684jieba** [github](https:\/\/github.com\/deepcs233\/jieba_fast)\n- \u4f7f\u7528cpython\u91cd\u5199\u4e86jieba\u5206\u8bcd\u5e93\u4e2d\u8ba1\u7b97DAG\u548cHMM\u4e2d\u7684vitrebi\u51fd\u6570\uff0c\u901f\u5ea6\u5f97\u5230\u5927\u5e45\u63d0\u5347\n\n**138\\. \u6b63\u5219\u8868\u8fbe\u5f0f\u6559\u7a0b** [github](https:\/\/github.com\/ziishaned\/learn-regex\/blob\/master\/translations\/README-cn.md)\n\n**139\\. \u4e2d\u6587\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6** [github](https:\/\/github.com\/ymcui\/Chinese-RC-Datasets)\n\n**140\\. \u57fa\u4e8eBERT\u7b49\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u53d6\u5f0f\u6458\u8981\u63d0\u53d6** [github](https:\/\/github.com\/Hellisotherpeople\/CX_DB8)\n\n**141\\. Python\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6587\u672c\u6458\u8981\u7684\u7efc\u5408\u6307\u5357** [link](https:\/\/mp.weixin.qq.com\/s\/gDZyTbM1nw3fbEnU--y3nQ)\n\n**142\\. \u77e5\u8bc6\u56fe\u8c31\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u8d44\u6599\u6574\u7406** [github](https:\/\/github.com\/lihanghang\/Knowledge-Graph)\n- \u6df1\u5ea6\u5b66\u4e60\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u5bf9\u8bdd\u7cfb\u7edf\u3002\u5305\u62ec\u77e5\u8bc6\u83b7\u53d6\u3001\u77e5\u8bc6\u5e93\u6784\u5efa\u3001\u77e5\u8bc6\u5e93\u5e94\u7528\u4e09\u5927\u6280\u672f\u7814\u7a76\u4e0e\u5e94\u7528\n\n**143\\. \u7ef4\u57fa\u5927\u89c4\u6a21\u5e73\u884c\u6587\u672c\u8bed\u6599** [github](https:\/\/github.com\/facebookresearch\/LASER\/tree\/master\/tasks\/WikiMatrix)\n- 85\u79cd\u8bed\u8a00\u30011620\u79cd\u8bed\u8a00\u5bf9\u3001135M\u5bf9\u7167\u53e5\n\n**144\\. StanfordNLP 0.2.0\uff1a\u7eafPython\u7248\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5305** [link](https:\/\/stanfordnlp.github.io\/stanfordnlp\/)\n\n**145\\. NeuralNLP-NeuralClassifier\uff1a\u817e\u8baf\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5206\u7c7b\u5de5\u5177** [github](https:\/\/github.com\/Tencent\/NeuralNLP-NeuralClassifier)\n\n**146\\. \u7aef\u5230\u7aef\u7684\u5c01\u95ed\u57df\u5bf9\u8bdd\u7cfb\u7edf** [github](https:\/\/github.com\/cdqa-suite\/cdQA)\n\n**147\\. \u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff1aNeuroNER vs. BertNER** [github](https:\/\/github.com\/EOA-AILab\/NER-Chinese)\n\n**148\\. \u65b0\u95fb\u4e8b\u4ef6\u7ebf\u7d22\u62bd\u53d6** [github](https:\/\/github.com\/liuhuanyong\/ImportantEventExtractor)\n- An exploration for Eventline (important news Rank organized by pulic time)\uff0c\u9488\u5bf9\u67d0\u4e00\u4e8b\u4ef6\u8bdd\u9898\u4e0b\u7684\u65b0\u95fb\u62a5\u9053\u96c6\u5408\uff0c\u901a\u8fc7\u4f7f\u7528docrank\u7b97\u6cd5\uff0c\u5bf9\u65b0\u95fb\u62a5\u9053\u8fdb\u884c\u91cd\u8981\u6027\u8bc6\u522b\uff0c\u5e76\u901a\u8fc7\u65b0\u95fb\u62a5\u9053\u65f6\u95f4\u6311\u9009\u51fa\u65f6\u95f4\u7ebf\u4e0a\u91cd\u8981\u65b0\u95fb\n\n**149\\. 2019\u5e74\u767e\u5ea6\u7684\u4e09\u5143\u7ec4\u62bd\u53d6\u6bd4\u8d5b\uff0c\u201c\u79d1\u5b66\u7a7a\u95f4\u961f\u201d\u6e90\u7801(\u7b2c7\u540d)** [github](https:\/\/github.com\/bojone\/kg-2019)\n\n**150\\. \u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u7684\u5f00\u653e\u57df\u6587\u672c\u77e5\u8bc6\u4e09\u5143\u7ec4\u62bd\u53d6\u548c\u77e5\u8bc6\u5e93\u6784\u5efa** [github](https:\/\/github.com\/lemonhu\/open-entity-relation-extraction)\n\n**151\\. \u4e2d\u6587\u7684GPT2\u8bad\u7ec3\u4ee3\u7801** [github](https:\/\/github.com\/Morizeyao\/GPT2-Chinese)\n\n**152\\. ML-NLP - \u673a\u5668\u5b66\u4e60(Machine Learning)\u3001NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0** [github](https:\/\/github.com\/NLP-LOVE\/ML-NLP)\n\n**153\\. nlp4han:\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5\/\u5206\u8bcd\/\u8bcd\u6027\u6807\u6ce8\/\u7ec4\u5757\/\u53e5\u6cd5\u5206\u6790\/\u8bed\u4e49\u5206\u6790\/NER\/N\u5143\u8bed\u6cd5\/HMM\/\u4ee3\u8bcd\u6d88\u89e3\/\u60c5\u611f\u5206\u6790\/\u62fc\u5199\u68c0\u67e5** [github](https:\/\/github.com\/kidden\/nlp4han)\n\n**154\\. XLM\uff1aFacebook\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b** [github](https:\/\/github.com\/facebookresearch\/XLM)\n\n**155\\. \u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u767e\u5ea6\u767e\u79d1\u4eba\u7269\u8bcd\u6761\u5c5e\u6027\u62bd\u53d6** [github](https:\/\/github.com\/sakuranew\/BERT-AttributeExtraction)\n\n**156\\. \u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u76f8\u5173\u7684\u5f00\u653e\u4efb\u52a1\uff0c\u6570\u636e\u96c6, \u4ee5\u53ca\u5f53\u524d\u6700\u4f73\u7ed3\u679c** [github](https:\/\/github.com\/didi\/ChineseNLP)\n\n**157\\. CoupletAI - \u57fa\u4e8eCNN+Bi-LSTM+Attention \u7684\u81ea\u52a8\u5bf9\u5bf9\u8054\u7cfb\u7edf** [github](https:\/\/github.com\/WiseDoge\/CoupletAI)\n\n**158\\. \u62bd\u8c61\u77e5\u8bc6\u56fe\u8c31\uff0c\u76ee\u524d\u89c4\u6a2150\u4e07\uff0c\u652f\u6301\u540d\u8bcd\u6027\u5b9e\u4f53\u3001\u72b6\u6001\u6027\u63cf\u8ff0\u3001\u4e8b\u4ef6\u6027\u52a8\u4f5c\u8fdb\u884c\u62bd\u8c61** [github](https:\/\/github.com\/liuhuanyong\/AbstractKnowledgeGraph)\n\n**159\\. MiningZhiDaoQACorpus - 580\u4e07\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u6570\u636e\u6316\u6398\u9879\u76ee** [github](\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u8bed\u6599\u5e93\uff0c\u5305\u62ec\u8d85\u8fc7580\u4e07\u7684\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u5e26\u6709\u95ee\u9898\u6807\u7b7e\u3002\u57fa\u4e8e\u8be5\u95ee\u7b54\u8bed\u6599\u5e93\uff0c\u53ef\u652f\u6301\u591a\u79cd\u5e94\u7528\uff0c\u5982\u903b\u8f91\u6316\u6398)\n\n**160\\. brat rapid annotation tool: \u5e8f\u5217\u6807\u6ce8\u5de5\u5177** [link](http:\/\/brat.nlplab.org\/index.html)\n\n**161\\. \u5927\u89c4\u6a21\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\uff1a\uff1a1.4\u4ebf\u5b9e\u4f53** [github](https:\/\/github.com\/ownthink\/KnowledgeGraphData)\n\n**162\\. \u6570\u636e\u589e\u5f3a\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u4ed6nlp\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53ca\u6548\u679c** [link](https:\/\/mp.weixin.qq.com\/s\/_aVwSWuYho_7MUT0LuFgVA)\n\n**163\\. allennlp\u9605\u8bfb\u7406\u89e3:\u652f\u6301\u591a\u79cd\u6570\u636e\u548c\u6a21\u578b** [github](https:\/\/github.com\/allenai\/allennlp-reading-comprehension)\n\n**164\\. PDF\u8868\u683c\u6570\u636e\u63d0\u53d6\u5de5\u5177** [github](https:\/\/github.com\/camelot-dev\/camelot)\n\n**165\\. Graphbrain\uff1aAI\u5f00\u6e90\u8f6f\u4ef6\u5e93\u548c\u79d1\u7814\u5de5\u5177\uff0c\u76ee\u7684\u662f\u4fc3\u8fdb\u81ea\u52a8\u610f\u4e49\u63d0\u53d6\u548c\u6587\u672c\u7406\u89e3\u4ee5\u53ca\u77e5\u8bc6\u7684\u63a2\u7d22\u548c\u63a8\u65ad** [github](https:\/\/github.com\/graphbrain\/graphbrain)\n\n**166\\. \u7b80\u5386\u81ea\u52a8\u7b5b\u9009\u7cfb\u7edf** [github](https:\/\/github.com\/JAIJANYANI\/Automated-Resume-Screening-System)\n\n**167\\. \u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u7b80\u5386\u81ea\u52a8\u6458\u8981** [github](https:\/\/github.com\/DataTurks-Engg\/Entity-Recognition-In-Resumes-SpaCy)\n\n**168\\. \u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6&\u57fa\u51c6\u6a21\u578b&\u8bed\u6599\u5e93&\u6392\u884c\u699c** [github](https:\/\/github.com\/brightmart\/ChineseGLUE)\n\n**169\\. \u6811\u6d1e OCR \u6587\u5b57\u8bc6\u522b** [github](https:\/\/github.com\/AnyListen\/tools-ocr)\n- \u4e00\u4e2ac++ OCR [github](https:\/\/github.com\/myhub\/tr)\n- \n**170\\. \u4ece\u5305\u542b\u8868\u683c\u7684\u626b\u63cf\u56fe\u7247\u4e2d\u8bc6\u522b\u8868\u683c\u548c\u6587\u5b57** [github](https:\/\/github.com\/bitdata\/ocrtable)\n\n**171\\. \u8bed\u58f0\u8fc1\u79fb** [github](https:\/\/github.com\/fighting41love\/become-yukarin)\n\n**172\\. Python\u53e3\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u82f1\u6587)** [github](https:\/\/github.com\/gooofy\/py-nltools)\n\n**173\\. similarity\uff1a\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5de5\u5177\u5305\uff0cjava\u7f16\u5199** [github](https:\/\/github.com\/shibing624\/similarity)\n- \u7528\u4e8e\u8bcd\u8bed\u3001\u77ed\u8bed\u3001\u53e5\u5b50\u3001\u8bcd\u6cd5\u5206\u6790\u3001\u60c5\u611f\u5206\u6790\u3001\u8bed\u4e49\u5206\u6790\u7b49\u76f8\u5173\u7684\u76f8\u4f3c\u5ea6\u8ba1\u7b97\n\n**174\\. \u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b** [github](https:\/\/github.com\/brightmart\/albert_zh)\n\n**175\\. Transformers 2.0** [github](https:\/\/github.com\/huggingface\/transformers)\n- \u652f\u6301TensorFlow 2.0 \u548c PyTorch \u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet\u2026) 8\u79cd\u67b6\u6784\/33\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\/102\u79cd\u8bed\u8a00\n\n**176\\. \u57fa\u4e8e\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6Audioset\u7684\u97f3\u9891\u589e\u5f3a** [github](https:\/\/github.com\/AppleHolic\/audioset_augmentor)\n\n\n**177\\. Poplar\uff1a\u7f51\u9875\u7248\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u5de5\u5177** [github](https:\/\/github.com\/synyi\/poplar)\n\n**178\\. \u56fe\u7247\u6587\u5b57\u53bb\u9664\uff0c\u53ef\u7528\u4e8e\u6f2b\u753b\u7ffb\u8bd1** [github](https:\/\/github.com\/yu45020\/Text_Segmentation_Image_Inpainting)\n\n**179\\. 186\u79cd\u8bed\u8a00\u7684\u6570\u5b57\u53eb\u6cd5\u5e93** [github](https:\/\/github.com\/google\/UniNum)\n\n**180\\. Amazon\u53d1\u5e03\u57fa\u4e8e\u77e5\u8bc6\u7684\u4eba-\u4eba\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u6570\u636e\u96c6** [github](https:\/\/github.com\/alexa\/alexa-prize-topical-chat-dataset\/)\n\n**181\\. \u4e2d\u6587\u6587\u672c\u7ea0\u9519\u6a21\u5757\u4ee3\u7801** [github](https:\/\/github.com\/zedom1\/error-detection)\n\n**182\\. \u7e41\u7b80\u4f53\u8f6c\u6362** [github](https:\/\/github.com\/berniey\/hanziconv)\n\n**183\\. Python\u5b9e\u73b0\u7684\u591a\u79cd\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u4ef7\u6307\u6807** [github](https:\/\/github.com\/cdimascio\/py-readability-metrics)\n\n**184\\. \u7c7b\u4f3c\u4e8e\u4eba\u540d\/\u5730\u540d\/\u7ec4\u7ec7\u673a\u6784\u540d\u7684\u547d\u540d\u4f53\u8bc6\u522b\u6570\u636e\u96c6** [github](https:\/\/github.com\/LG-1\/video_music_book_datasets)\n\n**185\\. \u4e1c\u5357\u5927\u5b66\u300a\u77e5\u8bc6\u56fe\u8c31\u300b\u7814\u7a76\u751f\u8bfe\u7a0b(\u8d44\u6599)** [github](https:\/\/github.com\/npubird\/KnowledgeGraphCourse)\n\n**186\\. \u82f1\u6587\u62fc\u5199\u68c0\u67e5\u5e93** [github](https:\/\/github.com\/barrust\/pyspellchecker)\n\n```\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\n\n# find those words that may be misspelled\nmisspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n\nfor word in misspelled:\n    # Get the one `most likely` answer\n    print(spell.correction(word))\n\n    # Get a list of `likely` options\n    print(spell.candidates(word))\n```\n**187\\. wwsearch\u662f\u4f01\u4e1a\u5fae\u4fe1\u540e\u53f0\u81ea\u7814\u7684\u5168\u6587\u68c0\u7d22\u5f15\u64ce** [github](https:\/\/github.com\/Tencent\/wwsearch)\n\n**188\\. CHAMELEON\uff1a\u6df1\u5ea6\u5b66\u4e60\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u5143\u67b6\u6784** [github](https:\/\/github.com\/gabrielspmoreira\/chameleon_recsys)\n\n**189\\. 8\u7bc7\u8bba\u6587\u68b3\u7406BERT\u76f8\u5173\u6a21\u578b\u8fdb\u5c55\u4e0e\u53cd\u601d** [github](https:\/\/www.msra.cn\/zh-cn\/news\/features\/bert)\n\n**190\\. DocSearch\uff1a\u514d\u8d39\u6587\u6863\u641c\u7d22\u5f15\u64ce** [github](https:\/\/github.com\/algolia\/docsearch)\n\n**191\\. LIDA\uff1a\u8f7b\u91cf\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u6807\u6ce8\u5de5\u5177** [github](https:\/\/github.com\/Wluper\/lida)\n\n**192\\. aili - the fastest in-memory index in the East \u4e1c\u534a\u7403\u6700\u5feb\u5e76\u53d1\u7d22\u5f15** [github](https:\/\/github.com\/UncP\/aili)\n\n\n**193\\. \u77e5\u8bc6\u56fe\u8c31\u8f66\u97f3\u5de5\u4f5c\u9879\u76ee** [github](https:\/\/github.com\/qiu997018209\/KnowledgeGraph)\n\n**194\\. \u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d44\u6e90\u5927\u5168** [github](https:\/\/github.com\/tokenmill\/awesome-nlg)\n- \u5185\u542b\u82f1\u6587\u6570\u636e\u3001\u8bba\u6587\u3001\u4ee3\u7801\n  \n**195\\. \u4e2d\u65e5\u97e9\u5206\u8bcd\u5e93mecab\u7684Python\u63a5\u53e3\u5e93** [github](https:\/\/github.com\/jeongukjae\/python-mecab)\n\n**196\\. \u4e2d\u6587\u6587\u672c\u6458\u8981\/\u5173\u952e\u8bcd\u63d0\u53d6** [github](https:\/\/github.com\/letiantian\/TextRank4ZH)\n\n**197\\. \u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5668 (featurizer)\uff0c\u63d0\u53d6\u6c49\u5b57\u7684\u7279\u5f81\uff08\u53d1\u97f3\u7279\u5f81\u3001\u5b57\u5f62\u7279\u5f81\uff09\u7528\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81** [github](https:\/\/github.com\/howl-anderson\/hanzi_char_featurizer)\n\n**198\\. \u4e2d\u6587\u751f\u6210\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4** [github](https:\/\/github.com\/CLUEbenchmark\/CLGE)\n\n**199\\. \u4e2d\u6587\u7f29\u5199\u6570\u636e\u96c6** [github](https:\/\/github.com\/zhangyics\/Chinese-abbreviation-dataset)\n\n**200\\. \u4e2d\u6587\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 - \u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6-\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b-\u8bed\u6599\u5e93-baseline-\u5de5\u5177\u5305-\u6392\u884c\u699c** [github](https:\/\/github.com\/CLUEbenchmark\/CLUE)\n\n**201\\. PySS3\uff1a\u9762\u5411\u53ef\u89e3\u91caAI\u7684SS3\u6587\u672c\u5206\u7c7b\u5668\u673a\u5668\u53ef\u89c6\u5316\u5de5\u5177** [github](https:\/\/github.com\/sergioburdisso\/pyss3)\n\n**202\\. \u4e2d\u6587NLP\u6570\u636e\u96c6\u5217\u8868** [github](https:\/\/github.com\/OYE93\/Chinese-NLP-Corpus)\n\n**203\\. COPE - \u683c\u5f8b\u8bd7\u7f16\u8f91\u7a0b\u5e8f** [github](https:\/\/github.com\/LingDong-\/cope)\n\n**204\\. doccano\uff1a\u57fa\u4e8e\u7f51\u9875\u7684\u5f00\u6e90\u534f\u540c\u591a\u8bed\u8a00\u6587\u672c\u6807\u6ce8\u5de5\u5177** [github](https:\/\/github.com\/doccano\/doccano)\n\n**205\\. PreNLP\uff1a\u81ea\u7136\u8bed\u8a00\u9884\u5904\u7406\u5e93** [github](https:\/\/github.com\/lyeoni\/prenlp)\n\n**206\\. \u7b80\u5355\u7684\u7b80\u5386\u89e3\u6790\u5668\uff0c\u7528\u6765\u4ece\u7b80\u5386\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f** [github](https:\/\/github.com\/OmkarPathak\/pyresparser)\n\n**207\\. \u7528\u4e8e\u4e2d\u6587\u95f2\u804a\u7684GPT2\u6a21\u578b\uff1aGPT2-chitchat** [github](https:\/\/github.com\/yangjianxin1\/GPT2-chitchat)\n\n**208\\. \u57fa\u4e8e\u68c0\u7d22\u804a\u5929\u673a\u5668\u4eba\u591a\u8f6e\u54cd\u5e94\u9009\u62e9\u76f8\u5173\u8d44\u6e90\u5217\u8868(Leaderboards\u3001Datasets\u3001Papers)** [github](https:\/\/github.com\/JasonForJoy\/Leaderboards-for-Multi-Turn-Response-Selection)\n\n**209\\. (Colab)\u62bd\u8c61\u6587\u672c\u6458\u8981\u5b9e\u73b0\u96c6\u9526(\u6559\u7a0b** [github](https:\/\/github.com\/theamrzaki\/text_summurization_abstractive_methods)\n\n**210\\. \u8bcd\u8bed\u62fc\u97f3\u6570\u636e** [github](https:\/\/github.com\/mozillazg\/phrase-pinyin-data)\n\n**211\\. \u9ad8\u6548\u6a21\u7cca\u641c\u7d22\u5de5\u5177** [github](https:\/\/github.com\/Yggdroot\/LeaderF)\n\n**212\\. NLP\u6570\u636e\u589e\u5e7f\u8d44\u6e90\u96c6** [github](https:\/\/github.com\/quincyliang\/nlp-data-augmentation)\n\n**213\\. \u5fae\u8f6f\u5bf9\u8bdd\u673a\u5668\u4eba\u6846\u67b6** [github](https:\/\/github.com\/microsoft\/botframework)\n\n**214\\. GitHub Typo Corpus\uff1a\u5927\u89c4\u6a21GitHub\u591a\u8bed\u8a00\u62fc\u5199\u9519\u8bef\/\u8bed\u6cd5\u9519\u8bef\u6570\u636e\u96c6** [github](https:\/\/github.com\/mhagiwara\/github-typo-corpus)\n\n**215\\. TextCluster\uff1a\u77ed\u6587\u672c\u805a\u7c7b\u9884\u5904\u7406\u6a21\u5757 Short text cluster** [github](https:\/\/github.com\/RandyPen\/TextCluster)\n\n**216\\. \u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\u6587\u672c\u89c4\u8303\u5316** [github](https:\/\/github.com\/speech-io\/chinese_text_normalization)\n\n**217\\. BLINK\uff1a\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u5e93** [github](https:\/\/github.com\/facebookresearch\/BLINK)\n\n**218\\. BertPunc\uff1a\u57fa\u4e8eBERT\u7684\u6700\u5148\u8fdb\u6807\u70b9\u4fee\u590d\u6a21\u578b** [github](https:\/\/github.com\/nkrnrnk\/BertPunc)\n\n**219\\. Tokenizer\uff1a\u5feb\u901f\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8bcd\u6761\u5316\u5e93** [github](https:\/\/github.com\/OpenNMT\/Tokenizer)\n\n**220\\. \u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b\u3001\u8bed\u6599\u5e93\u3001\u6392\u884c\u699c** [github](https:\/\/github.com\/CLUEbenchmark\/CLUE)\n\n**221\\. spaCy \u533b\u5b66\u6587\u672c\u6316\u6398\u4e0e\u4fe1\u606f\u63d0\u53d6** [github](https:\/\/github.com\/NLPatVCU\/medaCy)\n\n**222\\. NLP\u4efb\u52a1\u793a\u4f8b\u9879\u76ee\u4ee3\u7801\u96c6** [github](https:\/\/github.com\/explosion\/projects)\n\n**223\\. python\u62fc\u5199\u68c0\u67e5\u5e93** [github](https:\/\/github.com\/barrust\/pyspellchecker)\n\n**224\\. chatbot-list - \u884c\u4e1a\u5185\u5173\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u804a\u5929\u673a\u5668\u4eba\u7684\u5e94\u7528\u548c\u67b6\u6784\u3001\u7b97\u6cd5\u5206\u4eab\u548c\u4ecb\u7ecd** [github](https:\/\/github.com\/lizhe2004\/chatbot-list)\n\n**225\\. \u8bed\u97f3\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807(MOSNet, BSSEval, STOI, PESQ, SRMR)** [github](https:\/\/github.com\/aliutkus\/speechmetrics)\n\n**226\\. \u7528138GB\u8bed\u6599\u8bad\u7ec3\u7684\u6cd5\u6587RoBERTa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b** [link](https:\/\/camembert-model.fr\/)\n\n**227\\. BERT-NER-Pytorch\uff1a\u4e09\u79cd\u4e0d\u540c\u6a21\u5f0f\u7684BERT\u4e2d\u6587NER\u5b9e\u9a8c** [github](https:\/\/github.com\/lonePatient\/BERT-NER-Pytorch)\n\n**228\\. \u65e0\u9053\u8bcd\u5178 - \u6709\u9053\u8bcd\u5178\u7684\u547d\u4ee4\u884c\u7248\u672c\uff0c\u652f\u6301\u82f1\u6c49\u4e92\u67e5\u548c\u5728\u7ebf\u67e5\u8be2** [github](https:\/\/github.com\/ChestnutHeng\/Wudao-dict)\n\n**229\\. 2019\u5e74NLP\u4eae\u70b9\u56de\u987e** [download](https:\/\/pan.baidu.com\/s\/1h5gEPUhvY1HkUVc32eeX4w)\n- \u63d0\u53d6\u7801: yb6x \n\n**230\\. Chinese medical dialogue data \u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6** [github](https:\/\/github.com\/Toyhom\/Chinese-medical-dialogue-data)\n\n**231\\. \u6700\u597d\u7684\u6c49\u5b57\u6570\u5b57(\u4e2d\u6587\u6570\u5b57)-\u963f\u62c9\u4f2f\u6570\u5b57\u8f6c\u6362\u5de5\u5177** [github](https:\/\/github.com\/Wall-ee\/chinese2digits)\n\n**232\\. \u57fa\u4e8e\u767e\u79d1\u77e5\u8bc6\u5e93\u7684\u4e2d\u6587\u8bcd\u8bed\u591a\u8bcd\u4e49\/\u4e49\u9879\u83b7\u53d6\u4e0e\u7279\u5b9a\u53e5\u5b50\u8bcd\u8bed\u8bed\u4e49\u6d88\u6b67** [github](https:\/\/github.com\/liuhuanyong\/WordMultiSenseDisambiguation)\n\n**233\\. awesome-nlp-sentiment-analysis - \u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u539f\u56e0\u8bc6\u522b\u3001\u8bc4\u4ef7\u5bf9\u8c61\u548c\u8bc4\u4ef7\u8bcd\u62bd\u53d6** [github](https:\/\/github.com\/haiker2011\/awesome-nlp-sentiment-analysis)\n\n**234\\. LineFlow\uff1a\u9762\u5411\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684NLP\u6570\u636e\u9ad8\u6548\u52a0\u8f7d\u5668** [github](https:\/\/github.com\/tofunlp\/lineflow)\n\n**235\\. \u4e2d\u6587\u533b\u5b66NLP\u516c\u5f00\u8d44\u6e90\u6574\u7406** [github](https:\/\/github.com\/GanjinZero\/awesome_Chinese_medical_NLP)\n\n**236\\. MedQuAD\uff1a(\u82f1\u6587)\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6** [github](https:\/\/github.com\/abachaa\/MedQuAD)\n\n**237\\. \u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b57\u4e32\u89e3\u6790\u8f6c\u6362\u4e3a\u6574\u6570\u548c\u6d6e\u70b9\u6570** [github](https:\/\/github.com\/jaidevd\/numerizer)\n\n**238\\. Transfer Learning in Natural Language Processing (NLP)** [youtube](https:\/\/www.youtube.com\/watch?v=ly0TRNr7I_M)\n\n**239\\. \u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178** [github](https:\/\/github.com\/speech-io\/BigCiDian)\n\n**240\\. Tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668** [github](https:\/\/github.com\/huggingface\/tokenizers)\n\n**241\\. CLUENER \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b Fine Grained Named Entity Recognition** [github](https:\/\/github.com\/CLUEbenchmark\/CLUENER2020)\n\n**242\\. \u57fa\u4e8eBERT\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b** [github](https:\/\/github.com\/lonePatient\/BERT-NER-Pytorch)\n\n**243\\. \u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93** [github](https:\/\/github.com\/thunlp\/Chinese_Rumor_Dataset)\n\n**244\\. NLP\u6570\u636e\u96c6\/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868** [github](https:\/\/quantumstat.com\/dataset\/dataset.html)\n\n- \u5927\u591a\u6570\u4e3a\u82f1\u6587\u6570\u636e\n\n**245\\. nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801, \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(Word Embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u3001\u6587\u672c\u5206\u7c7b(Text Classificatin)\u3001\u6587\u672c\u751f\u6210(Text Generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(Text Similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow** [github](https:\/\/github.com\/msgi\/nlp-journey)\n\n**246\\. Python\u6587\u672c\u6316\u6398\/NLP\u5b9e\u6218\u793a\u4f8b** [github](https:\/\/github.com\/kavgan\/nlp-in-practice)\n\n**247\\. Blackstone\uff1a\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spaCy pipeline\u548cNLP\u6a21\u578b** [github](https:\/\/github.com\/ICLRandD\/Blackstone)\n\n**248\\. \u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d** [github](https:\/\/github.com\/paubric\/python-sirajnet)\n\n**249\\. \u4e2d\u6587 \u9884\u8bad\u7ec3 ELECTREA \u6a21\u578b: \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain Chinese Model** [github](https:\/\/github.com\/CLUEbenchmark\/ELECTRA)\n\n\n**250\\. albert-chinese-ner - \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578bALBERT\u505a\u4e2d\u6587NER** [github](https:\/\/github.com\/ProHiryu\/albert-chinese-ner)\n\n**251\\. \u57fa\u4e8eGPT2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210\/\u6587\u672c\u589e\u5e7f** [github](https:\/\/github.com\/prakhar21\/TextAugmentation-GPT2)\n\n**252\\. \u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6** [github](https:\/\/github.com\/ZhuiyiTechnology\/pretrained-models)\n\n**253\\. \u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305** [github](https:\/\/github.com\/yannvgn\/laserembeddings)\n\n**254\\. \u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\uff1a\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5** [github](https:\/\/github.com\/yannvgn\/laserembeddings)\n\n**255\\. \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868** [github](https:\/\/github.com\/zacanger\/profane-words)\n\n**256\\. attnvis\uff1aGPT2\u3001BERT\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316** [github](https:\/\/github.com\/SIDN-IAP\/attnvis)\n\n**257\\. CoVoST\uff1aFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587** [github](https:\/\/github.com\/facebookresearch\/covost)\n\n**258\\. Jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177 - \u4ee5BiLSTM\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd** [github](https:\/\/github.com\/ownthink\/Jiagu)\n\n**259\\. \u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa** [github](https:\/\/github.com\/chineseocr\/table-ocr)\n\n**260\\. NLP\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868** [github](https:\/\/github.com\/BaptisteBlouin\/EventExtractionPapers)\n\n**261\\. \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868** [github](https:\/\/github.com\/icoxfog417\/awesome-financial-nlp)\n\n**262\\. CLUEDatasetSearch - \u4e2d\u82f1\u6587NLP\u6570\u636e\u96c6\uff1a\u641c\u7d22\u6240\u6709\u4e2d\u6587NLP\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587NLP\u6570\u636e\u96c6** [github](https:\/\/github.com\/CLUEbenchmark\/CLUEDatasetSearch)\n\n**263\\. medical_NER - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b** [github](https:\/\/github.com\/pumpkinduo\/KnowledgeGraph_NER)\n\n**264\\. (\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66** [pdf](https:\/\/cdn1.sph.harvard.edu\/wp-content\/uploads\/sites\/1268\/2019\/10\/ci_hernanrobins_23oct19.pdf)\n\n**265\\. \u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599\/\u6570\u636e\u96c6\/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868** [github](https:\/\/github.com\/totogo\/awesome-knowledge-graph)\n\n**266\\. Forte\uff1a\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6** [github](https:\/\/github.com\/asyml\/forte)\n\n**267\\. Python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93** [github](https:\/\/github.com\/luozhouyang\/python-string-similarity)\n\n**268\\. PyLaia\uff1a\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305** [github](https:\/\/github.com\/jpuigcerver\/PyLaia)\n\n**269\\. TextFooler\uff1a\u9488\u5bf9\u6587\u672c\u5206\u7c7b\/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757** [github](https:\/\/github.com\/jind11\/TextFooler)\n\n**270\\. Haystack\uff1a\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(QA)\u6846\u67b6** [github](https:\/\/github.com\/deepset-ai\/haystack)\n\n**271\\. \u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177** [github](https:\/\/github.com\/dongrixinyu\/chinese_keyphrase_extractor)\n\n**272\\. pdf\u6587\u6863\u89e3\u6790\u76f8\u5173\u5de5\u5177\u5305**\n- pdf\u751f\u6210\n  - [fdfgen](https:\/\/github.com\/ccnmtl\/fdfgen): \u80fd\u591f\u81ea\u52a8\u521b\u5efapdf\u6587\u6863\uff0c\u5e76\u586b\u5199\u4fe1\u606f\n- pdf\u8868\u683c\u89e3\u6790\n  - [pdftabextract](https:\/\/github.com\/WZBSocialScienceCenter\/pdftabextract): \u7528\u4e8eOCR\u8bc6\u522b\u540e\u7684\u8868\u683c\u4fe1\u606f\u89e3\u6790\uff0c\u5f88\u5f3a\u5927\n  - [tabula-py](https:\/\/github.com\/chezou\/tabula-py): \u76f4\u63a5\u5c06pdf\u4e2d\u7684\u8868\u683c\u4fe1\u606f\u8f6c\u6362\u4e3apandas\u7684dataframe\uff0c\u6709java\u548cpython\u4e24\u79cd\u7248\u672c\u4ee3\u7801\n  - [pdfx](https:\/\/github.com\/metachris\/pdfx): \u81ea\u52a8\u62bd\u53d6\u51fa\u5f15\u7528\u53c2\u8003\u6587\u732e\uff0c\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684pdf\u6587\u4ef6\n  - [invoice2data](https:\/\/github.com\/invoice-x\/invoice2data): \u53d1\u7968pdf\u4fe1\u606f\u62bd\u53d6\n  - [camelot](https:\/\/github.com\/atlanhq\/camelot): pdf\u8868\u683c\u89e3\u6790\n  - [pdfplumber](https:\/\/github.com\/jsvine\/pdfplumber): pdf\u8868\u683c\u89e3\u6790\n  - [pdf\u6587\u6863\u4fe1\u606f\u62bd\u53d6](https:\/\/github.com\/jstockwin\/py-pdf-parser)\n- pdf\u8bed\u4e49\u5206\u5272\n  - [PubLayNet](https:\/\/go.ctolib.com\/ibm-aur-nlp-PubLayNet.html):\u80fd\u591f\u5212\u5206\u6bb5\u843d\u3001\u8bc6\u522b\u8868\u683c\u3001\u56fe\u7247\n- pdf\u8bfb\u53d6\u5de5\u5177\n  - [PDFMiner](https:\/\/github.com\/euske\/pdfminer)\uff1aPDFMiner\u80fd\u83b7\u53d6\u9875\u9762\u4e2d\u6587\u672c\u7684\u51c6\u786e\u4f4d\u7f6e\uff0c\u4ee5\u53ca\u5b57\u4f53\u6216\u884c\u7b49\u5176\u4ed6\u4fe1\u606f\u3002\u5b83\u8fd8\u6709\u4e00\u4e2aPDF\u8f6c\u6362\u5668\uff0c\u53ef\u4ee5\u5c06PDF\u6587\u4ef6\u8f6c\u6362\u6210\u5176\u4ed6\u6587\u672c\u683c\u5f0f(\u5982HTML)\u3002\u8fd8\u6709\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u6790\u5668PDF\uff0c\u53ef\u4ee5\u7528\u4e8e\u6587\u672c\u5206\u6790\u4ee5\u5916\u7684\u5176\u4ed6\u7528\u9014\u3002\n  - [PyPDF2](https:\/\/github.com\/mstamy2\/PyPDF2)\uff1aPyPDF 2\u662f\u4e00\u4e2apython PDF\u5e93\uff0c\u80fd\u591f\u5206\u5272\u3001\u5408\u5e76\u3001\u88c1\u526a\u548c\u8f6c\u6362PDF\u6587\u4ef6\u7684\u9875\u9762\u3002\u5b83\u8fd8\u53ef\u4ee5\u5411PDF\u6587\u4ef6\u4e2d\u6dfb\u52a0\u81ea\u5b9a\u4e49\u6570\u636e\u3001\u67e5\u770b\u9009\u9879\u548c\u5bc6\u7801\u3002\u5b83\u53ef\u4ee5\u4ecePDF\u68c0\u7d22\u6587\u672c\u548c\u5143\u6570\u636e\uff0c\u8fd8\u53ef\u4ee5\u5c06\u6574\u4e2a\u6587\u4ef6\u5408\u5e76\u5728\u4e00\u8d77\u3002\n  - [ReportLab](https:\/\/www.reportlab.com\/opensource\/)\uff1aReportLab\u80fd\u5feb\u901f\u521b\u5efaPDF \u6587\u6863\u3002\u7ecf\u8fc7\u65f6\u95f4\u8bc1\u660e\u7684\u3001\u8d85\u597d\u7528\u7684\u5f00\u6e90\u9879\u76ee\uff0c\u7528\u4e8e\u521b\u5efa\u590d\u6742\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684PDF\u6587\u6863\u548c\u81ea\u5b9a\u4e49\u77e2\u91cf\u56fe\u5f62\u3002\u5b83\u662f\u514d\u8d39\u7684\uff0c\u5f00\u6e90\u7684\uff0c\u7528Python\u7f16\u5199\u7684\u3002\u8be5\u8f6f\u4ef6\u5305\u6bcf\u6708\u4e0b\u8f7d5\u4e07\u591a\u6b21\uff0c\u662f\u6807\u51c6Linux\u53d1\u884c\u7248\u7684\u4e00\u90e8\u5206\uff0c\u5d4c\u5165\u5230\u8bb8\u591a\u4ea7\u54c1\u4e2d\uff0c\u5e76\u88ab\u9009\u4e2d\u4e3aWikipedia\u7684\u6253\u5370\/\u5bfc\u51fa\u529f\u80fd\u63d0\u4f9b\u52a8\u529b\u3002\n  - \n  \n**273\\. \u4e2d\u6587\u8bcd\u8bed\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5** [gihtub](https:\/\/github.com\/yaleimeng\/Final_word_Similarity)\n- \u7efc\u5408\u4e86\u540c\u4e49\u8bcd\u8bcd\u6797\u6269\u5c55\u7248\u4e0e\u77e5\u7f51\uff08Hownet\uff09\u7684\u8bcd\u8bed\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\uff0c\u8bcd\u6c47\u8986\u76d6\u66f4\u591a\u3001\u7ed3\u679c\u66f4\u51c6\u786e\u3002\n\n**274\\. \u4eba\u6c11\u65e5\u62a5\u8bed\u6599\u5e93\u5904\u7406\u5de5\u5177\u96c6** [github](https:\/\/github.com\/howl-anderson\/tools_for_corpus_of_people_daily)\n\n**275\\. stanza:\u65af\u5766\u798f\u56e2\u961fNLP\u5de5\u5177** [github](https:\/\/github.com\/stanfordnlp\/stanza)\n  - \u53ef\u5904\u7406\u516d\u5341\u591a\u79cd\u8bed\u8a00\n\n**276\\. \u4e00\u4e2a\u5927\u89c4\u6a21\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6** [github](https:\/\/github.com\/UCSD-AI4H\/Medical-Dialogue-System)\n- \u5305\u542b110\u4e07\u533b\u5b66\u54a8\u8be2\uff0c400\u4e07\u6761\u533b\u60a3\u5bf9\u8bdd\n\n**277\\. \u65b0\u51a0\u80ba\u708e\u76f8\u5173\u6570\u636e** \n- \u65b0\u51a0\u53ca\u5176\u4ed6\u7c7b\u578b\u80ba\u708e\u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6 [github](https:\/\/github.com\/UCSD-AI4H\/COVID-Dialogue)\n- \u6e05\u534e\u5927\u5b66\u7b49\u673a\u6784\u7684\u5f00\u653e\u6570\u636e\u6e90\uff08COVID-19\uff09[github](https:\/\/www.aminer.cn\/data-covid19\/)\n\n**278\\. DGL-KE \u56fe\u5d4c\u5165\u8868\u793a\u5b66\u4e60\u7b97\u6cd5** [github](https:\/\/github.com\/awslabs\/dgl-ke)\n\n**279\\. nlp-recipes\uff1a\u5fae\u8f6f\u51fa\u54c1--\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6700\u4f73\u5b9e\u8df5\u548c\u8303\u4f8b** [github](https:\/\/github.com\/microsoft\/nlp-recipes)\n\n**280\\. chinese_keyphrase_extractor (CKPE) - A tool for chinese keyphrase extraction \u4e00\u4e2a\u5feb\u901f\u4ece\u81ea\u7136\u8bed\u8a00\u6587\u672c\u4e2d\u63d0\u53d6\u548c\u8bc6\u522b\u5173\u952e\u77ed\u8bed\u7684\u5de5\u5177** [github](https:\/\/github.com\/dongrixinyu\/chinese_keyphrase_extractor)\n\n**281\\. \u4f7f\u7528GAN\u751f\u6210\u8868\u683c\u6570\u636e\uff08\u4ec5\u652f\u6301\u82f1\u6587\uff09** [github](https:\/\/github.com\/Diyago\/GAN-for-tabular-data)\n\n**282\\. Google\u53d1\u5e03Taskmaster-2\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u5bf9\u8bdd\u6570\u636e\u96c6** [github](https:\/\/github.com\/google-research-datasets\/Taskmaster\/tree\/master\/TM-2-2020)\n \n**283\\. BDCI2019\u91d1\u878d\u8d1f\u9762\u4fe1\u606f\u5224\u5b9a** [github](https:\/\/github.com\/A-Rain\/BDCI2019-Negative_Finance_Info_Judge)\n\n**284\\. \u7528\u795e\u7ecf\u7f51\u7edc\u7b26\u53f7\u63a8\u7406\u6c42\u89e3\u590d\u6742\u6570\u5b66\u65b9\u7a0b** [github](https:\/\/ai.facebook.com\/blog\/using-neural-networks-to-solve-advanced-mathematics-equations\/)\n\n**285\\. \u7ca4\u8bed\/\u82f1\u8bed\u4f1a\u8bdd\u53cc\u8bed\u8bed\u6599\u5e93** [github](https:\/\/github.com\/khiajohnson\/SpiCE-Corpus)\n\n**286\\. \u4e2d\u6587ELECTRA\u9884\u8bad\u7ec3\u6a21\u578b** [github](https:\/\/github.com\/ymcui\/Chinese-ELECTRA)\n\n**287\\. \u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4eba\u5458\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5b9e\u4f8b\u6559\u7a0b** [github](https:\/\/github.com\/graykode\/nlp-tutorial)\n\n**288\\. Parakeet\uff1a\u57fa\u4e8ePaddlePaddle\u7684\u6587\u672c-\u8bed\u97f3\u5408\u6210** [github](https:\/\/github.com\/PaddlePaddle\/Parakeet)\n\n**289\\. 103976\u4e2a\u82f1\u8bed\u5355\u8bcd\u5e93\uff08sql\u7248\uff0ccsv\u7248\uff0cExcel\u7248\uff09\u5305** [github](https:\/\/github.com\/1eez\/103976)\n\n**290\\. \u300a\u6d77\u8d3c\u738b\u300b\u77e5\u8bc6\u56fe\u8c31** [github](https:\/\/github.com\/mrbulb\/ONEPIECE-KG)\n\n**291\\. \u6cd5\u52a1\u667a\u80fd\u6587\u732e\u8d44\u6e90\u5217\u8868** [github](https:\/\/github.com\/thunlp\/LegalPapers)\n\n**292\\. Datasaur.ai \u5728\u7ebf\u6570\u636e\u6807\u6ce8\u5de5\u4f5c\u6d41\u7ba1\u7406\u5de5\u5177** [link](https:\/\/datasaur.ai)\n\n**293\\. (Java)\u51c6\u786e\u7684\u8bed\u97f3\u81ea\u7136\u8bed\u8a00\u68c0\u6d4b\u5e93** [github](https:\/\/github.com\/pemistahl\/lingua)\n\n**294\\. \u9762\u5411\u5404\u8bed\u79cd\/\u4efb\u52a1\u7684BERT\u6a21\u578b\u5927\u5217\u8868\/\u641c\u7d22\u5f15\u64ce** [link](https:\/\/bertlang.unibocconi.it)\n\n**295\\. CoVoST\uff1aFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93** [github](https:\/\/github.com\/facebookresearch\/covost)\n\n**296\\. \u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e2d\u6587\u5173\u952e\u8bcd\u62bd\u53d6\u65b9\u6cd5** [github](https:\/\/github.com\/sunyilgdx\/SIFRank_zh)\n\n**297\\. Fancy-NLP:\u7528\u4e8e\u5efa\u8bbe\u5546\u54c1\u753b\u50cf\u7684\u6587\u672c\u77e5\u8bc6\u6316\u6398\u5de5\u5177** [github](https:\/\/github.com\/boat-group\/fancy-nlp)\n\n**298\\. \u57fa\u4e8e\u767e\u5ea6webqa\u4e0edureader\u6570\u636e\u96c6\u8bad\u7ec3\u7684Albert Large QA\u6a21\u578b** [github](https:\/\/github.com\/wptoux\/albert-chinese-large-webqa\/tree\/master)\n\n**299\\. BERT\/CRF\u5b9e\u73b0\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b** [github](https:\/\/github.com\/Louis-udm\/NER-BERT-CRF)\n\n**300\\. ssc, Sound Shape Code, \u97f3\u5f62\u7801 - \u57fa\u4e8e\u201c\u97f3\u5f62\u7801\u201d\u7684\u4e2d\u6587\u5b57\u7b26\u4e32\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5** \n- [version 1](https:\/\/github.com\/qingyujean\/ssc)\n- [version 2](https:\/\/github.com\/wenyangchou\/SimilarCharactor)\n- [blog\/introduction](https:\/\/blog.csdn.net\/chndata\/article\/details\/41114771)\n\n**301\\. \u4e2d\u6587\u6307\u4ee3\u6d88\u89e3\u6570\u636e** [github](https:\/\/github.com\/CLUEbenchmark\/CLUEWSC2020)\n- [baidu ink](https:\/\/pan.baidu.com\/s\/1gKP_Mj-7KVfFWpjYvSvAAA)  code: a0qq\n\n**302\\. \u5168\u9762\u7b80\u4fbf\u7684\u4e2d\u6587 NLP \u5de5\u5177\u5305** [github](https:\/\/github.com\/dongrixinyu\/JioNLP)\n\n**303\\. \u4e2d\u6587\u5730\u5740\u5206\u8bcd\uff08\u5730\u5740\u5143\u7d20\u8bc6\u522b\u4e0e\u62bd\u53d6\uff09\uff0c\u901a\u8fc7\u5e8f\u5217\u6807\u6ce8\u8fdb\u884cNER** [github](https:\/\/github.com\/yihenglu\/chinese-address-segment)\n\n**304\\. \u7528Transformers(BERT, XLNet, Bart, Electra, Roberta, XLM-Roberta)\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd(\u6a21\u578b\u6bd4\u8f83)** [github](https:\/\/github.com\/renatoviolin\/next_word_prediction)\n\n**305\\. \u6587\u672c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6700\u5148\u8fdb\u89e3\u91ca\u5668\u5e93** [github](https:\/\/github.com\/interpretml\/interpret-text)\n\n**306\\. \u591a\u6587\u6863\u6458\u8981\u6570\u636e\u96c6** [github](https:\/\/github.com\/complementizer\/wcep-mds-dataset)\n\n**307\\. \u7528\u8bb0\u4e8b\u672c\u6e32\u67d33D\u56fe\u50cf** [github](https:\/\/github.com\/khalladay\/render-with-notepad)\n\n**308\\. char_featurizer - \u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5de5\u5177** [github](https:\/\/github.com\/charlesXu86\/char_featurizer)\n\n**309\\. SimBERT - \u57fa\u4e8eUniLM\u601d\u60f3\u3001\u878d\u68c0\u7d22\u4e0e\u751f\u6210\u4e8e\u4e00\u4f53\u7684BERT\u6a21\u578b** [github](https:\/\/github.com\/ZhuiyiTechnology\/simbert)\n\n**310\\. Python\u97f3\u9891\u7279\u5f81\u63d0\u53d6\u5305** [github](https:\/\/github.com\/novoic\/surfboard)\n\n**311\\. TensorFlow 2 \u5b9e\u73b0\u7684\u6587\u672c\u8bed\u97f3\u5408\u6210** [github](https:\/\/github.com\/as-ideas\/TransformerTTS)\n\n**312\\. \u60c5\u611f\u5206\u6790\u6280\u672f\uff1a\u8ba9\u667a\u80fd\u5ba2\u670d\u66f4\u61c2\u4eba\u7c7b\u60c5\u611f** [github](https:\/\/developer.aliyun.com\/article\/761513?utm_content=g_1000124809)\n\n**313\\. TensorFlow Hub\u6700\u65b0\u53d1\u5e0340+\u79cd\u8bed\u8a00\u7684\u65b0\u8bed\u8a00\u6a21\u578b(\u5305\u62ec\u4e2d\u6587)** [link](https:\/\/tfhub.dev\/google\/collections\/wiki40b-lm\/1)\n\n**314\\. \u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5668 (featurizer)\uff0c\u63d0\u53d6\u6c49\u5b57\u7684\u7279\u5f81\uff08\u53d1\u97f3\u7279\u5f81\u3001\u5b57\u5f62\u7279\u5f81\uff09\u7528\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81** [github](https:\/\/github.com\/howl-anderson\/hanzi_char_featurizer)\n\n**315\\. \u5de5\u4e1a\u754c\u5e38\u7528\u57fa\u4e8eDSSM\u5411\u91cf\u5316\u53ec\u56depipeline\u590d\u73b0** [github](https:\/\/github.com\/wangzhegeek\/DSSM-Lookalike)\n\n**316\\. \u4e0d\u5b58\u5728\u7684\u8bcd\uff1a\u7528GPT-2\u53d8\u4f53\u4ece\u5934\u751f\u6210\u65b0\u8bcd\u53ca\u5176\u5b9a\u4e49\u3001\u4f8b\u53e5** [github](https:\/\/github.com\/turtlesoupy\/this-word-does-not-exist)\n\n**317\\. TextAttack\uff1a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u5bf9\u6297\u6027\u653b\u51fb\u6846\u67b6** [github](https:\/\/github.com\/QData\/TextAttack)\n\n**318\\. \u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u8fdb\u5c55** [link](https:\/\/ai.facebook.com\/blog\/ai-advances-to-better-detect-hate-speech)\n\n**319\\. OPUS-100\uff1a\u4ee5\u82f1\u6587\u4e3a\u4e2d\u5fc3\u7684\u591a\u8bed(100\u79cd)\u5e73\u884c\u8bed\u6599** [github](https:\/\/github.com\/EdinburghNLP\/opus-100-corpus)\n\n**320\\. \u4ece\u8bba\u6587\u4e2d\u63d0\u53d6\u8868\u683c\u6570\u636e** [github](https:\/\/github.com\/paperswithcode\/axcell)\n\n**321\\. \u8ba9\u4eba\u4eba\u90fd\u53d8\u5f97\u201c\u5f6c\u5f6c\u6709\u793c\u201d\uff1a\u793c\u8c8c\u8fc1\u79fb\u4efb\u52a1\u2014\u2014\u5728\u4fdd\u7559\u610f\u4e49\u7684\u540c\u65f6\u5c06\u975e\u793c\u8c8c\u8bed\u53e5\u8f6c\u6362\u4e3a\u793c\u8c8c\u8bed\u53e5\uff0c\u63d0\u4f9b\u5305\u542b1.39M + \u5b9e\u4f8b\u7684\u6570\u636e\u96c6** [paper and code](https:\/\/arxiv.org\/abs\/2004.14257)\n\n**322\\. \u7528BERT\u5728\u8868\u683c\u4e2d\u5bfb\u627e\u7b54\u6848** [github](https:\/\/github.com\/google-research\/tapas)\n\n**323\\. PyTorch\u5b9e\u73b0\u7684BERT\u4e8b\u4ef6\u62bd\u53d6(ACE 2005 corpus)** [github](https:\/\/github.com\/nlpcl-lab\/bert-event-extraction)\n\n**324\\. \u8868\u683c\u95ee\u7b54\u7684\u7cfb\u5217\u6587\u7ae0**\n- [\u7b80\u4ecb](https:\/\/mp.weixin.qq.com\/s?__biz=MzAxMDk0OTI3Ng==&mid=2247484103&idx=2&sn=4a5b50557ab9178270866d812bcfc87f&chksm=9b49c534ac3e4c22de7c53ae5d986fac60a7641c0c072d4038d9d4efd6beb24a22df9f859d08&scene=21#wechat_redirect)\n- [\u6a21\u578b](https:\/\/mp.weixin.qq.com\/s?__biz=MzAxMDk0OTI3Ng==&mid=2247484103&idx=1&sn=73f37fbc1dbd5fdc2d4ad54f58693ef3&chksm=9b49c534ac3e4c222f6a320674b3728cf8567b9a16e6d66b8fdcf06703b05a16a9c9ed9d79a3&scene=21#wechat_redirect)\n- [\u5b8c\u7ed3\u7bc7](https:\/\/mp.weixin.qq.com\/s\/ee1DG_vO2qblqFC6zO97pA)\n\n**325\\. LibKGE\uff1a\u9762\u5411\u53ef\u590d\u73b0\u7814\u7a76\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u5e93** [github](https:\/\/github.com\/uma-pi1\/kge)\n\n**326\\. comparxiv :\u7528\u4e8e\u6bd4\u8f83arXiv\u4e0a\u4e24\u63d0\u4ea4\u7248\u672c\u5dee\u5f02\u7684\u547d\u4ee4** [pypi](https:\/\/pypi.org\/project\/comparxiv\/)\n\n**327\\. ViSQOL\uff1a\u97f3\u9891\u8d28\u91cf\u611f\u77e5\u5ba2\u89c2\u3001\u5b8c\u6574\u53c2\u8003\u6307\u6807\uff0c\u5206\u97f3\u9891\u3001\u8bed\u97f3\u4e24\u79cd\u6a21\u5f0f** [github](https:\/\/github.com\/google\/visqol)\n\n**328\\. \u65b9\u9762\u60c5\u611f\u5206\u6790\u5305** [github](https:\/\/github.com\/ScalaConsultants\/Aspect-Based-Sentiment-Analysis)\n\n**329\\. dstlr\uff1a\u975e\u7ed3\u6784\u5316\u6587\u672c\u53ef\u6269\u5c55\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5e73\u53f0** [github](https:\/\/github.com\/dstlry\/dstlr)\n\n**330\\. \u7531\u6587\u672c\u81ea\u52a8\u751f\u6210\u591a\u9879\u9009\u62e9\u9898** [github](https:\/\/github.com\/KristiyanVachev\/Question-Generation)\n\n**331\\. \u5927\u89c4\u6a21\u8de8\u9886\u57df\u4e2d\u6587\u4efb\u52a1\u5bfc\u5411\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u53ca\u6a21\u578bCrossWOZ** [paper & data](https:\/\/arxiv.org\/pdf\/2002.11893.pdf)\n\n**332\\. whatlies\uff1a\u8bcd\u5411\u91cf\u4ea4\u4e92\u53ef\u89c6\u5316** [spacy\n\u5de5\u5177](https:\/\/spacy.io\/universe\/project\/whatlies)\n\n**333\\. \u652f\u6301\u6279\u5e76\u884c\u7684LatticeLSTM\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b** [github](https:\/\/github.com\/LeeSureman\/Batch_Parallel_LatticeLSTM)\n\n**334\\. \u57fa\u4e8eAlbert\u3001Electra\uff0c\u7528\u7ef4\u57fa\u767e\u79d1\u6587\u672c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7684\u95ee\u7b54\u5f15\u64ce** [github](https:\/\/github.com\/renatoviolin\/Question-Answering-Albert-Electra)\n\n**335\\. Deepmatch\uff1a\u9488\u5bf9\u63a8\u8350\u3001\u5e7f\u544a\u548c\u641c\u7d22\u7684\u6df1\u5ea6\u5339\u914d\u6a21\u578b\u5e93** [github](https:\/\/github.com\/shenweichen\/DeepMatch)\n\n**336\\. \u8bed\u97f3\u5de5\u5177\u5408\u96c6**\n  - zhrtvc \u597d\u7528\u7684\u4e2d\u6587\u8bed\u97f3\u514b\u9686\u517c\u4e2d\u6587\u8bed\u97f3\u5408\u6210\u7cfb\u7edf [github](https:\/\/github.com\/KuangDD\/zhrtvc)\n  - aukit \u597d\u7528\u7684\u8bed\u97f3\u5904\u7406\u5de5\u5177\u7bb1\uff0c\u5305\u542b\u8bed\u97f3\u964d\u566a\u3001\u97f3\u9891\u683c\u5f0f\u8f6c\u6362\u3001\u7279\u5f81\u9891\u8c31\u751f\u6210\u7b49\u6a21\u5757 [github](https:\/\/github.com\/KuangDD\/aukit)\n  - phkit \u597d\u7528\u7684\u97f3\u7d20\u5904\u7406\u5de5\u5177\u7bb1\uff0c\u5305\u542b\u4e2d\u6587\u97f3\u7d20\u3001\u82f1\u6587\u97f3\u7d20\u3001\u6587\u672c\u8f6c\u62fc\u97f3\u3001\u6587\u672c\u6b63\u5219\u5316\u7b49\u6a21\u5757 [github](https:\/\/github.com\/KuangDD\/phkit)\n  - zhvoice \u4e2d\u6587\u8bed\u97f3\u8bed\u6599\uff0c\u8bed\u97f3\u66f4\u52a0\u6e05\u6670\u81ea\u7136\uff0c\u5305\u542b8\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\uff0c3200\u4e2a\u8bf4\u8bdd\u4eba\uff0c900\u5c0f\u65f6\u8bed\u97f3\uff0c1300\u4e07\u5b57 [github](https:\/\/github.com\/KuangDD\/zhvoice)\n\n**337\\. \u591a\u97f3\u5b57\u8bcd\u5178\u6570\u636e\u53ca\u4ee3\u7801** [github](https:\/\/github.com\/mozillazg\/phrase-pinyin-data)\n\n**338\\. audio\uff1a\u9762\u5411\u8bed\u97f3\u884c\u4e3a\u68c0\u6d4b\u3001\u4e8c\u503c\u5316\u3001\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u60c5\u611f\u8bc6\u522b\u7b49\u4efb\u52a1\u7684\u97f3\u9891\u6807\u6ce8\u5de5\u5177** [github](https:\/\/github.com\/midas-research\/audino)\n\n**339\\. \u5927\u89c4\u6a21\u3001\u7ed3\u6784\u5316\u3001\u4e2d\u82f1\u6587\u53cc\u8bed\u7684\u65b0\u51a0\u77e5\u8bc6\u56fe\u8c31(COKG-19)** [link](http:\/\/openkg.cn\/dataset\/39801d1b-0b51-4cde-a06c-62def5a70563)\n  - COKG-19\u5305\u542b\u4e86505\u4e2a\u6982\u5ff5\u3001393\u4e2a\u5c5e\u6027\u300126282\u4e2a\u5b9e\u4f8b\u548c32352\u4e2a\u77e5\u8bc6\u4e09\u5143\u7ec4\uff0c\u8986\u76d6\u4e86\u533b\u7597\u3001\u5065\u5eb7\u3001\u7269\u8d44\u3001\u9632\u63a7\u3001\u79d1\u7814\u548c\u4eba\u7269\u7b49\n\n**340\\. 132\u4e2a\u77e5\u8bc6\u56fe\u8c31\u7684\u6570\u636e\u96c6** [link](http:\/\/openkg.cn\/dataset)\n  - \u6db5\u76d6\u5e38\u8bc6\u3001\u57ce\u5e02\u3001\u91d1\u878d\u3001\u519c\u4e1a\u3001\u5730\u7406\u3001\u6c14\u8c61\u3001\u793e\u4ea4\u3001\u7269\u8054\u7f51\u3001\u533b\u7597\u3001\u5a31\u4e50\u3001\u751f\u6d3b\u3001\u5546\u4e1a\u3001\u51fa\u884c\u3001\u79d1\u6559\n\n**341\\. 42GB\u7684JD\u5ba2\u670d\u5bf9\u8bdd\u6570\u636e(CSDD)** [github](https:\/\/github.com\/jd-aig\/nlp_baai\/tree\/master\/pretrained_models_and_embeddings)\n  - 12\u4ebf\u53e5\u5b50\u8bad\u7ec3\u5f97\u5230\u7684word embedding\n\n**342\\. \u5408\u6210\u6570\u636e\u751f\u6210\u57fa\u51c6** [github](https:\/\/github.com\/sdv-dev\/SDGym)\n\n**343\\. \u6c49\u5b57\u3001\u8bcd\u8bed\u3001\u6210\u8bed\u67e5\u8be2\u63a5\u53e3** [github](https:\/\/github.com\/netnr\/zidian\/tree\/206028e5ce9a608afc583820df8dc2d1d4b61781)\n\n**344\\. \u4e2d\u6587\u95ee\u9898\u53e5\u5b50\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6bd4\u8d5b\u53ca\u65b9\u6848\u6c47\u603b** [github](https:\/\/github.com\/ShuaichiLi\/Chinese-sentence-similarity-task)\n\n**345\\. Texthero\uff1a\u6587\u672c\u6570\u636e\u9ad8\u6548\u5904\u7406\u5305\uff0c\u5305\u62ec\u9884\u5904\u7406\u3001\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5411\u91cf\u7a7a\u95f4\u5206\u6790\u3001\u6587\u672c\u53ef\u89c6\u5316\u7b49** [github](https:\/\/github.com\/jbesomi\/texthero)\n\n**346\\. SIMPdf\uff1aPython\u5199\u7684\u7b80\u5355PDF\u6587\u4ef6\u6587\u5b57\u7f16\u8f91\u5668** [github](https:\/\/github.com\/shashanoid\/Simpdf)\n\n**347\\. \u300a\u914d\u8272\u8f9e\u5178\u300b\u6570\u636e\u96c6** [github](https:\/\/github.com\/mattdesl\/dictionary-of-colour-combinations)\n\n**348\\. carefree-learn\uff1a(PyTorch)\u8868\u683c\u6570\u636e\u96c6\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60(AutoML)\u5305** [github](https:\/\/github.com\/carefree0910\/carefree-learn)\n\n**349\\. token2index\uff1a\u4e0ePyTorch\/Tensorflow\u517c\u5bb9\u7684\u5f3a\u5927\u8f7b\u91cf\u8bcd\u6761\u7d22\u5f15\u5e93** [github](https:\/\/github.com\/Kaleidophon\/token2index)\n\n**350\\. \u5f00\u6e90\u5bf9\u8bdd\u5f0f\u4fe1\u606f\u641c\u7d22\u5e73\u53f0** [github](https:\/\/github.com\/microsoft\/macaw)\n\n**351\\. \u5bf9\u8054\u6570\u636e** [github](https:\/\/github.com\/wb14123\/couplet-dataset)\n-  700,000 couplets, \u8d85\u8fc770\u4e07\u5bf9\u5bf9\u8054\n-  \u767e\u5ea6\u4e91\u76d8\uff1a[\u94fe\u63a5](https:\/\/pan.baidu.com\/s\/1BBXBsoIbkyI5eBRUjnpcTw)  \u5bc6\u7801:egpt\n\n**352\\. \u57fa\u4e8ePytorch\u7684Bert\u5e94\u7528\uff0c\u5305\u62ec\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u60c5\u611f\u5206\u6790\u3001\u6587\u672c\u5206\u7c7b\u4ee5\u53ca\u6587\u672c\u76f8\u4f3c\u5ea6\u7b49** [github](https:\/\/github.com\/rsanshierli\/EasyBert)\n\n**353\\. TaBERT\uff1a\u7406\u89e3\u8868\u683c\u6570\u636e\u67e5\u8be2\u7684\u65b0\u6a21\u578b** [paper](https:\/\/scontent-hkt1-1.xx.fbcdn.net\/v\/t39.8562-6\/106708899_597765107810230_1899215558892880563_n.pdf?_nc_cat=107&_nc_sid=ae5e01&_nc_ohc=4sN3TJwewSIAX8iliBD&_nc_ht=scontent-hkt1-1.xx&oh=eccb9795f027ff63be61ff4a5e337c02&oe=5F316505)\n\n**354\\. Dakshina\u6570\u636e\u96c6\uff1a\u5341\u4e8c\u79cd\u5357\u4e9a\u8bed\u8a00\u7684\u62c9\u4e01\/\u672c\u5730\u6587\u5b57\u5e73\u884c\u6570\u636e\u96c6\u5408** [github](https:\/\/github.com\/google-research-datasets\/dakshina)\n\n**355\\. NLP\u6807\u6ce8\u5e73\u53f0\u7efc\u8ff0** [github](https:\/\/github.com\/alvations\/annotate-questionnaire)\n\n**356\\. \u5c01\u95ed\u57df\u5fae\u8c03\u8868\u683c\u68c0\u6d4b** [github](https:\/\/github.com\/holms-ur\/fine-tuning)\n\n**357\\. \u6df1\u5ea6\u5b66\u4e60\u60c5\u611f\u6587\u672c\u8bed\u97f3\u5408\u6210** [github](https:\/\/github.com\/Emotional-Text-to-Speech\/dl-for-emo-tts)\n\n**358\\. \u4e2d\u6587\u5199\u4f5c\u6821\u5bf9\u5de5\u5177** [github](https:\/\/xiezuocat.com\/#\/)\n\n**359\\. \u7528Quora\u95ee\u9898\u5bf9\u8bad\u7ec3\u7684T5\u95ee\u9898\u610f\u8bd1(Paraphrase)** [github](https:\/\/github.com\/renatoviolin\/T5-paraphrase-generation)\n\n**360\\. \u60c5\u5883\u4e92\u52a8\u591a\u6a21\u6001\u5bf9\u8bdd\u6311\u62182020(DSTC9 2020)** [github](https:\/\/github.com\/facebookresearch\/simmc)\n\n**361\\. nlpgnn\uff1a\u56fe\u795e\u7ecf\u7f51\u7edc\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u7bb1** [github](https:\/\/github.com\/kyzhouhzau\/NLPGNN)\n\n**362\\. Macadam\uff1a\u4ee5Tensorflow(Keras)\u548cbert4keras\u4e3a\u57fa\u7840\uff0c\u4e13\u6ce8\u4e8e\u6587\u672c\u5206\u7c7b\u3001\u5e8f\u5217\u6807\u6ce8\u548c\u5173\u7cfb\u62bd\u53d6\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u5305** [github](https:\/\/github.com\/yongzhuo\/Macadam)\n\n**363\\. \u7528\u65b0\u7248nlp\u5e93\u52a0\u8f7d17GB+\u82f1\u6587\u7ef4\u57fa\u8bed\u6599\u53ea\u5360\u75289MB\u5185\u5b58\u904d\u5386\u901f\u5ea62-3 Gbit\/s** [github](https:\/\/gist.github.com\/thomwolf\/13ca2b2b172b2d17ac66685aa2eeba62)\n\n\n","104":"# Machine Learning From Scratch\n\n## About\nPython implementations of some of the fundamental Machine Learning models and algorithms from scratch.\n\nThe purpose of this project is not to produce as optimized and computationally efficient algorithms as possible\nbut rather to present the inner workings of them in a transparent and accessible way.\n\n## Table of Contents\n- [Machine Learning From Scratch](#machine-learning-from-scratch)\n  * [About](#about)\n  * [Table of Contents](#table-of-contents)\n  * [Installation](#installation)\n  * [Examples](#examples)\n    + [Polynomial Regression](#polynomial-regression)\n    + [Classification With CNN](#classification-with-cnn)\n    + [Density-Based Clustering](#density-based-clustering)\n    + [Generating Handwritten Digits](#generating-handwritten-digits)\n    + [Deep Reinforcement Learning](#deep-reinforcement-learning)\n    + [Image Reconstruction With RBM](#image-reconstruction-with-rbm)\n    + [Evolutionary Evolved Neural Network](#evolutionary-evolved-neural-network)\n    + [Genetic Algorithm](#genetic-algorithm)\n    + [Association Analysis](#association-analysis)\n  * [Implementations](#implementations)\n    + [Supervised Learning](#supervised-learning)\n    + [Unsupervised Learning](#unsupervised-learning)\n    + [Reinforcement Learning](#reinforcement-learning)\n    + [Deep Learning](#deep-learning)\n  * [Contact](#contact)\n\n## Installation\n    $ git clone https:\/\/github.com\/eriklindernoren\/ML-From-Scratch\n    $ cd ML-From-Scratch\n    $ python setup.py install\n\n## Examples\n### Polynomial Regression\n    $ python mlfromscratch\/examples\/polynomial_regression.py\n\n<p align=\"center\">\n    <img src=\"http:\/\/eriklindernoren.se\/images\/p_reg.gif\" width=\"640\"\\>\n<\/p>\n<p align=\"center\">\n    Figure: Training progress of a regularized polynomial regression model fitting <br>\n    temperature data measured in Link\u00f6ping, Sweden 2016.\n<\/p>\n\n### Classification With CNN\n    $ python mlfromscratch\/examples\/convolutional_neural_network.py\n\n    +---------+\n    | ConvNet |\n    +---------+\n    Input Shape: (1, 8, 8)\n    +----------------------+------------+--------------+\n    | Layer Type           | Parameters | Output Shape |\n    +----------------------+------------+--------------+\n    | Conv2D               | 160        | (16, 8, 8)   |\n    | Activation (ReLU)    | 0          | (16, 8, 8)   |\n    | Dropout              | 0          | (16, 8, 8)   |\n    | BatchNormalization   | 2048       | (16, 8, 8)   |\n    | Conv2D               | 4640       | (32, 8, 8)   |\n    | Activation (ReLU)    | 0          | (32, 8, 8)   |\n    | Dropout              | 0          | (32, 8, 8)   |\n    | BatchNormalization   | 4096       | (32, 8, 8)   |\n    | Flatten              | 0          | (2048,)      |\n    | Dense                | 524544     | (256,)       |\n    | Activation (ReLU)    | 0          | (256,)       |\n    | Dropout              | 0          | (256,)       |\n    | BatchNormalization   | 512        | (256,)       |\n    | Dense                | 2570       | (10,)        |\n    | Activation (Softmax) | 0          | (10,)        |\n    +----------------------+------------+--------------+\n    Total Parameters: 538570\n\n    Training: 100% [------------------------------------------------------------------------] Time: 0:01:55\n    Accuracy: 0.987465181058\n\n<p align=\"center\">\n    <img src=\"http:\/\/eriklindernoren.se\/images\/mlfs_cnn1.png\" width=\"640\">\n<\/p>\n<p align=\"center\">\n    Figure: Classification of the digit dataset using CNN.\n<\/p>\n\n### Density-Based Clustering\n    $ python mlfromscratch\/examples\/dbscan.py\n\n<p align=\"center\">\n    <img src=\"http:\/\/eriklindernoren.se\/images\/mlfs_dbscan.png\" width=\"640\">\n<\/p>\n<p align=\"center\">\n    Figure: Clustering of the moons dataset using DBSCAN.\n<\/p>\n\n### Generating Handwritten Digits\n    $ python mlfromscratch\/unsupervised_learning\/generative_adversarial_network.py\n\n    +-----------+\n    | Generator |\n    +-----------+\n    Input Shape: (100,)\n    +------------------------+------------+--------------+\n    | Layer Type             | Parameters | Output Shape |\n    +------------------------+------------+--------------+\n    | Dense                  | 25856      | (256,)       |\n    | Activation (LeakyReLU) | 0          | (256,)       |\n    | BatchNormalization     | 512        | (256,)       |\n    | Dense                  | 131584     | (512,)       |\n    | Activation (LeakyReLU) | 0          | (512,)       |\n    | BatchNormalization     | 1024       | (512,)       |\n    | Dense                  | 525312     | (1024,)      |\n    | Activation (LeakyReLU) | 0          | (1024,)      |\n    | BatchNormalization     | 2048       | (1024,)      |\n    | Dense                  | 803600     | (784,)       |\n    | Activation (TanH)      | 0          | (784,)       |\n    +------------------------+------------+--------------+\n    Total Parameters: 1489936\n\n    +---------------+\n    | Discriminator |\n    +---------------+\n    Input Shape: (784,)\n    +------------------------+------------+--------------+\n    | Layer Type             | Parameters | Output Shape |\n    +------------------------+------------+--------------+\n    | Dense                  | 401920     | (512,)       |\n    | Activation (LeakyReLU) | 0          | (512,)       |\n    | Dropout                | 0          | (512,)       |\n    | Dense                  | 131328     | (256,)       |\n    | Activation (LeakyReLU) | 0          | (256,)       |\n    | Dropout                | 0          | (256,)       |\n    | Dense                  | 514        | (2,)         |\n    | Activation (Softmax)   | 0          | (2,)         |\n    +------------------------+------------+--------------+\n    Total Parameters: 533762\n\n\n<p align=\"center\">\n    <img src=\"http:\/\/eriklindernoren.se\/images\/gan_mnist5.gif\" width=\"640\">\n<\/p>\n<p align=\"center\">\n    Figure: Training progress of a Generative Adversarial Network generating <br>\n    handwritten digits.\n<\/p>\n\n### Deep Reinforcement Learning\n    $ python mlfromscratch\/examples\/deep_q_network.py\n\n    +----------------+\n    | Deep Q-Network |\n    +----------------+\n    Input Shape: (4,)\n    +-------------------+------------+--------------+\n    | Layer Type        | Parameters | Output Shape |\n    +-------------------+------------+--------------+\n    | Dense             | 320        | (64,)        |\n    | Activation (ReLU) | 0          | (64,)        |\n    | Dense             | 130        | (2,)         |\n    +-------------------+------------+--------------+\n    Total Parameters: 450\n\n<p align=\"center\">\n    <img src=\"http:\/\/eriklindernoren.se\/images\/mlfs_dql1.gif\" width=\"640\">\n<\/p>\n<p align=\"center\">\n    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.\n<\/p>\n\n### Image Reconstruction With RBM\n    $ python mlfromscratch\/examples\/restricted_boltzmann_machine.py\n\n<p align=\"center\">\n    <img src=\"http:\/\/eriklindernoren.se\/images\/rbm_digits1.gif\" width=\"640\">\n<\/p>\n<p align=\"center\">\n    Figure: Shows how the network gets better during training at reconstructing <br>\n    the digit 2 in the MNIST dataset.\n<\/p>\n\n### Evolutionary Evolved Neural Network\n    $ python mlfromscratch\/examples\/neuroevolution.py\n\n    +---------------+\n    | Model Summary |\n    +---------------+\n    Input Shape: (64,)\n    +----------------------+------------+--------------+\n    | Layer Type           | Parameters | Output Shape |\n    +----------------------+------------+--------------+\n    | Dense                | 1040       | (16,)        |\n    | Activation (ReLU)    | 0          | (16,)        |\n    | Dense                | 170        | (10,)        |\n    | Activation (Softmax) | 0          | (10,)        |\n    +----------------------+------------+--------------+\n    Total Parameters: 1210\n\n    Population Size: 100\n    Generations: 3000\n    Mutation Rate: 0.01\n\n    [0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]\n    [1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]\n    ...\n    [2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]\n    Test set accuracy: 96.7%\n\n<p align=\"center\">\n    <img src=\"http:\/\/eriklindernoren.se\/images\/evo_nn4.png\" width=\"640\">\n<\/p>\n<p align=\"center\">\n    Figure: Classification of the digit dataset by a neural network which has<br>\n    been evolutionary evolved.\n<\/p>\n\n### Genetic Algorithm\n    $ python mlfromscratch\/examples\/genetic_algorithm.py\n\n    +--------+\n    |   GA   |\n    +--------+\n    Description: Implementation of a Genetic Algorithm which aims to produce\n    the user specified target string. This implementation calculates each\n    candidate's fitness based on the alphabetical distance between the candidate\n    and the target. A candidate is selected as a parent with probabilities proportional\n    to the candidate's fitness. Reproduction is implemented as a single-point\n    crossover between pairs of parents. Mutation is done by randomly assigning\n    new characters with uniform probability.\n\n    Parameters\n    ----------\n    Target String: 'Genetic Algorithm'\n    Population Size: 100\n    Mutation Rate: 0.05\n\n    [0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]\n    [1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]\n    [2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]\n    [3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]\n    [4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]\n    ...\n    [292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]\n    [293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]\n    [294 Answer: 'Genetic Algorithm']\n\n### Association Analysis\n    $ python mlfromscratch\/examples\/apriori.py\n    +-------------+\n    |   Apriori   |\n    +-------------+\n    Minimum Support: 0.25\n    Minimum Confidence: 0.8\n    Transactions:\n        [1, 2, 3, 4]\n        [1, 2, 4]\n        [1, 2]\n        [2, 3, 4]\n        [2, 3]\n        [3, 4]\n        [2, 4]\n    Frequent Itemsets:\n        [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]\n    Rules:\n        1 -> 2 (support: 0.43, confidence: 1.0)\n        4 -> 2 (support: 0.57, confidence: 0.8)\n        [1, 4] -> 2 (support: 0.29, confidence: 1.0)\n\n\n## Implementations\n### Supervised Learning\n- [Adaboost](mlfromscratch\/supervised_learning\/adaboost.py)\n- [Bayesian Regression](mlfromscratch\/supervised_learning\/bayesian_regression.py)\n- [Decision Tree](mlfromscratch\/supervised_learning\/decision_tree.py)\n- [Elastic Net](mlfromscratch\/supervised_learning\/regression.py)\n- [Gradient Boosting](mlfromscratch\/supervised_learning\/gradient_boosting.py)\n- [K Nearest Neighbors](mlfromscratch\/supervised_learning\/k_nearest_neighbors.py)\n- [Lasso Regression](mlfromscratch\/supervised_learning\/regression.py)\n- [Linear Discriminant Analysis](mlfromscratch\/supervised_learning\/linear_discriminant_analysis.py)\n- [Linear Regression](mlfromscratch\/supervised_learning\/regression.py)\n- [Logistic Regression](mlfromscratch\/supervised_learning\/logistic_regression.py)\n- [Multi-class Linear Discriminant Analysis](mlfromscratch\/supervised_learning\/multi_class_lda.py)\n- [Multilayer Perceptron](mlfromscratch\/supervised_learning\/multilayer_perceptron.py)\n- [Naive Bayes](mlfromscratch\/supervised_learning\/naive_bayes.py)\n- [Neuroevolution](mlfromscratch\/supervised_learning\/neuroevolution.py)\n- [Particle Swarm Optimization of Neural Network](mlfromscratch\/supervised_learning\/particle_swarm_optimization.py)\n- [Perceptron](mlfromscratch\/supervised_learning\/perceptron.py)\n- [Polynomial Regression](mlfromscratch\/supervised_learning\/regression.py)\n- [Random Forest](mlfromscratch\/supervised_learning\/random_forest.py)\n- [Ridge Regression](mlfromscratch\/supervised_learning\/regression.py)\n- [Support Vector Machine](mlfromscratch\/supervised_learning\/support_vector_machine.py)\n- [XGBoost](mlfromscratch\/supervised_learning\/xgboost.py)\n\n### Unsupervised Learning\n- [Apriori](mlfromscratch\/unsupervised_learning\/apriori.py)\n- [Autoencoder](mlfromscratch\/unsupervised_learning\/autoencoder.py)\n- [DBSCAN](mlfromscratch\/unsupervised_learning\/dbscan.py)\n- [FP-Growth](mlfromscratch\/unsupervised_learning\/fp_growth.py)\n- [Gaussian Mixture Model](mlfromscratch\/unsupervised_learning\/gaussian_mixture_model.py)\n- [Generative Adversarial Network](mlfromscratch\/unsupervised_learning\/generative_adversarial_network.py)\n- [Genetic Algorithm](mlfromscratch\/unsupervised_learning\/genetic_algorithm.py)\n- [K-Means](mlfromscratch\/unsupervised_learning\/k_means.py)\n- [Partitioning Around Medoids](mlfromscratch\/unsupervised_learning\/partitioning_around_medoids.py)\n- [Principal Component Analysis](mlfromscratch\/unsupervised_learning\/principal_component_analysis.py)\n- [Restricted Boltzmann Machine](mlfromscratch\/unsupervised_learning\/restricted_boltzmann_machine.py)\n\n### Reinforcement Learning\n- [Deep Q-Network](mlfromscratch\/reinforcement_learning\/deep_q_network.py)\n\n### Deep Learning\n  + [Neural Network](mlfromscratch\/deep_learning\/neural_network.py)\n  + [Layers](mlfromscratch\/deep_learning\/layers.py)\n    * Activation Layer\n    * Average Pooling Layer\n    * Batch Normalization Layer\n    * Constant Padding Layer\n    * Convolutional Layer\n    * Dropout Layer\n    * Flatten Layer\n    * Fully-Connected (Dense) Layer\n    * Fully-Connected RNN Layer\n    * Max Pooling Layer\n    * Reshape Layer\n    * Up Sampling Layer\n    * Zero Padding Layer\n  + Model Types\n    * [Convolutional Neural Network](mlfromscratch\/examples\/convolutional_neural_network.py)\n    * [Multilayer Perceptron](mlfromscratch\/examples\/multilayer_perceptron.py)\n    * [Recurrent Neural Network](mlfromscratch\/examples\/recurrent_neural_network.py)\n\n## Contact\nIf there's some implementation you would like to see here or if you're just feeling social,\nfeel free to [email](mailto:eriklindernoren@gmail.com) me or connect with me on [LinkedIn](https:\/\/www.linkedin.com\/in\/eriklindernoren\/).\n","105":"<h1 align=\"center\">Rasa Open Source<\/h1>\n\n<div align=\"center\">\n\n[![Join the chat on Rasa Community Forum](https:\/\/img.shields.io\/badge\/forum-join%20discussions-brightgreen.svg)](https:\/\/forum.rasa.com\/?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![PyPI version](https:\/\/badge.fury.io\/py\/rasa.svg)](https:\/\/badge.fury.io\/py\/rasa)\n[![Supported Python Versions](https:\/\/img.shields.io\/pypi\/pyversions\/rasa.svg)](https:\/\/pypi.python.org\/pypi\/rasa)\n[![Build Status](https:\/\/github.com\/RasaHQ\/rasa\/workflows\/Continuous%20Integration\/badge.svg)](https:\/\/github.com\/RasaHQ\/rasa\/actions)\n[![Coverage Status](https:\/\/coveralls.io\/repos\/github\/RasaHQ\/rasa\/badge.svg?branch=main)](https:\/\/coveralls.io\/github\/RasaHQ\/rasa?branch=main)\n[![Documentation Status](https:\/\/img.shields.io\/badge\/docs-stable-brightgreen.svg)](https:\/\/rasa.com\/docs)\n![Documentation Build](https:\/\/img.shields.io\/netlify\/d2e447e4-5a5e-4dc7-be5d-7c04ae7ff706?label=Documentation%20Build)\n[![FOSSA Status](https:\/\/app.fossa.com\/api\/projects\/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git.svg?type=shield)](https:\/\/app.fossa.com\/projects\/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git?ref=badge_shield)\n[![PRs Welcome](https:\/\/img.shields.io\/badge\/PRs-welcome-brightgreen.svg?style=flat-square)](https:\/\/github.com\/orgs\/RasaHQ\/projects\/23)\n\n<\/div>\n\n<a href=\"https:\/\/grnh.se\/05a908c02us\" target=\"_blank\"><img align=\"center\" src=\"https:\/\/www.rasa.com\/assets\/img\/github\/hiring_banner.png\" alt=\"An image with Sara, the Rasa mascot, standing next to a roadmap with future Rasa milestones: identifying unsuccessful conversations at scale, continuous model evaluation, controllable NLG and breaking free from intents. Are you excited about these milestones? Help us make these ideas become reality - we're hiring!\" title=\"We're hiring! Learn more\"><\/a>\n\n<hr \/>\n\n\ud83d\udca1 **Rasa Open Source 3.0 is here!** \ud83d\udca1\n\n[2.8](https:\/\/github.com\/RasaHQ\/rasa\/milestone\/39) is the last minor in the 2.x series.\nYou can still contribute new features and improvements which we plan to release alongside\nupdates to 3.0. Read more about [our contributor guidelines](#how-to-contribute).\n\n<hr \/>\n\n<img align=\"right\" height=\"244\" src=\"https:\/\/www.rasa.com\/assets\/img\/sara\/sara-open-source-2.0.png\" alt=\"An image of Sara, the Rasa mascot bird, holding a flag that reads Open Source with one wing, and a wrench in the other\" title=\"Rasa Open Source\">\n\nRasa is an open source machine learning framework to automate text-and voice-based conversations. With Rasa, you can build contextual assistants on:\n- Facebook Messenger\n- Slack\n- Google Hangouts\n- Webex Teams\n- Microsoft Bot Framework\n- Rocket.Chat\n- Mattermost\n- Telegram\n- Twilio\n- Your own custom conversational channels\n\nor voice assistants as:\n- Alexa Skills\n- Google Home Actions\n\nRasa helps you build contextual assistants capable of having layered conversations with\nlots of back-and-forth. In order for a human to have a meaningful exchange with a contextual\nassistant, the assistant needs to be able to use context to build on things that were previously\ndiscussed \u2013 Rasa enables you to build assistants that can do this in a scalable way.\n\nThere's a lot more background information in this\n[blog post](https:\/\/medium.com\/rasa-blog\/a-new-approach-to-conversational-software-2e64a5d05f2a).\n\n---\n- **What does Rasa do? \ud83e\udd14**\n  [Check out our Website](https:\/\/rasa.com\/)\n\n- **I'm new to Rasa \ud83d\ude04**\n  [Get Started with Rasa](https:\/\/rasa.com\/docs\/getting-started\/)\n\n- **I'd like to read the detailed docs \ud83e\udd13**\n  [Read The Docs](https:\/\/rasa.com\/docs\/)\n\n- **I'm ready to install Rasa \ud83d\ude80**\n  [Installation](https:\/\/rasa.com\/docs\/rasa\/user-guide\/installation\/)\n\n- **I want to learn how to use Rasa \ud83d\ude80**\n  [Tutorial](https:\/\/rasa.com\/docs\/rasa\/user-guide\/rasa-tutorial\/)\n\n- **I have a question \u2753**\n  [Rasa Community Forum](https:\/\/forum.rasa.com\/)\n\n- **I would like to contribute \ud83e\udd17**\n  [How to Contribute](#how-to-contribute)\n\n---\n## Where to get help\n\nThere is extensive documentation in the [Rasa Docs](https:\/\/rasa.com\/docs\/rasa).\nMake sure to select the correct version so you are looking at\nthe docs for the version you installed.\n\nPlease use [Rasa Community Forum](https:\/\/forum.rasa.com) for quick answers to\nquestions.\n\n### README Contents:\n- [How to contribute](#how-to-contribute)\n- [Development Internals](#development-internals)\n- [Releases](#releases)\n- [License](#license)\n\n### How to contribute\nWe are very happy to receive and merge your contributions into this repository!\n\nTo contribute via pull request, follow these steps:\n\n1. Create an issue describing the feature you want to work on (or\n   have a look at the [contributor board](https:\/\/github.com\/orgs\/RasaHQ\/projects\/23))\n2. Write your code, tests and documentation, and format them with ``black``\n3. Create a pull request describing your changes\n\nFor more detailed instructions on how to contribute code, check out these [code contributor guidelines](CONTRIBUTING.md).\n\nYou can find more information about how to contribute to Rasa (in lots of\ndifferent ways!) [on our website.](http:\/\/rasa.com\/community\/contribute).\n\nYour pull request will be reviewed by a maintainer, who will get\nback to you about any necessary changes or questions. You will\nalso be asked to sign a\n[Contributor License Agreement](https:\/\/cla-assistant.io\/RasaHQ\/rasa).\n\n\n## Development Internals\n\n### Installing Poetry\n\nRasa uses Poetry for packaging and dependency management. If you want to build it from source,\nyou have to install Poetry first. This is how it can be done:\n\n```bash\ncurl -sSL https:\/\/raw.githubusercontent.com\/python-poetry\/poetry\/master\/get-poetry.py | python\n```\n\nThere are several other ways to install Poetry. Please, follow\n[the official guide](https:\/\/python-poetry.org\/docs\/#installation) to see all possible options.\n\n### Managing environments\n\nThe official [Poetry guide](https:\/\/python-poetry.org\/docs\/managing-environments\/) suggests to use\n[pyenv](https:\/\/github.com\/pyenv\/pyenv) or any other similar tool to easily switch between Python versions.\nThis is how it can be done:\n\n```bash\npyenv install 3.7.9\npyenv local 3.7.9  # Activate Python 3.7.9 for the current project\n```\n*Note*: If you have trouble installing a specific version of python on your system\nit might be worth trying other supported versions.\n\nBy default, Poetry will try to use the currently activated Python version to create the virtual environment\nfor the current project automatically. You can also create and activate a virtual environment manually \u2014 in this\ncase, Poetry should pick it up and use it to install the dependencies. For example:\n\n```bash\npython -m venv .venv\nsource .venv\/bin\/activate\n```\n\nYou can make sure that the environment is picked up by executing\n\n```bash\npoetry env info\n```\n\n### Building from source\n\nTo install dependencies and `rasa` itself in editable mode execute\n\n```bash\nmake install\n```\n\n*Note for macOS users*: under macOS Big Sur we've seen some compiler issues for \ndependencies. Using `export SYSTEM_VERSION_COMPAT=1` before the installation helped. \n\n### Running and changing the documentation\n\nFirst of all, install all the required dependencies:\n\n```bash\nmake install install-docs\n```\n\nAfter the installation has finished, you can run and view the documentation\nlocally using:\n\n```bash\nmake livedocs\n```\n\nIt should open a new tab with the local version of the docs in your browser;\nif not, visit http:\/\/localhost:3000 in your browser.\nYou can now change the docs locally and the web page will automatically reload\nand apply your changes.\n\n### Running the Tests\n\nIn order to run the tests, make sure that you have the development requirements installed:\n\n```bash\nmake prepare-tests-ubuntu # Only on Ubuntu and Debian based systems\nmake prepare-tests-macos  # Only on macOS\n```\n\nThen, run the tests:\n\n```bash\nmake test\n```\n\nThey can also be run at multiple jobs to save some time:\n\n```bash\nJOBS=[n] make test\n```\n\nWhere `[n]` is the number of jobs desired. If omitted, `[n]` will be automatically chosen by pytest.\n\n\n### Running the Integration Tests\n\nIn order to run the integration tests, make sure that you have the development requirements installed:\n\n```bash\nmake prepare-tests-ubuntu # Only on Ubuntu and Debian based systems\nmake prepare-tests-macos  # Only on macOS\n```\n\nThen, you'll need to start services with the following command which uses\n[Docker Compose](https:\/\/docs.docker.com\/compose\/install\/):\n\n```bash\nmake run-integration-containers\n```\n\nFinally, you can run the integration tests like this:\n\n```bash\nmake test-integration\n```\n\n\n### Resolving merge conflicts\n\nPoetry doesn't include any solution that can help to resolve merge conflicts in\nthe lock file `poetry.lock` by default.\nHowever, there is a great tool called [poetry-merge-lock](https:\/\/poetry-merge-lock.readthedocs.io\/en\/latest\/).\nHere is how you can install it:\n\n```bash\npip install poetry-merge-lock\n```\n\nJust execute this command to resolve merge conflicts in `poetry.lock` automatically:\n\n```bash\npoetry-merge-lock\n```\n\n### Build a Docker image locally\n\nIn order to build a Docker image on your local machine execute the following command:\n\n```bash\nmake build-docker\n```\n\nThe Docker image is available on your local machine as `rasa:localdev`.\n\n### Code Style\n\nTo ensure a standardized code style we use the formatter [black](https:\/\/github.com\/ambv\/black).\nTo ensure our type annotations are correct we use the type checker [pytype](https:\/\/github.com\/google\/pytype).\nIf your code is not formatted properly or doesn't type check, GitHub will fail to build.\n\n#### Formatting\n\nIf you want to automatically format your code on every commit, you can use [pre-commit](https:\/\/pre-commit.com\/).\nJust install it via `pip install pre-commit` and execute `pre-commit install` in the root folder.\nThis will add a hook to the repository, which reformats files on every commit.\n\nIf you want to set it up manually, install black via `poetry install`.\nTo reformat files execute\n```\nmake formatter\n```\n\n#### Type Checking\n\nIf you want to check types on the codebase, install `mypy` using `poetry install`.\nTo check the types execute\n```\nmake types\n```\n\n### Deploying documentation updates\n\nWe use `Docusaurus v2` to build docs for tagged versions and for the `main` branch.\nThe static site that gets built is pushed to the `documentation` branch of this repo.\n\nWe host the site on netlify. On `main` branch builds (see `.github\/workflows\/documentation.yml`), we push the built docs to\nthe `documentation` branch. Netlify automatically re-deploys the docs pages whenever there is a change to that branch.\n\n## Releases\n### Release Timeline for Minor Releases\n**For Rasa Open Source, we usually commit to time-based releases, specifically on a monthly basis.**\nThis means that we commit beforehand to releasing a specific version of Rasa Open Source on a specific day,\nand we cannot be 100% sure what will go in a release, because certain features may not be ready.\n\nAt the beginning of each quarter, the Rasa team will review the scheduled release dates for all products and make sure\nthey work for the projected work we have planned for the quarter, as well as work well across products.\n\n**Once the dates are settled upon, we update the respective [milestones](https:\/\/github.com\/RasaHQ\/rasa\/milestones).**\n\n### Cutting a Major \/ Minor release\n#### A week before release day\n\n1. **Make sure the [milestone](https:\/\/github.com\/RasaHQ\/rasa\/milestones) already exists and is scheduled for the\ncorrect date.**\n2. **Take a look at the issues & PRs that are in the milestone**: does it look about right for the release highlights\nwe are planning to ship? Does it look like anything is missing? Don't worry about being aware of every PR that should\nbe in, but it's useful to take a moment to evaluate what's assigned to the milestone.\n3. **Post a message on the engineering Slack channel**, letting the team know you'll be the one cutting the upcoming\nrelease, as well as:\n    1. Providing the link to the appropriate milestone\n    2. Reminding everyone to go over their issues and PRs and please assign them to the milestone\n    3. Reminding everyone of the scheduled date for the release\n\n#### A day before release day\n\n1. **Go over the milestone and evaluate the status of any PR merging that's happening. Follow up with people on their\nbugs and fixes.** If the release introduces new bugs or regressions that can't be fixed in time, we should discuss on\nSlack about this and take a decision on how to move forward. If the issue is not ready to be merged in time, we remove the issue \/ PR from the milestone and notify the PR owner and the product manager on Slack about it. The PR \/ issue owners are responsible for\ncommunicating any issues which might be release relevant. Postponing the release should be considered as an edge case scenario.\n\n#### Release day! \ud83d\ude80\n\n1. **At the start of the day, post a small message on slack announcing release day!** Communicate you'll be handling\nthe release, and the time you're aiming to start releasing (again, no later than 4pm, as issues may arise and\ncause delays). This message should be posted early in the morning and before moving forward with any of the steps of the release, \n   in order to give enough time to people to check their PRs and issues. That way they can plan any remaining work. A template of the slack message can be found [here](https:\/\/rasa-hq.slack.com\/archives\/C36SS4N8M\/p1613032208137500?thread_ts=1612876410.068400&cid=C36SS4N8M).\n   The release time should be communicated transparently so that others can plan potentially necessary steps accordingly. If there are bigger changes this should be communicated.\n2. Make sure the milestone is empty (everything has been either merged or moved to the next milestone)\n3. Once everything in the milestone is taken care of, post a small message on Slack communicating you are about to\nstart the release process (in case anything is missing).\n4. **You may now do the release by following the instructions outlined in the\n[Rasa Open Source README](#steps-to-release-a-new-version) !**\n\n#### After a Major release\n\nAfter a Major release has been completed, please follow [these instructions to complete the documentation update](.\/docs\/README.md#manual-steps-after-a-new-version).\n\n### Steps to release a new version\nReleasing a new version is quite simple, as the packages are build and distributed by GitHub Actions.\n\n*Terminology*:\n* micro release (third version part increases): 1.1.2 -> 1.1.3\n* minor release (second version part increases): 1.1.3 -> 1.2.0\n* major release (first version part increases): 1.2.0 -> 2.0.0\n\n*Release steps*:\n1. Make sure all dependencies are up to date (**especially Rasa SDK**)\n    - For Rasa SDK, except in the case of a micro release, that means first creating a [new Rasa SDK release](https:\/\/github.com\/RasaHQ\/rasa-sdk#steps-to-release-a-new-version) (make sure the version numbers between the new Rasa and Rasa SDK releases match)\n    - Once the tag with the new Rasa SDK release is pushed and the package appears on [pypi](https:\/\/pypi.org\/project\/rasa-sdk\/), the dependency in the rasa repository can be resolved (see below).\n2. In case of a minor release, create a new branch that corresponds to the new release, e.g. \n   ```bash\n    git checkout -b 1.2.x\n    git push origin 1.2.x\n    ```\n3. Switch to the branch you want to cut the release from (`main` in case of a major, the `<major>.<minor>.x` branch for minors and micros)\n    - Update the `rasa-sdk` entry in `pyproject.toml` with the new release version and run `poetry update`. This creates a new `poetry.lock` file with all dependencies resolved.\n    - Commit the changes with `git commit -am \"bump rasa-sdk dependency\"` but do not push them. They will be automatically picked up by the following step.\n4. If this is a major release, update the list of actively maintained versions [in the README](#actively-maintained-versions) and in [the docs](.\/docs\/docs\/actively-maintained-versions.mdx).\n5. Run `make release`\n6. Create a PR against the release branch (e.g. `1.2.x`)\n7. Once your PR is merged, tag a new release (this SHOULD always happen on the release branch), e.g. using\n    ```bash\n    git checkout 1.2.x\n    git pull origin 1.2.x\n    git tag 1.2.0 -m \"next release\"\n    git push origin 1.2.0 --tags\n    ```\n    GitHub will build this tag and publish the build artifacts.\n8. After all the steps are completed and if everything goes well then we should see a message automatically posted in the company's Slack (`product` channel) like this [one](https:\/\/rasa-hq.slack.com\/archives\/C7B08Q5FX\/p1614354499046600)\n9. If no message appears in the channel then you can do the following checks:\n    - Check the workflows in [Github Actions](https:\/\/github.com\/RasaHQ\/rasa\/actions) and make sure that the merged PR of the current release is completed successfully. To easily find your PR you can use the filters `event: push` and `branch: <version number>` (example on release 2.4 you can see [here](https:\/\/github.com\/RasaHQ\/rasa\/actions\/runs\/643344876))\n    - If the workflow is not completed, then try to re run the workflow in case that solves the problem\n    - If the problem persists, check also the log files and try to find the root cause of the issue\n    - If you still cannot resolve the error, contact the infrastructure team by providing any helpful information from your investigation\n10.  After the message is posted correctly in the `product` channel, check also in the `product-engineering-alerts` channel if there are any alerts related to the Rasa Open Source release like this [one](https:\/\/rasa-hq.slack.com\/archives\/C01585AN2NP\/p1615486087001000)\n    \n### Cutting a Micro release\n\nMicro releases are simpler to cut, since they are meant to contain only bugfixes.\n\n**The only things you need to do to cut a micro are:**\n\n1. Notify the engineering team on Slack that you are planning to cut a micro, in case someone has an important fix\nto add.\n2. Make sure the bugfix(es) are in the release branch you will use (p.e if you are cutting a `2.0.4` micro, you will\nneed your fixes to be on the `2.0.x` release branch). All micros must come from a `.x` branch!\n3. Once you're ready to release the Rasa Open Source micro, checkout the branch, run `make release` and follow the\nsteps + get the PR merged.\n4. Once the PR is in, pull the `.x` branch again and push the tag!\n\n### Actively maintained versions\n\nWe're actively maintaining _any minor on our latest major release_ and _the latest minor of the previous major release_.\nCurrently, this means the following minor versions will receive bugfixes updates:\n- 2.8\n- Every minor version on 3.x\n\n## License\nLicensed under the Apache License, Version 2.0.\nCopyright 2022 Rasa Technologies GmbH. [Copy of the license](LICENSE.txt).\n\nA list of the Licenses of the dependencies of the project can be found at\nthe bottom of the\n[Libraries Summary](https:\/\/libraries.io\/github\/RasaHQ\/rasa).\n","106":"# imgaug\n\nThis python library helps you with augmenting images for your machine learning projects.\nIt converts a set of input images into a new, much larger set of slightly altered images.\n\n[![Build Status](https:\/\/travis-ci.org\/aleju\/imgaug.svg?branch=master)](https:\/\/travis-ci.org\/aleju\/imgaug)\n[![codecov](https:\/\/codecov.io\/gh\/aleju\/imgaug\/branch\/master\/graph\/badge.svg)](https:\/\/codecov.io\/gh\/aleju\/imgaug)\n[![Codacy Badge](https:\/\/api.codacy.com\/project\/badge\/Grade\/1370ce38e99e40af842d47a8dd721444)](https:\/\/www.codacy.com\/app\/aleju\/imgaug?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=aleju\/imgaug&amp;utm_campaign=Badge_Grade)\n\n<table>\n\n<tr>\n<th>&nbsp;<\/th>\n<th>Image<\/th>\n<th>Heatmaps<\/th>\n<th>Seg. Maps<\/th>\n<th>Keypoints<\/th>\n<th>Bounding Boxes,<br>Polygons<\/th>\n<\/tr>\n\n<!-- Line 1: Original Input -->\n<tr>\n<td><em>Original Input<\/em><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/noop_image.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input images\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/noop_heatmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input heatmaps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/noop_segmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input segmentation maps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/noop_kps.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input keypoints\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/noop_bbs.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input bounding boxes\"><\/td>\n<\/tr>\n\n<!-- Line 2: Gauss. Noise + Contrast + Sharpen -->\n<tr>\n<td>Gauss. Noise<br>+&nbsp;Contrast<br>+&nbsp;Sharpen<\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/non_geometric_image.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to images\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/non_geometric_heatmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to heatmaps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/non_geometric_segmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to segmentation maps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/non_geometric_kps.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to keypoints\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/non_geometric_bbs.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to bounding boxes\"><\/td>\n<\/tr>\n\n<!-- Line 3: Affine -->\n<tr>\n<td>Affine<\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/affine_image.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"affine augmentations, applied to images\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/affine_heatmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"affine augmentations, applied to heatmaps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/affine_segmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"affine augmentations, applied to segmentation maps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/affine_kps.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"affine augmentations, applied to keypoints\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/affine_bbs.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"affine augmentations, applied to bounding boxes\"><\/td>\n<\/tr>\n\n<!-- Line 4: Crop + Pad -->\n<tr>\n<td>Crop<br>+&nbsp;Pad<\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/cropandpad_image.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"crop and pad augmentations, applied to images\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/cropandpad_heatmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"crop and pad augmentations, applied to heatmaps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/cropandpad_segmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"crop and pad augmentations, applied to segmentation maps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/cropandpad_kps.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"crop and pad augmentations, applied to keypoints\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/cropandpad_bbs.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"crop and pad augmentations, applied to bounding boxes\"><\/td>\n<\/tr>\n\n<!-- Line 5: Fliplr + Perspective -->\n<tr>\n<td>Fliplr<br>+&nbsp;Perspective<\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/fliplr_perspective_image.jpg\" height=\"83\" width=\"124\" alt=\"Horizontal flip and perspective transform augmentations, applied to images\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/fliplr_perspective_heatmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"Horizontal flip and perspective transform augmentations, applied to heatmaps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/fliplr_perspective_segmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"Horizontal flip and perspective transform augmentations, applied to segmentation maps\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/fliplr_perspective_kps.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"Horizontal flip and perspective transform augmentations, applied to keypoints\"><\/td>\n<td><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/small_overview\/fliplr_perspective_bbs.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"Horizontal flip and perspective transform augmentations, applied to bounding boxes\"><\/td>\n<\/tr>\n\n<\/table>\n\n\n**More (strong) example augmentations of one input image:**\n\n![64 quokkas](https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/examples_grid.jpg?raw=true \"64 quokkas\")\n\n\n## Table of Contents\n\n1. [Features](#features)\n2. [Installation](#installation)\n3. [Documentation](#documentation)\n4. [Recent Changes](#recent_changes)\n5. [Example Images](#example_images)\n6. [Code Examples](#code_examples)\n7. [Citation](#citation)\n\n\n<a name=\"features\"\/>\n\n## Features\n\n* Many augmentation techniques\n  * E.g. affine transformations, perspective transformations, contrast changes, gaussian noise, dropout of regions, hue\/saturation changes, cropping\/padding, blurring, ...\n  * Optimized for high performance\n  * Easy to apply augmentations only to some images\n  * Easy to apply augmentations in random order\n* Support for\n  * Images (full support for uint8, for other dtypes see [documentation](https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/dtype_support.html))\n  * Heatmaps (float32), Segmentation Maps (int), Masks (bool)\n    * May be smaller\/larger than their corresponding images. *No* extra lines of code needed for e.g. crop.\n  * Keypoints\/Landmarks (int\/float coordinates)\n  * Bounding Boxes (int\/float coordinates)\n  * Polygons (int\/float coordinates)\n  * Line Strings (int\/float coordinates)\n* Automatic alignment of sampled random values\n  * Example: Rotate image and segmentation map on it by the same value sampled from `uniform(-10\u00b0, 45\u00b0)`. (0 extra lines of code.)\n* Probability distributions as parameters\n  * Example: Rotate images by values sampled from `uniform(-10\u00b0, 45\u00b0)`.\n  * Example: Rotate images by values sampled from `ABS(N(0, 20.0))*(1+B(1.0, 1.0))`\", where `ABS(.)` is the absolute function, `N(.)` the gaussian distribution and `B(.)` the beta distribution.\n* Many helper functions\n  * Example: Draw heatmaps, segmentation maps, keypoints, bounding boxes, ...\n  * Example: Scale segmentation maps, average\/max pool of images\/maps, pad images to aspect\n    ratios (e.g. to square them)\n  * Example: Convert keypoints to distance maps, extract pixels within bounding boxes from images, clip polygon to the image plane, ...\n* Support for augmentation on multiple CPU cores\n\n\n<a name=\"installation\"\/>\n\n## Installation\n\nThe library supports python 2.7 and 3.4+.\n\n### Installation: Anaconda\n\nTo install the library in anaconda, perform the following commands:\n```bash\nconda config --add channels conda-forge\nconda install imgaug\n```\n\nYou can deinstall the library again via `conda remove imgaug`.\n\n### Installation: pip\n\nThen install imgaug either via pypi (can lag behind the github version):\n```bash\npip install imgaug\n```\n\nor install the latest version directly from github:\n```bash\npip install git+https:\/\/github.com\/aleju\/imgaug.git\n```\n\nFor more details, see the [install guide](https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/installation.html)\n\nTo deinstall the library, just execute `pip uninstall imgaug`.\n\n\n<a name=\"documentation\"\/>\n\n## Documentation\n\nExample jupyter notebooks:\n  * [Load and Augment an Image](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/A01%20-%20Load%20and%20Augment%20an%20Image.ipynb)\n  * [Multicore Augmentation](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/A03%20-%20Multicore%20Augmentation.ipynb)\n  * Augment and work with: [Keypoints\/Landmarks](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/B01%20-%20Augment%20Keypoints.ipynb),\n    [Bounding Boxes](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/B02%20-%20Augment%20Bounding%20Boxes.ipynb),\n    [Polygons](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/B03%20-%20Augment%20Polygons.ipynb),\n    [Line Strings](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/B06%20-%20Augment%20Line%20Strings.ipynb),\n    [Heatmaps](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/B04%20-%20Augment%20Heatmaps.ipynb),\n    [Segmentation Maps](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/B05%20-%20Augment%20Segmentation%20Maps.ipynb) \n\nMore notebooks: [imgaug-doc\/notebooks](https:\/\/github.com\/aleju\/imgaug-doc\/tree\/master\/notebooks).\n\nExample ReadTheDocs pages:\n* [Quick example code on how to use the library](http:\/\/imgaug.readthedocs.io\/en\/latest\/source\/examples_basics.html)\n* [Overview of all Augmenters](https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview_of_augmenters.html)\n* [API](http:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api.html)\n\nMore RTD documentation: [imgaug.readthedocs.io](http:\/\/imgaug.readthedocs.io\/en\/latest\/source\/examples_basics.html).\n\nAll documentation related files of this project are hosted in the\nrepository [imgaug-doc](https:\/\/github.com\/aleju\/imgaug-doc).\n\n\n<a name=\"recent_changes\"\/>\n\n## Recent Changes\n\n* **0.4.0**: Added new augmenters, changed backend to batchwise augmentation,\n  support for numpy 1.18 and python 3.8.\n* **0.3.0**: Reworked segmentation map augmentation, adapted to numpy 1.17+\n  random number sampling API, several new augmenters.\n* **0.2.9**: Added polygon augmentation, added line string augmentation,\n  simplified augmentation interface.\n* **0.2.8**: Improved performance, dtype support and multicore augmentation.\n\nSee [changelogs\/](changelogs\/) for more details.\n\n\n<a name=\"example_images\"\/>\n\n## Example Images\n\nThe images below show examples for most augmentation techniques.\n\nValues written in the form `(a, b)` denote a uniform distribution,\ni.e. the value is randomly picked from the interval `[a, b]`.\nLine strings are supported by (almost) all augmenters, but are not explicitly\nvisualized here.\n\n<table>\n\n<tr><td colspan=\"5\"><strong>meta<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#identity\">Identity<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#channelshuffle\">ChannelShuffle<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/meta\/identity.gif\" height=\"148\" width=\"100\" alt=\"Identity\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/meta\/channelshuffle.gif\" height=\"148\" width=\"100\" alt=\"ChannelShuffle\"><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#sequential\">Sequential<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#someof\">SomeOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#oneof\">OneOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#sometimes\">Sometimes<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#withchannels\">WithChannels<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#lambda\">Lambda<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#assertlambda\">AssertLambda<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#assertshape\">AssertShape<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#removecbasbyoutofimagefraction\">RemoveCBAsByOutOfImageFraction<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/meta.html#clipcbastoimageplanes\">ClipCBAsToImagePlanes<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>arithmetic<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#add\">Add<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#add\">Add<\/a><br\/>(per_channel=True)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#additivegaussiannoise\">AdditiveGaussianNoise<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#additivegaussiannoise\">AdditiveGaussianNoise<\/a><br\/>(per_channel=True)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#multiply\">Multiply<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/add.gif\" height=\"148\" width=\"100\" alt=\"Add\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/add_per_channel_true.gif\" height=\"148\" width=\"100\" alt=\"Add per_channel=True\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/additivegaussiannoise.gif\" height=\"148\" width=\"100\" alt=\"AdditiveGaussianNoise\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/additivegaussiannoise_per_channel_true.gif\" height=\"148\" width=\"100\" alt=\"AdditiveGaussianNoise per_channel=True\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/multiply.gif\" height=\"148\" width=\"100\" alt=\"Multiply\"><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#cutout\">Cutout<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#dropout\">Dropout<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#coarsedropout\">CoarseDropout<\/a><br\/>(p=0.2)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#coarsedropout\">CoarseDropout<\/a><br\/>(p=0.2, per_channel=True)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#dropout2d\">Dropout2d<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/cutout.gif\" height=\"148\" width=\"100\" alt=\"Cutout\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/dropout.gif\" height=\"148\" width=\"100\" alt=\"Dropout\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/coarsedropout_p_0_2.gif\" height=\"148\" width=\"100\" alt=\"CoarseDropout p=0.2\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/coarsedropout_p_0_2_per_channel_true.gif\" height=\"148\" width=\"100\" alt=\"CoarseDropout p=0.2, per_channel=True\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/dropout2d.gif\" height=\"148\" width=\"100\" alt=\"Dropout2d\"><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#saltandpepper\">SaltAndPepper<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#coarsesaltandpepper\">CoarseSaltAndPepper<\/a><br\/>(p=0.2)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#invert\">Invert<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#solarize\">Solarize<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#jpegcompression\">JpegCompression<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/saltandpepper.gif\" height=\"148\" width=\"100\" alt=\"SaltAndPepper\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/coarsesaltandpepper_p_0_2.gif\" height=\"148\" width=\"100\" alt=\"CoarseSaltAndPepper p=0.2\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/invert.gif\" height=\"148\" width=\"100\" alt=\"Invert\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/solarize.gif\" height=\"148\" width=\"100\" alt=\"Solarize\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/arithmetic\/jpegcompression.gif\" height=\"148\" width=\"100\" alt=\"JpegCompression\"><\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#addelementwise\">AddElementwise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#additivelaplacenoise\">AdditiveLaplaceNoise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#additivepoissonnoise\">AdditivePoissonNoise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#multiplyelementwise\">MultiplyElementwise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#totaldropout\">TotalDropout<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#replaceelementwise\">ReplaceElementwise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#impulsenoise\">ImpulseNoise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#salt\">Salt<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#pepper\">Pepper<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#coarsesalt\">CoarseSalt<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#coarsepepper\">CoarsePepper<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/arithmetic.html#solarize\">Solarize<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>artistic<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/artistic.html#cartoon\">Cartoon<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/artistic\/cartoon.gif\" height=\"144\" width=\"128\" alt=\"Cartoon\"><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>blend<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalpha\">BlendAlpha<\/a><br\/>with EdgeDetect(1.0)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphasimplexnoise\">BlendAlphaSimplexNoise<\/a><br\/>with EdgeDetect(1.0)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphafrequencynoise\">BlendAlphaFrequencyNoise<\/a><br\/>with EdgeDetect(1.0)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphasomecolors\">BlendAlphaSomeColors<\/a><br\/>with RemoveSaturation(1.0)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalpharegulargrid\">BlendAlphaRegularGrid<\/a><br\/>with Multiply((0.0, 0.5))<\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blend\/blendalpha_with_edgedetect_1_0.gif\" height=\"148\" width=\"100\" alt=\"BlendAlpha with EdgeDetect1.0\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blend\/blendalphasimplexnoise_with_edgedetect_1_0.gif\" height=\"148\" width=\"100\" alt=\"BlendAlphaSimplexNoise with EdgeDetect1.0\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blend\/blendalphafrequencynoise_with_edgedetect_1_0.gif\" height=\"148\" width=\"100\" alt=\"BlendAlphaFrequencyNoise with EdgeDetect1.0\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blend\/blendalphasomecolors_with_removesaturation_1_0.gif\" height=\"144\" width=\"128\" alt=\"BlendAlphaSomeColors with RemoveSaturation1.0\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blend\/blendalpharegulargrid_with_multiply_0_0_0_5.gif\" height=\"148\" width=\"100\" alt=\"BlendAlphaRegularGrid with Multiply0.0, 0.5\"><\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphamask\">BlendAlphaMask<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphaelementwise\">BlendAlphaElementwise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphaverticallineargradient\">BlendAlphaVerticalLinearGradient<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphahorizontallineargradient\">BlendAlphaHorizontalLinearGradient<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphasegmapclassids\">BlendAlphaSegMapClassIds<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphaboundingboxes\">BlendAlphaBoundingBoxes<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blend.html#blendalphacheckerboard\">BlendAlphaCheckerboard<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_blend.html#imgaug.augmenters.blend.SomeColorsMaskGen\">SomeColorsMaskGen<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_blend.html#imgaug.augmenters.blend.HorizontalLinearGradientMaskGen\">HorizontalLinearGradientMaskGen<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_blend.html#imgaug.augmenters.blend.VerticalLinearGradientMaskGen\">VerticalLinearGradientMaskGen<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_blend.html#imgaug.augmenters.blend.RegularGridMaskGen\">RegularGridMaskGen<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_blend.html#imgaug.augmenters.blend.CheckerboardMaskGen\">CheckerboardMaskGen<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_blend.html#imgaug.augmenters.blend.SegMapClassIdsMaskGen\">SegMapClassIdsMaskGen<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_blend.html#imgaug.augmenters.blend.BoundingBoxesMaskGen\">BoundingBoxesMaskGen<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_blend.html#imgaug.augmenters.blend.InvertMaskGen\">InvertMaskGen<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>blur<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blur.html#gaussianblur\">GaussianBlur<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blur.html#averageblur\">AverageBlur<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blur.html#medianblur\">MedianBlur<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blur.html#bilateralblur\">BilateralBlur<\/a><br\/>(sigma_color=250,<br\/>sigma_space=250)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blur.html#motionblur\">MotionBlur<\/a><br\/>(angle=0)<\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blur\/gaussianblur.gif\" height=\"148\" width=\"100\" alt=\"GaussianBlur\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blur\/averageblur.gif\" height=\"148\" width=\"100\" alt=\"AverageBlur\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blur\/medianblur.gif\" height=\"148\" width=\"100\" alt=\"MedianBlur\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blur\/bilateralblur_sigma_color_250_sigma_space_250.gif\" height=\"148\" width=\"100\" alt=\"BilateralBlur sigma_color=250, sigma_space=250\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blur\/motionblur_angle_0.gif\" height=\"148\" width=\"100\" alt=\"MotionBlur angle=0\"><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blur.html#motionblur\">MotionBlur<\/a><br\/>(k=5)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/blur.html#meanshiftblur\">MeanShiftBlur<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blur\/motionblur_k_5.gif\" height=\"148\" width=\"100\" alt=\"MotionBlur k=5\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/blur\/meanshiftblur.gif\" height=\"148\" width=\"100\" alt=\"MeanShiftBlur\"><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>collections<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/collections.html#randaugment\">RandAugment<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/collections\/randaugment.gif\" height=\"148\" width=\"100\" alt=\"RandAugment\"><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>color<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#multiplyandaddtobrightness\">MultiplyAndAddToBrightness<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#multiplyhueandsaturation\">MultiplyHueAndSaturation<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#multiplyhue\">MultiplyHue<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#multiplysaturation\">MultiplySaturation<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#addtohueandsaturation\">AddToHueAndSaturation<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/multiplyandaddtobrightness.gif\" height=\"148\" width=\"100\" alt=\"MultiplyAndAddToBrightness\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/multiplyhueandsaturation.gif\" height=\"148\" width=\"100\" alt=\"MultiplyHueAndSaturation\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/multiplyhue.gif\" height=\"148\" width=\"100\" alt=\"MultiplyHue\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/multiplysaturation.gif\" height=\"148\" width=\"100\" alt=\"MultiplySaturation\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/addtohueandsaturation.gif\" height=\"148\" width=\"100\" alt=\"AddToHueAndSaturation\"><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#grayscale\">Grayscale<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#removesaturation\">RemoveSaturation<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#changecolortemperature\">ChangeColorTemperature<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#kmeanscolorquantization\">KMeansColorQuantization<\/a><br\/>(to_colorspace=RGB)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#uniformcolorquantization\">UniformColorQuantization<\/a><br\/>(to_colorspace=RGB)<\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/grayscale.gif\" height=\"148\" width=\"100\" alt=\"Grayscale\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/removesaturation.gif\" height=\"148\" width=\"100\" alt=\"RemoveSaturation\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/changecolortemperature.gif\" height=\"148\" width=\"100\" alt=\"ChangeColorTemperature\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/kmeanscolorquantization_to_colorspace_rgb.gif\" height=\"148\" width=\"100\" alt=\"KMeansColorQuantization to_colorspace=RGB\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/color\/uniformcolorquantization_to_colorspace_rgb.gif\" height=\"148\" width=\"100\" alt=\"UniformColorQuantization to_colorspace=RGB\"><\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#withcolorspace\">WithColorspace<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#withbrightnesschannels\">WithBrightnessChannels<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#multiplybrightness\">MultiplyBrightness<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#addtobrightness\">AddToBrightness<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#withhueandsaturation\">WithHueAndSaturation<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#addtohue\">AddToHue<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#addtosaturation\">AddToSaturation<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#changecolorspace\">ChangeColorspace<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#posterize\">Posterize<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>contrast<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#gammacontrast\">GammaContrast<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#gammacontrast\">GammaContrast<\/a><br\/>(per_channel=True)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#sigmoidcontrast\">SigmoidContrast<\/a><br\/>(cutoff=0.5)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#sigmoidcontrast\">SigmoidContrast<\/a><br\/>(gain=10)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#logcontrast\">LogContrast<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/gammacontrast.gif\" height=\"148\" width=\"100\" alt=\"GammaContrast\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/gammacontrast_per_channel_true.gif\" height=\"148\" width=\"100\" alt=\"GammaContrast per_channel=True\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/sigmoidcontrast_cutoff_0_5.gif\" height=\"148\" width=\"100\" alt=\"SigmoidContrast cutoff=0.5\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/sigmoidcontrast_gain_10.gif\" height=\"148\" width=\"100\" alt=\"SigmoidContrast gain=10\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/logcontrast.gif\" height=\"148\" width=\"100\" alt=\"LogContrast\"><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#linearcontrast\">LinearContrast<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#allchannelshistogramequalization\">AllChannels-<\/a><br\/>HistogramEqualization<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#histogramequalization\">HistogramEqualization<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#allchannelsclahe\">AllChannelsCLAHE<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#clahe\">CLAHE<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/linearcontrast.gif\" height=\"148\" width=\"100\" alt=\"LinearContrast\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/allchannels_histogramequalization.gif\" height=\"148\" width=\"100\" alt=\"AllChannels- HistogramEqualization\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/histogramequalization.gif\" height=\"148\" width=\"100\" alt=\"HistogramEqualization\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/allchannelsclahe.gif\" height=\"148\" width=\"100\" alt=\"AllChannelsCLAHE\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/contrast\/clahe.gif\" height=\"148\" width=\"100\" alt=\"CLAHE\"><\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/contrast.html#equalize\">Equalize<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>convolutional<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/convolutional.html#sharpen\">Sharpen<\/a><br\/>(alpha=1)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/convolutional.html#emboss\">Emboss<\/a><br\/>(alpha=1)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/convolutional.html#edgedetect\">EdgeDetect<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/convolutional.html#directededgedetect\">DirectedEdgeDetect<\/a><br\/>(alpha=1)<\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/convolutional\/sharpen_alpha_1.gif\" height=\"148\" width=\"100\" alt=\"Sharpen alpha=1\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/convolutional\/emboss_alpha_1.gif\" height=\"148\" width=\"100\" alt=\"Emboss alpha=1\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/convolutional\/edgedetect.gif\" height=\"148\" width=\"100\" alt=\"EdgeDetect\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/convolutional\/directededgedetect_alpha_1.gif\" height=\"148\" width=\"100\" alt=\"DirectedEdgeDetect alpha=1\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/convolutional.html#convolve\">Convolve<\/a><\/td>\n<\/tr>\n<tr>\n<td colspan=\"5\"><strong>debug<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/debug.html#savedebugimageeverynbatches\">SaveDebugImageEveryNBatches<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>edges<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/edges.html#canny\">Canny<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/edges\/canny.gif\" height=\"148\" width=\"100\" alt=\"Canny\"><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>flip<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/flip.html#fliplr\">Fliplr<\/a><\/sub><\/td>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/flip.html#flipud\">Flipud<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/flip\/fliplr.gif\" height=\"148\" width=\"300\" alt=\"Fliplr\"><\/td>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/flip\/flipud.gif\" height=\"148\" width=\"300\" alt=\"Flipud\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#horizontalflip\">HorizontalFlip<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/color.html#verticalflip\">VerticalFlip<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>geometric<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#affine\">Affine<\/a><\/sub><\/td>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#affine\">Affine: Modes<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/affine.gif\" height=\"148\" width=\"300\" alt=\"Affine\"><\/td>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/affine_modes.gif\" height=\"148\" width=\"300\" alt=\"Affine: Modes\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#affine\">Affine: cval<\/a><\/sub><\/td>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#piecewiseaffine\">PiecewiseAffine<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/affine_cval.gif\" height=\"148\" width=\"300\" alt=\"Affine: cval\"><\/td>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/piecewiseaffine.gif\" height=\"148\" width=\"300\" alt=\"PiecewiseAffine\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#perspectivetransform\">PerspectiveTransform<\/a><\/sub><\/td>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#elastictransformation\">ElasticTransformation<\/a><br\/>(sigma=1.0)<\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/perspectivetransform.gif\" height=\"148\" width=\"300\" alt=\"PerspectiveTransform\"><\/td>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/elastictransformation_sigma_1_0.gif\" height=\"148\" width=\"300\" alt=\"ElasticTransformation sigma=1.0\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#elastictransformation\">ElasticTransformation<\/a><br\/>(sigma=4.0)<\/sub><\/td>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#rot90\">Rot90<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/elastictransformation_sigma_4_0.gif\" height=\"148\" width=\"300\" alt=\"ElasticTransformation sigma=4.0\"><\/td>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/rot90.gif\" height=\"148\" width=\"300\" alt=\"Rot90\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#withpolarwarping\">WithPolarWarping<\/a><br\/>+Affine<\/sub><\/td>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#jigsaw\">Jigsaw<\/a><br\/>(5x5 grid)<\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/withpolarwarping_affine.gif\" height=\"148\" width=\"300\" alt=\"WithPolarWarping +Affine\"><\/td>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/geometric\/jigsaw_5x5_grid.gif\" height=\"148\" width=\"300\" alt=\"Jigsaw 5x5 grid\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#scalex\">ScaleX<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#scaley\">ScaleY<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#translatex\">TranslateX<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#translatey\">TranslateY<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/geometric.html#rotate\">Rotate<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>imgcorruptlike<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#glassblur\">GlassBlur<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#defocusblur\">DefocusBlur<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#zoomblur\">ZoomBlur<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#snow\">Snow<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#spatter\">Spatter<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/imgcorruptlike\/glassblur.gif\" height=\"148\" width=\"100\" alt=\"GlassBlur\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/imgcorruptlike\/defocusblur.gif\" height=\"148\" width=\"100\" alt=\"DefocusBlur\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/imgcorruptlike\/zoomblur.gif\" height=\"148\" width=\"100\" alt=\"ZoomBlur\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/imgcorruptlike\/snow.gif\" height=\"148\" width=\"100\" alt=\"Snow\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/imgcorruptlike\/spatter.gif\" height=\"148\" width=\"100\" alt=\"Spatter\"><\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#gaussiannoise\">GaussianNoise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#shotnoise\">ShotNoise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#impulsenoise\">ImpulseNoise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#specklenoise\">SpeckleNoise<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#gaussianblur\">GaussianBlur<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#motionblur\">MotionBlur<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#fog\">Fog<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#frost\">Frost<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#contrast\">Contrast<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#brightness\">Brightness<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#saturate\">Saturate<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#jpegcompression\">JpegCompression<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#pixelate\">Pixelate<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/imgcorruptlike.html#elastictransform\">ElasticTransform<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>pillike<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#autocontrast\">Autocontrast<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#enhancecolor\">EnhanceColor<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#enhancesharpness\">EnhanceSharpness<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filteredgeenhancemore\">FilterEdgeEnhanceMore<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filtercontour\">FilterContour<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pillike\/autocontrast.gif\" height=\"148\" width=\"100\" alt=\"Autocontrast\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pillike\/enhancecolor.gif\" height=\"148\" width=\"100\" alt=\"EnhanceColor\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pillike\/enhancesharpness.gif\" height=\"148\" width=\"100\" alt=\"EnhanceSharpness\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pillike\/filteredgeenhancemore.gif\" height=\"148\" width=\"100\" alt=\"FilterEdgeEnhanceMore\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pillike\/filtercontour.gif\" height=\"148\" width=\"100\" alt=\"FilterContour\"><\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#solarize\">Solarize<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#posterize\">Posterize<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#equalize\">Equalize<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#enhancecontrast\">EnhanceContrast<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#enhancebrightness\">EnhanceBrightness<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filterblur\">FilterBlur<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filtersmooth\">FilterSmooth<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filtersmoothmore\">FilterSmoothMore<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filteredgeenhance\">FilterEdgeEnhance<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filterfindedges\">FilterFindEdges<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filteremboss\">FilterEmboss<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filtersharpen\">FilterSharpen<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#filterdetail\">FilterDetail<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pillike.html#affine\">Affine<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>pooling<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pooling.html#averagepooling\">AveragePooling<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pooling.html#maxpooling\">MaxPooling<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pooling.html#minpooling\">MinPooling<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/pooling.html#medianpooling\">MedianPooling<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pooling\/averagepooling.gif\" height=\"148\" width=\"100\" alt=\"AveragePooling\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pooling\/maxpooling.gif\" height=\"148\" width=\"100\" alt=\"MaxPooling\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pooling\/minpooling.gif\" height=\"148\" width=\"100\" alt=\"MinPooling\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/pooling\/medianpooling.gif\" height=\"148\" width=\"100\" alt=\"MedianPooling\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>segmentation<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/segmentation.html#superpixels\">Superpixels<\/a><br\/>(p_replace=1)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/segmentation.html#superpixels\">Superpixels<\/a><br\/>(n_segments=100)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/segmentation.html#uniformvoronoi\">UniformVoronoi<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/segmentation.html#regulargridvoronoi\">RegularGridVoronoi: rows\/cols<\/a><br\/>(p_drop_points=0)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/segmentation.html#regulargridvoronoi\">RegularGridVoronoi: p_drop_points<\/a><br\/>(n_rows=n_cols=30)<\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/segmentation\/superpixels_p_replace_1.gif\" height=\"148\" width=\"100\" alt=\"Superpixels p_replace=1\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/segmentation\/superpixels_n_segments_100.gif\" height=\"148\" width=\"100\" alt=\"Superpixels n_segments=100\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/segmentation\/uniformvoronoi.gif\" height=\"148\" width=\"100\" alt=\"UniformVoronoi\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/segmentation\/regulargridvoronoi_rows_cols_p_drop_points_0.gif\" height=\"148\" width=\"100\" alt=\"RegularGridVoronoi: rows\/cols p_drop_points=0\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/segmentation\/regulargridvoronoi_p_drop_points_n_rows_n_cols_30.gif\" height=\"148\" width=\"100\" alt=\"RegularGridVoronoi: p_drop_points n_rows=n_cols=30\"><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/segmentation.html#regulargridvoronoi\">RegularGridVoronoi: p_replace<\/a><br\/>(n_rows=n_cols=16)<\/sub><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/segmentation\/regulargridvoronoi_p_replace_n_rows_n_cols_16.gif\" height=\"148\" width=\"100\" alt=\"RegularGridVoronoi: p_replace n_rows=n_cols=16\"><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/segmentation.html#voronoi\">Voronoi<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/segmentation.html#relativeregulargridvoronoi\">RelativeRegularGridVoronoi<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_segmentation.html#imgaug.augmenters.segmentation.RegularGridPointsSampler\">RegularGridPointsSampler<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_segmentation.html#imgaug.augmenters.segmentation.RelativeRegularGridPointsSampler\">RelativeRegularGridPointsSampler<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_segmentation.html#imgaug.augmenters.segmentation.DropoutPointsSampler\">DropoutPointsSampler<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_segmentation.html#imgaug.augmenters.segmentation.UniformPointsSampler\">UniformPointsSampler<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_segmentation.html#imgaug.augmenters.segmentation.SubsamplingPointsSampler\">SubsamplingPointsSampler<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>size<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#cropandpad\">CropAndPad<\/a><\/sub><\/td>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#crop\">Crop<\/a><\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/size\/cropandpad.gif\" height=\"148\" width=\"300\" alt=\"CropAndPad\"><\/td>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/size\/crop.gif\" height=\"148\" width=\"300\" alt=\"Crop\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#pad\">Pad<\/a><\/sub><\/td>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#padtofixedsize\">PadToFixedSize<\/a><br\/>(height'=height+32,<br\/>width'=width+32)<\/sub><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/size\/pad.gif\" height=\"148\" width=\"300\" alt=\"Pad\"><\/td>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/size\/padtofixedsize_height_height_32_width_width_32.gif\" height=\"148\" width=\"300\" alt=\"PadToFixedSize height'=height+32, width'=width+32\"><\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#croptofixedsize\">CropToFixedSize<\/a><br\/>(height'=height-32,<br\/>width'=width-32)<\/sub><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n<td colspan=\"2\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/size\/croptofixedsize_height_height_32_width_width_32.gif\" height=\"148\" width=\"300\" alt=\"CropToFixedSize height'=height-32, width'=width-32\"><\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<td>&nbsp;<\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#resize\">Resize<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#croptomultiplesof\">CropToMultiplesOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#padtomultiplesof\">PadToMultiplesOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#croptopowersof\">CropToPowersOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#padtopowersof\">PadToPowersOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#croptoaspectratio\">CropToAspectRatio<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#padtoaspectratio\">PadToAspectRatio<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#croptosquare\">CropToSquare<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#padtosquare\">PadToSquare<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centercroptofixedsize\">CenterCropToFixedSize<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centerpadtofixedsize\">CenterPadToFixedSize<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centercroptomultiplesof\">CenterCropToMultiplesOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centerpadtomultiplesof\">CenterPadToMultiplesOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centercroptopowersof\">CenterCropToPowersOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centerpadtopowersof\">CenterPadToPowersOf<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centercroptoaspectratio\">CenterCropToAspectRatio<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centerpadtoaspectratio\">CenterPadToAspectRatio<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centercroptosquare\">CenterCropToSquare<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#centerpadtosquare\">CenterPadToSquare<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/size.html#keepsizebyresize\">KeepSizeByResize<\/a><\/td>\n<\/tr>\n<tr><td colspan=\"5\"><strong>weather<\/strong><\/td><\/tr>\n<tr>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/weather.html#fastsnowylandscape\">FastSnowyLandscape<\/a><br\/>(lightness_multiplier=2.0)<\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/weather.html#clouds\">Clouds<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/weather.html#fog\">Fog<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/weather.html#snowflakes\">Snowflakes<\/a><\/sub><\/td>\n<td colspan=\"1\"><sub><a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/weather.html#rain\">Rain<\/a><\/sub><\/td>\n<\/tr>\n<tr>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/weather\/fastsnowylandscape_lightness_multiplier_2_0.gif\" height=\"144\" width=\"128\" alt=\"FastSnowyLandscape lightness_multiplier=2.0\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/weather\/clouds.gif\" height=\"144\" width=\"128\" alt=\"Clouds\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/weather\/fog.gif\" height=\"144\" width=\"128\" alt=\"Fog\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/weather\/snowflakes.gif\" height=\"144\" width=\"128\" alt=\"Snowflakes\"><\/td>\n<td colspan=\"1\"><img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/augmenter_videos\/weather\/rain.gif\" height=\"144\" width=\"128\" alt=\"Rain\"><\/td>\n<\/tr>\n<tr>\n\n<\/tr>\n<tr>\n<td colspan=\"5\">See also: <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/weather.html#cloudlayer\">CloudLayer<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/weather.html#snowflakeslayer\">SnowflakesLayer<\/a>, <a href=\"https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/overview\/weather.html#rainlayer\">RainLayer<\/a><\/td>\n<\/tr>\n\n<\/table>\n\n\n\n<a name=\"code_examples\"\/>\n\n\n## Code Examples\n\n### Example: Simple Training Setting\n\nA standard machine learning situation.\nTrain on batches of images and augment each batch via crop, horizontal\nflip (\"Fliplr\") and gaussian blur:\n```python\nimport numpy as np\nimport imgaug.augmenters as iaa\n\ndef load_batch(batch_idx):\n    # dummy function, implement this\n    # Return a numpy array of shape (N, height, width, #channels)\n    # or a list of (height, width, #channels) arrays (may have different image\n    # sizes).\n    # Images should be in RGB for colorspace augmentations.\n    # (cv2.imread() returns BGR!)\n    # Images should usually be in uint8 with values from 0-255.\n    return np.zeros((128, 32, 32, 3), dtype=np.uint8) + (batch_idx % 255)\n\ndef train_on_images(images):\n    # dummy function, implement this\n    pass\n\n# Pipeline:\n# (1) Crop images from each side by 1-16px, do not resize the results\n#     images back to the input size. Keep them at the cropped size.\n# (2) Horizontally flip 50% of the images.\n# (3) Blur images using a gaussian kernel with sigma between 0.0 and 3.0.\nseq = iaa.Sequential([\n    iaa.Crop(px=(1, 16), keep_size=False),\n    iaa.Fliplr(0.5),\n    iaa.GaussianBlur(sigma=(0, 3.0))\n])\n\nfor batch_idx in range(100):\n    images = load_batch(batch_idx)\n    images_aug = seq(images=images)  # done by the library\n    train_on_images(images_aug)\n```\n\n\n### Example: Very Complex Augmentation Pipeline\n\nApply a very heavy augmentation pipeline to images (used to create the image \nat the very top of this readme):\n```python\nimport numpy as np\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\n# random example images\nimages = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n\n# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\nsometimes = lambda aug: iaa.Sometimes(0.5, aug)\n\n# Define our sequence of augmentation steps that will be applied to every image\n# All augmenters with per_channel=0.5 will sample one value _per image_\n# in 50% of all cases. In all other cases they will sample new values\n# _per channel_.\n\nseq = iaa.Sequential(\n    [\n        # apply the following augmenters to most images\n        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n        iaa.Flipud(0.2), # vertically flip 20% of all images\n        # crop images by -5% to 10% of their height\/width\n        sometimes(iaa.CropAndPad(\n            percent=(-0.05, 0.1),\n            pad_mode=ia.ALL,\n            pad_cval=(0, 255)\n        )),\n        sometimes(iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n            rotate=(-45, 45), # rotate by -45 to +45 degrees\n            shear=(-16, 16), # shear by -16 to +16 degrees\n            order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n            cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n        )),\n        # execute 0 to 5 of the following (less important) augmenters per image\n        # don't execute all of them, as that would often be way too strong\n        iaa.SomeOf((0, 5),\n            [\n                sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n                iaa.OneOf([\n                    iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n                    iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n                    iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n                ]),\n                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n                # search either for all edges or for directed edges,\n                # blend the result with the original image using a blobby mask\n                iaa.SimplexNoiseAlpha(iaa.OneOf([\n                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n                    iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n                ])),\n                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n                iaa.OneOf([\n                    iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n                    iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n                ]),\n                iaa.Invert(0.05, per_channel=True), # invert color channels\n                iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n                iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n                # either change the brightness of the whole image (sometimes\n                # per channel) or change the brightness of subareas\n                iaa.OneOf([\n                    iaa.Multiply((0.5, 1.5), per_channel=0.5),\n                    iaa.FrequencyNoiseAlpha(\n                        exponent=(-4, 0),\n                        first=iaa.Multiply((0.5, 1.5), per_channel=True),\n                        second=iaa.LinearContrast((0.5, 2.0))\n                    )\n                ]),\n                iaa.LinearContrast((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n                iaa.Grayscale(alpha=(0.0, 1.0)),\n                sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n            ],\n            random_order=True\n        )\n    ],\n    random_order=True\n)\nimages_aug = seq(images=images)\n```\n\n\n### Example: Augment Images and Keypoints\n\nAugment images and keypoints\/landmarks on the same images:\n```python\nimport numpy as np\nimport imgaug.augmenters as iaa\n\nimages = np.zeros((2, 128, 128, 3), dtype=np.uint8)  # two example images\nimages[:, 64, 64, :] = 255\npoints = [\n    [(10.5, 20.5)],  # points on first image\n    [(50.5, 50.5), (60.5, 60.5), (70.5, 70.5)]  # points on second image\n]\n\nseq = iaa.Sequential([\n    iaa.AdditiveGaussianNoise(scale=0.05*255),\n    iaa.Affine(translate_px={\"x\": (1, 5)})\n])\n\n# augment keypoints and images\nimages_aug, points_aug = seq(images=images, keypoints=points)\n\nprint(\"Image 1 center\", np.argmax(images_aug[0, 64, 64:64+6, 0]))\nprint(\"Image 2 center\", np.argmax(images_aug[1, 64, 64:64+6, 0]))\nprint(\"Points 1\", points_aug[0])\nprint(\"Points 2\", points_aug[1])\n```\nNote that all coordinates in `imgaug` are subpixel-accurate, which is\nwhy `x=0.5, y=0.5` denotes the center of the top left pixel.\n\n\n### Example: Augment Images and Bounding Boxes\n\n```python\nimport numpy as np\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\nimages = np.zeros((2, 128, 128, 3), dtype=np.uint8)  # two example images\nimages[:, 64, 64, :] = 255\nbbs = [\n    [ia.BoundingBox(x1=10.5, y1=15.5, x2=30.5, y2=50.5)],\n    [ia.BoundingBox(x1=10.5, y1=20.5, x2=50.5, y2=50.5),\n     ia.BoundingBox(x1=40.5, y1=75.5, x2=70.5, y2=100.5)]\n]\n\nseq = iaa.Sequential([\n    iaa.AdditiveGaussianNoise(scale=0.05*255),\n    iaa.Affine(translate_px={\"x\": (1, 5)})\n])\n\nimages_aug, bbs_aug = seq(images=images, bounding_boxes=bbs)\n```\n\n\n### Example: Augment Images and Polygons\n\n```python\nimport numpy as np\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\nimages = np.zeros((2, 128, 128, 3), dtype=np.uint8)  # two example images\nimages[:, 64, 64, :] = 255\npolygons = [\n    [ia.Polygon([(10.5, 10.5), (50.5, 10.5), (50.5, 50.5)])],\n    [ia.Polygon([(0.0, 64.5), (64.5, 0.0), (128.0, 128.0), (64.5, 128.0)])]\n]\n\nseq = iaa.Sequential([\n    iaa.AdditiveGaussianNoise(scale=0.05*255),\n    iaa.Affine(translate_px={\"x\": (1, 5)})\n])\n\nimages_aug, polygons_aug = seq(images=images, polygons=polygons)\n```\n\n\n### Example: Augment Images and LineStrings\n\nLineStrings are similar to polygons, but are not closed, may intersect with\nthemselves and don't have an inner area.\n```python\nimport numpy as np\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\nimages = np.zeros((2, 128, 128, 3), dtype=np.uint8)  # two example images\nimages[:, 64, 64, :] = 255\nls = [\n    [ia.LineString([(10.5, 10.5), (50.5, 10.5), (50.5, 50.5)])],\n    [ia.LineString([(0.0, 64.5), (64.5, 0.0), (128.0, 128.0), (64.5, 128.0),\n                    (128.0, 0.0)])]\n]\n\nseq = iaa.Sequential([\n    iaa.AdditiveGaussianNoise(scale=0.05*255),\n    iaa.Affine(translate_px={\"x\": (1, 5)})\n])\n\nimages_aug, ls_aug = seq(images=images, line_strings=ls)\n```\n\n\n### Example: Augment Images and Heatmaps\n\nHeatmaps are dense float arrays with values between `0.0` and `1.0`.\nThey can be used e.g. when training models to predict facial landmark\nlocations. Note that the heatmaps here have lower height and width than the\nimages. `imgaug` handles that case automatically. The crop pixel amounts will\nbe halved for the heatmaps.\n\n```python\nimport numpy as np\nimport imgaug.augmenters as iaa\n\n# Standard scenario: You have N RGB-images and additionally 21 heatmaps per\n# image. You want to augment each image and its heatmaps identically.\nimages = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\nheatmaps = np.random.random(size=(16, 64, 64, 1)).astype(np.float32)\n\nseq = iaa.Sequential([\n    iaa.GaussianBlur((0, 3.0)),\n    iaa.Affine(translate_px={\"x\": (-40, 40)}),\n    iaa.Crop(px=(0, 10))\n])\n\nimages_aug, heatmaps_aug = seq(images=images, heatmaps=heatmaps)\n```\n\n\n### Example: Augment Images and Segmentation Maps\n\nThis is similar to heatmaps, but the dense arrays have dtype `int32`.\nOperations such as resizing will automatically use nearest neighbour\ninterpolation.\n\n```python\nimport numpy as np\nimport imgaug.augmenters as iaa\n\n# Standard scenario: You have N=16 RGB-images and additionally one segmentation\n# map per image. You want to augment each image and its heatmaps identically.\nimages = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\nsegmaps = np.random.randint(0, 10, size=(16, 64, 64, 1), dtype=np.int32)\n\nseq = iaa.Sequential([\n    iaa.GaussianBlur((0, 3.0)),\n    iaa.Affine(translate_px={\"x\": (-40, 40)}),\n    iaa.Crop(px=(0, 10))\n])\n\nimages_aug, segmaps_aug = seq(images=images, segmentation_maps=segmaps)\n```\n\n\n### Example: Visualize Augmented Images\n\nQuickly show example results of your augmentation sequence:\n```python\nimport numpy as np\nimport imgaug.augmenters as iaa\n\nimages = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\nseq = iaa.Sequential([iaa.Fliplr(0.5), iaa.GaussianBlur((0, 3.0))])\n\n# Show an image with 8*8 augmented versions of image 0 and 8*8 augmented\n# versions of image 1. Identical augmentations will be applied to\n# image 0 and 1.\nseq.show_grid([images[0], images[1]], cols=8, rows=8)\n```\n\n\n### Example: Visualize Augmented Non-Image Data\n\n`imgaug` contains many helper function, among these functions to quickly\nvisualize augmented non-image results, such as bounding boxes or heatmaps.\n\n```python\nimport numpy as np\nimport imgaug as ia\n\nimage = np.zeros((64, 64, 3), dtype=np.uint8)\n\n# points\nkps = [ia.Keypoint(x=10.5, y=20.5), ia.Keypoint(x=60.5, y=60.5)]\nkpsoi = ia.KeypointsOnImage(kps, shape=image.shape)\nimage_with_kps = kpsoi.draw_on_image(image, size=7, color=(0, 0, 255))\nia.imshow(image_with_kps)\n\n# bbs\nbbsoi = ia.BoundingBoxesOnImage([\n    ia.BoundingBox(x1=10.5, y1=20.5, x2=50.5, y2=30.5)\n], shape=image.shape)\nimage_with_bbs = bbsoi.draw_on_image(image)\nimage_with_bbs = ia.BoundingBox(\n    x1=50.5, y1=10.5, x2=100.5, y2=16.5\n).draw_on_image(image_with_bbs, color=(255, 0, 0), size=3)\nia.imshow(image_with_bbs)\n\n# polygons\npsoi = ia.PolygonsOnImage([\n    ia.Polygon([(10.5, 20.5), (50.5, 30.5), (10.5, 50.5)])\n], shape=image.shape)\nimage_with_polys = psoi.draw_on_image(\n    image, alpha_points=0, alpha_face=0.5, color_lines=(255, 0, 0))\nia.imshow(image_with_polys)\n\n# heatmaps\nhms = ia.HeatmapsOnImage(np.random.random(size=(32, 32, 1)).astype(np.float32),\n                         shape=image.shape)\nimage_with_hms = hms.draw_on_image(image)\nia.imshow(image_with_hms)\n```\n\nLineStrings and segmentation maps support similar methods as shown above.\n\n\n### Example: Using Augmenters Only Once \n\nWhile the interface is adapted towards re-using instances of augmenters\nmany times, you are also free to use them only once. The overhead to\ninstantiate the augmenters each time is usually negligible.\n\n```python\nfrom imgaug import augmenters as iaa\nimport numpy as np\n\nimages = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n\n# always horizontally flip each input image\nimages_aug = iaa.Fliplr(1.0)(images=images)\n\n# vertically flip each input image with 90% probability\nimages_aug = iaa.Flipud(0.9)(images=images)\n\n# blur 50% of all images using a gaussian kernel with a sigma of 3.0\nimages_aug = iaa.Sometimes(0.5, iaa.GaussianBlur(3.0))(images=images)\n```\n\n\n### Example: Multicore Augmentation\n\nImages can be augmented in **background processes** using the\nmethod `augment_batches(batches, background=True)`, where `batches` is\na list\/generator of\n[imgaug.augmentables.batches.UnnormalizedBatch](https:\/\/imgaug.readthedocs.io\/en\/latest\/_modules\/imgaug\/augmentables\/batches.html#UnnormalizedBatch)\nor\n[imgaug.augmentables.batches.Batch](https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmentables_batches.html#imgaug.augmentables.batches.Batch).\nThe following example augments a list of image batches in the background:\n```python\nimport skimage.data\nimport imgaug as ia\nimport imgaug.augmenters as iaa\nfrom imgaug.augmentables.batches import UnnormalizedBatch\n\n# Number of batches and batch size for this example\nnb_batches = 10\nbatch_size = 32\n\n# Example augmentation sequence to run in the background\naugseq = iaa.Sequential([\n    iaa.Fliplr(0.5),\n    iaa.CoarseDropout(p=0.1, size_percent=0.1)\n])\n\n# For simplicity, we use the same image here many times\nastronaut = skimage.data.astronaut()\nastronaut = ia.imresize_single_image(astronaut, (64, 64))\n\n# Make batches out of the example image (here: 10 batches, each 32 times\n# the example image)\nbatches = []\nfor _ in range(nb_batches):\n    batches.append(UnnormalizedBatch(images=[astronaut] * batch_size))\n\n# Show the augmented images.\n# Note that augment_batches() returns a generator.\nfor images_aug in augseq.augment_batches(batches, background=True):\n    ia.imshow(ia.draw_grid(images_aug.images_aug, cols=8))\n```\n\nIf you need more control over the background augmentation, e.g. to set\nseeds, control the number of used CPU cores or constraint the memory usage,\nsee the corresponding\n[multicore augmentation notebook](https:\/\/nbviewer.jupyter.org\/github\/aleju\/imgaug-doc\/blob\/master\/notebooks\/A03%20-%20Multicore%20Augmentation.ipynb)\nor the API about\n[Augmenter.pool()](https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_augmenters_meta.html#imgaug.augmenters.meta.Augmenter.pool)\nand\n[imgaug.multicore.Pool](https:\/\/imgaug.readthedocs.io\/en\/latest\/source\/api_multicore.html#imgaug.multicore.Pool).\n\n\n### Example: Probability Distributions as Parameters\n\nMost augmenters support using tuples `(a, b)` as a shortcut to denote\n`uniform(a, b)` or lists `[a, b, c]` to denote a set of allowed values from\nwhich one will be picked randomly. If you require more complex probability\ndistributions (e.g. gaussians, truncated gaussians or poisson distributions)\nyou can use stochastic parameters from `imgaug.parameters`:\n\n```python\nimport numpy as np\nfrom imgaug import augmenters as iaa\nfrom imgaug import parameters as iap\n\nimages = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n\n# Blur by a value sigma which is sampled from a uniform distribution\n# of range 10.1 <= x < 13.0.\n# The convenience shortcut for this is: GaussianBlur((10.1, 13.0))\nblurer = iaa.GaussianBlur(10 + iap.Uniform(0.1, 3.0))\nimages_aug = blurer(images=images)\n\n# Blur by a value sigma which is sampled from a gaussian distribution\n# N(1.0, 0.1), i.e. sample a value that is usually around 1.0.\n# Clip the resulting value so that it never gets below 0.1 or above 3.0.\nblurer = iaa.GaussianBlur(iap.Clip(iap.Normal(1.0, 0.1), 0.1, 3.0))\nimages_aug = blurer(images=images)\n```\n\nThere are many more probability distributions in the library, e.g. truncated\ngaussian distribution, poisson distribution or beta distribution.\n\n\n### Example: WithChannels\n\nApply an augmenter only to specific image channels:\n```python\nimport numpy as np\nimport imgaug.augmenters as iaa\n\n# fake RGB images\nimages = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n\n# add a random value from the range (-30, 30) to the first two channels of\n# input images (e.g. to the R and G channels)\naug = iaa.WithChannels(\n  channels=[0, 1],\n  children=iaa.Add((-30, 30))\n)\n\nimages_aug = aug(images=images)\n```\n\n\n<a name=\"citation\"\/>\n\n## Citation\n\n<!--\nNote: the table only lists people who have their real names (publicly)\nset in their github\n\nList of username-realname matching based on\nhttps:\/\/github.com\/aleju\/imgaug\/graphs\/contributors ordered by commits:\n\nwkentaro            Wada, Kentaro\nErotemic            Crall, Jon\nstnk20              Tanaka, Satoshi\njgraving            Graving, Jake\ncreinders           Reinders, Christoph     (lastname not public on github, guessed from username)\nSarthakYadav        Yadav, Sarthak\nnektor211           ?\njoybanerjee08       Banerjee, Joy\ngaborvecsei         Vecsei, G\u00e1bor\nadamwkraft          Kraft, Adam\nZhengRui            Rui, Zheng\nBorda               Borovec, Jirka\nvallentin           Vallentin, Christian\nss18                Zhydenko, Semen\nkilsenp             Pfeiffer, Kilian\nkacper1095          ?\nismaelfm            Fern\u00e1ndez, Ismael\nfmder               De Rainville, Fran\u00e7ois-Michel\nfchouteau           ?\nchi-hung            Weng, Chi-Hung\napatsekin           ?\nabnera              Ayala-Acevedo, Abner\nRephaelMeudec       Meudec, Raphael\nPetemir             Laporte, Matias\n\n-->\nIf this library has helped you during your research, feel free to cite it:\n```latex\n@misc{imgaug,\n  author = {Jung, Alexander B.\n            and Wada, Kentaro\n            and Crall, Jon\n            and Tanaka, Satoshi\n            and Graving, Jake\n            and Reinders, Christoph\n            and Yadav, Sarthak\n            and Banerjee, Joy\n            and Vecsei, G\u00e1bor\n            and Kraft, Adam\n            and Rui, Zheng\n            and Borovec, Jirka\n            and Vallentin, Christian\n            and Zhydenko, Semen\n            and Pfeiffer, Kilian\n            and Cook, Ben\n            and Fern\u00e1ndez, Ismael\n            and De Rainville, Fran\u00e7ois-Michel\n            and Weng, Chi-Hung\n            and Ayala-Acevedo, Abner\n            and Meudec, Raphael\n            and Laporte, Matias\n            and others},\n  title = {{imgaug}},\n  howpublished = {\\url{https:\/\/github.com\/aleju\/imgaug}},\n  year = {2020},\n  note = {Online; accessed 01-Feb-2020}\n}\n```\n","107":"![ChatterBot: Machine learning in Python](https:\/\/i.imgur.com\/b3SCmGT.png)\n\n# ChatterBot\n\nChatterBot is a machine-learning based conversational dialog engine build in\nPython which makes it possible to generate responses based on collections of\nknown conversations. The language independent design of ChatterBot allows it\nto be trained to speak any language.\n\n[![Package Version](https:\/\/img.shields.io\/pypi\/v\/chatterbot.svg)](https:\/\/pypi.python.org\/pypi\/chatterbot\/)\n[![Python 3.6](https:\/\/img.shields.io\/badge\/python-3.6-blue.svg)](https:\/\/www.python.org\/downloads\/release\/python-360\/)\n[![Django 2.0](https:\/\/img.shields.io\/badge\/Django-2.0-blue.svg)](https:\/\/docs.djangoproject.com\/en\/2.1\/releases\/2.0\/)\n[![Requirements Status](https:\/\/requires.io\/github\/gunthercox\/ChatterBot\/requirements.svg?branch=master)](https:\/\/requires.io\/github\/gunthercox\/ChatterBot\/requirements\/?branch=master)\n[![Build Status](https:\/\/travis-ci.org\/gunthercox\/ChatterBot.svg?branch=master)](https:\/\/travis-ci.org\/gunthercox\/ChatterBot)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/chatterbot\/badge\/?version=stable)](http:\/\/chatterbot.readthedocs.io\/en\/stable\/?badge=stable)\n[![Coverage Status](https:\/\/img.shields.io\/coveralls\/gunthercox\/ChatterBot.svg)](https:\/\/coveralls.io\/r\/gunthercox\/ChatterBot)\n[![Code Climate](https:\/\/codeclimate.com\/github\/gunthercox\/ChatterBot\/badges\/gpa.svg)](https:\/\/codeclimate.com\/github\/gunthercox\/ChatterBot)\n[![Join the chat at https:\/\/gitter.im\/chatterbot\/Lobby](https:\/\/badges.gitter.im\/chatterbot\/Lobby.svg)](https:\/\/gitter.im\/chatterbot\/Lobby?utm_source=badge&utm_medium=badge&utm_content=badge)\n\nAn example of typical input would be something like this:\n\n> **user:** Good morning! How are you doing?  \n> **bot:**  I am doing very well, thank you for asking.  \n> **user:** You're welcome.  \n> **bot:** Do you like hats?  \n\n## How it works\n\nAn untrained instance of ChatterBot starts off with no knowledge of how to communicate. Each time a user enters a statement, the library saves the text that they entered and the text that the statement was in response to. As ChatterBot receives more input the number of responses that it can reply and the accuracy of each response in relation to the input statement increase. The program selects the closest matching response by searching for the closest matching known statement that matches the input, it then returns the most likely response to that statement based on how frequently each response is issued by the people the bot communicates with.\n\n## Installation\n\nThis package can be installed from [PyPi](https:\/\/pypi.python.org\/pypi\/ChatterBot) by running:\n\n```\npip install chatterbot\n```\n\n## Basic Usage\n\n```\nfrom chatterbot import ChatBot\nfrom chatterbot.trainers import ChatterBotCorpusTrainer\n\nchatbot = ChatBot('Ron Obvious')\n\n# Create a new trainer for the chatbot\ntrainer = ChatterBotCorpusTrainer(chatbot)\n\n# Train the chatbot based on the english corpus\ntrainer.train(\"chatterbot.corpus.english\")\n\n# Get a response to an input statement\nchatbot.get_response(\"Hello, how are you today?\")\n```\n\n# Training data\n\nChatterBot comes with a data utility module that can be used to train chat bots.\nAt the moment there is training data for over a dozen languages in this module.\nContributions of additional training data or training data\nin other languages would be greatly appreciated. Take a look at the data files\nin the [chatterbot-corpus](https:\/\/github.com\/gunthercox\/chatterbot-corpus)\npackage if you are interested in contributing.\n\n```\nfrom chatterbot.trainers import ChatterBotCorpusTrainer\n\n# Create a new trainer for the chatbot\ntrainer = ChatterBotCorpusTrainer(chatbot)\n\n# Train based on the english corpus\ntrainer.train(\"chatterbot.corpus.english\")\n\n# Train based on english greetings corpus\ntrainer.train(\"chatterbot.corpus.english.greetings\")\n\n# Train based on the english conversations corpus\ntrainer.train(\"chatterbot.corpus.english.conversations\")\n```\n\n**Corpus contributions are welcome! Please make a pull request.**\n\n# [Documentation](https:\/\/chatterbot.readthedocs.io\/)\n\nView the [documentation](https:\/\/chatterbot.readthedocs.io\/)\nfor ChatterBot on Read the Docs.\n\nTo build the documentation yourself using [Sphinx](http:\/\/www.sphinx-doc.org\/), run:\n\n```\nsphinx-build -b html docs\/ build\/\n```\n\n# Examples\n\nFor examples, see the [examples](https:\/\/github.com\/gunthercox\/ChatterBot\/tree\/master\/examples)\ndirectory in this project's git repository.\n\nThere is also an example [Django project using ChatterBot](https:\/\/github.com\/gunthercox\/ChatterBot\/tree\/master\/examples), as well as an example [Flask project using ChatterBot](https:\/\/github.com\/chamkank\/flask-chatterbot).\n\n# History\n\nSee release notes for changes https:\/\/github.com\/gunthercox\/ChatterBot\/releases\n\n# Development pattern for contributors\n\n1. [Create a fork](https:\/\/help.github.com\/articles\/fork-a-repo\/) of\n   the [main ChatterBot repository](https:\/\/github.com\/gunthercox\/ChatterBot) on GitHub.\n2. Make your changes in a branch named something different from `master`, e.g. create\n   a new branch `my-pull-request`.\n3. [Create a pull request](https:\/\/help.github.com\/articles\/creating-a-pull-request\/).\n4. Please follow the [Python style guide for PEP-8](https:\/\/www.python.org\/dev\/peps\/pep-0008\/).\n5. Use the projects [built-in automated testing](https:\/\/chatterbot.readthedocs.io\/en\/latest\/testing.html).\n   to help make sure that your contribution is free from errors.\n\n# License\n\nChatterBot is licensed under the [BSD 3-clause license](https:\/\/opensource.org\/licenses\/BSD-3-Clause).\n","108":"=============================================\nMLflow: A Machine Learning Lifecycle Platform\n=============================================\n\nMLflow is a platform to streamline machine learning development, including tracking experiments, packaging code\ninto reproducible runs, and sharing and deploying models. MLflow offers a set of lightweight APIs that can be\nused with any existing machine learning application or library (TensorFlow, PyTorch, XGBoost, etc), wherever you\ncurrently run ML code (e.g. in notebooks, standalone applications or the cloud). MLflow's current components are:\n\n* `MLflow Tracking <https:\/\/mlflow.org\/docs\/latest\/tracking.html>`_: An API to log parameters, code, and\n  results in machine learning experiments and compare them using an interactive UI.\n* `MLflow Projects <https:\/\/mlflow.org\/docs\/latest\/projects.html>`_: A code packaging format for reproducible\n  runs using Conda and Docker, so you can share your ML code with others.\n* `MLflow Models <https:\/\/mlflow.org\/docs\/latest\/models.html>`_: A model packaging format and tools that let\n  you easily deploy the same model (from any ML library) to batch and real-time scoring on platforms such as\n  Docker, Apache Spark, Azure ML and AWS SageMaker.\n* `MLflow Model Registry <https:\/\/mlflow.org\/docs\/latest\/model-registry.html>`_: A centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of MLflow Models.\n\n|docs| |labeling| |examples| |cross-version-tests| |r-devel| |pypi| |conda-forge| |cran| |maven| |license| |downloads| |slack| |twitter|\n\n.. |docs| image:: https:\/\/img.shields.io\/badge\/docs-latest-success.svg?style=for-the-badge\n    :target: https:\/\/mlflow.org\/docs\/latest\/index.html\n    :alt: Latest Docs\n.. |labeling| image:: https:\/\/img.shields.io\/github\/workflow\/status\/mlflow\/mlflow\/Labeling?label=Labeling&style=for-the-badge&logo=github\n    :target: https:\/\/github.com\/mlflow\/mlflow\/actions?query=workflow%3ALabeling\n    :alt: Labeling Action Status\n.. |examples| image:: https:\/\/img.shields.io\/github\/workflow\/status\/mlflow\/mlflow\/Examples?event=schedule&label=Examples&style=for-the-badge&logo=github\n    :target: https:\/\/github.com\/mlflow\/mlflow\/actions?query=workflow%3AExamples+event%3Aschedule\n    :alt: Examples Action Status\n.. |cross-version-tests| image:: https:\/\/img.shields.io\/github\/workflow\/status\/mlflow\/mlflow\/Cross%20version%20tests?event=schedule&label=Cross%20version%20tests&style=for-the-badge&logo=github\n    :target: https:\/\/github.com\/mlflow\/mlflow\/actions?query=workflow%3ACross%2Bversion%2Btests+event%3Aschedule\n.. |r-devel| image:: https:\/\/img.shields.io\/github\/workflow\/status\/mlflow\/mlflow\/R?event=schedule&label=r-devel&style=for-the-badge&logo=github\n    :target: https:\/\/github.com\/mlflow\/mlflow\/actions?query=workflow%3AR+event%3Aschedule\n    :alt: Examples Action Status\n.. |pypi| image:: https:\/\/img.shields.io\/pypi\/v\/mlflow.svg?style=for-the-badge&logo=pypi&logoColor=white\n    :target: https:\/\/pypi.org\/project\/mlflow\/\n    :alt: Latest Python Release\n.. |conda-forge| image:: https:\/\/img.shields.io\/conda\/vn\/conda-forge\/mlflow.svg?style=for-the-badge&logo=anaconda\n    :target: https:\/\/anaconda.org\/conda-forge\/mlflow\n    :alt: Latest Conda Release\n.. |cran| image:: https:\/\/img.shields.io\/cran\/v\/mlflow.svg?style=for-the-badge&logo=r\n    :target: https:\/\/cran.r-project.org\/package=mlflow\n    :alt: Latest CRAN Release\n.. |maven| image:: https:\/\/img.shields.io\/maven-central\/v\/org.mlflow\/mlflow-parent.svg?style=for-the-badge&logo=apache-maven\n    :target: https:\/\/mvnrepository.com\/artifact\/org.mlflow\n    :alt: Maven Central\n.. |license| image:: https:\/\/img.shields.io\/badge\/license-Apache%202-brightgreen.svg?style=for-the-badge&logo=apache\n    :target: https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/LICENSE.txt\n    :alt: Apache 2 License\n.. |downloads| image:: https:\/\/img.shields.io\/pypi\/dw\/mlflow?style=for-the-badge&logo=pypi&logoColor=white\n    :target: https:\/\/pepy.tech\/project\/mlflow\n    :alt: Total Downloads\n.. |slack| image:: https:\/\/img.shields.io\/badge\/slack-@mlflow--users-CF0E5B.svg?logo=slack&logoColor=white&labelColor=3F0E40&style=for-the-badge\n    :target: `Slack`_\n    :alt: Slack\n.. |twitter| image:: https:\/\/img.shields.io\/twitter\/follow\/MLflow?style=for-the-badge&labelColor=00ACEE&logo=twitter&logoColor=white\n    :target: https:\/\/twitter.com\/MLflow\n    :alt: Account Twitter\n\n\n\n.. _Slack: https:\/\/join.slack.com\/t\/mlflow-users\/shared_invite\/zt-g6qwro5u-odM7pRnZxNX_w56mcsHp8g\n\nInstalling\n----------\nInstall MLflow from PyPI via ``pip install mlflow``\n\nMLflow requires ``conda`` to be on the ``PATH`` for the projects feature.\n\nNightly snapshots of MLflow master are also available `here <https:\/\/mlflow-snapshots.s3-us-west-2.amazonaws.com\/>`_.\n\nInstall a lower dependency subset of MLflow from PyPI via ``pip install mlflow-skinny``\nExtra dependencies can be added per desired scenario.\nFor example, ``pip install mlflow-skinny pandas numpy`` allows for mlflow.pyfunc.log_model support.\n\nDocumentation\n-------------\nOfficial documentation for MLflow can be found at https:\/\/mlflow.org\/docs\/latest\/index.html.\n\nRoadmap\n-------\nThe current MLflow Roadmap is available at https:\/\/github.com\/mlflow\/mlflow\/milestone\/3. We are\nseeking contributions to all of our roadmap items with the ``help wanted`` label. Please see the\n`Contributing`_ section for more information.\n\nCommunity\n---------\nFor help or questions about MLflow usage (e.g. \"how do I do X?\") see the `docs <https:\/\/mlflow.org\/docs\/latest\/index.html>`_\nor `Stack Overflow <https:\/\/stackoverflow.com\/questions\/tagged\/mlflow>`_.\n\nTo report a bug, file a documentation issue, or submit a feature request, please open a GitHub issue.\n\nFor release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\nor join us on `Slack`_.\n\nRunning a Sample App With the Tracking API\n------------------------------------------\nThe programs in ``examples`` use the MLflow Tracking API. For instance, run::\n\n    python examples\/quickstart\/mlflow_tracking.py\n\nThis program will use `MLflow Tracking API <https:\/\/mlflow.org\/docs\/latest\/tracking.html>`_,\nwhich logs tracking data in ``.\/mlruns``. This can then be viewed with the Tracking UI.\n\n\nLaunching the Tracking UI\n-------------------------\nThe MLflow Tracking UI will show runs logged in ``.\/mlruns`` at `<http:\/\/localhost:5000>`_.\nStart it with::\n\n    mlflow ui\n\n**Note:** Running ``mlflow ui`` from within a clone of MLflow is not recommended - doing so will\nrun the dev UI from source. We recommend running the UI from a different working directory,\nspecifying a backend store via the ``--backend-store-uri`` option. Alternatively, see\ninstructions for running the dev UI in the `contributor guide <CONTRIBUTING.rst>`_.\n\n\nRunning a Project from a URI\n----------------------------\nThe ``mlflow run`` command lets you run a project packaged with a MLproject file from a local path\nor a Git URI::\n\n    mlflow run examples\/sklearn_elasticnet_wine -P alpha=0.4\n\n    mlflow run https:\/\/github.com\/mlflow\/mlflow-example.git -P alpha=0.4\n\nSee ``examples\/sklearn_elasticnet_wine`` for a sample project with an MLproject file.\n\n\nSaving and Serving Models\n-------------------------\nTo illustrate managing models, the ``mlflow.sklearn`` package can log scikit-learn models as\nMLflow artifacts and then load them again for serving. There is an example training application in\n``examples\/sklearn_logistic_regression\/train.py`` that you can run as follows::\n\n    $ python examples\/sklearn_logistic_regression\/train.py\n    Score: 0.666\n    Model saved in run <run-id>\n\n    $ mlflow models serve --model-uri runs:\/<run-id>\/model\n\n    $ curl -d '{\"columns\":[0],\"index\":[0,1],\"data\":[[1],[-1]]}' -H 'Content-Type: application\/json'  localhost:5000\/invocations\n\n\nContributing\n------------\nWe happily welcome contributions to MLflow. We are also seeking contributions to items on the\n`MLflow Roadmap <https:\/\/github.com\/mlflow\/mlflow\/milestone\/3>`_. Please see our\n`contribution guide <CONTRIBUTING.rst>`_ to learn more about contributing to MLflow.","109":"<div align=\"center\">\n<img src=\"docs\/img\/nni_logo.png\" width=\"600\"\/>\n<\/div>\n\n<br\/>\n\n[![MIT licensed](https:\/\/img.shields.io\/badge\/license-MIT-brightgreen.svg)](LICENSE)\n[![Issues](https:\/\/img.shields.io\/github\/issues-raw\/Microsoft\/nni.svg)](https:\/\/github.com\/Microsoft\/nni\/issues?q=is%3Aissue+is%3Aopen)\n[![Bugs](https:\/\/img.shields.io\/github\/issues\/Microsoft\/nni\/bug.svg)](https:\/\/github.com\/Microsoft\/nni\/issues?q=is%3Aissue+is%3Aopen+label%3Abug)\n[![Pull Requests](https:\/\/img.shields.io\/github\/issues-pr-raw\/Microsoft\/nni.svg)](https:\/\/github.com\/Microsoft\/nni\/pulls?q=is%3Apr+is%3Aopen)\n[![Version](https:\/\/img.shields.io\/github\/release\/Microsoft\/nni.svg)](https:\/\/github.com\/Microsoft\/nni\/releases)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/nni\/badge\/?version=stable)](https:\/\/nni.readthedocs.io\/en\/stable\/?badge=stable)\n[![](https:\/\/img.shields.io\/github\/contributors-anon\/microsoft\/nni)](https:\/\/github.com\/microsoft\/nni\/graphs\/contributors)\n\n\n\n[<img src=\"docs\/img\/readme_banner.png\" width=\"100%\"\/>](https:\/\/nni.readthedocs.io\/en\/stable)\n\nNNI automates feature engineering, neural architecture search, hyperparameter tuning, and model compression for deep learning. Find the latest features, API, examples and tutorials in our **[official documentation](https:\/\/nni.readthedocs.io\/) ([\u7b80\u4f53\u4e2d\u6587\u7248\u70b9\u8fd9\u91cc](https:\/\/nni.readthedocs.io\/zh\/stable))**.\n\n## What's NEW! &nbsp;<a href=\"#nni-released-reminder\"><img width=\"48\" src=\"docs\/img\/release_icon.png\"><\/a>\n\n* **New release**: [v2.7 is available](https:\/\/github.com\/microsoft\/nni\/releases\/tag\/v2.7) - _released on Apr-18-2022_\n* **New demo available**: [Youtube entry](https:\/\/www.youtube.com\/channel\/UCKcafm6861B2mnYhPbZHavw) | [Bilibili \u5165\u53e3](https:\/\/space.bilibili.com\/1649051673) - _last updated on Apr-18-2022_\n* **New webinar**: [Introducing Retiarii: A deep learning exploratory-training framework on NNI](https:\/\/note.microsoft.com\/MSR-Webinar-Retiarii-Registration-Live.html) - _scheduled on June-24-2021_\n* **Newly upgraded documentation**: [Doc upgraded](https:\/\/nni.readthedocs.io\/en\/stable)\n\n\n## Installation\n\nSee the [NNI installation guide](https:\/\/nni.readthedocs.io\/en\/stable\/installation.html) to install from pip, or build from source.\n\nTo install the current release:\n\n```\n$ pip install nni\n```\n\nTo update NNI to the latest version, add `--upgrade` flag to the above commands.\n\n## NNI capabilities in a glance\n\n<img src=\"docs\/img\/overview.svg\" width=\"100%\"\/>\n\n<table>\n<tbody>\n<tr align=\"center\" valign=\"bottom\">\n<td><\/td>\n<td>\n<b>Hyperparameter Tuning<\/b>\n<img src=\"docs\/img\/bar.png\" \/>\n<\/td>\n<td>\n<b>Neural Architecture Search<\/b>\n<img src=\"docs\/img\/bar.png\" \/>\n<\/td>\n<td>\n<b>Model Compression<\/b>\n<img src=\"docs\/img\/bar.png\" \/>\n<\/td>\n<\/tr>\n<tr valign=\"top\">\n<td align=\"center\" valign=\"middle\">\n<b>Algorithms<\/b>\n<\/td>\n<td>\n<ul>\n<li><b>Exhaustive search<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.gridsearch_tuner.GridSearchTuner\">Grid Search<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.random_tuner.RandomTuner\">Random<\/a><\/li>\n<\/ul>\n<li><b>Heuristic search<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.hyperopt_tuner.HyperoptTuner\">Anneal<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.evolution_tuner.EvolutionTuner\">Evolution<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.hyperband_advisor.Hyperband\">Hyperband<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.pbt_tuner.PBTTuner\">PBT<\/a><\/li>\n<\/ul>\n<li><b>Bayesian optimization<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.bohb_advisor.BOHB\">BOHB<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.dngo_tuner.DNGOTuner\">DNGO<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.gp_tuner.GPTuner\">GP<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.metis_tuner.MetisTuner\">Metis<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.smac_tuner.SMACTuner\">SMAC<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/reference\/hpo.html#nni.algorithms.hpo.tpe_tuner.TpeTuner\">TPE<\/a><\/li>\n<\/ul>\n<\/ul>\n<\/td>\n<td>\n<ul>\n<li><b>Multi-trial<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#grid-search-strategy\">Grid Search<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#policy-based-rl-strategy\">Policy Based RL<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#random-strategy\">Random<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#regularized-evolution-strategy\">Regularized Evolution<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#tpe-strategy\">TPE<\/a><\/li>\n<\/ul>\n<li><b>One-shot<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#darts-strategy\">DARTS<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#enas-strategy\">ENAS<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#fbnet-strategy\">FBNet<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#proxylessnas-strategy\">ProxylessNAS<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/nas\/exploration_strategy.html#spos-strategy\">SPOS<\/a><\/li>\n<\/ul>\n<\/ul>\n<\/td>\n<td>\n<ul>\n<li><b>Pruning<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/pruner.html#level-pruner\">Level<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/pruner.html#l1-norm-pruner\">L1 Norm<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/pruner.html#taylor-fo-weight-pruner\">Taylor FO Weight<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/pruner.html#movement-pruner\">Movement<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/pruner.html#agp-pruner\">AGP<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/pruner.html#auto-compress-pruner\">Auto Compress<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/index.html\">More...<\/a><\/li>\n<\/ul>\n<li><b>Quantization<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/quantizer.html#naive-quantizer\">Naive<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/quantizer.html#qat-quantizer\">QAT<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/quantizer.html#lsq-quantizer\">LSQ<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/quantizer.html#observer-quantizer\">Observer<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/quantizer.html#dorefa-quantizer\">DoReFa<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/compression\/quantizer.html#bnn-quantizer\">BNN<\/a><\/li>\n<\/ul>\n<\/ul>\n<\/td>\n<tr align=\"center\" valign=\"bottom\">\n<td><\/td>\n<td>\n<b>Supported Frameworks<\/b>\n<img src=\"docs\/img\/bar.png\" \/>\n<\/td>\n<td>\n<b>Training Services<\/b>\n<img src=\"docs\/img\/bar.png\" \/>\n<\/td>\n<td>\n<b>Tutorials<\/b>\n<img src=\"docs\/img\/bar.png\" \/>\n<\/td>\n<\/tr>\n<tr valign=\"top\">\n<td align=\"center\" valign=\"middle\">\n<b>Supports<\/b>\n<\/td>\n<td>\n<ul>\n<li>PyTorch<\/li>\n<li>TensorFlow<\/li>\n<li>Scikit-learn<\/li>\n<li>XGBoost<\/li>\n<li>LightGBM<\/li>\n<li>MXNet<\/li>\n<li>Caffe2<\/li>\n<li>More...<\/li>\n<\/ul>\n<\/td>\n<td>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/local.html\">Local machine<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/remote.html\">Remote SSH servers<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/aml.html\">Azure Machine Learning (AML)<\/a><\/li>\n<li><b>Kubernetes Based<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/openpai.html\">OpenAPI<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/kubeflow.html\">Kubeflow<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/frameworkcontroller.html\">FrameworkController<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/adaptdl.html\">AdaptDL<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/paidlc.html\">PAI DLC<\/a><\/li>\n<\/ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/experiment\/hybrid.html\">Hybrid training services<\/a><\/li>\n<\/ul>\n<\/td>\n<td>\n<ul>\n<li><b>HPO<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/tutorials\/hpo_quickstart_pytorch\/main.html\">PyTorch<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/tutorials\/hpo_quickstart_tensorflow\/main.html\">TensorFlow<\/a><\/li>\n<\/ul>\n<li><b>NAS<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/tutorials\/hello_nas.html\">Hello NAS<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/tutorials\/nasbench_as_dataset.html\">NAS Benchmarks<\/a><\/li>\n<\/ul>\n<li><b>Compression<\/b><\/li>\n<ul>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/tutorials\/pruning_quick_start_mnist.html\">Pruning<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/tutorials\/pruning_speed_up.html\">Pruning Speedup<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/tutorials\/quantization_quick_start_mnist.html\">Quantization<\/a><\/li>\n<li><a href=\"https:\/\/nni.readthedocs.io\/en\/latest\/tutorials\/quantization_speed_up.html\">Quantization Speedup<\/a><\/li>\n<\/ul>\n<\/ul>\n<\/td>\n<\/tbody>\n<\/table>\n\n<img src=\"docs\/static\/img\/webui.gif\" alt=\"webui\" width=\"100%\"\/>\n\n## Resources\n\n* [NNI Documentation Homepage](https:\/\/nni.readthedocs.io\/en\/stable)\n* [NNI Installation Guide](https:\/\/nni.readthedocs.io\/en\/stable\/installation.html)\n* [NNI Examples](https:\/\/nni.readthedocs.io\/en\/latest\/examples.html)\n* [Python API Reference](https:\/\/nni.readthedocs.io\/en\/latest\/reference\/python_api.html)\n* [Releases (Change Log)](https:\/\/nni.readthedocs.io\/en\/latest\/release.html)\n* [Related Research and Publications](https:\/\/nni.readthedocs.io\/en\/latest\/notes\/research_publications.html)\n* [Youtube Channel of NNI](https:\/\/www.youtube.com\/channel\/UCKcafm6861B2mnYhPbZHavw)\n* [Bilibili Space of NNI](https:\/\/space.bilibili.com\/1649051673)\n* [Webinar of Introducing Retiarii: A deep learning exploratory-training framework on NNI](https:\/\/note.microsoft.com\/MSR-Webinar-Retiarii-Registration-Live.html)\n* [Community Discussions](https:\/\/github.com\/microsoft\/nni\/discussions)\n\n## Contribution guidelines\n\nIf you want to contribute to NNI, be sure to review the [contribution guidelines](https:\/\/nni.readthedocs.io\/en\/stable\/notes\/contributing.html), which includes instructions of submitting feedbacks, best coding practices, and code of conduct.\n\nWe use [GitHub issues](https:\/\/github.com\/microsoft\/nni\/issues) to track tracking requests and bugs.\nPlease use [NNI Discussion](https:\/\/github.com\/microsoft\/nni\/discussions) for general questions and new ideas.\nFor questions of specific use cases, please go to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/nni).\n\nParticipating discussions via the following IM groups is also welcomed.\n\n|Gitter||WeChat|\n|----|----|----|\n|![image](https:\/\/user-images.githubusercontent.com\/39592018\/80665738-e0574a80-8acc-11ea-91bc-0836dc4cbf89.png)| OR |![image](https:\/\/github.com\/scarlett2018\/nniutil\/raw\/master\/wechat.png)|\n\nOver the past few years, NNI has received thousands of feedbacks on GitHub issues, and pull requests from hundreds of contributors.\nWe appreciate all contributions from community to make NNI thrive.\n\n<img src=\"https:\/\/img.shields.io\/github\/contributors-anon\/microsoft\/nni\"\/>\n\n<a href=\"https:\/\/github.com\/microsoft\/nni\/graphs\/contributors\"><img src=\"https:\/\/contrib.rocks\/image?repo=microsoft\/nni&max=240&columns=18\" \/><\/a>\n\n## Test status\n\n### Essentials\n\n| Type | Status |\n| :---: | :---: |\n| Fast test | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/fast%20test?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=54&branchName=master) |\n| Full linux | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/full%20test%20-%20linux?repoName=microsoft%2Fnni&branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=62&repoName=microsoft%2Fnni&branchName=master) |\n| Full windows | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/full%20test%20-%20windows?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=63&branchName=master) |\n\n### Training services\n\n| Type | Status |\n| :---: | :---: |\n| Remote - linux to linux | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/integration%20test%20-%20remote%20-%20linux%20to%20linux?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=64&branchName=master) |\n| Remote - linux to windows | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/integration%20test%20-%20remote%20-%20linux%20to%20windows?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=67&branchName=master) |\n| Remote - windows to linux | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/integration%20test%20-%20remote%20-%20windows%20to%20linux?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=68&branchName=master) |\n| OpenPAI | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/integration%20test%20-%20openpai%20-%20linux?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=65&branchName=master) |\n| Frameworkcontroller | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/integration%20test%20-%20frameworkcontroller?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=70&branchName=master) |\n| Kubeflow | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/integration%20test%20-%20kubeflow?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=69&branchName=master) |\n| Hybrid | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/integration%20test%20-%20hybrid?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=79&branchName=master) |\n| AzureML | [![Build Status](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_apis\/build\/status\/integration%20test%20-%20aml?branchName=master)](https:\/\/msrasrg.visualstudio.com\/NNIOpenSource\/_build\/latest?definitionId=78&branchName=master) |\n\n## Related Projects\n\nTargeting at openness and advancing state-of-art technology, [Microsoft Research (MSR)](https:\/\/www.microsoft.com\/en-us\/research\/group\/systems-and-networking-research-group-asia\/) had also released few other open source projects.\n\n* [OpenPAI](https:\/\/github.com\/Microsoft\/pai) : an open source platform that provides complete AI model training and resource management capabilities, it is easy to extend and supports on-premise, cloud and hybrid environments in various scale.\n* [FrameworkController](https:\/\/github.com\/Microsoft\/frameworkcontroller) : an open source general-purpose Kubernetes Pod Controller that orchestrate all kinds of applications on Kubernetes by a single controller.\n* [MMdnn](https:\/\/github.com\/Microsoft\/MMdnn) : A comprehensive, cross-framework solution to convert, visualize and diagnose deep neural network models. The \"MM\" in MMdnn stands for model management and \"dnn\" is an acronym for deep neural network.\n* [SPTAG](https:\/\/github.com\/Microsoft\/SPTAG) : Space Partition Tree And Graph (SPTAG) is an open source library for large scale vector approximate nearest neighbor search scenario.\n* [nn-Meter](https:\/\/github.com\/microsoft\/nn-Meter) : An accurate inference latency predictor for DNN models on diverse edge devices.\n\nWe encourage researchers and students leverage these projects to accelerate the AI development and research.\n\n## License\n\nThe entire codebase is under [MIT license](LICENSE).\n","110":"# numpy-ml\nEver wish you had an inefficient but somewhat legible collection of machine\nlearning algorithms implemented exclusively in NumPy? No?\n\n## Installation\n\n### For rapid experimentation\nTo use this code as a starting point for ML prototyping \/ experimentation, just clone the repository, create a new [virtualenv](https:\/\/pypi.org\/project\/virtualenv\/), and start hacking:\n\n```sh\n$ git clone https:\/\/github.com\/ddbourgin\/numpy-ml.git\n$ cd numpy-ml && virtualenv npml && source npml\/bin\/activate\n$ pip3 install -r requirements-dev.txt\n```\n\n### As a package\nIf you don't plan to modify the source, you can also install numpy-ml as a\nPython package: `pip3 install -u numpy_ml`.\n\nThe reinforcement learning agents train on environments defined in the [OpenAI\ngym](https:\/\/github.com\/openai\/gym). To install these alongside numpy-ml, you\ncan use `pip3 install -u 'numpy_ml[rl]'`.\n\n## Documentation\nFor more details on the available models, see the [project documentation](https:\/\/numpy-ml.readthedocs.io\/).\n\n## Available models\n<details>\n  <summary>Click to expand!<\/summary>\n\n1. **Gaussian mixture model**\n    - EM training\n\n2. **Hidden Markov model**\n    - Viterbi decoding\n    - Likelihood computation\n    - MLE parameter estimation via Baum-Welch\/forward-backward algorithm\n\n3. **Latent Dirichlet allocation** (topic model)\n    - Standard model with MLE parameter estimation via variational EM\n    - Smoothed model with MAP parameter estimation via MCMC\n\n4. **Neural networks**\n    * Layers \/ Layer-wise ops\n        - Add\n        - Flatten\n        - Multiply\n        - Softmax\n        - Fully-connected\/Dense\n        - Sparse evolutionary connections\n        - LSTM\n        - Elman-style RNN\n        - Max + average pooling\n        - Dot-product attention\n        - Embedding layer\n        - Restricted Boltzmann machine (w. CD-n training)\n        - 2D deconvolution (w. padding and stride)\n        - 2D convolution (w. padding, dilation, and stride)\n        - 1D convolution (w. padding, dilation, stride, and causality)\n    * Modules\n        - Bidirectional LSTM\n        - ResNet-style residual blocks (identity and convolution)\n        - WaveNet-style residual blocks with dilated causal convolutions\n        - Transformer-style multi-headed scaled dot product attention\n    * Regularizers\n        - Dropout\n    * Normalization\n        - Batch normalization (spatial and temporal)\n        - Layer normalization (spatial and temporal)\n    * Optimizers\n        - SGD w\/ momentum\n        - AdaGrad\n        - RMSProp\n        - Adam\n    * Learning Rate Schedulers\n        - Constant\n        - Exponential\n        - Noam\/Transformer\n        - Dlib scheduler\n    * Weight Initializers\n        - Glorot\/Xavier uniform and normal\n        - He\/Kaiming uniform and normal\n        - Standard and truncated normal\n    * Losses\n        - Cross entropy\n        - Squared error\n        - Bernoulli VAE loss\n        - Wasserstein loss with gradient penalty\n        - Noise contrastive estimation loss\n    * Activations\n        - ReLU\n        - Tanh\n        - Affine\n        - Sigmoid\n        - Leaky ReLU\n        - ELU\n        - SELU\n        - GELU\n        - Exponential\n        - Hard Sigmoid\n        - Softplus\n    * Models\n        - Bernoulli variational autoencoder\n        - Wasserstein GAN with gradient penalty\n        - word2vec encoder with skip-gram and CBOW architectures\n    * Utilities\n        - `col2im` (MATLAB port)\n        - `im2col` (MATLAB port)\n        - `conv1D`\n        - `conv2D`\n        - `deconv2D`\n        - `minibatch`\n\n5. **Tree-based models**\n    - Decision trees (CART)\n    - [Bagging] Random forests\n    - [Boosting] Gradient-boosted decision trees\n\n6. **Linear models**\n    - Ridge regression\n    - Logistic regression\n    - Ordinary least squares\n    - Weighted linear regression\n    - Generalized linear model (log, logit, and identity link)\n    - Gaussian naive Bayes classifier\n    - Bayesian linear regression w\/ conjugate priors\n        - Unknown mean, known variance (Gaussian prior)\n        - Unknown mean, unknown variance (Normal-Gamma \/ Normal-Inverse-Wishart prior)\n\n7. **n-Gram sequence models**\n    - Maximum likelihood scores\n    - Additive\/Lidstone smoothing\n    - Simple Good-Turing smoothing\n\n8. **Multi-armed bandit models**\n    - UCB1\n    - LinUCB\n    - Epsilon-greedy\n    - Thompson sampling w\/ conjugate priors\n        - Beta-Bernoulli sampler\n    - LinUCB\n\n8. **Reinforcement learning models**\n    - Cross-entropy method agent\n    - First visit on-policy Monte Carlo agent\n    - Weighted incremental importance sampling Monte Carlo agent\n    - Expected SARSA agent\n    - TD-0 Q-learning agent\n    - Dyna-Q \/ Dyna-Q+ with prioritized sweeping\n\n9. **Nonparameteric models**\n    - Nadaraya-Watson kernel regression\n    - k-Nearest neighbors classification and regression\n    - Gaussian process regression\n\n10. **Matrix factorization**\n    - Regularized alternating least-squares\n    - Non-negative matrix factorization\n\n11. **Preprocessing**\n    - Discrete Fourier transform (1D signals)\n    - Discrete cosine transform (type-II) (1D signals)\n    - Bilinear interpolation (2D signals)\n    - Nearest neighbor interpolation (1D and 2D signals)\n    - Autocorrelation (1D signals)\n    - Signal windowing\n    - Text tokenization\n    - Feature hashing\n    - Feature standardization\n    - One-hot encoding \/ decoding\n    - Huffman coding \/ decoding\n    - Byte pair encoding \/ decoding\n    - Term frequency-inverse document frequency (TF-IDF) encoding\n    - MFCC encoding\n\n12. **Utilities**\n    - Similarity kernels\n    - Distance metrics\n    - Priority queue\n    - Ball tree\n    - Discrete sampler\n    - Graph processing and generators\n<\/details>\n\n## Contributing\n\nAm I missing your favorite model? Is there something that could be cleaner \/\nless confusing? Did I mess something up? Submit a PR! The only requirement is\nthat your models are written with just the [Python standard\nlibrary](https:\/\/docs.python.org\/3\/library\/) and [NumPy](https:\/\/www.numpy.org\/). The\n[SciPy library](https:\/\/scipy.github.io\/devdocs\/) is also permitted under special\ncircumstances ;)\n\nSee full contributing guidelines [here](.\/CONTRIBUTING.md).\n","111":"\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Logo%20with%20text%20for%20GitHub%20Top.png\" alt=\"\u4eba\u9593\u306e\u305f\u3081\u306ePythonGUI \">\n  <h2 align=\"center\">\u4eba\u9593\u306e\u305f\u3081\u306ePythonGUI<\/h2>\n<\/p>\n\ntkinter\u3001Qt\u3001WxPython\u3001\u304a\u3088\u3073Remi\uff08\u30d6\u30e9\u30a6\u30b6\u30d9\u30fc\u30b9\uff09\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u3001\u3088\u308a\u30b7\u30f3\u30d7\u30eb\u306a\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u30a6\u30a3\u30f3\u30c9\u30a6\u5b9a\u7fa9\u306f\u521d\u5fc3\u8005\u304c\u7406\u89e3\u3059\u308bPython\u30b3\u30a2\u30c7\u30fc\u30bf\u578b\uff08\u30ea\u30b9\u30c8\u3068\u8f9e\u66f8\uff09\u3092\u4f7f\u7528\u3057\u3066\u7c21\u7565\u5316\u3055\u308c\u307e\u3059\u3002\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u304b\u3089\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u6e21\u3059\u30e2\u30c7\u30eb\u306b\u30a4\u30d9\u30f3\u30c8\u51e6\u7406\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3055\u3089\u306b\u5358\u7d14\u5316\u304c\u884c\u308f\u308c\u307e\u3059\u3002 \n\n\u30b3\u30fc\u30c9\u306f\u3088\u308a\u591a\u304f\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3059\u308b\u306e\u306b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u6301\u3064*\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093*\u3002\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f\u7406\u89e3\u3057\u3084\u3059\u3044\u3082\u306e\u3067\u3059\u304c\u3001\u5fc5\u305a\u3057\u3082*\u5358\u7d14*\u306a\u554f\u984c\u3060\u3051\u306b\u5236\u9650\u3055\u308c\u308b\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\n\u305f\u3060\u3057\u4e00\u90e8\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306fPySimpleGUI\u306b\u306f\u9069\u3057\u3066\u3044\u307e\u305b\u3093\u3002 \u5b9a\u7fa9\u4e0a\u3001PySimpleGUI\u306f\u57fa\u76e4\u3068\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6a5f\u80fd\u306e\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u3069\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304cPySimpleGUI\u306b\u9069\u3057\u3066\u3044\u3066\u3069\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u9069\u3057\u3066\u3044\u306a\u3044\u304b\u3092\u6b63\u78ba\u306b\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u306f\u96e3\u3057\u3044\u3067\u3059\u3002 \u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u8a73\u7d30\u306b\u3088\u3063\u3066\u7570\u306a\u308a\u307e\u3059\u3002\u30a8\u30af\u30bb\u30eb\u3092\u8a73\u7d30\u306b\u8907\u88fd\u3059\u308b\u3053\u3068\u306fPySimpleGUI\u306b\u9069\u3057\u3066\u3044\u306a\u3044\u3082\u306e\u306e\u4f8b\u3067\u3059\u3002\n\n[Japanese version of this readme](https:\/\/github.com\/PySimpleGUI\/PySimpleGUI\/blob\/master\/readme.ja.md).\n\n<a href=\"https:\/\/www.buymeacoffee.com\/PySimpleGUI\" target=\"_blank\"><img src=\"https:\/\/cdn.buymeacoffee.com\/buttons\/v2\/arial-yellow.png\" alt=\"Buy Me A Coffee\" width=\"217px\" ><\/a>\n\n<!-- I could use a coffee!  It fuels consultants, editors, domain registration and so many other things required for PySimpleGUI to be a thriving project. -->\n\u30b3\u30fc\u30d2\u30fc\u304c\u98f2\u307f\u305f\u3044\u3067\u3059!  \u30b3\u30f3\u30b5\u30eb\u30bf\u30f3\u30c8\u3001\u30a8\u30c7\u30a3\u30bf\u30fc\u3001\u30c9\u30e1\u30a4\u30f3\u767b\u9332\u306a\u3069\u3001PySimpleGUI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304c\u7e41\u6804\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u591a\u304f\u306e\u3082\u306e\u3092\u307e\u304b\u306a\u3048\u307e\u3059\u3002\n\n<hr>\n\n# \u7d71\u8a08 :chart_with_upwards_trend:\n\n\n## PyPI\u306e\u7d71\u8a08\u3068\u30d0\u30fc\u30b8\u30e7\u30f3\n\n| TK | TK 2.7 | Qt| WxPython | Web (Remi) |\n| -- | -- | -- | -- | -- |\n| ![tkinter](https:\/\/img.shields.io\/pypi\/dm\/pysimplegui?label=tkinter) | ![tkinter 2.7 downloads](https:\/\/img.shields.io\/pypi\/dm\/pysimplegui27?label=tkinter%202.7) | ![qt](https:\/\/img.shields.io\/pypi\/dm\/pysimpleguiqt?label=qt) | ![wx](https:\/\/img.shields.io\/pypi\/dm\/pysimpleguiwx?label=wx) | ![web](https:\/\/img.shields.io\/pypi\/dm\/pysimpleguiweb?label=web) |\n| [![tkinter](http:\/\/pepy.tech\/badge\/pysimplegui)](http:\/\/pepy.tech\/project\/pysimplegui) | [![tkinter27](https:\/\/pepy.tech\/badge\/pysimplegui27)](https:\/\/pepy.tech\/project\/pysimplegui27) | [![Downloads](https:\/\/pepy.tech\/badge\/pysimpleguiqt)](https:\/\/pepy.tech\/project\/pysimpleguiqt) | [![Downloads](https:\/\/pepy.tech\/badge\/pysimpleguiwx)](https:\/\/pepy.tech\/project\/pysimpleguiWx) | [![Downloads](https:\/\/pepy.tech\/badge\/pysimpleguiweb)](https:\/\/pepy.tech\/project\/pysimpleguiWeb) |\n| ![tkinter](https:\/\/img.shields.io\/pypi\/v\/pysimplegui.svg?label=tkinter%20PyPI%20Ver&color=red) | ![tkinter 2.7](https:\/\/img.shields.io\/pypi\/v\/pysimplegui27.svg?label=tkinter%202.7%20PyPI%20Ver&color=red) | ![qt](https:\/\/img.shields.io\/pypi\/v\/pysimpleguiqt.svg?label=qt%20PyPI%20Ver&color=red) | ![wx](https:\/\/img.shields.io\/pypi\/v\/pysimpleguiwx.svg?label=wx%20PyPI%20Ver&color=red) | ![web](https:\/\/img.shields.io\/pypi\/v\/pysimpleguiweb.svg?label=web%20PyPI%20Ver&color=red) | \n|  [![PyPI pyversions](https:\/\/img.shields.io\/pypi\/pyversions\/PySimpleGUI)](https:\/\/pypi.python.org\/pypi\/PySimpleGUI\/)  |  [![PyPI pyversions](https:\/\/img.shields.io\/pypi\/pyversions\/PySimpleGUI27)](https:\/\/pypi.python.org\/pypi\/PySimpleGUI27\/)  | [![PyPI pyversions](https:\/\/img.shields.io\/pypi\/pyversions\/PySimpleGUIQt)](https:\/\/pypi.python.org\/pypi\/PySimpleGUIQt\/) | [![PyPI pyversions](https:\/\/img.shields.io\/pypi\/pyversions\/PySimpleGUIWx)](https:\/\/pypi.python.org\/pypi\/PySimpleGUIWx\/) | [![PyPI pyversions](https:\/\/img.shields.io\/pypi\/pyversions\/PySimpleGUIWeb)](https:\/\/pypi.python.org\/pypi\/PySimpleGUIWeb\/) |\n\n\n--------------------------\n\n## GitHub\u306e\u7d71\u8a08\n\n\n\n\n|  Issues | Commit Activity | Stars | Docs | \n| -- | -- | -- | -- |\n| ![GitHub issues](https:\/\/img.shields.io\/github\/issues-raw\/PySimpleGUI\/PySimpleGUI?color=blue)  | ![commit activity](https:\/\/img.shields.io\/github\/commit-activity\/m\/PySimpleGUI\/PySimpleGUI.svg?color=blue) | ![stars](https:\/\/img.shields.io\/github\/stars\/PySimpleGUI\/PySimpleGUI.svg?label=stars&maxAge=2592000) | ![Documentation Status](https:\/\/readthedocs.org\/projects\/pysimplegui\/badge\/?version=latest) |\n|  ![GitHub closed issues](https:\/\/img.shields.io\/github\/issues-closed-raw\/PySimpleGUI\/PySimpleGUI?color=blue) | ![last commit](https:\/\/img.shields.io\/github\/last-commit\/PySimpleGUI\/PySimpleGUI.svg?color=blue)  |  |\n\n\n\n\n\n<p align=\"center\">\n  <img src=\"https:\/\/github-readme-stats.vercel.app\/api\/?username=PySimpleGUI&bg_color=3e7bac&title_color=ffdd55&icon_color=ffdd55&text_color=ffdd55&show_icons=true&count_private=true\">\n<\/p>\n\n\n\n<hr>\n\n# PySimpleGUI\u3068\u306f\u4f55\u3067\u3059\u304b:question:\n\nPySimpleGUI\u306f\u3042\u3089\u3086\u308b\u30ec\u30d9\u30eb\u306ePython\u30d7\u30ed\u30b0\u30e9\u30de\u304cGUI\u3092\u4f5c\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308bPython\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u3059\u3002\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3092\u542b\u3080\u300c\u30ec\u30a4\u30a2\u30a6\u30c8\u300d\u3092\u4f7f\u7528\u3057\u3066\u3001GUI\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u6307\u5b9a\u3057\u307e\u3059\uff08PySimpleGUI\u3067\u306f\u300c\u30a8\u30ec\u30e1\u30f3\u30c8\u300d\u3068\u547c\u3073\u307e\u3059\uff09\u3002 \u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b4\u3064\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u3044\u305a\u308c\u304b\u3092\u4f7f\u7528\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3057\u3066\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u8868\u793a\u3084\u64cd\u4f5c\u3059\u308b\u306e\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u3001tkinter\u3001Qt\u3001WxPython\u3001WxPython\u307e\u305f\u306fRemi\u304c\u542b\u307e\u308c\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u300c\u30e9\u30c3\u30d1\u30fc\u300d\u3068\u547c\u3076\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\n\nPySimpleGUI\u306f\u300c\u30dc\u30a4\u30e9\u30fc\u30d7\u30ec\u30fc\u30c8\u30b3\u30fc\u30c9\u300d\u306e\u591a\u304f\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u57fa\u3068\u306a\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u76f4\u63a5\u8a18\u8ff0\u3059\u308b\u3088\u308a\u3082\u5358\u7d14\u3067\u77ed\u304b\u3044\u30b3\u30fc\u30c9\u306b\u306a\u308a\u307e\u3059\u3002\n\u3055\u3089\u306b\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306f\u3001\u671b\u3093\u3060\u7d50\u679c\u3092\u5f97\u308b\u305f\u3081\u306b\u3001\u5fc5\u8981\u306a\u30b3\u30fc\u30c9\u3092\u3067\u304d\u308b\u3060\u3051\u5c11\u306a\u304f\u3059\u308b\u3088\u3046\u306b\u5358\u7d14\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u4f7f\u7528\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3084\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u3082\u3088\u308a\u307e\u3059\u304cPySimpleGUI\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u3044\u305a\u308c\u304b\u3092\u76f4\u63a5\u4f7f\u7528\u3057\u3066\u540c\u3058\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u3088\u308a\u3082\u3001\u30b3\u30fc\u30c9\u306e\u91cf\u306f1\/2\u304b\u30891\/10\u7a0b\u5ea6\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u76ee\u6a19\u306f\u4f7f\u7528\u3057\u3066\u3044\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u4e0a\u306e\u7279\u5b9a\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3084\u30b3\u30fc\u30c9\u3092\u30ab\u30d7\u30bb\u30eb\u5316\/\u975e\u8868\u793a\u306b\u3059\u308b\u3053\u3068\u3067\u3059\u304c\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u4f9d\u5b58\u3057\u3066\u3044\u308b\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3084\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u76f4\u63a5\u30a2\u30af\u30bb\u30b9\u3067\u304d\u307e\u3059\u3002\n\u8a2d\u5b9a\u3084\u6a5f\u80fd\u304c\u307e\u3060\u516c\u958b\u3055\u308c\u3066\u304a\u3089\u305aPySimpleGUI API\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044\u5834\u5408\u3067\u3082\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u906e\u65ad\u3055\u308c\u3066\u307e\u305b\u3093\u3002PySimpleGUI\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u81ea\u4f53\u3092\u76f4\u63a5\u5909\u66f4\u305b\u305a\u306b\u6a5f\u80fd\u3092\u62e1\u5f35\u3067\u304d\u307e\u3059\u3002\n\n\n## \u300cGUI\u306e\u30ae\u30e3\u30c3\u30d7\u300d\u3092\u57cb\u3081\u308b\n\nPython\u306f\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0 \u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306b\u591a\u304f\u306e\u4eba\u3005\u3092\u62db\u3044\u3066\u3044\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u6570\u3068\u6271\u3046\u9818\u57df\u306e\u7bc4\u56f2\u306f\u6c17\u304c\u9060\u304f\u306a\u308a\u307e\u3059 \u3057\u304b\u3057\u591a\u304f\u306e\u5834\u5408\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3068\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u4e00\u63e1\u308a\u306e\u4eba\u3005\u4ee5\u5916\u306e\u624b\u306e\u5c4a\u304b\u306a\u3044\u3068\u3053\u308d\u306b\u3042\u308a\u307e\u3059\u3002Python\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u5927\u534a\u306f\"\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\"\u30d9\u30fc\u30b9\u3067\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u7cfb\u306e\u4eba\u306f\u30c6\u30ad\u30b9\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3092\u4ecb\u3057\u3066\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u3068\u3084\u308a\u53d6\u308a\u3059\u308b\u3053\u3068\u306b\u6163\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u3053\u306e\u554f\u984c\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306b\u554f\u984c\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u307b\u3068\u3093\u3069\u306e\u300c\u666e\u901a\u306e\u4eba\u300d\u306f\u554f\u984c\u3092\u62b1\u3048\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30b8\u30bf\u30eb\u30fb\u30c7\u30a3\u30d0\u30a4\u30c9\u3001\u300cGUI\u306e\u30ae\u30e3\u30c3\u30d7\u300d\u304c\u751f\u307f\u51fa\u3055\u308c\u307e\u3059\u3002\n\u30d7\u30ed\u30b0\u30e9\u30e0\u306bGUI\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u305d\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3088\u308a\u591a\u304f\u306e\u4eba\u306b\u77e5\u3063\u3066\u3082\u3089\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3088\u308a\u89aa\u3057\u307f\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002GUI\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30fc\u30b9\u306b\u6163\u308c\u3066\u3044\u308b\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u3067\u3042\u3063\u3066\u3082\u3001\u3044\u304f\u3064\u304b\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u64cd\u4f5c\u3092\u7c21\u5358\u306b\u3067\u304d\u307e\u3059\u3002 \u305d\u3057\u3066\u6700\u5f8c\u306bGUI\u3092\u5fc5\u8981\u3068\u3059\u308b\u554f\u984c\u3082\u3042\u308a\u307e\u3059\u3002   \n\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/GUI%20Gap%202020.png\" width=\"600px\">\n<\/p>\n\n\n<hr>\n\n# \u79c1\u306b\u3064\u3044\u3066 :wave:\n\u3053\u3093\u306b\u3061\u306f\uff01 \u79c1\u306f\u30de\u30a4\u30af\u3067\u3059\u3002 GitHub\u306ePySimpleGUI\u3067\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066PySimpleGUI\u3092\u7d99\u7d9a\u7684\u306b\u524d\u9032\u3055\u305b\u7d9a\u3051\u3066\u3044\u307e\u3059\u3002\u79c1\u306f\u663c\u3068\u591c\u3001\u305d\u3057\u3066\u9031\u672b\u3082\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068PySimpleGUI\u30e6\u30fc\u30b6\u30fc\u306b\u6367\u3052\u3066\u304d\u307e\u3057\u305f\u3002\u79c1\u305f\u3061\u306e\u6210\u529f\u306f\u6700\u7d42\u7684\u306b\u5171\u6709\u3055\u308c\u307e\u3059\u3002 \u3042\u306a\u305f\u304c\u6210\u529f\u3057\u305f\u3068\u304d\u306b\u79c1\u306f\u6210\u529f\u3057\u3066\u3044\u307e\u3059\u3002\n\nPython\u3067\u306f\u76f8\u5bfe\u7684\u306a\u65b0\u4eba\u3067\u3059\u304c\u300170\u5e74\u4ee3\u304b\u3089\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u66f8\u3044\u3066\u304d\u307e\u3057\u305f\u3002 \u79c1\u306e\u30ad\u30e3\u30ea\u30a2\u306e\u5927\u534a\u306f\u30b7\u30ea\u30b3\u30f3\u30d0\u30ec\u30fc\u3067\u306e\u88fd\u54c1\u958b\u767a\u306b\u8cbb\u3084\u3055\u308c\u307e\u3057\u305f\u3002\u81ea\u5206\u304c\u958b\u767a\u3057\u3066\u304d\u305f\u4f01\u696d\u88fd\u54c1\u3068\u540c\u3058\u3088\u3046\u306a\u3001\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30ea\u30ba\u30e0\u3068\u732e\u8eab\u3092PySimpleGUI\u306b\u3082\u305f\u3089\u3057\u307e\u3059\u3002\u4eca\u3001\u3042\u306a\u305f\u306f\u79c1\u306e\u9867\u5ba2\u3067\u3059\u3002\n\n\n## \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u6a19 :goal_net:\n\nPySimpleGUI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u91cd\u8981\u306a\u76ee\u6a19\u306f\u4ee5\u4e0b\u306e2\u3064\u3067\u3059\u3002\n\n* \u697d\u3057\u3080\u3053\u3068\n* \u3042\u306a\u305f\u306e\u6210\u529f\n\n\u771f\u9762\u76ee\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30b4\u30fc\u30eb\u3068\u3057\u3066**\u697d\u3057\u3080**\u3068\u3044\u3046\u306e\u306f\u5909\u306b\u805e\u3053\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u3053\u308c\u306f\u771f\u9762\u76ee\u306a\u76ee\u6a19\u3067\u3059\u3002\u79c1\u306f\u3053\u308c\u3089\u306eGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u66f8\u304f\u3053\u3068\u306f\u3068\u3066\u3082\u697d\u3057\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u305d\u306e\u7406\u7531\u306e1\u3064\u306f\u3001\u5b8c\u5168\u306a\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u4f5c\u6210\u306b\u304b\u304b\u308b\u6642\u9593\u304c\u3044\u304b\u306b\u77ed\u3044\u304b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u3082\u3057\u79c1\u9054\u304c\u30d7\u30ed\u30bb\u30b9\u3092\u697d\u3057\u3093\u3067\u3044\u306a\u3044\u5834\u5408\u306f\u3001\u8ab0\u304b\u304c\u3042\u304d\u3089\u3081\u3066\u3044\u307e\u3059\u3002\n\n\u81a8\u5927\u306a\u91cf\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3001\u30af\u30c3\u30af\u30d6\u30c3\u30af\u3001\u3059\u3050\u306b\u4f7f\u3048\u308b100\u7a2e\u985e\u4ee5\u4e0a\u306e\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u3001\u8a73\u7d30\u306a\u30b3\u30fc\u30eb\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3001YouTube\u306e\u30d3\u30c7\u30aa\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u306eTrinket\u306e\u30c7\u30e2\u306a\u3069\u3001\u3059\u3079\u3066\u304c\u697d\u3057\u3044\u4f53\u9a13\u3092\u751f\u307f\u51fa\u3059\u305f\u3081\u306b\u4f5c\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n**\u3042\u306a\u305f\u306e\u6210\u529f**\u306f\u5171\u901a\u306e\u76ee\u6a19\u3067\u3059\u3002 PySimpleGUI\u306f\u958b\u767a\u8005\u5411\u3051\u306b\u69cb\u7bc9\u3055\u308c\u307e\u3057\u305f\u3002\u3042\u306a\u305f\u306f\u79c1\u306e\u4ef2\u9593\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u3068PySimpleGUI\u306e\u5171\u540c\u4f5c\u696d\u306e\u7d50\u679c\u3092\u898b\u308b\u306e\u306f\u4e88\u60f3\u5916\u306e\u5831\u916c\u3067\u3057\u305f\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3084\u305d\u306e\u4ed6\u306e\u8cc7\u6599\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u69cb\u7bc9\u306b\u5f79\u7acb\u3066\u3066\u304f\u3060\u3055\u3044\u3002\u30c8\u30e9\u30d6\u30eb\u306b\u906d\u9047\u3057\u305f\u5834\u5408\u306f\u3001[PySimpleGUI GitHub \u306e\u554f\u984c](http:\/\/Issues.PySimpleGUI.org)\u3067Issue\u3092\u958b\u3044\u3066\u30d8\u30eb\u30d7\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u30b5\u30dd\u30fc\u30c8\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u898b\u3066\u304f\u3060\u3055\u3044\u3002\n\n<hr>\n\n# \u6559\u80b2\u30ea\u30bd\u30fc\u30b9 :books:\n\nwww.PySimpleGUI.org\u306f\u899a\u3048\u3084\u3059\u304f\u3001\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u5834\u6240\u3067\u3059\u3002\u4e0a\u90e8\u306b\u306f\u3044\u304f\u3064\u304b\u306e\u7570\u306a\u308b\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8868\u3059\u30bf\u30d6\u304c\u3042\u308a\u307e\u3059\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f\u300cRead The Docs\u300d\u306b\u8a18\u8f09\u3055\u308c\u3066\u304a\u308a\u3001\u5404\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u76ee\u6b21\u304c\u3042\u308a\u3001\u691c\u7d22\u3082\u7c21\u5358\u3067\u3059\u3002\n\n\u6570\u767e\u30da\u30fc\u30b8\u306e\u6587\u66f8\u5316\u3055\u308c\u305f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3068\u6570\u767e\u306e\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u3042\u308a\u3001\u3042\u306a\u305f\u304c\u975e\u5e38\u306b\u901f\u304f\u52b9\u679c\u3092\u767a\u63ee\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\n\u5358\u4e00\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u5b66\u3076\u306e\u306b\u6570\u65e5\u307e\u305f\u306f\u6570\u9031\u9593\u6295\u8cc7\u3059\u308b\u3088\u308a\u3082\u3001PySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u5348\u5f8c\u306e\u6642\u9593\u3060\u3051\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u5b8c\u6210\u3055\u305b\u3089\u308c\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n\n## \u4f8b1 - \u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\n\n\u3053\u306e\u30bf\u30a4\u30d7\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u304c1\u56de\u8868\u793a\u3055\u308c\u3066\u53ce\u96c6\u3055\u308c\u305f\u5024\u304c\u9589\u3058\u3089\u308c\u308b\u305f\u3081\u3001\u300c\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u300d\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u547c\u3070\u308c\u307e\u3059\u3002 \u30ef\u30fc\u30c9\u30d7\u30ed\u30bb\u30c3\u30b5\u306e\u3088\u3046\u306b\u9577\u3044\u9593\u958b\u3044\u305f\u307e\u307e\u306b\u306a\u3063\u3066\u3044\u307e\u305b\u3093\u3002\n\n\n### \u5358\u7d14\u306aPySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u89e3\u5256\u5b66\n\nPySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u306f5\u3064\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\n\n\n\n```python\nimport PySimpleGUI as sg                                 # \u30d1\u30fc\u30c8 1 - \u30a4\u30f3\u30dd\u30fc\u30c8\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5185\u5bb9\u3092\u5b9a\u7fa9\u3059\u308b\nlayout = [  [sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],     # \u30d1\u30fc\u30c8 2 - \u30ec\u30a4\u30a2\u30a6\u30c8\n            [sg.Input()],\n            [sg.Button('\u306f\u3044')] ]\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\nwindow = sg.Window('\u30a6\u30a3\u30f3\u30c9\u30a6\u30bf\u30a4\u30c8\u30eb', layout)      # \u30d1\u30fc\u30c8 3- \u30a6\u30a3\u30f3\u30c9\u30a6\u5b9a\u7fa9\n                                                \n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u3001\u5bfe\u8a71\u3059\u308b\nevent, values = window.read()                   # \u30d1\u30fc\u30c8 4- \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u307e\u305f\u306f Window.read \u547c\u3073\u51fa\u3057\n\n# \u53ce\u96c6\u3055\u308c\u305f\u60c5\u5831\u3067\u4f55\u304b\u3092\u3059\u308b\nprint('\u30cf\u30ed\u30fc ', values[0], \"PySimpleGUI\u3092\u8a66\u3057\u3066\u304f\u308c\u3066\u3042\u308a\u304c\u3068\u3046!\")\n\n# \u753b\u9762\u304b\u3089\u524a\u9664\u3057\u3066\u7d42\u4e86\nwindow.close()                                  #\u30d1\u30fc\u30c8 5 - \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u308b\n```\n\n\u30b3\u30fc\u30c9\u306f\u3001\u4ee5\u4e0b\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u751f\u6210\u3057\u307e\u3059\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/WhatsYourNameBlank1.jpg\">\n<\/p>\n\n\n<hr>\n\n## \u4f8b2 - \u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30a6\u30a3\u30f3\u30c9\u30a6\n\n\u3053\u306e\u4f8b\u3067\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u308b\u304b\u3001\u307e\u305f\u306f [\u7d42\u4e86] \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u307e\u3067\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u753b\u9762\u4e0a\u306b\u6b8b\u308a\u307e\u3059\u3002\u5148\u307b\u3069\u898b\u305f\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u4e3b\u306a\u9055\u3044\u306f\u3001\u300c\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u300d\u306e\u8ffd\u52a0\u3067\u3059\u3002\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u304b\u3089\u30a4\u30d9\u30f3\u30c8\u3068\u5165\u529b\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u4e2d\u5fc3\u306f\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306b\u306a\u308a\u307e\u3059\u3002\n\n```python\nimport PySimpleGUI as sg\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5185\u5bb9\u3092\u5b9a\u7fa9\u3059\u308b\nlayout = [[sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],\n          [sg.Input(key='-\u5165\u529b-')],\n          [sg.Text(size=(55,1), key='-\u51fa\u529b-')],\n          [sg.Button('\u306f\u3044'), sg.Button('\u7d42\u4e86')]]\n\n# \u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\nwindow = sg.Window('\u30a6\u30a3\u30f3\u30c9\u30a6\u30bf\u30a4\u30c8\u30eb',\u30ec\u30a4\u30a2\u30a6\u30c8)\n\n# \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u3092\u4f7f\u7528\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u3001\u5bfe\u8a71\u3059\u308b\nwhile True:\n    event, values = window.read()\n# \u30e6\u30fc\u30b6\u30fc\u304c\u7d42\u4e86\u3057\u305f\u3044\u306e\u304b\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u9589\u3058\u3089\u308c\u305f\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\n    if event == sg.WINDOW_CLOSED or event == '\u7d42\u4e86':\n        break\n\n    # Output a message to the window\n    window['-\u51fa\u529b-'].update('\u30cf\u30ed\u30fc ' + values['-\u5165\u529b-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\")\n\n# \u753b\u9762\u304b\u3089\u524a\u9664\u3057\u3066\u7d42\u4e86\nwindow.close()\n```\n\n\u4ee5\u4e0b\u306f\u3001\u4f8b2\u304c\u4f5c\u6210\u3059\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/WhatsYourNameBlank.jpg\">\n<\/p>\n\n\n\n\u5165\u529b\u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u5024\u3092\u5165\u529b\u3057\u3066 [OK] \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u5f8c\u306e\u8868\u793a\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/HelloWorld1.jpg\">\n<\/p>\n\n\n\u3053\u306e\u4f8b\u3068\u30ef\u30f3\u30b7\u30e7\u30c3\u30c8 \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u9055\u3044\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\u307e\u305a\u3001\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u9055\u3044\u306b\u6c17\u3065\u304f\u3067\u3057\u3087\u3046\u3002\u3068\u304f\u306b2\u3064\u306e\u5909\u66f4\u304c\u91cd\u8981\u3067\u3059\u30021\u3064\u306f`Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u3068`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e1\u3064\u306b`key`\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u300ckey\u300d\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u540d\u524d\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 \u307e\u305f\u306fPython\u306e\u8f9e\u66f8\u30ad\u30fc\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 `Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u30ad\u30fc\u306f\u3001\u30b3\u30fc\u30c9\u306e\u5f8c\u534a\u3067\u8f9e\u66f8\u30ad\u30fc\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\n\n\n\u3082\u30461\u3064\u306e\u9055\u3044\u306f\u3001\u3053\u306e `Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u8ffd\u52a0\u3067\u3059:\n```python\n          [sg.Text(size=(40,1), key='-OUTPUT-')],\n```\n\n\u3059\u3067\u306b\u30ab\u30d0\u30fc\u3057\u3066\u3044\u308b\u300c\u30ad\u30fc\u300d\u3068\u3044\u30462\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u3042\u308a\u307e\u3059\u3002 `Size`\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u6587\u5b57\u6570\u306e\u30b5\u30a4\u30ba\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 \u3053\u306e\u5834\u5408\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f\u5e4540\u6587\u5b57\u3001\u9ad8\u30551\u6587\u5b57\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u30c6\u30ad\u30b9\u30c8\u306e\u6587\u5b57\u5217\u304c\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u306e\u3067\u7a7a\u767d\u3067\u8868\u793a\u3055\u308c\u3066\u3044\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u4f5c\u6210\u3055\u308c\u305f\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u306f\u7a7a\u767d\u884c\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\n\u307e\u305f \u3001[\u7d42\u4e86]\u30dc\u30bf\u30f3\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\n\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306b\u306f\u3001\u304a\u306a\u3058\u307f\u306e`window.read()`\u547c\u3073\u51fa\u3057\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u8aad\u307f\u8fbc\u3093\u3060\u5f8c\u306b\u7d9a\u304f\u306e\u306f\u3001\u3053\u306eif\u6587\u3067\u3059\u3002\n```python\n    if event == sg.WINDOW_CLOSED or event == '\u7d42\u4e86':\n        break\n```\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u300cX\uff08\u9589\u3058\u308b\uff09\u300d\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9589\u3058\u305f\u304b\u3001\u307e\u305f\u306f\u300c\u7d42\u4e86\u300d\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002 \u3053\u308c\u3089\u306e\u3044\u305a\u308c\u304b\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001\u30b3\u30fc\u30c9\u306f\u30a4\u30d9\u30f3\u30c8 \u30eb\u30fc\u30d7\u304b\u3089\u629c\u3051\u51fa\u3057\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u9589\u3058\u3089\u308c\u305a\u3001\u300c\u7d42\u4e86\u300d\u30dc\u30bf\u30f3\u304c\u30af\u30ea\u30c3\u30af\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u52d5\u4f5c\u304c\u7d99\u7d9a\u3055\u308c\u307e\u3059\u3002 \u8d77\u3053\u308a\u3046\u308b\u552f\u4e00\u306e\u4e8b\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u300cOK\u300d\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u305f\u3053\u3068\u3067\u3059\u3002 \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u306e\u6700\u5f8c\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002\n\n\n\n\n```python\n    window['-OUTPUT-'].update('\u30cf\u30ed\u30fc  ' + values['-INPUT-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\")\n```\n\n\u3053\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u306f\u3001`-OUTPUT-`\u30ad\u30fc \u3092\u6301\u3064`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u6587\u5b57\u5217\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002`window['-OUTPUT-']`\u306f`-OUTPUT-`\u30ad\u30fc\u3092\u6301\u3064\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u691c\u7d22\u3057\u307e\u3059\u3002 \u30ad\u30fc\u306f\u3001\u7a7a\u767d\u306e`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u5c5e\u3057\u307e\u3059\u3002 \u30a8\u30ec\u30e1\u30f3\u30c8\u304c\u691c\u7d22\u304b\u3089\u8fd4\u3055\u308c\u308b\u3068\u3001\u305d\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306e`update`\u30e1\u30bd\u30c3\u30c9\u304c\u547c\u3073\u51fa\u3055\u308c\u307e\u3059\u3002 \u307b\u3068\u3093\u3069\u3059\u3079\u3066\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306f`update`\u30e1\u30bd\u30c3\u30c9\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u5024\u3084\u69cb\u6210\u3092\u5909\u66f4\u3057\u305f\u308a\u3059\u308b\u306e\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n\u30c6\u30ad\u30b9\u30c8\u3092\u9ec4\u8272\u306b\u3057\u305f\u3044\u5834\u5408\u306f\u3001`update`\u30e1\u30bd\u30c3\u30c9\u306b`text_color`\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u8ffd\u52a0\u3057\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u51e6\u7406\u3057\u307e\u3059\u3002\n```python\n    window['-\u51fa\u529b-'].update('\u30cf\u30ed\u30fc ' + values['-\u5165\u529b-'] + \"! PySimpleGUI \u3092\u304a\u8a66\u3057\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\", text_color='yellow')\n```\n\n`text_color`\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u8ffd\u52a0\u3057\u305f\u5f8c\u3001\u3053\u308c\u304c\u65b0\u3057\u3044\u7d50\u679c\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u306a\u308a\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/HelloWorldYellow.jpg\">\n<\/p>\n\n\n\u5404\u30a8\u30ec\u30e1\u30f3\u30c8\u3067\u4f7f\u7528\u3067\u304d\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306f[call reference\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](http:\/\/calls.PySimpleGUI.org)\u3068docstrings\u306e\u4e21\u65b9\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002PySimpleGUI\u306b\u306f\u5229\u7528\u53ef\u80fd\u306a\u3059\u3079\u3066\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u7406\u89e3\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u8c4a\u5bcc\u306a\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306e`update'`\u30e1\u30bd\u30c3\u30c9\u3092\u691c\u7d22\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5b9a\u7fa9\u304c\u898b\u3064\u304b\u308a\u307e\u3059:\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/TextUpdate.jpg\">\n<\/p>\n\n\n\u3054\u89a7\u306e\u3088\u3046\u306b\u3001\u3044\u304f\u3064\u304b\u306e\u3082\u306e\u306f\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u5909\u66f4\u3067\u304d\u307e\u3059\u3002 call reference\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306fPySimpleGUI\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u7c21\u5358\u306b\u3059\u308b\u8cb4\u91cd\u306a\u30ea\u30bd\u30fc\u30b9\u3067\u3059\u3002\n\n<hr>\n\n## \u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u304a\u3082\u3057\u308d\u3044\u3067\u3059\uff08\u7b11\uff09! :laughing:\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u300c\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u300d(LOL)\u3067\u3059\u3002 \u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u300c\u884c\u300d\u306b\u5206\u5272\u3055\u308c\u307e\u3059\u3002 \u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5404\u884c\u306f\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u30ea\u30b9\u30c8\u306b\u306a\u308a\u307e\u3059\u3002 \u3059\u3079\u3066\u306e\u30ea\u30b9\u30c8\u3092\u9023\u7d50\u3059\u308b\u3068\u3001\u30ec\u30a4\u30a2\u30a6\u30c8\u304c\u3067\u304d\u3042\u304c\u308a\u307e\u3059\u3002\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002\n\n\u884c\u306e\u5b9a\u7fa9\u3092\u7c21\u5358\u306b\u78ba\u8a8d\u306b\u3059\u308b\u305f\u3081\u3001\u5404\u884c\u306b\u8ffd\u52a0\u3067 'Text' \u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u81ea\u4f53\u306f\u4ee5\u524d\u3068\u540c\u3058\u3067\u3059:\n\n```python\nlayout = [  [sg.Text('\u30e9\u30a4\u30f3 1'), sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\")],\n            [sg.Text('\u30e9\u30a4\u30f3 2'), sg.Input()],\n            [sg.Text('\u30e9\u30a4\u30f3 3'), sg.Button('\u306f\u3044')] ]\n```\n\n\u3053\u306e\u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u5404\u884c\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u5185\u306e\u884c\u306b\u8868\u793a\u3055\u308c\u308b\u30a8\u30ec\u30e1\u30f3\u30c8\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002\n\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/rows.jpg\">\n<\/p>\n\n\n\n\u30ea\u30b9\u30c8\u3092\u4f7f\u7528\u3057\u3066GUI\u3092\u5b9a\u7fa9\u3059\u308b\u5834\u5408\u3001\u4ed6\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u7528\u3057\u3066GUI\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u884c\u3046\u65b9\u6cd5\u306b\u304f\u3089\u3079\u3066\u3044\u304f\u3064\u304b\u306e\u5927\u304d\u306a\u5229\u70b9\u304c\u3042\u308a\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001Python\u306e\u30ea\u30b9\u30c8\u5185\u5305\u8868\u8a18\u3092\u5229\u7528\u3057\u3066\u30011\u884c\u306e\u30b3\u30fc\u30c9\u3067\u30dc\u30bf\u30f3\u306e\u30b0\u30ea\u30c3\u30c9\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n\n\u6b21\u306e3\u884c\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002\n\n```python\nimport PySimpleGUI as sg\n\nlayout = [[sg.Button(f'{row}, {col}') for col in range(4)] for row in range(4)]\n\nevent, values = sg.Window('List Comprehensions', layout).read(close=True)\n```\n\n\u30dc\u30bf\u30f3\u306e4 x 4\u30b0\u30ea\u30c3\u30c9\u3092\u6301\u3064\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u751f\u6210\u3057\u307e\u3059\u3002\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/4x4grid.jpg\">\n<\/p>\n\n\u300c\u697d\u3057\u3080\u300d\u304c\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u7684\u306e1\u3064\u3067\u3042\u308b\u3053\u3068\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002 Python\u306e\u5f37\u529b\u306a\u57fa\u672c\u6a5f\u80fd\u3092GUI\u306e\u554f\u984c\u306b\u76f4\u63a5\u9069\u7528\u3059\u308b\u306e\u306f\u697d\u3057\u3044\u3067\u3059\u3002GUI\u3092\u4f5c\u6210\u3059\u308b\u30b3\u30fc\u30c9\u306e\u30da\u30fc\u30b8\u306e\u4ee3\u308f\u308a\u306b\u3001\u6570\u884c\uff08\u307e\u305f\u306f\u591a\u304f\u306e\u5834\u5408\u306f1\u884c\uff09\u306e\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n## \u30b3\u30fc\u30c9\u306e\u6298\u308a\u305f\u305f\u307f\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30b3\u30fc\u30c9\u30921\u884c\u306e\u30b3\u30fc\u30c9\u306b\u51dd\u7e2e\u3067\u304d\u307e\u3059\u3002 \u30ec\u30a4\u30a2\u30a6\u30c8\u306e\u5b9a\u7fa9\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u4f5c\u6210\u3001\u8868\u793a\u3001\u304a\u3088\u3073\u30c7\u30fc\u30bf\u53ce\u96c6\u306f\u3059\u3079\u3066\u3001\u6b21\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u66f8\u3051\u307e\u3059\u3002\n\n```python\nevent, values = sg.Window('Window Title', [[sg.Text(\"\u304a\u540d\u524d\u306f\u4f55\u3067\u3059\u304b\uff1f\")],[sg.Input()],[sg.Button('\u306f\u3044')]]).read(close=True)\n```\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/WhatsYourName.jpg\">\n<\/p>\n\n\n\u540c\u3058\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u8868\u793a\u3055\u308c\u3001PySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3092\u793a\u3059\u4f8b\u3068\u540c\u3058\u5024\u304c\u8fd4\u3055\u308c\u307e\u3059\u3002 \u975e\u5e38\u306b\u5c11\u306a\u3044\u91cf\u3067\u591a\u304f\u306e\u3053\u3068\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u305f\u3081\u3001Python\u30b3\u30fc\u30c9\u306b\u3059\u3070\u3084\u304f\u7c21\u5358\u306bGUI\u3092\u8ffd\u52a0\u3067\u304d\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u3092\u8868\u793a\u3057\u3066\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u306e\u9078\u629e\u3092\u5f97\u305f\u3044\u5834\u5408\u306f\u30011\u30da\u30fc\u30b8\u306e\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304f1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u77ed\u7e2e\u30a8\u30a4\u30ea\u30a2\u30b9\u3092\u4f7f\u7528\u3057\u3066\u3088\u308a\u5c11\u306a\u3044\u6587\u5b57\u6570\u3067\u30b3\u30fc\u30c9\u306e\u30b9\u30da\u30fc\u30b9\u3092\u3055\u3089\u306b\u77ed\u304f\u3067\u304d\u307e\u3059\u3002\u3059\u3079\u3066\u306e\u30a8\u30ec\u30e1\u30f3\u30c8\u306b\u306f\u3001\u4f7f\u7528\u3067\u304d\u308b\u77ed\u3044\u540d\u524d\u304c1\u3064\u4ee5\u4e0a\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001`Text`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f\u5358\u306b`T`\u3068\u3057\u3066\u66f8\u3051\u307e\u3059\u3002`Input`\u30a8\u30ec\u30e1\u30f3\u30c8\u306f `I`\u3001`Button`\u306f`B`\u3068\u66f8\u3051\u307e\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306b\u306a\u308a\u307e\u3059:\n\n```python\nevent, values = sg.Window('Window Title', [[sg.T(\"\u3042\u306a\u305f\u306e\u540d\u524d\u306f\u4f55\u3067\u3059\u304b?\")],[sg.I()],[sg.B('\u306f\u3044')]]).read(close=True)\n```\n\n\n### \u30b3\u30fc\u30c9\u306e\u79fb\u690d\u6027\n\nPySimpleGUI\u306f\u73fe\u5728\u30014\u3064\u306ePython\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002 \u4f7f\u7528\u3059\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u3001import\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u3066\u6307\u5b9a\u3057\u307e\u3059\u3002 \u30a4\u30f3\u30dd\u30fc\u30c8\u3092\u5909\u66f4\u3059\u308b\u3068\u3001\u57fa\u672c\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u5909\u66f4\u3055\u308c\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3088\u3063\u3066\u306f\u3001\u5225\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u306fimport\u6587\u4ee5\u5916\u306e\u5909\u66f4\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002 \u4e0a\u8a18\u306e\u4f8b\u3067\u306f\u3001\u30a4\u30f3\u30dd\u30fc\u30c8\u3092`PySimpleGUI`\u304b\u3089`PySimpleGUIQt`\u3001`PySimpleGUIWx`\u3001`PySimpleGUIWeb`\u3001`PySimpleGUIWeb`\u306b\u5909\u66f4\u3059\u308b\u3068\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u5909\u66f4\u3055\u308c\u307e\u3059\u3002\n\n| \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u30a4\u30f3\u30dd\u30fc\u30c8 | \u7d50\u679c\u30a6\u30a3\u30f3\u30c9\u30a6 |\n|--|--|\n| PySimpleGUI |  ![](https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/ex1-tkinter.jpg) |\n| PySimpleGUIQt |  ![](https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/ex1-Qt.jpg) |\n| PySimpleGUIWx |  ![](https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/ex1-WxPython.jpg) |\n| PySimpleGUIWeb |  ![](https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/ex1-Remi.jpg) |\n\n\n\nGUI\u306e\u30b3\u30fc\u30c9\u3092\u3042\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u5225\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u79fb\u690d\u3059\u308b\uff08\u305f\u3068\u3048\u3070\u3001\u30b3\u30fc\u30c9\u3092tkinter\u304b\u3089Qt\u306b\u5909\u66f4\u3059\u308b\uff09\u306b\u306f\u3001\u901a\u5e38\u306f\u30b3\u30fc\u30c9\u306e\u66f8\u304d\u63db\u3048\u304c\u5fc5\u8981\u3067\u3059\u3002PySimpleGUI\u306f\u3001\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u9593\u306e\u7c21\u5358\u306a\u79fb\u52d5\u3092\u53ef\u80fd\u306b\u3059\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u5834\u5408\u306b\u3088\u3063\u3066\u306f\u3044\u304f\u3064\u304b\u306e\u5909\u66f4\u304c\u5fc5\u8981\u3067\u3059\u304c\u76ee\u7684\u306f\u6700\u5c0f\u9650\u306e\u5909\u66f4\u3067\u79fb\u690d\u6027\u306e\u9ad8\u3044\u30b3\u30fc\u30c9\u3092\u4f5c\u308b\u3053\u3068\u3067\u3059\u3002 \n\n\u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u306a\u3069\u306e\u4e00\u90e8\u306e\u6a5f\u80fd\u306f\u3001\u3059\u3079\u3066\u306e\u30dd\u30fc\u30c8\u3067\u4f7f\u7528\u3067\u304d\u306a\u3044\u3067\u3059\u3002 \u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u6a5f\u80fd\u306fQt\u304a\u3088\u3073WxPython\u30dd\u30fc\u30c8\u3067\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\u3055\u308c\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u306ftkinter\u3067\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002\u30b7\u30b9\u30c6\u30e0\u30c8\u30ec\u30a4\u30a2\u30a4\u30b3\u30f3\u306f\u3001PySimpleGUIWeb\u30dd\u30fc\u30c8\u3067\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u307e\u305b\u3093\u3002\n\n##  \u30e9\u30f3\u30bf\u30a4\u30e0\u74b0\u5883\n\n|\u74b0\u5883 |\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b |\n|--|--|\n|\u30d1\u30a4\u30bd\u30f3| Python  3.4+ |\n|\u30aa\u30da\u30ec\u30fc\u30c6\u30a3\u30f3\u30b0 \u30b7\u30b9\u30c6\u30e0 |\u30a6\u30a3\u30f3\u30c9\u30a6\u30ba,Linux,\u30de\u30c3\u30af |\n|\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2 |\u30c7\u30b9\u30af\u30c8\u30c3\u30d7PC,\u30ce\u30fc\u30c8\u30d1\u30bd\u30b3\u30f3,\u30e9\u30ba\u30d9\u30ea\u30fc\u30d1\u30a4,PyDroid3 \u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30a2\u30f3\u30c9\u30ed\u30a4\u30c9\u30c7\u30d0\u30a4\u30b9 |\n|\u30aa\u30f3\u30e9\u30a4\u30f3 |repli.it,Trinket.com\uff08\u3069\u3061\u3089\u3082\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067tkinter\u3092\u5b9f\u884c\u3059\u308b\uff09|\n|GUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af |tkinter, pyside2, WxPython, Remi |\n\n\n## \u7d71\u5408\n200\u4ee5\u4e0a\u306e\u300c\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u300d\u306e\u4e2d\u306b\u306f\u3001\u591a\u304f\u306e\u4eba\u6c17\u306ePython\u30d1\u30c3\u30b1\u30fc\u30b8\u3092GUI\u306b\u7d71\u5408\u3059\u308b\u65b9\u6cd5\u306e\u4f8b\u304c\u898b\u3064\u304b\u308a\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306bMatplotlib\u306e\u63cf\u753b\u3092\u57cb\u3081\u8fbc\u307f\u305f\u3044\u3067\u3059\u304b\uff1f\u3001\u554f\u984c\u3042\u308a\u307e\u305b\u3093\u3001 \u30c7\u30e2\u30b3\u30fc\u30c9\u3092\u30b3\u30d4\u30fc\u3059\u308b\u3068\u5373\u5ea7\u306b\u3042\u306a\u305f\u306e\u5922\u306eMatplotlib\u306e\u63cf\u753b\u3092\u3042\u306a\u305f\u306eGUI\u306b\u7d44\u307f\u8fbc\u3081\u307e\u3059\u3002  \n\n\u3053\u308c\u3089\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3084\u305d\u306e\u4ed6\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u3001\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0\u3084\u30c7\u30e2\u30ec\u30dd\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001GUI\u306b\u5165\u308c\u308b\u6e96\u5099\u304c\u3067\u304d\u3066\u3044\u307e\u3059\u3002\n\n|\u30d1\u30c3\u30b1\u30fc\u30b8 |\u8aac\u660e |\n|--|--|\n Matplotlib |\u30b0\u30e9\u30d5\u3084\u30d7\u30ed\u30c3\u30c8\u306e\u591a\u304f\u306e\u7a2e\u985e |\n OpenCV |\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\uff08AI\u3067\u3088\u304f\u4f7f\u7528\u3055\u308b\uff09 |\n VLC |\u30d3\u30c7\u30aa\u518d\u751f |\n pymunk |\u7269\u7406\u30a8\u30f3\u30b8\u30f3|\n psutil |\u30b7\u30b9\u30c6\u30e0\u74b0\u5883\u306e\u7d71\u8a08 |\n prawn |Reddit  API |\njson |PySimpleGUI\u306f\u3001\u300c\u30e6\u30fc\u30b6\u30fc\u8a2d\u5b9a\u300d\u3092\u683c\u7d0d\u3059\u308b\u7279\u5225\u306aAPI\u3092\u30e9\u30c3\u30d7\u3057\u307e\u3059\u3002 |\n weather |\u304a\u5929\u6c17\u30a2\u30d7\u30ea\u3092\u4f5c\u308b\u305f\u3081\u306b\u3044\u304f\u3064\u304b\u306e\u5929\u6c17API\u3068\u7d71\u5408 |\n mido |MIDI\u306e\u518d\u751f |\n beautiful soup |\u30a6\u30a7\u30d6\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\uff08GitHub issue\u30a6\u30a9\u30c3\u30c1\u30e3\u30fc\u3067\u306e\u4f8b\uff09 |\n\n<hr>\n\n# \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb :floppy_disk:\n\n\nPySimpleGUI\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u4e00\u822c\u7684\u306b2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002\n\n1. PyPI\u304b\u3089pip\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\n2. PySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d5\u30a9\u30eb\u30c0\u30fc\u306b\u914d\u7f6e\u3057\u307e\u3059\n\n\n### Pip\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n\u73fe\u5728\u63d0\u6848\u3055\u308c\u3066\u3044\u308b`pip`\u30b3\u30de\u30f3\u30c9\u3092\u547c\u3073\u51fa\u3059\u65b9\u6cd5\u306f\u3001Python\u3092\u4f7f\u3063\u3066\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u3057\u3066\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u3059\u3002 \u4ee5\u524d\u306f\u3001`pip`\u307e\u305f\u306f`pip3`\u30b3\u30de\u30f3\u30c9\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\/\u30b7\u30a7\u30eb\u4e0a\u3067\n\u76f4\u63a5\u5b9f\u884c\u3055\u308c\u307e\u3057\u305f\u3002 \u63d0\u6848\u3055\u308c\u305f\u65b9\u6cd5\u306f\u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002\n\nWindows\u306e\u521d\u671f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n`python -m pip install PySimpleGUI`\n\nLinux\u304a\u3088\u3073macOS\u306e\u521d\u671f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n`python3 -m pip install PySimpleGUI`\n\n`pip`\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3059\u308b\u306b\u306f\u3001\u5358\u306b2\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc`--upgrade --no-cache-dir`\u3092\u6307\u5b9a\u3059\u308b\u3060\u3051\u3067\u3059\u3002\n\nWindows\u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n`python -m pip install --upgrade --no-cache-dir PySimpleGUI`\n\nLinux\u304a\u3088\u3073macOS\u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\n\n`python3 -m pip install --upgrade --no-cache-dir PySimpleGUI`\n\n\n### \u5358\u4e00\u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nPySimpleGUI\u306fRaspberry Pi\u306e\u3088\u3046\u306a\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u306b\u63a5\u7d9a\u3055\u308c\u3066\u3044\u306a\u3044\u30b7\u30b9\u30c6\u30e0\u3067\u3082\u3001\u7c21\u5358\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u308b\u3088\u3046\u306b\u5358\u4e00\u306e.py \u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002 PySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3068\u540c\u3058\u30d5\u30a9\u30eb\u30c0\u30fc\u306b\u7f6e\u304f\u3060\u3051\u3067\u3059\u3002Python\u306f\u30a4\u30f3\u30dd\u30fc\u30c8\u6642\u306b\u30ed\u30fc\u30ab\u30eb\u306e\u30b3\u30d4\u30fc\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n.py\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408\u306f\u3001PyPI\u304b\u3089\u5165\u624b\u3059\u308b\u304b\u3001\u6700\u65b0\u306e\u672a\u30ea\u30ea\u30fc\u30b9\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5b9f\u884c\u3057\u305f\u3044\u5834\u5408\u306fGitHub\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\n\nPyPI\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u306b\u306f\u3001wheel\u307e\u305f\u306f.gz\u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u89e3\u51cd\u3057\u307e\u3059\u3002.whl\u30d5\u30a1\u30a4\u30eb\u3092zip\u306b\u30ea\u30cd\u30fc\u30e0\u3059\u308b\u3068\u3001\u901a\u5e38\u306ezip\u30d5\u30a1\u30a4\u30eb\u3068\u540c\u3058\u3088\u3046\u306b\u958b\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30d5\u30a9\u30eb\u30c0\u30fc\u306e\u4e2d\u306b\u306fPySimpleGUI.py\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d5\u30a9\u30eb\u30c0\u30fc\u306b\u30b3\u30d4\u30fc\u3059\u308b\u3068\u5b8c\u4e86\u3067\u3059\u3002\n\ntkinter\u30d0\u30fc\u30b8\u30e7\u30f3\u306ePySimpleGUI\u306ePyPI\u30ea\u30f3\u30af\u3067\u3059\nhttps:\/\/pypi.org\/project\/PySimpleGUI\/#files\n\nGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\u3001\u3053\u3061\u3089\u3067\u78ba\u8a8d\u3067\u304d\u307e\u3059\nhttps:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/PySimpleGUI.py\n\n\n\u300c\u305d\u3046\u3060\u3051\u3069\u3001\u5de8\u5927\u306a\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u30921\u3064\u3060\u3051\u6301\u3064\u306e\u306f\u306a\u3093\u3066\u3072\u3069\u3044\u8003\u3048\u3060\u300d\u3068\u4eca\u3001\u8003\u3048\u3066\u3044\u308b\u4eba\u3082\u3044\u308b\u3067\u3057\u3087\u3046\u3002 \u3053\u308c\u306f*\u6642\u306b\u306f*\u3072\u3069\u3044\u8003\u3048\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002 \u4eca\u56de\u306f\u3001\u30e1\u30ea\u30c3\u30c8\u306f\u30c7\u30e1\u30ea\u30c3\u30c8\u3092\u5927\u5e45\u306b\u4e0a\u56de\u308a\u307e\u3057\u305f\u3002 \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u6982\u5ff5\u306e\u591a\u304f\u306f\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u307e\u305f\u306f\u4e3b\u89b3\u7684\u306a\u3082\u306e\u3067\u3059\u3002 \u4e00\u90e8\u306e\u4eba\u304c\u671b\u3080\u306e\u3068\u540c\u3058\u304f\u3089\u3044\u3059\u3079\u3066\u304c\u767d\u9ed2\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u591a\u304f\u306e\u5834\u5408\u3001\u3053\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b\u7b54\u3048\u306f\u300c\u6b21\u7b2c\u300d\u3067\u3059\u3002\n\n\n\n## \u30ae\u30e3\u30e9\u30ea\u30fc :art:\n\n\u30e6\u30fc\u30b6\u30fc\u304c\u6295\u7a3f\u3057\u305fGUI\u3068GitHub\u306b\u3042\u308bGUI\u306e\u3088\u308a\u6b63\u5f0f\u306a\u30ae\u30e3\u30e9\u30ea\u30fc\u306e\u4f5c\u6210\u306f\u9032\u884c\u4e2d\u3067\u3059\u304c\u3001readme\u3092\u4f5c\u6210\u6642\u70b9\u3067\u306f\u307e\u3060\u5b8c\u6210\u3057\u3066\u3044\u307e\u305b\u3093\u3002\u73fe\u5728\u307e\u3068\u307e\u3063\u3066\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u898b\u308c\u308b\u5834\u6240\u306f2\u304b\u6240\u3042\u308a\u307e\u3059\u3002\u9858\u308f\u304f\u306f\u4eba\u3005\u304c\u4f5c\u3063\u3066\u3044\u308b\u7d20\u6674\u3089\u3057\u3044\u4f5c\u54c1\u3092\u6b63\u5f53\u5316\u3059\u308b\u305f\u3081\u306eWiki\u3084\u305d\u306e\u4ed6\u306e\u4ed5\u7d44\u307f\u304c\u3059\u3050\u306b\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u308b\u3053\u3068\u3092\u9858\u3063\u3066\u3044\u307e\u3059\u3002\n\n### \u30e6\u30fc\u30b6\u30fc\u304c\u63d0\u51fa\u3057\u305f\u30ae\u30e3\u30e9\u30ea\u30fc\n\n1\u3064\u76ee\u306f\u3001GitHub\u306b\u3042\u308b[\u30e6\u30fc\u30b6\u30fc\u304c\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092\u63d0\u51fa\u3057\u305fissue](https:\/\/github.com\/PySimpleGUI\/PySimpleGUI\/issues\/10)\u3067\u3059\u3002 \u3053\u308c\u306f\u3001\u4eba\u3005\u304c\u4f5c\u3063\u305f\u3082\u306e\u3092\u62ab\u9732\u3059\u308b\u305f\u3081\u306e\u975e\u516c\u5f0f\u306a\u65b9\u6cd5\u3067\u3059\u3002 \u7406\u60f3\u7684\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u30b9\u30bf\u30fc\u30c8\u3067\u3057\u305f\u3002\n\n### \u5927\u91cf\u306b\u30b9\u30af\u30e9\u30c3\u30d7\u3055\u308c\u305fGitHub\u306e\u753b\u50cf\n\n2\u3064\u76ee\u306f\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3068\u4f1d\u3048\u3089\u308c\u3066\u3044\u308bGitHub\u306e1,000\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304b\u96c6\u3081\u305f[3,000\u4ee5\u4e0a\u306e\u753b\u50cf\u306e\u5927\u898f\u6a21\u306a\u30ae\u30e3\u30e9\u30ea\u30fc ](https:\/\/www.dropbox.com\/sh\/g67ms0darox0i2p\/AAAMrkIM6C64nwHLDkboCWnaa?dl=0)\u3067\u3059\u3002 \u624b\u4f5c\u696d\u3067\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3055\u308c\u3066\u304a\u308a\u521d\u671f\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u305f\u53e4\u3044\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u304c\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u305d\u3053\u306b\u3042\u306a\u305f\u306e\u60f3\u50cf\u529b\u3092\u5f15\u304d\u8d77\u3053\u3059\u4f55\u304b\u304c\u898b\u3064\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n<hr>\n\n# PySimpleGUI\u306e\u7528\u9014\u306b\u3064\u3044\u3066 :hammer:\n\n\u6b21\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001PySimpleGUI\u306e\u7528\u9014\u306e\u4e00\u90e8\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 GitHub\u3060\u3051\u3067\u30821,000\u4ee5\u4e0a\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3060\u3051\u306e\u591a\u304f\u306e\u4eba\u3005\u306e\u53ef\u80fd\u6027\u304c\u5e83\u304c\u3063\u305f\u3053\u3068\u306f\u672c\u5f53\u306b\u9a5a\u304f\u3079\u304d\u3053\u3068\u3067\u3059\u3002 \u591a\u304f\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u4ee5\u524d\u306bPython\u3067GUI\u3092\u4f5c\u6210\u3057\u3088\u3046\u3068\u3057\u3066\u5931\u6557\u3057\u305f\u3068\u8a71\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u5f7c\u3089\u304cPySimpleGUI\u3092\u8a66\u3057\u3066\u307f\u3066\u6700\u7d42\u7684\u306b\u81ea\u5206\u306e\u5922\u3092\u9054\u6210\u3067\u304d\u305f\u3068\u8a71\u3057\u3066\u3044\u307e\u3059\u3002\n\n## \u6700\u521d\u306eGUI\n\n\u3082\u3061\u308d\u3093\u3001PySimpleGUI\u306e\u3082\u3063\u3068\u3082\u512a\u308c\u305f\u4f7f\u3044\u65b9\u306e1\u3064\u306fPython\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306eGUI\u3092\u4f5c\u308b\u3053\u3068\u3067\u3059\u3002 \u30d5\u30a1\u30a4\u30eb\u540d\u3092\u30ea\u30af\u30a8\u30b9\u30c8\u3059\u308b\u3060\u3051\u306e\u5c0f\u3055\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304b\u3089\u958b\u59cb\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u305f\u3081\u306b\u306f\u3001`popup`\u3068\u547c\u3070\u308c\u308b\u300c\u30cf\u30a4\u30ec\u30d9\u30eb\u95a2\u6570\u300d\u306e1\u3064\u30921\u56de\u547c\u3073\u51fa\u3059\u3060\u3051\u3067\u6e08\u307f\u307e\u3059\u3002 \u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u306b\u306f\u3042\u3089\u3086\u308b\u7a2e\u985e\u304c\u3042\u308a\u3001\u4e00\u90e8\u306f\u60c5\u5831\u3092\u53ce\u96c6\u3057\u307e\u3059\n\n`popup`\u81ea\u4f53\u3067\u60c5\u5831\u3092\u8868\u793a\u3059\u308b\u305f\u3081\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002print\u3068\u540c\u3058\u3088\u3046\u306b\u8907\u6570\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u6e21\u305b\u307e\u3059\u3002\u60c5\u5831\u3092\u53d6\u5f97\u3057\u305f\u3044\u5834\u5408\u306f\u3001`popup_get_filename`\u306e\u3088\u3046\u306b`popup_get_`\u3067\u59cb\u307e\u308b\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\u3057\u307e\u3059\u3002\n\n\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u6307\u5b9a\u3059\u308b\u4ee3\u308f\u308a\u306b\u3001\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e1\u884c\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u300c\u666e\u901a\u306e\u4eba\u300d\u304c\u5feb\u9069\u306b\u4f7f\u7528\u3067\u304d\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u5909\u8eab\u3057\u307e\u3059\u3002\n\n\n```python\nimport PySimpleGUI as sg\n\nfilename = sg.popup_get_file('\u51e6\u7406\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044')\nsg.popup('\u5165\u529b\u3057\u305f', filename)\n```\n\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u30012\u3064\u306e\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u8868\u793a\u3057\u307e\u3059\u3002 1\u3064\u306f\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u3067\u3001\u5165\u529b\u30dc\u30c3\u30af\u30b9\u306e\u95b2\u89a7\u3084\u30da\u30fc\u30b9\u30c8\u304c\u3067\u304d\u307e\u3059\u3002  \n\n<p align=\"center\">\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/popupgetfilename.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/popupgetfilename.jpg\"  alt=\"img\" width=\"400px\"><\/a>\n<\/p>\n\n\u3082\u3046\u4e00\u65b9\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u53ce\u96c6\u3055\u308c\u305f\u5185\u5bb9\u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\n<p align=\"center\">\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/popupyouentered.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/popupyouentered.jpg\"  alt=\"img\" width=\"175px\"><\/a>\n\n<\/p>\n\n\n<br>\n\n## Rainmeter\u98a8\u30b9\u30bf\u30a4\u30eb\u30a6\u30a3\u30f3\u30c9\u30a6\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/RainmeterStyleWidgets.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/RainmeterStyleWidgets.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\nGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u3067\u306f\u898b\u6804\u3048\u306e\u826f\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u4f5c\u6210\u3067\u304d\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u7d30\u90e8\u306b\u6ce8\u610f\u3059\u308b\u3053\u3068\u3067\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u9b45\u529b\u7684\u306b\u898b\u305b\u308b\u305f\u3081\u306b\u3044\u304f\u3064\u304b\u306e\u3053\u3068\u3092\u304a\u3053\u306a\u3048\u307e\u3059\u3002 PySimpleGUI\u306f\u3001\u8272\u3084\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u524a\u9664\u306a\u3069\u306e\u6a5f\u80fd\u3092\u3088\u308a\u7c21\u5358\u306b\u64cd\u4f5c\u3067\u304d\u307e\u3059\u3002 \u305d\u306e\u7d50\u679c\u3001\u5178\u578b\u7684\u306atkinter\u306e\u3088\u3046\u306b\u306f\u898b\u3048\u306a\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u3053\u3053\u3067\u306f\u3001\u5178\u578b\u7684\u306atkinter\u306e\u3088\u3046\u306b\u898b\u3048\u306a\u3044\u30a6\u30a3\u30f3\u30c9\u30a6\u3092Windows\u3067\u4f5c\u6210\u3059\u308b\u65b9\u6cd5\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u3053\u306e\u4f8b\u3067\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u30bf\u30a4\u30c8\u30eb \u30d0\u30fc\u304c\u524a\u9664\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u7d50\u679c\u3068\u3057\u3066\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u30d7\u30ed\u30b0\u30e9\u30e0\u306eRainmeter\u3088\u3046\u306b\u898b\u3048\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u3067\u304d\u3042\u304c\u308a\u307e\u3059\u3002\n\n<br><br>\n\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u900f\u660e\u5ea6\u3082\u7c21\u5358\u306b\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002 \u540c\u3058Rainmeter\u30b9\u30bf\u30a4\u30eb\u306e\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u4ed6\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002 \u534a\u900f\u660e\u306a\u306e\u3067\u3001\u8584\u6697\u304f\u8868\u793a\u3055\u308c\u308b\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/semi-transparent.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/semi-transparent.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\n\n\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u524a\u9664\u3068\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u534a\u900f\u660e\u5316\u306e\u4e21\u65b9\u306e\u52b9\u679c\u306f\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b2\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u5b9f\u73fe\u3057\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306fPySimpleGUI\u304c\u3044\u304b\u306b\u6a5f\u80fd\u306b\u7c21\u5358\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u304b\u3092\u793a\u3059\u4f8b\u3067\u3059\u3002 \u307e\u305f\u3001PySimpleGUI \u306e\u30b3\u30fc\u30c9\u306fGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u9593\u3067\u79fb\u690d\u53ef\u80fd\u306a\u306e\u3067\u3001Qt\u306e\u3088\u3046\u306a\u4ed6\u306e\u30dd\u30fc\u30c8\u3067\u3082\u540c\u3058\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002\n\n\n\u4f8b\uff11\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u4f5c\u6210\u306e\u547c\u3073\u51fa\u3057\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306b\u5909\u66f4\u3059\u308b\u3068\u540c\u69d8\u306e\u534a\u900f\u660e\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002\n```python\nwindow = sg.Window('My window', layout, no_titlebar=True, alpha_channel=0.5)\n```\n\n## \u30b2\u30fc\u30e0\n\n\u30b2\u30fc\u30e0\u958b\u767a\u7528\u306eSDK\u3068\u3057\u3066\u306f\u7279\u306b\u8a18\u8ff0\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u306f\u30b2\u30fc\u30e0\u306e\u958b\u767a\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u3057\u307e\u3059\u3002\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Chess.png\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Chess.png\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\u3053\u306e\u30c1\u30a7\u30b9\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u30c1\u30a7\u30b9\u3092\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u30c1\u30a7\u30b9AI\u300cStockfish\u300d\u3092\u7d71\u5408\u3057\u307e\u3059\u3002\n<br><br><br><br><br><br><br><br><br>\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Minesweeper.gif\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Minesweeper.gif\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u306e\u3044\u304f\u3064\u304b\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u304c\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\u3002\n\n<br><br><br><br>\n<br><br><br><br><br>\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/minesweeper_israel_dryer.png\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/minesweeper_israel_dryer.png\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n<br><br><br><br><br><br><br><br><br><br>\n\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Solitaire.gif\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Solitaire.gif\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n<br><br>\n\nPySimpleGUI\u306e`Graph`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3059\u308b\u3068\u753b\u50cf\u306e\u64cd\u4f5c\u304c\u7c21\u5358\u306a\u306e\u3067\u3001\u30ab\u30fc\u30c9\u30b2\u30fc\u30e0\u306fPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u7c21\u5358\u3067\u3059\u3002\n\n\u30b2\u30fc\u30e0\u958b\u767a\u7528\u306eSDK\u3068\u3057\u3066\u66f8\u304b\u308c\u305f\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u306f\u30b2\u30fc\u30e0\u306e\u958b\u767a\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u3057\u307e\u3059\u3002<br><br>\n<br><br>\n<br><br><br>\n\n\n## \u30e1\u30c7\u30a3\u30a2\u306e\u30ad\u30e3\u30d7\u30c1\u30e3\u3068\u518d\u751f\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/OpenCV.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/OpenCV.jpg\"  alt=\"img\" align=\"right\" width=\"400px\"><\/a>\n\n\nWEB\u30ab\u30e1\u30e9\u304b\u3089\u30d3\u30c7\u30aa\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3057\u3066GUI\u3067\u8868\u793a\u3059\u308b\u306e\u306b\u306f\u3001PySimpleGUI\u306e\u30b3\u30fc\u30c9\u3067\u306f4\u884c\u3067\u3067\u304d\u307e\u3059\u3002 \u3055\u3089\u306b\u5370\u8c61\u7684\u306a\u306e\u306f\u3053\u3089\u306e4\u884c\u306e\u30b3\u30fc\u30c9\u304ctkinter\u3001Qt\u3001\u304a\u3088\u3073 Web\u30dd\u30fc\u30c8\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002  tkinter\u3092\u4f7f\u7528\u3057\u3066\u753b\u50cf\u3092\u8868\u793a\u3059\u308b\u306e\u3068\u540c\u3058\u30b3\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u30d6\u30e9\u30a6\u30b6\u3067Web\u30ab\u30e1\u30e9\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u304c\u8868\u793a\u3067\u304d\u307e\u3059\u3002\n\n\u307e\u305f\u3001VLC\u30d7\u30ec\u30fc\u30e4\u30fc\u3092\u4f7f\u3063\u3066\u3001\u30aa\u30fc\u30c7\u30a3\u30aa\u3084\u30d3\u30c7\u30aa\u306a\u3069\u306e\u30e1\u30c7\u30a3\u30a2\u518d\u751f\u3082\u53ef\u80fd\u3067\u3059\u3002\u30c7\u30e2\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u5b9f\u969b\u306e\u4f5c\u696d\u4f8b\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u306ereadme\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u308b\u5185\u5bb9\u306f\u5168\u3066\u3001\u3042\u306a\u305f\u81ea\u8eab\u306e\u5275\u4f5c\u306e\u51fa\u767a\u70b9\u3068\u3057\u3066\u5229\u7528\u3067\u304d\u307e\u3059\u3002\n<br><br><br><br><br>\n<br><br><br><br><br>\n<br><br>\n## \u4eba\u5de5\u77e5\u80fd\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/YOLO_GIF.gif\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/YOLO_GIF.gif\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\n\nAI\u3068Python\u306f\u9577\u3044\u9593\u3001\u3053\u306e2\u3064\u304c\u7d44\u307f\u5408\u308f\u3055\u308c\u305f\u3068\u304d\u306e\u30b9\u30fc\u30d1\u30fc\u30d1\u30ef\u30fc\u3068\u3057\u3066\u8a8d\u8b58\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u591a\u304f\u306e\u5834\u5408\u3001\u30e6\u30fc\u30b6\u30fc\u304cGUI\u3092\u4f7f\u7528\u3057\u3066\u3053\u308c\u3089\u306eAI\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8eab\u8fd1\u306b\u64cd\u4f5c\u3059\u308b\u65b9\u6cd5\u304c\u6b20\u3051\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u308c\u3089\u306eYOLO\u306e\u30c7\u30e2\u306f\u3001GUI\u304cAI\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u306e\u5bfe\u8a71\u306b\u304a\u3044\u3066\u3044\u304b\u306b\u5927\u304d\u306a\u9055\u3044\u3092\u3082\u305f\u3089\u3059\u304b\u306e\u7d20\u6674\u3089\u3057\u3044\u4f8b\u3067\u3059\u3002 \u3053\u308c\u3089\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u4e0b\u90e8\u306b\u3042\u308b2\u3064\u306e\u30b9\u30e9\u30a4\u30c0\u30fc\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u3053\u306e2\u3064\u306e\u30b9\u30e9\u30a4\u30c0\u30fc\u306f\u3001YOLO\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u4f7f\u7528\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 \n\n\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066YOLO\u30c7\u30e2\u3092\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u5834\u5408\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u8d77\u52d5\u3059\u308b\u3068\u304d\u306b\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u8a2d\u5b9a\u3057\u3001\u305d\u306e\u5b9f\u884c\u65b9\u6cd5\u3092\u78ba\u8a8d\u3057\u3066\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u505c\u6b62\u3057\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u5909\u66f4\u3057\u3066\u6700\u5f8c\u306b\u65b0\u3057\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u518d\u8d77\u52d5\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n<br><br><br><br>\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/YOLO%20Object%20Detection.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/YOLO%20Object%20Detection.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\u3053\u308c\u3089\u306e\u30b9\u30c6\u30c3\u30d7\u3068\u3001GUI\u3092\u4f7f\u7528\u3057\u3066\u5b9f\u884c\u3067\u304d\u308b\u64cd\u4f5c\u3068\u6bd4\u8f03\u3057\u3066\u307f\u307e\u3059\u3002 GUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u3053\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u3067\u5909\u66f4\u3067\u304d\u307e\u3059\u3002 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u304b\u306b\u3064\u3044\u3066\u3001\u3059\u3050\u306b\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3092\u5f97\u3089\u308c\u307e\u3059\u3002\n\n\n\n<br><br><br><br><br>\n<br><br>\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Colorizer.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Colorizer.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\n\u516c\u958b\u3055\u308c\u3066\u3044\u308bAI\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u306f\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u52d5\u304b\u3059\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u975e\u5e38\u306b\u591a\u304f\u5b58\u5728\u3057\u307e\u3059\u3002 \u3053\u308c\u81ea\u4f53\u306f\u5927\u304d\u306a\u30cf\u30fc\u30c9\u30eb\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u30ab\u30e9\u30fc\u30ea\u30f3\u30b0\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5165\u529b\/\u8cbc\u308a\u4ed8\u3051\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3057\u3066\u3001\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30eb\u30d3\u30e5\u30fc\u30a2\u30fc\u3067\u958b\u304f\u306e\u306f\u5341\u5206\u300c\u9762\u5012\u304f\u3055\u3044\u300d\u3067\u3059\u3002\n\n\nGUI\u306b\u306f\u3001**\u30e6\u30fc\u30b6\u30fc\u30a8\u30af\u30b9\u30da\u30ea\u30a8\u30f3\u30b9\u3092\u5909\u66f4\u3059\u308b**\u3092\u300cGUI\u30ae\u30e3\u30c3\u30d7\u300d\u306b\u5909\u5316\u3055\u305b\u308b\u529b\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30ab\u30e9\u30fc\u30e9\u30a4\u30ba\u306e\u4f8b\u3067\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306f\u753b\u50cf\u304c\u683c\u7d0d\u3055\u308c\u3066\u305f\u30d5\u30a9\u30eb\u30c0\u30fc\u3092\u6307\u5b9a\u3057\u3066\u3001\u753b\u50cf\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3060\u3051\u3067\u30ab\u30e9\u30fc\u30ea\u30f3\u30b0\u3068\u7d50\u679c\u8868\u793a\u306e\u4e21\u65b9\u3092\u884c\u3048\u307e\u3059\u3002  \n\u30ab\u30e9\u30fc\u30e9\u30a4\u30ba\u3092\u884c\u3046\u30d7\u30ed\u30b0\u30e9\u30e0\/\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u81ea\u7531\u306b\u5229\u7528\u53ef\u80fd\u3067\u3001\u4f7f\u7528\u53ef\u80fd\u3067\u3057\u305f\u3002 \u4e0d\u8db3\u3057\u3066\u3044\u305f\u306e\u306fGUI\u304c\u3082\u305f\u3089\u3059\u4f7f\u3044\u3084\u3059\u3055\u3067\u3059\u3002\n\n\n<hr>\n\n## \u30b0\u30e9\u30d5\u5316\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/CPU%20Cores%20Dashboard%202.gif\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/CPU%20Cores%20Dashboard%202.gif\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\nGUI\u3067\u306e\u30c7\u30fc\u30bf\u306e\u8868\u793a\u3068\u64cd\u4f5c\u306fPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u7c21\u5358\u3067\u3059\u3002\u3044\u304f\u3064\u304b\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u3042\u308a\u307e\u3059\u3002\n\u7d44\u307f\u8fbc\u307f\u306e\u63cf\u753b\/\u30b0\u30e9\u30d5\u4f5c\u6210\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u30ab\u30b9\u30bf\u30e0\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u3053\u306eCPU\u4f7f\u7528\u7387\u30e2\u30cb\u30bf\u306f`Graph`\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n<br><br>\n<br><br>\n<br><br>\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Matplotlib.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Matplotlib.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\nMatplotlib\u306fPython\u30e6\u30fc\u30b6\u30fc\u306b\u4eba\u6c17\u304c\u3042\u308a\u307e\u3059\u3002 PySimpleGUI\u306f\u3001Matplotlib\u306e\u30b0\u30e9\u30d5\u3092GUI\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u76f4\u63a5\u57cb\u3081\u8fbc\u3081\u307e\u3059\u3002 Matplotlib\u306e\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u6a5f\u80fd\u3092\u4fdd\u6301\u3057\u305f\u3044\u5834\u5408\u306f\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u3092\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u57cb\u3081\u8fbc\u3080\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Matplotlib2.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Matplotlib2.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\nPySimpleGUI\u306e\u30ab\u30e9\u30fc\u30c6\u30fc\u30de\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u307b\u3068\u3093\u3069\u306e\u4eba\u304cMatplotlib\u3067\u4f5c\u6210\u3059\u308b\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30e9\u30d5\u3088\u308a\u3082\u4e00\u6bb5\u4e0a\u306e\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n<br><br>\n\n<hr>\n\n## \u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\n\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/JumpCutter.png\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/JumpCutter.png\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\u524d\u8ff0\u306e\u300cGUI\u30ae\u30e3\u30c3\u30d7\u300d\u306f\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u7c21\u5358\u306b\u89e3\u6c7a\u3067\u304d\u307e\u3059\u3002 GUI\u3092\u8ffd\u52a0\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u7528\u610f\u3059\u308b\u5fc5\u8981\u3082\u3042\u308a\u307e\u305b\u3093\u3002\u300c\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u300dGUI \u306f\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u6e21\u3059\u60c5\u5831\u3092\u53ce\u96c6\u3059\u308bGUI\u3067\u3059\u3002\n\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9GUI\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u3068\u3063\u3066\u30e6\u30fc\u30b6\u30fc\u304c\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30fb\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30a4\u30b9\u3092\u4f7f\u3044\u5fc3\u5730\u3088\u304f\u611f\u3058\u306a\u304b\u3063\u305f\u305f\u3081\u306b\u3001\u4ee5\u524d\u306f\u4f7f\u3044\u305f\u304c\u3089\u306a\u304b\u3063\u305f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u914d\u5e03\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u65b9\u6cd5\u3067\u3059\u3002\u3053\u308c\u3089\u306eGUI\u306f\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u305f\u3081\u306e\u552f\u4e00\u306e\u9078\u629e\u80a2\u3067\u3059\u3002\n\u3053\u306e\u4f8b\u306f\u3001\u300cJump Cutter\u300d\u3068\u3044\u3046\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30d5\u30ed\u30f3\u30c8\u30a8\u30f3\u30c9\u3067\u3059\u3002 \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306fGUI\u3092\u3068\u304a\u3057\u3066\u53ce\u96c6\u3055\u308c\u3066\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u4f7f\u7528\u3057\u3066\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u304c\u69cb\u7bc9\u3055\u308c\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u51fa\u529b\u304cGUI\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3055\u308c\u3066\u30b3\u30de\u30f3\u30c9\u304c\u5b9f\u884c\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u4f8b\u3067\u306f\u3001\u5b9f\u884c\u3055\u308c\u305f\u30b3\u30de\u30f3\u30c9\u304c\u9ec4\u8272\u3067\u8868\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n<br><br>\n<hr>\n\n## Raspberry Pi\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Raspberry%20Pi.jpg\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Raspberry%20Pi.jpg\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\nPySimpleGUI\u306fPython 3.4\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u305f\u3081\u3001Raspberry Pi\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306eGUI\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002 \u30bf\u30c3\u30c1\u30b9\u30af\u30ea\u30fc\u30f3\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3068\u3068\u304f\u306b\u3046\u307e\u304f\u6a5f\u80fd\u3057\u307e\u3059\u3002 \u30e2\u30cb\u30bf\u30fc\u304c\u63a5\u7d9a\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001PySimpleGUIWeb\u3092\u4f7f\u7528\u3057\u3066Pi\u3092\u5236\u5fa1\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n<br><br>\n<br><br>\n<br><br>\n<br><br><br>\n<hr>\n\n\n## \u9ad8\u5ea6\u306a\u6a5f\u80fd\u3078\u306e\u7c21\u5358\u306a\u30a2\u30af\u30bb\u30b9\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Customized%20Titlebar.gif\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Customized%20Titlebar.gif\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\n\u57fa\u790e\u3068\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u591a\u304f\u306e\u6a5f\u80fd\u306b\u975e\u5e38\u306b\u7c21\u5358\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u305f\u3081\u3001GUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u76f4\u63a5\u4f7f\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u306f\u898b\u3048\u306a\u3044\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u308b\u305f\u3081\u306e\u6a5f\u80fd\u3092\u7d44\u307f\u5408\u308f\u305b\u3089\u308c\u307e\u3059\u3002\n\n\u305f\u3068\u3048\u3070\u3001tkinter\u3084\u305d\u306e\u4ed6\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3057\u3066\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u306e\u8272\u3084\u5916\u898b\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u304c\u3001PySimpleGUI\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u30ab\u30b9\u30bf\u30e0\u30bf\u30a4\u30c8\u30eb\u30d0\u30fc\u3092\u6301\u3063\u3066\u3044\u308b\u304b\u306e\u3088\u3046\u306b\u8868\u793a\u3055\u308c\u308b\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n<br><br><br>\n\n<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Desktop%20Bouncing%20Balls.gif\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/Desktop%20Bouncing%20Balls.gif\"  alt=\"img\" align=\"right\" width=\"500px\"><\/a>\n\n\u4fe1\u3058\u3089\u308c\u306a\u3044\u3053\u3068\u306b\u3001\u3053\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u306f\u30b9\u30af\u30ea\u30fc\u30f3\u30bb\u30fc\u30d0\u30fc\u306e\u3088\u3046\u306b\u898b\u3048\u308b\u3082\u306e\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306btkinter\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u30a6\u30a3\u30f3\u30c9\u30a6\u3067\u306ftkinter \u306f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304b\u3089\u80cc\u666f\u3092\u5b8c\u5168\u306b\u53d6\u308a\u9664\u3051\u307e\u3059\u3002 \u7e70\u308a\u8fd4\u3057\u307e\u3059\u304cPySimpleGUI\u306f\u3053\u308c\u3089\u306e\u6a5f\u80fd\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3092\u7c21\u5358\u306b\u3057\u307e\u3059\u3002 \u900f\u660e\u306a\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f\u3001`Window`\u3092\u4f5c\u6210\u3059\u308b\u547c\u3073\u51fa\u3057\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u30921\u3064\u8ffd\u52a0\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 1\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u5909\u66f4\u3060\u3051\u3067\u3001\u6b21\u306e\u52b9\u679c\u3092\u6301\u3064\u5358\u7d14\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002\n\n\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u4e0a\u306e\u3059\u3079\u3066\u306e\u3082\u306e\u3092\u30d5\u30eb\u30b9\u30af\u30ea\u30fc\u30f3\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u64cd\u4f5c\u3067\u304d\u307e\u3059\u3002\n<hr>\n\n# \u30c6\u30fc\u30de\n\n\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30ec\u30fc\u306eGUI\u306b\u3046\u3093\u3056\u308a\u3057\u307e\u3057\u305f\u304b?  PySimpleGUI \u306f`theme`\u95a2\u6570\u306e\u547c\u3073\u51fa\u3057\u3092\u884c\u3046\u3053\u3060\u3051\u3067\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u898b\u305f\u76ee\u3092\u7d20\u6575\u306b\u3057\u307e\u3059\u3002 150\u7a2e\u985e\u4ee5\u4e0a\u306e\u30ab\u30e9\u30fc\u30c6\u30fc\u30de\u3092\u9078\u629e\u3067\u304d\u307e\u3059:\n<p align=\"center\">\n<a href=\"\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/ThemePreview.jpg\"  alt=\"img\" width=\"900px\"><\/a>\n<\/p>\n\n\n\u307b\u3068\u3093\u3069\u306eGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u306f\u3001\u4f5c\u6210\u3059\u308b\u3059\u3079\u3066\u306e\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u8272\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002  PySimpleGUI\u306f\u3001\u3053\u306e\u96d1\u7528\u3092\u4ee3\u308f\u308a\u306b\u884c\u3044\u81ea\u52d5\u7684\u306b\u9078\u629e\u3057\u305f\u30c6\u30fc\u30de\u306b\u5408\u308f\u305b\u3066\u30a8\u30ec\u30e1\u30f3\u30c8\u3092\u8272\u4ed8\u3051\u3057\u307e\u3059\u3002\n\n\u30c6\u30fc\u30de\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u524d\u306b\u30c6\u30fc\u30de\u540d\u3092\u6307\u5b9a\u3057\u3066`theme`\u95a2\u6570\u3092\u547c\u3073\u51fa\u3057\u307e\u3059\u3002\u8aad\u307f\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u30b9\u30da\u30fc\u30b9\u3092\u8ffd\u52a0\u3067\u304d\u307e\u3059\u3002 \u30c6\u30fc\u30de\u3092\u300cdark grey 9\u300d\u306b\u8a2d\u5b9a\u3059\u308b\u306b\u306f\n```python\nimport PySimpleGUI as sg\n\nsg.theme('dark grey 9')\n```\n\n\u3053\u306e1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u30a6\u30a3\u30f3\u30c9\u30a6\u306e\u5916\u89b3\u3092\u5b8c\u5168\u306b\u5909\u66f4\u3057\u307e\u3059:\n<p align=\"center\">\n<a href=\"\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/DarkGreyJapanese.jpg\"  alt=\"img\" width=\"400px\"><\/a>\n<\/p>\n\n\n\u30c6\u30fc\u30de\u306f\u3001\u80cc\u666f\u3001\u30c6\u30ad\u30b9\u30c8\u3001\u5165\u529b\u80cc\u666f\u3001\u5165\u529b\u30c6\u30ad\u30b9\u30c8\u3001\u304a\u3088\u3073\u30dc\u30bf\u30f3\u306e\u8272\u3092\u5909\u66f4\u3057\u307e\u3057\u305f\u3002 \u3053\u306e\u3088\u3046\u306a\u914d\u8272\u3092\u5909\u66f4\u3059\u308b\u4ed6\u306eGUI\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u306f\u3001\u5404\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u8272\u3092\u500b\u5225\u306b\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u30b3\u30fc\u30c9\u3092\u4f55\u5ea6\u3082\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n<hr>\n\n# \u30b5\u30dd\u30fc\u30c8:muscle:\n\n\u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u306f[\u30c9\u30ad\u30e5\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3](http:\/\/www.PySimpleGUI.org)\u3068[\u30c7\u30e2\u30d7\u30ed\u30b0\u30e9\u30e0](http:\/\/Demos.PySimpleGUI.org)\u3067\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002 \u3067\u3059\u3002\u3082\u3057\u307e\u3060\u8cea\u554f\u304c\u3042\u3063\u305f\u308a\u3001\u30d8\u30eb\u30d7\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f...\u554f\u984c\u3042\u308a\u307e\u305b\u3093...\u30d8\u30eb\u30d7\u306f\u7121\u6599\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002PySimpleGUI\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3067[Issue\u3092\u63d0\u51fa](http:\/\/Issues.PySimpleGUI.org)\u3059\u308b\u3060\u3051\u3067\u3001\u52a9\u3051\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\n\n\u307b\u3068\u3093\u3069\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u4f1a\u793e\u306f\u3001\u30d0\u30b0\u30ec\u30dd\u30fc\u30c8\u306b\u4ed8\u968f\u3059\u308b\u30d5\u30a9\u30fc\u30e0\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002 \u305d\u308c\u306f\u60aa\u3044\u53d6\u5f15\u3067\u306f\u3042\u308a\u307e\u305b\u3093.\u30d5\u30a9\u30fc\u30e0\u306b\u5fc5\u8981\u4e8b\u9805\u8a18\u5165\u3059\u308c\u3070\u3001\u7121\u6599\u3067\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30b5\u30dd\u30fc\u30c8\u3092\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\u3053\u306e\u60c5\u5831\u306f\u52b9\u7387\u7684\u306b\u56de\u7b54\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\n\nPySimpleGUI\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u756a\u53f7\u3084\u57fa\u306b\u306a\u308bGUI\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306a\u3069\u306e\u60c5\u5831\u3092\u8981\u6c42\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u554f\u984c\u306e\u89e3\u6c7a\u306b\u5f79\u7acb\u3064\u304b\u3082\u3057\u308c\u306a\u3044\u9805\u76ee\u306e\u30c1\u30a7\u30c3\u30af\u30ea\u30b9\u30c8\u3082\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\n***\u30d5\u30a9\u30fc\u30e0\u306b\u8a18\u5165\u3057\u3066\u304f\u3060\u3055\u3044 \u3002*** \u3000\u3042\u306a\u305f\u306b\u306f\u7121\u610f\u5473\u306b\u611f\u3058\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u307b\u3093\u306e\u4e00\u77ac\u3067\u3059\u304c\u82e6\u75db\u306b\u611f\u3058\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u8a18\u5165\u306f\u3042\u306a\u305f\u304c\u3088\u308a\u65e9\u304f\u89e3\u6c7a\u7b56\u3092\u5f97\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u3082\u3057\u3042\u306a\u305f\u304c\u30b9\u30d4\u30fc\u30c7\u30a3\u30fc\u306a\u56de\u7b54\u3068\u89e3\u6c7a\u3092\u5f97\u308b\u305f\u3081\u306b\u5f79\u7acb\u3064\u5fc5\u8981\u306a\u60c5\u5831\u3067\u306a\u3051\u308c\u3070\u3001\u8a18\u5165\u306f\u5fc5\u8981\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u300c\u79c1\u306f\u3042\u306a\u305f\u3092\u52a9\u3051\u308b\u305f\u3081\u306b\u79c1\u3092\u52a9\u3051\u308b\u300d\u3002\n\n\n# \u652f\u63f4\u3059\u308b\t<a href=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/PSGSuperHero.png\"><img src=\"https:\/\/raw.githubusercontent.com\/PySimpleGUI\/PySimpleGUI\/master\/images\/for_readme\/PSGSuperHero.png\"  alt=\"img\"  width=\"90px\"><\/a>\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u8ca1\u653f\u7684\u652f\u63f4\u306f\u975e\u5e38\u306b\u9ad8\u304f\u8a55\u4fa1\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u6b63\u76f4\u306b\u8a00\u3046\u3068\u3001\u7d4c\u6e08\u7684\u306a\u63f4\u52a9\u304c\u5fc5\u8981\u3067\u3059\u3002 \u30e9\u30a4\u30c8\u3092\u3064\u3051\u7d9a\u3051\u308b\u3060\u3051\u3067\u9ad8\u4fa1\u3067\u3059\u3002 \u30c9\u30e1\u30a4\u30f3\u540d\u767b\u9332\u3001\u30c8\u30ea\u30f3\u30b1\u30c3\u30c8\u3001\u30b3\u30f3\u30b5\u30eb\u30c6\u30a3\u30f3\u30b0\u30d8\u30eb\u30d7\u306a\u3069\u306e\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u306e\u9577\u3044\u30ea\u30b9\u30c8\u306f\u3001\u3059\u3050\u306b\u304b\u306a\u308a\u306e\u7d4c\u5e38\u30b3\u30b9\u30c8\u306b\u52a0\u7b97\u3055\u308c\u307e\u3059\u3002\n\nPySimpleGUI\u306e\u958b\u767a\u306f\u5b89\u304f\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u611b\u60c5\u3092\u3053\u3081\u3066\u958b\u767a\u3057\u305f\u3068\u306f\u3044\u3048\u4f55\u5e74\u306b\u3082\u308f\u305f\u3063\u3066\u975e\u5e38\u306b\u624b\u9593\u306e\u304b\u304b\u308b\u958b\u767a\u3067\u3057\u305f\u3002\u3053\u3093\u306b\u3061\u306e\u59ff\u306b\u306a\u308b\u306e\u306b\u304b\u306a\u308a\u306e\u6642\u9593\u3092\u3064\u3044\u3084\u3057\u307e\u3057\u305f\u3002\u73fe\u5728\u3082\u7d9a\u3051\u3066\u3044\u307e\u3059\u3002\n\nPySimpleGUI\u306b\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u3042\u308a\u3001\u305d\u306e\u307e\u307e\u6b8b\u308b\u3053\u3068\u304c\u3067\u304d\u308c\u3070\u7d20\u6674\u3089\u3057\u3044\u3053\u3068\u3067\u3059\u3002 \u304a\u5ba2\u69d8\u307e\u305f\u306f\u304a\u5ba2\u69d8\u306e\u4f1a\u793e (\u7279\u306b\u4f01\u696d\u3067PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u5834\u5408) \u304c\u3001PySimpleGUI\u3092\u4f7f\u7528\u3057\u3066\u7d4c\u6e08\u7684\u306b\u5229\u76ca\u3092\u5f97\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5bff\u547d\u3092\u5ef6\u9577\u3059\u308b\u6a5f\u4f1a\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\n\n###\u3000Buy Me a Coffee\n\n\u300cBuy Me a Coffee\u300d\u306f\u3001\u958b\u767a\u8005\u3092\u516c\u7684\u306b\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u65b9\u6cd5\u3067\u3059\u3002 \u7d20\u65e9\u304f\u3001\u7c21\u5358\u306b\u3001\u8ca2\u732e\u306f\u8a18\u9332\u3055\u308c\u308b\u306e\u3067\u3001\u3042\u306a\u305f\u304cPySimpleGUI\u306e\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3042\u308b\u3053\u3068\u3092\u4ed6\u306e\u4eba\u306b\u898b\u305b\u3089\u308c\u307e\u3059\u3002\u5bc4\u4ed8\u3092\u975e\u516c\u958b\u306b\u3082\u3067\u304d\u307e\u3059\u3002\n\n<a href=\"https:\/\/www.buymeacoffee.com\/PySimpleGUI\" target=\"_blank\"><img src=\"https:\/\/cdn.buymeacoffee.com\/buttons\/v2\/arial-yellow.png\" alt=\"Buy Me A Coffee\" width=\"217px\" ><\/a>\n\n\n\n\n### GitHub\u30b9\u30dd\u30f3\u30b5\u30fc\n\n<a href=\"https:\/\/github.com\/sponsors\/PySimpleGUI\" target=\"_blank\"><img src=\"https:\/\/img.shields.io\/static\/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&link=%3Curl%3E&color=f88379\"><\/a>\n\n[GitHub\u5b9a\u671f\u7684\u306a\u30b9\u30dd\u30f3\u30b5\u30fc\u30b7\u30c3\u30d7](https:\/\/github.com\/sponsors\u30fc\/PySimpleGUI)\u306f\u3001\u7d99\u7d9a\u7684\u306b\u3055\u307e\u3056\u307e\u306a\u30ec\u30d9\u30eb\u306e\u30b5\u30dd\u30fc\u30c8\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u30b9\u30dd\u30f3\u30b5\u30fc\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u591a\u304f\u306e\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u304c\u4f01\u696d\u30ec\u30d9\u30eb\u306e\u30b9\u30dd\u30f3\u30b5\u30fc\u30b7\u30c3\u30d7\u3092\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u91d1\u92ad\u7684\u306b\u8ca2\u732e\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3001\u975e\u5e38\u306b\u3042\u308a\u304c\u305f\u3044\u3067\u3059\u3002\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u958b\u767a\u8005\u3067\u3042\u308b\u3053\u3068\u306f\u3001\u7d4c\u6e08\u7684\u306b\u56f0\u96e3\u3067\u3059\u3002YouTube\u52d5\u753b\u306e\u30af\u30ea\u30a8\u30a4\u30bf\u30fc\u306f\u3001\u52d5\u753b\u4f5c\u6210\u3067\u751f\u8a08\u3092\u7acb\u3066\u3066\u3044\u307e\u3059\u3002\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u306b\u3068\u3063\u3066\u306f\u307e\u3060\u305d\u308c\u307b\u3069\u7c21\u5358\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\n\n# \u611f\u8b1d\u306e\u6c17\u6301\u3061\u3092\u3053\u3081\u3066\n\n<!--\nTo everyone that's helped, in whatever fashion, I'm very very grateful.\n\nEven taking a moment to say \"thank you\" helps, and a HUGE number of you have done that.  It's been an amazing number actually.  I value these thanks and find inspiration in the words alone.  Every message is a little push forward.  It adds a little bit of energy and keeps the whole project's momentum.  I'm so very grateful to everyone that's helped in whatever form it's been.\n-->\n\u3069\u3093\u306a\u5f62\u3067\u3082\u5354\u529b\u3057\u3066\u304f\u308c\u305f\u7686\u3055\u3093\u306b\u306f\u3068\u3066\u3082\u611f\u8b1d\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u4e00\u77ac\u3067\u3082 \"\u3042\u308a\u304c\u3068\u3046 \"\u3068\u8a00\u3063\u3066\u304f\u308c\u308b\u3060\u3051\u3067\u3082\u52a9\u304b\u308b\u3057\u3001\u3068\u3066\u3082\u591a\u304f\u306e\u4eba\u305f\u3061\u304c\u304c\u305d\u3046\u3057\u3066\u304f\u308c\u307e\u3057\u305f\u305f\u3002 \u5b9f\u969b\u3001\u9a5a\u304f\u3079\u304d\u4eba\u6570\u3067\u3059\u3002 \u79c1\u306f\u3053\u306e\u611f\u8b1d\u306e\u6c17\u6301\u3061\u3092\u5927\u5207\u306b\u3057\u3066\u305d\u306e\u8a00\u8449\u3060\u3051\u3067\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u5f97\u3066\u3044\u307e\u3059\u3002 \u3059\u3079\u3066\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u5c11\u3057\u305a\u3064\u524d\u9032\u3057\u3066\u3044\u307e\u3059\u3002 \u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u3061\u3087\u3063\u3068\u3057\u305f\u30a8\u30cd\u30eb\u30ae\u30fc\u3068\u306a\u3063\u3066\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u5168\u4f53\u306e\u52e2\u3044\u3092\u4fdd\u3063\u3066\u3044\u307e\u3059\u3002 \u3069\u306e\u3088\u3046\u306a\u5f62\u3067\u3042\u308c\u5354\u529b\u3057\u3066\u304f\u308c\u305f\u7686\u3055\u3093\u306b\u306f\u672c\u5f53\u306b\u611f\u8b1d\u3057\u3066\u3044\u307e\u3059\u3002\n\n# Contributing  \ud83d\udc77\n\n# \u8ca2\u732e:construction_worker:\n\n\u73fe\u5728\u3001PySimpleGUI\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u30e9\u30a4\u30bb\u30f3\u30b9\u3055\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u81ea\u4f53\u306f\u88fd\u54c1\u306e\u3088\u3046\u306b\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u306f\u53d7\u3051\u4ed8\u3051\u3066\u3044\u307e\u305b\u3093\u3002\n\n\u30b3\u30fc\u30c9\u306b\u8ca2\u732e\u3059\u308b\u6700\u826f\u306e\u65b9\u6cd5\u306e1\u3064\u306f\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u66f8\u3044\u3066\u516c\u958b\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u306f\u4ed6\u306e\u30e6\u30fc\u30b6\u30fc\u304c\u4f5c\u3063\u305f\u3082\u306e\u3092\u898b\u3066\u523a\u6fc0\u3092\u53d7\u3051\u307e\u3059\u3002GitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3066\u30b3\u30fc\u30c9\u3092\u6295\u7a3f\u3057te\n\u3001\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8\u3092readme\u30d5\u30a1\u30a4\u30eb\u306b\u5165\u308c\u307e\u3057\u3087\u3046\u3002\n\n\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a5f\u80fd\u304c\u3042\u3063\u305f\u308a\u3001\u6a5f\u80fd\u5f37\u5316\u3092\u63d0\u6848\u3057\u305f\u3044\u5834\u5408\u306f\u3001[issue\u3092\u958b\u3044\u3066\u304f\u3060\u3055\u3044](https:\/\/github.com\/PySimpleGUI\/PySimpleGUI\/issues\/new?assignees=&labels=&template=issue-form---must-fill-in-this-form-with-every-new-issue-submitted.md&title=%5B+Enhancement%2FBug%2FQuestion%5D+My+problem+is.) \u3002\n\n\n# \u7279\u5225\u306a\u611f\u8b1d :pray:\n\n<!--\nThis version of the PySimpleGUI readme wouldn't have come together without the help from @M4cs. He's a fantastic developer and has been a PySimpleGUI supporter since the project's launch. @israel-dryer is another long-term supporter and has written several PySimpleGUI programs that pushed the envelope of the package's capabilities. The unique minesweeper that uses an image for the board was created by Israel. @jason990420 surprised many when he published the first card game using PySimpleGUI that you see pictured above as well as the first minesweeper game made with PySimpleGUI. @Chr0nicT is the youngest developer I've worked with, ever, on projects. This kid shocks me on a regular basis. Ask for a capability, such as the PySimpleGUI GitHub Issues form error checking bot, and it simply happens regardless of the technologies involved. I'm fortunate that we were introduced. Someday he's going to be whisked away, but until then we're all benefiting from his talent. The Japanese version of the readme was greatly improved with help from @okajun35. @nngogol has had a very large impact on the project, also getting involved with PySimpleGUI in the first year of initial release. He wrote a designer, came up with the familiar window[key] lookup syntax, wrote the tools that create the documentation, designed the first set of doc strings as well as tools that generate the online documenation using the PySimpleGUI code itself. PySimpleGUI would not be where it is today were it not for the help of these individuals.\n\nThe more than 2,200 GitHub repos that use PySimpleGUI are owed a \"Thank You\" as well, for it is you that has been the inspiration that fuels this project's engine.\n\nThe overseas users that post on Twitter overnight are the spark that starts the day's work on PySimpleGUI. They've been a source of positive energy that gets the development engine started and ready to run every day. As a token of appreciation, this readme file has been translated into Japanese.\n\nYou've all been the best user community an Open Source developer could hope for.\n-->\n\n\u3053\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306ePySimpleGUIreadme\u306f\u3001[@M4cs](https:\/\/github.com\/M4cs)\u306e\u52a9\u3051\u304c\u306a\u3051\u308c\u3070\u5b8c\u6210\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u5f7c\u306f\u7d20\u6674\u3089\u3057\u3044\u958b\u767a\u8005\u3067\u3042\u308a\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u7acb\u3061\u4e0a\u3052\u4ee5\u6765\u3001PySimpleGUI\u306e\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3059\u3002 [@israel-dryer](https:\/\/github.com\/israel-dryer)\u306f\u3001\u3082\u30461\u3064\u306e\u9577\u671f\u7684\u306a\u30b5\u30dd\u30fc\u30bf\u30fc\u3067\u3042\u308a\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u6a5f\u80fd\u306e\u9650\u754c\u3092\u62bc\u3057\u4e0a\u3052\u308b\u3044\u304f\u3064\u304b\u306ePySimpleGUI\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\u30dc\u30fc\u30c9\u306e\u753b\u50cf\u3092\u4f7f\u7528\u3059\u308b\u30e6\u30cb\u30fc\u30af\u306a\u6383\u6d77\u8247\u306f\u3001israel\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002 [@jason990420](https:\/\/github.com\/jason990420)\u306f\u3001\u4e0a\u306e\u5199\u771f\u306b\u3042\u308bPySimpleGUI\u3092\u4f7f\u7528\u3057\u305f\u6700\u521d\u306e\u30ab\u30fc\u30c9\u30b2\u30fc\u30e0\u3068\u3001PySimpleGUI\u3067\u4f5c\u6210\u3055\u308c\u305f\u6700\u521d\u306e\u30de\u30a4\u30f3\u30b9\u30a4\u30fc\u30d1\u30b2\u30fc\u30e0\u3092\u516c\u958b\u3057\u305f\u3068\u304d\u306b\u591a\u304f\u306e\u4eba\u3092\u9a5a\u304b\u305b\u307e\u3057\u305f[@Chr0nicT](https:\/\/github.com\/Chr0nicT)\u306f\u3053\u308c\u307e\u3067\u4e00\u7dd2\u306b\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u9032\u3081\u3066\u304d\u305f\u4e2d\u3067\u6700\u5e74\u5c11\u306e\u958b\u767a\u8005\u3067\u3059\u3002\u3053\u306e\u5b50\u306f\u5b9a\u671f\u7684\u306b\u79c1\u3092\u9a5a\u304b\u305b\u3066\u304f\u308c\u307e\u3059\u3002\u4f8b\u3048\u3070PySimpleGUI\u306eGitHub Issues\u30d5\u30a9\u30fc\u30e0\u306e\u30a8\u30e9\u30fc\u30c1\u30a7\u30c3\u30af\u30dc\u30c3\u30c8\u306e\u3088\u3046\u306a\u6a5f\u80fd\u3092\u6c42\u3081\u308b\u3068\u3001\u95a2\u4fc2\u3059\u308b\u6280\u8853\u306b\u95a2\u308f\u3089\u305a\u7c21\u5358\u306b\u5b9f\u73fe\u3057\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u3002\u7e01\u304c\u3042\u3063\u3066\u51fa\u4f1a\u3044\u307e\u3057\u305f\u3002\u3044\u3064\u306e\u65e5\u304b\u5f7c\u306f\u53bb\u3063\u3066\u3057\u307e\u3046\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u305d\u308c\u307e\u3067\u306f\u79c1\u305f\u3061\u306f\u5f7c\u306e\u624d\u80fd\u304b\u3089\u6069\u6075\u3092\u53d7\u3051\u3066\u3044\u307e\u3059\u3002\u65e5\u672c\u8a9e\u7248\u306ereadme\u306f[@okajun35](https:\/\/github.com\/okajun35)\u306e\u52a9\u3051\u3092\u501f\u308a\u3066\u5927\u5e45\u306b\u6539\u5584\u3055\u308c\u307e\u3057\u305f\u3002 [@nngogol](https:\/\/github.com\/nngogol)\u306f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u975e\u5e38\u306b\u5927\u304d\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u3001\u521d\u671f\u30ea\u30ea\u30fc\u30b9\u306e\u6700\u521d\u306e\u5e74\u306b\u306f PySimpleGUI\u306b\u95a2\u308f\u3063\u3066\u304f\u308c\u307e\u3057\u305f\u3002\u5f7c\u306f\u30c7\u30b6\u30a4\u30ca\u30fc\u3092\u66f8\u304d\u3001\u304a\u306a\u3058\u307f\u306ewindow[key] \u30eb\u30c3\u30af\u30a2\u30c3\u30d7\u30b7\u30f3\u30bf\u30c3\u30af\u30b9\u3092\u8003\u6848\u3057\u307e\u3057\u305f\u3002\u307e\u305f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u30c4\u30fc\u30eb\u3092\u66f8\u304d\u3044\u3066\u6700\u521d\u306edoc strings\u3092\u8a2d\u5b9a\u3057\u3066\u3001\u3001PySimpleGUI\u30b3\u30fc\u30c9\u81ea\u4f53\u3092\u4f7f\u3063\u3066\u30aa\u30f3\u30e9\u30a4\u30f3\u30fb\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u751f\u6210\u3059\u308b\u30c4\u30fc\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u4eba\u3005\u306e\u52a9\u3051\u304c\u306a\u3051\u308c\u3070PySimpleGUI\u306f\u4eca\u65e5\u306e\u3088\u3046\u306b\u306f\u306a\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n\nPySimpleGUI\u3092\u4f7f\u7528\u3059\u308b1,200\u3092\u8d85\u3048\u308bGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3082\u300c\u3042\u308a\u304c\u3068\u3046\u300d\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u3002\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30a8\u30f3\u30b8\u30f3\u3092\u52d5\u304b\u3057\u3066\u3044\u308b\u306e\u306f\u3001\u3042\u306a\u305f\u306e\u30a4\u30f3\u30b9\u30d4\u30ec\u30fc\u30b7\u30e7\u30f3\u3067\u3059\u3002\n\n\u4e00\u6669\u4e2dTwitter\u306b\u6295\u7a3f\u3057\u3066\u304f\u308c\u308b\u6d77\u5916\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u3001PySimpleGUI\u306e\u4e00\u65e5\u306e\u4f5c\u696d\u3092\u59cb\u3081\u308b\u304d\u3063\u304b\u3051\u3068\u306a\u308a\u307e\u3059\u3002\u5f7c\u3089\u306f\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u30a8\u30cd\u30eb\u30ae\u30fc\u306e\u6e90\u3067\u3042\u308a\u3001\u958b\u767a\u30a8\u30f3\u30b8\u30f3\u3092\u59cb\u52d5\u3055\u305b\u3001\u6bce\u65e5\u7a3c\u50cd\u3055\u305b\u308b\u6e96\u5099\u3092\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002\u611f\u8b1d\u306e\u610f\u3092\u8fbc\u3081\u3066\u3001\u3053\u306ereadme\u30d5\u30a1\u30a4\u30eb\u3092[\u65e5\u672c\u8a9e](https:\/\/github.com\/PySimpleGUI\/PySimpleGUI\/blob\/master\/readme.ja.md)\u306b\u7ffb\u8a33\u3057\u307e\u3057\u305f\u3002\n\n\u7686\u3055\u3093\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u304c\u671b\u3080\u6700\u9ad8\u306e\u30e6\u30fc\u30b6\u30fc\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3067\u3059\u3002\n\n\n\n&copy; Copyright 2021 PySimpleGUI \n","112":"<!-- markdownlint-disable -->\n<h1 align=\"center\">\n    Best-of Machine Learning with Python\n    <br>\n<\/h1>\n\n<p align=\"center\">\n    <strong>\ud83c\udfc6&nbsp; A ranked list of awesome machine learning Python libraries. Updated weekly.<\/strong>\n<\/p>\n\n<p align=\"center\">\n    <a href=\"https:\/\/github.com\/ml-tooling\/best-of\" title=\"Best-of-badge\"><img src=\"http:\/\/bit.ly\/3o3EHNN\"><\/a>\n    <a href=\"#Contents\" title=\"Project Count\"><img src=\"https:\/\/img.shields.io\/badge\/projects-900-blue.svg?color=5ac4bf\"><\/a>\n    <a href=\"#Contribution\" title=\"Contributions are welcome\"><img src=\"https:\/\/img.shields.io\/badge\/contributions-welcome-green.svg\"><\/a>\n    <a href=\"https:\/\/github.com\/ml-tooling\/best-of-ml-python\/releases\" title=\"Best-of Updates\"><img src=\"https:\/\/img.shields.io\/github\/release-date\/ml-tooling\/best-of-ml-python?color=green&label=updated\"><\/a>\n    <a href=\"https:\/\/mltooling.substack.com\/subscribe\" title=\"Subscribe to newsletter\"><img src=\"http:\/\/bit.ly\/2Md9rxM\"><\/a>\n    <a href=\"https:\/\/twitter.com\/mltooling\" title=\"Follow on Twitter\"><img src=\"https:\/\/img.shields.io\/twitter\/follow\/mltooling.svg?style=social&label=Follow\"><\/a>\n<\/p>\n\nThis curated list contains 900 awesome open-source projects with a total of 3.3M stars grouped into 34 categories. All projects are ranked by a project-quality score, which is calculated based on various metrics automatically collected from GitHub and different package managers. If you like to add or update projects, feel free to open an [issue](https:\/\/github.com\/ml-tooling\/best-of-ml-python\/issues\/new\/choose), submit a [pull request](https:\/\/github.com\/ml-tooling\/best-of-ml-python\/pulls), or directly edit the [projects.yaml](https:\/\/github.com\/ml-tooling\/best-of-ml-python\/edit\/main\/projects.yaml). Contributions are very welcome!\n\n---\n\n<p align=\"center\">\n     \ud83e\uddd9\u200d\u2642\ufe0f&nbsp; Discover other <a href=\"https:\/\/best-of.org\">best-of lists<\/a> or create <a href=\"https:\/\/github.com\/best-of-lists\/best-of\/blob\/main\/create-best-of-list.md\">your own<\/a>.<br>\n    \ud83d\udceb&nbsp; Subscribe to our <a href=\"https:\/\/mltooling.substack.com\/subscribe\">newsletter<\/a> for updates and trending projects.\n<\/p>\n\n---\n\n\n## Contents\n\n- [Machine Learning Frameworks](#machine-learning-frameworks) _56 projects_\n- [Data Visualization](#data-visualization) _50 projects_\n- [Text Data & NLP](#text-data--nlp) _96 projects_\n- [Image Data](#image-data) _60 projects_\n- [Graph Data](#graph-data) _36 projects_\n- [Audio Data](#audio-data) _28 projects_\n- [Geospatial Data](#geospatial-data) _22 projects_\n- [Financial Data](#financial-data) _25 projects_\n- [Time Series Data](#time-series-data) _26 projects_\n- [Medical Data](#medical-data) _19 projects_\n- [Tabular Data](#tabular-data) _5 projects_\n- [Optical Character Recognition](#optical-character-recognition) _12 projects_\n- [Data Containers & Structures](#data-containers--structures) _0 projects_\n- [Data Loading & Extraction](#data-loading--extraction) _2 projects_\n- [Web Scraping & Crawling](#web-scraping--crawling) _1 projects_\n- [Data Pipelines & Streaming](#data-pipelines--streaming) _43 projects_\n- [Distributed Machine Learning](#distributed-machine-learning) _33 projects_\n- [Hyperparameter Optimization & AutoML](#hyperparameter-optimization--automl) _47 projects_\n- [Reinforcement Learning](#reinforcement-learning) _23 projects_\n- [Recommender Systems](#recommender-systems) _16 projects_\n- [Privacy Machine Learning](#privacy-machine-learning) _6 projects_\n- [Workflow & Experiment Tracking](#workflow--experiment-tracking) _39 projects_\n- [Model Serialization & Deployment](#model-serialization--deployment) _16 projects_\n- [Model Interpretability](#model-interpretability) _50 projects_\n- [Vector Similarity Search (ANN)](#vector-similarity-search-ann) _12 projects_\n- [Probabilistics & Statistics](#probabilistics--statistics) _22 projects_\n- [Adversarial Robustness](#adversarial-robustness) _9 projects_\n- [GPU Utilities](#gpu-utilities) _18 projects_\n- [Tensorflow Utilities](#tensorflow-utilities) _15 projects_\n- [Jax Utilities](#jax-utilities) _2 projects_\n- [Sklearn Utilities](#sklearn-utilities) _17 projects_\n- [Pytorch Utilities](#pytorch-utilities) _32 projects_\n- [Database Clients](#database-clients) _1 projects_\n- [Others](#others) _61 projects_\n\n## Explanation\n- \ud83e\udd47\ud83e\udd48\ud83e\udd49&nbsp; Combined project-quality score\n- \u2b50\ufe0f&nbsp; Star count from GitHub\n- \ud83d\udc23&nbsp; New project _(less than 6 months old)_\n- \ud83d\udca4&nbsp; Inactive project _(6 months no activity)_\n- \ud83d\udc80&nbsp; Dead project _(12 months no activity)_\n- \ud83d\udcc8\ud83d\udcc9&nbsp; Project is trending up or down\n- \u2795&nbsp; Project was recently added\n- \u2757\ufe0f&nbsp; Warning _(e.g. missing\/risky license)_\n- \ud83d\udc68\u200d\ud83d\udcbb&nbsp; Contributors count from GitHub\n- \ud83d\udd00&nbsp; Fork count from GitHub\n- \ud83d\udccb&nbsp; Issue count from GitHub\n- \u23f1\ufe0f&nbsp; Last update timestamp on package manager\n- \ud83d\udce5&nbsp; Download count from package manager\n- \ud83d\udce6&nbsp; Number of dependent projects\n- <img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; Tensorflow related project\n- <img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; Sklearn related project\n- <img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; PyTorch related project\n- <img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; MxNet related project\n- <img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; Apache Spark related project\n- <img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; Jupyter related project\n- <img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; PaddlePaddle related project\n- <img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; Pandas related project\n- <img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\">&nbsp; Jax related project\n\n<br>\n\n## Machine Learning Frameworks\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_General-purpose machine learning and deep learning frameworks._\n\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/tensorflow\">Tensorflow<\/a><\/b> (\ud83e\udd4755 \u00b7  \u2b50 170K) - An Open Source Machine Learning Framework for Everyone. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/tensorflow) (\ud83d\udc68\u200d\ud83d\udcbb 4K \u00b7 \ud83d\udd00 87K \u00b7 \ud83d\udce6 190K \u00b7 \ud83d\udccb 35K - 7% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/tensorflow\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow) (\ud83d\udce5 14M \/ month \u00b7 \ud83d\udce6 14K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install tensorflow\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorflow) (\ud83d\udce5 3.3M \u00b7 \u23f1\ufe0f 06.02.2022):\n\t```\n\tconda install -c conda-forge tensorflow\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/tensorflow\/tensorflow) (\ud83d\udce5 65M \u00b7 \u2b50 2K \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tdocker pull tensorflow\/tensorflow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-learn\/scikit-learn\">scikit-learn<\/a><\/b> (\ud83e\udd4751 \u00b7  \u2b50 50K) - scikit-learn: machine learning in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-learn\/scikit-learn) (\ud83d\udc68\u200d\ud83d\udcbb 2.6K \u00b7 \ud83d\udd00 23K \u00b7 \ud83d\udce5 790 \u00b7 \ud83d\udce6 340K \u00b7 \ud83d\udccb 10K - 22% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-learn\/scikit-learn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scikit-learn) (\ud83d\udce5 33M \/ month \u00b7 \ud83d\udce6 25K \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install scikit-learn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/scikit-learn) (\ud83d\udce5 12M \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tconda install -c conda-forge scikit-learn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/pytorch\">PyTorch<\/a><\/b> (\ud83e\udd4749 \u00b7  \u2b50 56K) - Tensors and Dynamic neural networks in Python with strong GPU.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/pytorch) (\ud83d\udc68\u200d\ud83d\udcbb 3.2K \u00b7 \ud83d\udd00 15K \u00b7 \ud83d\udce5 3.3K \u00b7 \ud83d\udccb 29K - 38% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/pytorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torch) (\ud83d\udce5 8M \/ month \u00b7 \ud83d\udce6 7K \u00b7 \u23f1\ufe0f 10.03.2022):\n\t```\n\tpip install torch\n\t```\n- [Conda](https:\/\/anaconda.org\/pytorch\/pytorch) (\ud83d\udce5 17M \u00b7 \u23f1\ufe0f 10.03.2022):\n\t```\n\tconda install -c pytorch pytorch\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/keras-team\/keras\">Keras<\/a><\/b> (\ud83e\udd4744 \u00b7  \u2b50 55K) - Deep Learning for humans. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/keras-team\/keras) (\ud83d\udc68\u200d\ud83d\udcbb 1.1K \u00b7 \ud83d\udd00 19K \u00b7 \ud83d\udccb 11K - 2% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/keras-team\/keras\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/keras) (\ud83d\udce5 8.8M \/ month \u00b7 \ud83d\udce6 280 \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tpip install keras\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/keras) (\ud83d\udce5 2.2M \u00b7 \u23f1\ufe0f 04.02.2022):\n\t```\n\tconda install -c conda-forge keras\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dmlc\/xgboost\">XGBoost<\/a><\/b> (\ud83e\udd4744 \u00b7  \u2b50 23K) - Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dmlc\/xgboost) (\ud83d\udc68\u200d\ud83d\udcbb 560 \u00b7 \ud83d\udd00 8.4K \u00b7 \ud83d\udce5 4K \u00b7 \ud83d\udce6 30K \u00b7 \ud83d\udccb 4.4K - 6% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dmlc\/xgboost\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/xgboost) (\ud83d\udce5 7.8M \/ month \u00b7 \ud83d\udce6 1.3K \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tpip install xgboost\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/xgboost) (\ud83d\udce5 2.6M \u00b7 \u23f1\ufe0f 17.02.2022):\n\t```\n\tconda install -c conda-forge xgboost\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/jax\">jax<\/a><\/b> (\ud83e\udd4743 \u00b7  \u2b50 17K) - Composable transformations of Python+NumPy programs: differentiate,.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/jax) (\ud83d\udc68\u200d\ud83d\udcbb 390 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udce6 4.2K \u00b7 \ud83d\udccb 3.3K - 33% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/jax\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/jax) (\ud83d\udce5 420K \/ month \u00b7 \ud83d\udce6 300 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install jax\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/jaxlib) (\ud83d\udce5 300K \u00b7 \u23f1\ufe0f 10.04.2022):\n\t```\n\tconda install -c conda-forge jaxlib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/statsmodels\/statsmodels\">StatsModels<\/a><\/b> (\ud83e\udd4743 \u00b7  \u2b50 7.3K) - Statsmodels: statistical modeling and econometrics in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/statsmodels\/statsmodels) (\ud83d\udc68\u200d\ud83d\udcbb 370 \u00b7 \ud83d\udd00 2.4K \u00b7 \ud83d\udce5 26 \u00b7 \ud83d\udce6 62K \u00b7 \ud83d\udccb 4.8K - 47% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/statsmodels\/statsmodels\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/statsmodels) (\ud83d\udce5 8.2M \/ month \u00b7 \ud83d\udce6 4.5K \u00b7 \u23f1\ufe0f 08.02.2022):\n\t```\n\tpip install statsmodels\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/statsmodels) (\ud83d\udce5 6M \u00b7 \u23f1\ufe0f 11.02.2022):\n\t```\n\tconda install -c conda-forge statsmodels\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/apache\/spark\">PySpark<\/a><\/b> (\ud83e\udd4842 \u00b7  \u2b50 33K) - Apache Spark Python API. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/apache\/spark) (\ud83d\udc68\u200d\ud83d\udcbb 2.7K \u00b7 \ud83d\udd00 25K \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/apache\/spark\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyspark) (\ud83d\udce5 20M \/ month \u00b7 \ud83d\udce6 790 \u00b7 \u23f1\ufe0f 26.01.2022):\n\t```\n\tpip install pyspark\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyspark) (\ud83d\udce5 1.6M \u00b7 \u23f1\ufe0f 26.01.2022):\n\t```\n\tconda install -c conda-forge pyspark\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\">pytorch-lightning<\/a><\/b> (\ud83e\udd4842 \u00b7  \u2b50 18K) - The lightweight PyTorch wrapper for high-performance.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning) (\ud83d\udc68\u200d\ud83d\udcbb 670 \u00b7 \ud83d\udd00 2.3K \u00b7 \ud83d\udce5 6.5K \u00b7 \ud83d\udce6 8.7K \u00b7 \ud83d\udccb 5K - 9% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch-lightning) (\ud83d\udce5 2.8M \/ month \u00b7 \ud83d\udce6 360 \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install pytorch-lightning\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pytorch-lightning) (\ud83d\udce5 430K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge pytorch-lightning\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/LightGBM\">LightGBM<\/a><\/b> (\ud83e\udd4842 \u00b7  \u2b50 14K) - A fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/LightGBM) (\ud83d\udc68\u200d\ud83d\udcbb 260 \u00b7 \ud83d\udd00 3.5K \u00b7 \ud83d\udce5 150K \u00b7 \ud83d\udce6 13K \u00b7 \ud83d\udccb 2.6K - 6% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/LightGBM\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lightgbm) (\ud83d\udce5 7.2M \/ month \u00b7 \ud83d\udce6 580 \u00b7 \u23f1\ufe0f 07.01.2022):\n\t```\n\tpip install lightgbm\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/lightgbm) (\ud83d\udce5 990K \u00b7 \u23f1\ufe0f 08.01.2022):\n\t```\n\tconda install -c conda-forge lightgbm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PaddlePaddle\/Paddle\">PaddlePaddle<\/a><\/b> (\ud83e\udd4841 \u00b7  \u2b50 18K) - PArallel Distributed Deep LEarning: Machine Learning.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PaddlePaddle\/Paddle) (\ud83d\udc68\u200d\ud83d\udcbb 720 \u00b7 \ud83d\udd00 4.5K \u00b7 \ud83d\udce5 15K \u00b7 \ud83d\udce6 110 \u00b7 \ud83d\udccb 15K - 18% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PaddlePaddle\/Paddle\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/paddlepaddle) (\ud83d\udce5 73K \/ month \u00b7 \ud83d\udce6 48 \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tpip install paddlepaddle\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/catboost\/catboost\">Catboost<\/a><\/b> (\ud83e\udd4840 \u00b7  \u2b50 6.5K) - A fast, scalable, high performance Gradient Boosting on Decision.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/catboost\/catboost) (\ud83d\udc68\u200d\ud83d\udcbb 990 \u00b7 \ud83d\udd00 1K \u00b7 \ud83d\udce5 79K \u00b7 \ud83d\udccb 1.8K - 21% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/catboost\/catboost\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/catboost) (\ud83d\udce5 2.9M \/ month \u00b7 \ud83d\udce6 220 \u00b7 \u23f1\ufe0f 07.04.2022):\n\t```\n\tpip install catboost\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/catboost) (\ud83d\udce5 970K \u00b7 \u23f1\ufe0f 07.04.2022):\n\t```\n\tconda install -c conda-forge catboost\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/apache\/incubator-mxnet\">MXNet<\/a><\/b> (\ud83e\udd4839 \u00b7  \u2b50 20K) - Lightweight, Portable, Flexible Distributed\/Mobile Deep Learning.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/apache\/incubator-mxnet) (\ud83d\udc68\u200d\ud83d\udcbb 970 \u00b7 \ud83d\udd00 6.9K \u00b7 \ud83d\udce5 25K \u00b7 \ud83d\udccb 9.7K - 20% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/apache\/incubator-mxnet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mxnet) (\ud83d\udce5 260K \/ month \u00b7 \ud83d\udce6 280 \u00b7 \u23f1\ufe0f 25.03.2022):\n\t```\n\tpip install mxnet\n\t```\n- [Conda](https:\/\/anaconda.org\/anaconda\/mxnet) (\ud83d\udce5 7.3K \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c anaconda mxnet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/fastai\/fastai\">Fastai<\/a><\/b> (\ud83e\udd4838 \u00b7  \u2b50 22K) - The fastai deep learning library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/fastai\/fastai) (\ud83d\udc68\u200d\ud83d\udcbb 610 \u00b7 \ud83d\udd00 7.2K \u00b7 \ud83d\udccb 1.6K - 7% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/fastai\/fastai\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fastai) (\ud83d\udce5 250K \/ month \u00b7 \ud83d\udce6 300 \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tpip install fastai\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jina-ai\/jina\">Jina<\/a><\/b> (\ud83e\udd4838 \u00b7  \u2b50 15K) - Cloud-native neural search framework for kind of data. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jina-ai\/jina) (\ud83d\udc68\u200d\ud83d\udcbb 150 \u00b7 \ud83d\udd00 1.9K \u00b7 \ud83d\udce6 270 \u00b7 \ud83d\udccb 1.4K - 4% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/jina-ai\/jina\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/jina) (\ud83d\udce5 48K \/ month \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install jina\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/jina-core) (\ud83d\udce5 4.3K \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tconda install -c conda-forge jina-core\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/jinaai\/jina) (\ud83d\udce5 1.1M \u00b7 \u2b50 7 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tdocker pull jinaai\/jina\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Theano\/Theano\">Theano<\/a><\/b> (\ud83e\udd4838 \u00b7  \u2b50 9.6K) - Theano was a Python library that allows you to define, optimize, and.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Theano\/Theano) (\ud83d\udc68\u200d\ud83d\udcbb 380 \u00b7 \ud83d\udd00 2.5K \u00b7 \ud83d\udce6 12K \u00b7 \ud83d\udccb 2.8K - 24% open \u00b7 \u23f1\ufe0f 23.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/Theano\/Theano\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/theano) (\ud83d\udce5 250K \/ month \u00b7 \ud83d\udce6 2.8K \u00b7 \u23f1\ufe0f 27.07.2020):\n\t```\n\tpip install theano\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/theano) (\ud83d\udce5 1.9M \u00b7 \u23f1\ufe0f 16.03.2022):\n\t```\n\tconda install -c conda-forge theano\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/apache\/flink\">PyFlink<\/a><\/b> (\ud83e\udd4837 \u00b7  \u2b50 19K) - Apache Flink Python API. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/apache\/flink) (\ud83d\udc68\u200d\ud83d\udcbb 1.5K \u00b7 \ud83d\udd00 11K \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/apache\/flink\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/apache-flink) (\ud83d\udce5 13K \/ month \u00b7 \ud83d\udce6 15 \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tpip install apache-flink\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/explosion\/thinc\">Thinc<\/a><\/b> (\ud83e\udd4837 \u00b7  \u2b50 2.5K) - A refreshing functional take on deep learning, compatible with your favorite.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/explosion\/thinc) (\ud83d\udc68\u200d\ud83d\udcbb 51 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udce6 21K \u00b7 \ud83d\udccb 130 - 18% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/explosion\/thinc\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/thinc) (\ud83d\udce5 4M \/ month \u00b7 \ud83d\udce6 610 \u00b7 \u23f1\ufe0f 15.03.2022):\n\t```\n\tpip install thinc\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/thinc) (\ud83d\udce5 2M \u00b7 \u23f1\ufe0f 15.03.2022):\n\t```\n\tconda install -c conda-forge thinc\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/chainer\/chainer\">Chainer<\/a><\/b> (\ud83e\udd4836 \u00b7  \u2b50 5.7K) - A flexible framework of neural networks for deep learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/chainer\/chainer) (\ud83d\udc68\u200d\ud83d\udcbb 320 \u00b7 \ud83d\udd00 1.4K \u00b7 \ud83d\udce6 2.6K \u00b7 \ud83d\udccb 2K - 0% open \u00b7 \u23f1\ufe0f 05.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/chainer\/chainer\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/chainer) (\ud83d\udce5 19K \/ month \u00b7 \ud83d\udce6 400 \u00b7 \u23f1\ufe0f 05.01.2022):\n\t```\n\tpip install chainer\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/chainer) (\ud83d\udce5 7.4K \u00b7 \u23f1\ufe0f 21.01.2022):\n\t```\n\tconda install -c conda-forge chainer\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/flax\">Flax<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 2.9K) - Flax is a neural network library for JAX that is designed for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/flax) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 340 \u00b7 \ud83d\udce5 38 \u00b7 \ud83d\udce6 890 \u00b7 \ud83d\udccb 510 - 25% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/flax\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/flax) (\ud83d\udce5 120K \/ month \u00b7 \ud83d\udce6 66 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install flax\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/flax) (\ud83d\udce5 3.5K \u00b7 \u23f1\ufe0f 23.03.2022):\n\t```\n\tconda install -c conda-forge flax\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\">Vowpal Wabbit<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 7.9K) - Vowpal Wabbit is a machine learning system which pushes the.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit) (\ud83d\udc68\u200d\ud83d\udcbb 320 \u00b7 \ud83d\udd00 1.8K \u00b7 \ud83d\udccb 1.2K - 11% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vowpalwabbit) (\ud83d\udce5 53K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tpip install vowpalwabbit\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/vowpalwabbit) (\ud83d\udce5 58K \u00b7 \u23f1\ufe0f 07.04.2022):\n\t```\n\tconda install -c conda-forge vowpalwabbit\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/ignite\">Ignite<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 4K) - High-level library to help with training and evaluating neural.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/ignite) (\ud83d\udc68\u200d\ud83d\udcbb 170 \u00b7 \ud83d\udd00 540 \u00b7 \ud83d\udccb 1.1K - 12% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/ignite\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch-ignite) (\ud83d\udce5 140K \/ month \u00b7 \ud83d\udce6 81 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install pytorch-ignite\n\t```\n- [Conda](https:\/\/anaconda.org\/pytorch\/ignite) (\ud83d\udce5 86K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c pytorch ignite\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ROCmSoftwarePlatform\/tensorflow-upstream\">tensorflow-upstream<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 590) - TensorFlow ROCm port. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ROCmSoftwarePlatform\/tensorflow-upstream) (\ud83d\udc68\u200d\ud83d\udcbb 4K \u00b7 \ud83d\udd00 68 \u00b7 \ud83d\udce5 20 \u00b7 \ud83d\udccb 330 - 17% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ROCmSoftwarePlatform\/tensorflow-upstream\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-rocm) (\ud83d\udce5 1.6K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 11.04.2022):\n\t```\n\tpip install tensorflow-rocm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/apple\/turicreate\">Turi Create<\/a><\/b> (\ud83e\udd4932 \u00b7  \u2b50 11K) - Turi Create simplifies the development of custom machine learning.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/apple\/turicreate) (\ud83d\udc68\u200d\ud83d\udcbb 85 \u00b7 \ud83d\udd00 1.1K \u00b7 \ud83d\udce5 5.7K \u00b7 \ud83d\udce6 300 \u00b7 \ud83d\udccb 1.8K - 27% open \u00b7 \u23f1\ufe0f 29.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/apple\/turicreate\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/turicreate) (\ud83d\udce5 28K \/ month \u00b7 \ud83d\udce6 19 \u00b7 \u23f1\ufe0f 30.09.2020):\n\t```\n\tpip install turicreate\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ludwig-ai\/ludwig\">Ludwig<\/a><\/b> (\ud83e\udd4932 \u00b7  \u2b50 8.2K) - Data-centric declarative deep learning framework. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ludwig-ai\/ludwig) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 970 \u00b7 \ud83d\udce6 110 \u00b7 \ud83d\udccb 720 - 25% open \u00b7 \u23f1\ufe0f 30.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ludwig-ai\/ludwig\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ludwig) (\ud83d\udce5 2.9K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 08.03.2022):\n\t```\n\tpip install ludwig\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorpack\/tensorpack\">tensorpack<\/a><\/b> (\ud83e\udd4932 \u00b7  \u2b50 6.2K) - A Neural Net Training Interface on TensorFlow, with focus.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorpack\/tensorpack) (\ud83d\udc68\u200d\ud83d\udcbb 58 \u00b7 \ud83d\udd00 1.8K \u00b7 \ud83d\udce5 130 \u00b7 \ud83d\udce6 1K \u00b7 \ud83d\udccb 1.3K - 0% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorpack\/tensorpack\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorpack) (\ud83d\udce5 21K \/ month \u00b7 \ud83d\udce6 46 \u00b7 \u23f1\ufe0f 22.01.2021):\n\t```\n\tpip install tensorpack\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorpack) (\ud83d\udce5 880 \u00b7 \u23f1\ufe0f 06.02.2022):\n\t```\n\tconda install -c conda-forge tensorpack\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mlpack\/mlpack\">mlpack<\/a><\/b> (\ud83e\udd4932 \u00b7  \u2b50 4K) - mlpack: a scalable C++ machine learning library --. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mlpack\/mlpack) (\ud83d\udc68\u200d\ud83d\udcbb 290 \u00b7 \ud83d\udd00 1.4K \u00b7 \ud83d\udccb 1.5K - 6% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mlpack\/mlpack\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mlpack) (\ud83d\udce5 290 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 28.10.2020):\n\t```\n\tpip install mlpack\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mlpack) (\ud83d\udce5 99K \u00b7 \u23f1\ufe0f 09.11.2021):\n\t```\n\tconda install -c conda-forge mlpack\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepmind\/sonnet\">Sonnet<\/a><\/b> (\ud83e\udd4931 \u00b7  \u2b50 9.3K) - TensorFlow-based neural network library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepmind\/sonnet) (\ud83d\udc68\u200d\ud83d\udcbb 53 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udce6 820 \u00b7 \ud83d\udccb 170 - 13% open \u00b7 \u23f1\ufe0f 07.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepmind\/sonnet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dm-sonnet) (\ud83d\udce5 22K \/ month \u00b7 \ud83d\udce6 52 \u00b7 \u23f1\ufe0f 27.03.2020):\n\t```\n\tpip install dm-sonnet\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sonnet) (\ud83d\udce5 13K \u00b7 \u23f1\ufe0f 14.11.2020):\n\t```\n\tconda install -c conda-forge sonnet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/arogozhnikov\/einops\">einops<\/a><\/b> (\ud83e\udd4931 \u00b7  \u2b50 5K) - Deep learning operations reinvented (for pytorch, tensorflow, jax and others). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/arogozhnikov\/einops) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 210 \u00b7 \ud83d\udce6 2.6K \u00b7 \ud83d\udccb 110 - 30% open \u00b7 \u23f1\ufe0f 01.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/arogozhnikov\/einops\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/einops) (\ud83d\udce5 1.7M \/ month \u00b7 \ud83d\udce6 200 \u00b7 \u23f1\ufe0f 04.03.2022):\n\t```\n\tpip install einops\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/einops) (\ud83d\udce5 14K \u00b7 \u23f1\ufe0f 04.03.2022):\n\t```\n\tconda install -c conda-forge einops\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/clab\/dynet\">dyNET<\/a><\/b> (\ud83e\udd4931 \u00b7  \u2b50 3.3K) - DyNet: The Dynamic Neural Network Toolkit. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/clab\/dynet) (\ud83d\udc68\u200d\ud83d\udcbb 160 \u00b7 \ud83d\udd00 700 \u00b7 \ud83d\udce5 5.6K \u00b7 \ud83d\udce6 210 \u00b7 \ud83d\udccb 920 - 28% open \u00b7 \u23f1\ufe0f 11.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/clab\/dynet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dyNET) (\ud83d\udce5 21K \/ month \u00b7 \ud83d\udce6 28 \u00b7 \u23f1\ufe0f 21.10.2020):\n\t```\n\tpip install dyNET\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepmind\/dm-haiku\">Haiku<\/a><\/b> (\ud83e\udd4931 \u00b7  \u2b50 1.9K) - JAX-based neural network library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepmind\/dm-haiku) (\ud83d\udc68\u200d\ud83d\udcbb 59 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 420 \u00b7 \ud83d\udccb 150 - 25% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepmind\/dm-haiku\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dm-haiku) (\ud83d\udce5 95K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 14.02.2022):\n\t```\n\tpip install dm-haiku\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dm-haiku) (\ud83d\udce5 2.5K \u00b7 \u23f1\ufe0f 14.02.2022):\n\t```\n\tconda install -c conda-forge dm-haiku\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/sony\/nnabla\">Neural Network Libraries<\/a><\/b> (\ud83e\udd4930 \u00b7  \u2b50 2.5K) - Neural Network Libraries. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/sony\/nnabla) (\ud83d\udc68\u200d\ud83d\udcbb 66 \u00b7 \ud83d\udd00 320 \u00b7 \ud83d\udce5 540 \u00b7 \ud83d\udccb 78 - 42% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/sony\/nnabla\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nnabla) (\ud83d\udce5 3.9K \/ month \u00b7 \ud83d\udce6 53 \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tpip install nnabla\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/amaiya\/ktrain\">ktrain<\/a><\/b> (\ud83e\udd4930 \u00b7  \u2b50 980) - ktrain is a Python library that makes deep learning and AI more.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/amaiya\/ktrain) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udce6 290 \u00b7 \ud83d\udccb 410 - 0% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/amaiya\/ktrain\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ktrain) (\ud83d\udce5 27K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 28.03.2022):\n\t```\n\tpip install ktrain\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/skorch-dev\/skorch\">skorch<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 4.5K) - A scikit-learn compatible neural network library that wraps.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/skorch-dev\/skorch) (\ud83d\udc68\u200d\ud83d\udcbb 49 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udce6 490 \u00b7 \ud83d\udccb 440 - 11% open \u00b7 \u23f1\ufe0f 24.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/skorch-dev\/skorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/skorch) (\ud83d\udce5 21K \/ month \u00b7 \ud83d\udce6 33 \u00b7 \u23f1\ufe0f 31.10.2021):\n\t```\n\tpip install skorch\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/skorch) (\ud83d\udce5 530K \u00b7 \u23f1\ufe0f 30.11.2021):\n\t```\n\tconda install -c conda-forge skorch\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/neural-tangents\">Neural Tangents<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.8K) - Fast and Easy Infinite Neural Networks in Python. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/neural-tangents) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce5 230 \u00b7 \ud83d\udce6 34 \u00b7 \ud83d\udccb 120 - 33% open \u00b7 \u23f1\ufe0f 13.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/neural-tangents\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/neural-tangents) (\ud83d\udce5 710 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 23.02.2022):\n\t```\n\tpip install neural-tangents\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nubank\/fklearn\">fklearn<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.4K) - fklearn: Functional Machine Learning. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nubank\/fklearn) (\ud83d\udc68\u200d\ud83d\udcbb 43 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udce6 12 \u00b7 \ud83d\udccb 44 - 50% open \u00b7 \u23f1\ufe0f 18.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nubank\/fklearn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fklearn) (\ud83d\udce5 6.8K \/ month \u00b7 \u23f1\ufe0f 30.12.2021):\n\t```\n\tpip install fklearn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/XiaoMi\/mace\">mace<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 4.6K) - MACE is a deep learning inference framework optimized for mobile.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/XiaoMi\/mace) (\ud83d\udc68\u200d\ud83d\udcbb 63 \u00b7 \ud83d\udd00 790 \u00b7 \ud83d\udce5 1.4K \u00b7 \ud83d\udccb 660 - 6% open \u00b7 \u23f1\ufe0f 11.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/XiaoMi\/mace\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/towhee-io\/towhee\">Towhee<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 430) - A framework that provides a simple API for developing ML-driven data.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/towhee-io\/towhee) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 85 \u00b7 \ud83d\udce5 9 \u00b7 \ud83d\udccb 320 - 14% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/towhee-io\/towhee\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/towhee) (\ud83d\udce5 470 \/ month \u00b7 \u23f1\ufe0f 30.03.2022):\n\t```\n\tpip install towhee\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/objax\">Objax<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 690) - Objax is a machine learning framework that provides an Object.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/objax) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 59 \u00b7 \ud83d\udce6 21 \u00b7 \ud83d\udccb 100 - 43% open \u00b7 \u23f1\ufe0f 18.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/objax\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/objax) (\ud83d\udce5 220 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 31.01.2022):\n\t```\n\tpip install objax\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/neoml-lib\/neoml\">NeoML<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 670) - Machine learning framework for both deep learning and traditional.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/neoml-lib\/neoml) (\ud83d\udc68\u200d\ud83d\udcbb 30 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udccb 71 - 38% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/neoml-lib\/neoml\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/neoml) (\ud83d\udce5 190 \/ month \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install neoml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Xtra-Computing\/thundersvm\">ThunderSVM<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1.4K) - ThunderSVM: A Fast SVM Library on GPUs and CPUs. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Xtra-Computing\/thundersvm) (\ud83d\udc68\u200d\ud83d\udcbb 34 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udce5 2.4K \u00b7 \ud83d\udccb 210 - 29% open \u00b7 \u23f1\ufe0f 09.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Xtra-Computing\/thundersvm\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/thundersvm) (\ud83d\udce5 910 \/ month \u00b7 \u23f1\ufe0f 13.03.2020):\n\t```\n\tpip install thundersvm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/poets-ai\/elegy\">elegy<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 350) - A High Level API for Deep Learning in JAX. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/poets-ai\/elegy) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 23 \u00b7 \ud83d\udccb 96 - 31% open \u00b7 \u23f1\ufe0f 23.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/poets-ai\/elegy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/elegy) (\ud83d\udce5 920 \/ month \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tpip install elegy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/serengil\/chefboost\">chefboost<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 320) - A Lightweight Decision Tree Framework supporting regular algorithms:.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/serengil\/chefboost) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 85 \u00b7 \ud83d\udce6 23 \u00b7 \ud83d\udccb 22 - 22% open \u00b7 \u23f1\ufe0f 23.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/serengil\/chefboost\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/chefboost) (\ud83d\udce5 830 \/ month \u00b7 \u23f1\ufe0f 16.02.2022):\n\t```\n\tpip install chefboost\n\t```\n<\/details>\n<details><summary>Show 13 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/davisking\/dlib\">dlib<\/a><\/b> (\ud83e\udd4839 \u00b7  \u2b50 11K) - A toolkit for making real world machine learning and data analysis.. <code><a href=\"https:\/\/tldrlegal.com\/search?q=BSL-1.0\">\u2757\ufe0fBSL-1.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/tflearn\/tflearn\">TFlearn<\/a><\/b> (\ud83e\udd4932 \u00b7  \u2b50 9.6K \u00b7 \ud83d\udc80) - Deep learning library featuring a higher-level API for TensorFlow. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/microsoft\/CNTK\">CNTK<\/a><\/b> (\ud83e\udd4931 \u00b7  \u2b50 17K \u00b7 \ud83d\udc80) - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Lasagne\/Lasagne\">Lasagne<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 3.8K \u00b7 \ud83d\udc80) - Lightweight library to build and train neural networks in Theano. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/mindsdb\/mindsdb\">MindsDB<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 6.6K) - In-Database Machine Learning. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/numenta\/nupic\">NuPIC<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 6.3K \u00b7 \ud83d\udc80) - Numenta Platform for Intelligent Computing is an implementation.. <code><a href=\"http:\/\/bit.ly\/3pwmjO5\">\u2757\ufe0fAGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/shogun-toolbox\/shogun\">SHOGUN<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 2.9K \u00b7 \ud83d\udc80) - Unified and efficient Machine Learning. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/aksnzhy\/xlearn\">xLearn<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 3K \u00b7 \ud83d\udc80) - High performance, easy-to-use, and scalable machine learning (ML).. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/itdxer\/neupy\">NeuPy<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 710 \u00b7 \ud83d\udc80) - NeuPy is a Tensorflow based python library for prototyping and building.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/NervanaSystems\/neon\">neon<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 3.9K \u00b7 \ud83d\udc80) - Intel Nervana reference deep learning framework committed to best.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/pytorchbearer\/torchbearer\">Torchbearer<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 630 \u00b7 \ud83d\udc80) - torchbearer: A model fitting library for PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/Xtra-Computing\/thundergbm\">ThunderGBM<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 620 \u00b7 \ud83d\udc80) - ThunderGBM: Fast GBDTs and Random Forests on GPUs. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/facebookresearch\/StarSpace\">StarSpace<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 3.8K \u00b7 \ud83d\udc80) - Learning embeddings for classification, retrieval and ranking. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n<\/details>\n<br>\n\n## Data Visualization\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_General-purpose and task-specific data visualization libraries._\n\n<details><summary><b><a href=\"https:\/\/github.com\/matplotlib\/matplotlib\">Matplotlib<\/a><\/b> (\ud83e\udd4749 \u00b7  \u2b50 15K) - matplotlib: plotting with Python. <code><a href=\"http:\/\/bit.ly\/35wkF7y\">Python-2.0<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/matplotlib\/matplotlib) (\ud83d\udc68\u200d\ud83d\udcbb 1.4K \u00b7 \ud83d\udd00 6.4K \u00b7 \ud83d\udce6 550K \u00b7 \ud83d\udccb 8.8K - 20% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/matplotlib\/matplotlib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/matplotlib) (\ud83d\udce5 27M \/ month \u00b7 \ud83d\udce6 54K \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install matplotlib\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/matplotlib) (\ud83d\udce5 12M \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge matplotlib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bokeh\/bokeh\">Bokeh<\/a><\/b> (\ud83e\udd4743 \u00b7  \u2b50 16K \u00b7 \ud83d\udcc9) - Interactive Data Visualization in the browser, from Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bokeh\/bokeh) (\ud83d\udc68\u200d\ud83d\udcbb 600 \u00b7 \ud83d\udd00 3.9K \u00b7 \ud83d\udce6 51K \u00b7 \ud83d\udccb 6.9K - 10% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bokeh\/bokeh\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/bokeh) (\ud83d\udce5 4.6M \/ month \u00b7 \ud83d\udce6 3.5K \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install bokeh\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/bokeh) (\ud83d\udce5 7.1M \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tconda install -c conda-forge bokeh\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/plotly\/plotly.py\">Plotly<\/a><\/b> (\ud83e\udd4742 \u00b7  \u2b50 11K) - The interactive graphing library for Python (includes Plotly Express). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/plotly\/plotly.py) (\ud83d\udc68\u200d\ud83d\udcbb 200 \u00b7 \ud83d\udd00 2.1K \u00b7 \ud83d\udce6 9 \u00b7 \ud83d\udccb 2.3K - 49% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/plotly\/plotly.py\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/plotly) (\ud83d\udce5 7.3M \/ month \u00b7 \ud83d\udce6 4K \u00b7 \u23f1\ufe0f 05.04.2022):\n\t```\n\tpip install plotly\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/plotly) (\ud83d\udce5 2.5M \u00b7 \u23f1\ufe0f 05.04.2022):\n\t```\n\tconda install -c conda-forge plotly\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/plotlywidget) (\ud83d\udce5 44K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 12.01.2021):\n\t```\n\tnpm install plotlywidget\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mwaskom\/seaborn\">Seaborn<\/a><\/b> (\ud83e\udd4741 \u00b7  \u2b50 9.4K) - Statistical data visualization in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mwaskom\/seaborn) (\ud83d\udc68\u200d\ud83d\udcbb 170 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce5 220 \u00b7 \ud83d\udce6 160K \u00b7 \ud83d\udccb 2K - 5% open \u00b7 \u23f1\ufe0f 03.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mwaskom\/seaborn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/seaborn) (\ud83d\udce5 8.4M \/ month \u00b7 \ud83d\udce6 8.8K \u00b7 \u23f1\ufe0f 16.08.2021):\n\t```\n\tpip install seaborn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/seaborn) (\ud83d\udce5 3.7M \u00b7 \u23f1\ufe0f 16.08.2021):\n\t```\n\tconda install -c conda-forge seaborn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/plotly\/dash\">dash<\/a><\/b> (\ud83e\udd4739 \u00b7  \u2b50 16K) - Analytical Web Apps for Python, R, Julia, and Jupyter. No JavaScript Required. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/plotly\/dash) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 1.7K \u00b7 \ud83d\udce6 190 \u00b7 \ud83d\udccb 1.3K - 47% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/plotly\/dash\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dash) (\ud83d\udce5 850K \/ month \u00b7 \ud83d\udce6 1.2K \u00b7 \u23f1\ufe0f 29.03.2022):\n\t```\n\tpip install dash\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dash) (\ud83d\udce5 430K \u00b7 \u23f1\ufe0f 30.03.2022):\n\t```\n\tconda install -c conda-forge dash\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/altair-viz\/altair\">Altair<\/a><\/b> (\ud83e\udd4739 \u00b7  \u2b50 7.5K) - Declarative statistical visualization library for Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/altair-viz\/altair) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 660 \u00b7 \ud83d\udce6 26K \u00b7 \ud83d\udccb 1.6K - 14% open \u00b7 \u23f1\ufe0f 25.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/altair-viz\/altair\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/altair) (\ud83d\udce5 6.6M \/ month \u00b7 \ud83d\udce6 340 \u00b7 \u23f1\ufe0f 29.12.2021):\n\t```\n\tpip install altair\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/altair) (\ud83d\udce5 1.1M \u00b7 \u23f1\ufe0f 29.12.2021):\n\t```\n\tconda install -c conda-forge altair\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ydataai\/pandas-profiling\">pandas-profiling<\/a><\/b> (\ud83e\udd4837 \u00b7  \u2b50 8.9K \u00b7 \ud83d\udcc8) - Create HTML profiling reports from pandas DataFrame.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ydataai\/pandas-profiling) (\ud83d\udc68\u200d\ud83d\udcbb 86 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udce6 7.7K \u00b7 \ud83d\udccb 570 - 20% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ydataai\/pandas-profiling\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pandas-profiling) (\ud83d\udce5 1.1M \/ month \u00b7 \ud83d\udce6 160 \u00b7 \u23f1\ufe0f 27.09.2021):\n\t```\n\tpip install pandas-profiling\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pandas-profiling) (\ud83d\udce5 200K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c conda-forge pandas-profiling\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lmcinnes\/umap\">UMAP<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 5.6K) - Uniform Manifold Approximation and Projection. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lmcinnes\/umap) (\ud83d\udc68\u200d\ud83d\udcbb 99 \u00b7 \ud83d\udd00 630 \u00b7 \ud83d\udce6 5.2K \u00b7 \ud83d\udccb 620 - 52% open \u00b7 \u23f1\ufe0f 26.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lmcinnes\/umap\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/umap-learn) (\ud83d\udce5 680K \/ month \u00b7 \ud83d\udce6 310 \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tpip install umap-learn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/umap-learn) (\ud83d\udce5 1M \u00b7 \u23f1\ufe0f 14.04.2022):\n\t```\n\tconda install -c conda-forge umap-learn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/holoviz\/holoviews\">HoloViews<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 2.2K) - With Holoviews, your data visualizes itself. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/holoviz\/holoviews) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 350 \u00b7 \ud83d\udccb 2.8K - 30% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/holoviz\/holoviews\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/holoviews) (\ud83d\udce5 430K \/ month \u00b7 \ud83d\udce6 210 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install holoviews\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/holoviews) (\ud83d\udce5 690K \u00b7 \u23f1\ufe0f 16.02.2022):\n\t```\n\tconda install -c conda-forge holoviews\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/@pyviz\/jupyterlab_pyviz) (\ud83d\udce5 1.8K \/ month \u00b7 \u23f1\ufe0f 24.05.2020):\n\t```\n\tnpm install @pyviz\/jupyterlab_pyviz\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/xflr6\/graphviz\">Graphviz<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 1.2K) - Simple Python interface for Graphviz. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/xflr6\/graphviz) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce6 31K \u00b7 \ud83d\udccb 140 - 4% open \u00b7 \u23f1\ufe0f 17.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/xflr6\/graphviz\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/graphviz) (\ud83d\udce5 9.3M \/ month \u00b7 \ud83d\udce6 3K \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tpip install graphviz\n\t```\n- [Conda](https:\/\/anaconda.org\/anaconda\/python-graphviz) (\ud83d\udce5 20K \u00b7 \u23f1\ufe0f 04.02.2021):\n\t```\n\tconda install -c anaconda python-graphviz\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pyecharts\/pyecharts\">pyecharts<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 12K \u00b7 \ud83d\udcc9) - Python Echarts Plotting Library. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pyecharts\/pyecharts) (\ud83d\udc68\u200d\ud83d\udcbb 30 \u00b7 \ud83d\udd00 2.6K \u00b7 \ud83d\udce6 2.2K \u00b7 \ud83d\udccb 1.6K - 2% open \u00b7 \u23f1\ufe0f 25.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pyecharts\/pyecharts\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyecharts) (\ud83d\udce5 68K \/ month \u00b7 \ud83d\udce6 220 \u00b7 \u23f1\ufe0f 16.11.2021):\n\t```\n\tpip install pyecharts\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pyqtgraph\/pyqtgraph\">PyQtGraph<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 2.8K) - Fast data visualization and GUI tools for scientific \/ engineering.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pyqtgraph\/pyqtgraph) (\ud83d\udc68\u200d\ud83d\udcbb 220 \u00b7 \ud83d\udd00 920 \u00b7 \ud83d\udccb 1K - 31% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pyqtgraph\/pyqtgraph\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyqtgraph) (\ud83d\udce5 89K \/ month \u00b7 \ud83d\udce6 790 \u00b7 \u23f1\ufe0f 04.03.2022):\n\t```\n\tpip install pyqtgraph\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyqtgraph) (\ud83d\udce5 240K \u00b7 \u23f1\ufe0f 05.03.2022):\n\t```\n\tconda install -c conda-forge pyqtgraph\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pyvista\/pyvista\">PyVista<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 1.2K) - 3D plotting and mesh analysis through a streamlined interface for.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pyvista\/pyvista) (\ud83d\udc68\u200d\ud83d\udcbb 87 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udce5 600 \u00b7 \ud83d\udce6 730 \u00b7 \ud83d\udccb 790 - 29% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pyvista\/pyvista\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyvista) (\ud83d\udce5 57K \/ month \u00b7 \ud83d\udce6 100 \u00b7 \u23f1\ufe0f 18.04.2022):\n\t```\n\tpip install pyvista\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyvista) (\ud83d\udce5 160K \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tconda install -c conda-forge pyvista\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/voxel51\/fiftyone\">FiftyOne<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 1.2K) - Visualize, create, and debug image and video datasets.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/voxel51\/fiftyone) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 740 - 31% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/voxel51\/fiftyone\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fiftyone) (\ud83d\udce5 28K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tpip install fiftyone\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/amueller\/word_cloud\">wordcloud<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 8.7K) - A little word cloud generator in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/amueller\/word_cloud) (\ud83d\udc68\u200d\ud83d\udcbb 65 \u00b7 \ud83d\udd00 2.2K \u00b7 \ud83d\udccb 480 - 23% open \u00b7 \u23f1\ufe0f 26.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/amueller\/word_cloud\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/wordcloud) (\ud83d\udce5 670K \/ month \u00b7 \ud83d\udce6 710 \u00b7 \u23f1\ufe0f 11.11.2020):\n\t```\n\tpip install wordcloud\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/wordcloud) (\ud83d\udce5 270K \u00b7 \u23f1\ufe0f 15.11.2021):\n\t```\n\tconda install -c conda-forge wordcloud\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/has2k1\/plotnine\">plotnine<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 3.1K \u00b7 \ud83d\udcc8) - A grammar of graphics for Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/has2k1\/plotnine) (\ud83d\udc68\u200d\ud83d\udcbb 94 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udccb 490 - 12% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/has2k1\/plotnine\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/plotnine) (\ud83d\udce5 250K \/ month \u00b7 \ud83d\udce6 200 \u00b7 \u23f1\ufe0f 25.03.2021):\n\t```\n\tpip install plotnine\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/plotnine) (\ud83d\udce5 160K \u00b7 \u23f1\ufe0f 25.03.2021):\n\t```\n\tconda install -c conda-forge plotnine\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/vispy\/vispy\">VisPy<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 2.9K) - High-performance interactive 2D\/3D data visualization library. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/vispy\/vispy) (\ud83d\udc68\u200d\ud83d\udcbb 170 \u00b7 \ud83d\udd00 590 \u00b7 \ud83d\udce6 730 \u00b7 \ud83d\udccb 1.3K - 21% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/vispy\/vispy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vispy) (\ud83d\udce5 53K \/ month \u00b7 \ud83d\udce6 96 \u00b7 \u23f1\ufe0f 18.04.2022):\n\t```\n\tpip install vispy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/vispy) (\ud83d\udce5 220K \u00b7 \u23f1\ufe0f 18.04.2022):\n\t```\n\tconda install -c conda-forge vispy\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/vispy) (\ud83d\udce5 24 \/ month \u00b7 \u23f1\ufe0f 15.03.2020):\n\t```\n\tnpm install vispy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/holoviz\/datashader\">datashader<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 2.8K) - Quickly and accurately render even the largest data. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/holoviz\/datashader) (\ud83d\udc68\u200d\ud83d\udcbb 47 \u00b7 \ud83d\udd00 350 \u00b7 \ud83d\udce6 1.1K \u00b7 \ud83d\udccb 500 - 25% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/holoviz\/datashader\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/datashader) (\ud83d\udce5 44K \/ month \u00b7 \ud83d\udce6 95 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install datashader\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/datashader) (\ud83d\udce5 290K \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tconda install -c conda-forge datashader\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/finos\/perspective\">Perspective<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 4.5K) - A data visualization and analytics component, especially.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/finos\/perspective) (\ud83d\udc68\u200d\ud83d\udcbb 68 \u00b7 \ud83d\udd00 460 \u00b7 \ud83d\udce6 240 \u00b7 \ud83d\udccb 520 - 15% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/finos\/perspective\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/perspective-python) (\ud83d\udce5 1.8K \/ month \u00b7 \ud83d\udce6 9 \u00b7 \u23f1\ufe0f 14.03.2022):\n\t```\n\tpip install perspective-python\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/perspective) (\ud83d\udce5 52K \u00b7 \u23f1\ufe0f 29.04.2022):\n\t```\n\tconda install -c conda-forge perspective\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/@finos\/perspective-jupyterlab) (\ud83d\udce5 2.5K \/ month \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tnpm install @finos\/perspective-jupyterlab\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bqplot\/bqplot\">bqplot<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 3.3K) - Plotting library for IPython\/Jupyter notebooks. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bqplot\/bqplot) (\ud83d\udc68\u200d\ud83d\udcbb 56 \u00b7 \ud83d\udd00 460 \u00b7 \ud83d\udce6 30 \u00b7 \ud83d\udccb 580 - 39% open \u00b7 \u23f1\ufe0f 08.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bqplot\/bqplot\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/bqplot) (\ud83d\udce5 69K \/ month \u00b7 \ud83d\udce6 92 \u00b7 \u23f1\ufe0f 11.02.2022):\n\t```\n\tpip install bqplot\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/bqplot) (\ud83d\udce5 950K \u00b7 \u23f1\ufe0f 11.02.2022):\n\t```\n\tconda install -c conda-forge bqplot\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/bqplot) (\ud83d\udce5 22K \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 11.02.2022):\n\t```\n\tnpm install bqplot\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/man-group\/dtale\">D-Tale<\/a><\/b> (\ud83e\udd4930 \u00b7  \u2b50 3.4K) - Visualizer for pandas data structures. <code><a href=\"https:\/\/tldrlegal.com\/search?q=LGPL-2.1\">\u2757\ufe0fLGPL-2.1<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/man-group\/dtale) (\ud83d\udc68\u200d\ud83d\udcbb 23 \u00b7 \ud83d\udd00 270 \u00b7 \ud83d\udce6 350 \u00b7 \ud83d\udccb 460 - 8% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/man-group\/dtale\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dtale) (\ud83d\udce5 63K \/ month \u00b7 \ud83d\udce6 12 \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tpip install dtale\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dtale) (\ud83d\udce5 110K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge dtale\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ResidentMario\/missingno\">missingno<\/a><\/b> (\ud83e\udd4930 \u00b7  \u2b50 3.2K) - Missing data visualization module for Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ResidentMario\/missingno) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 400 \u00b7 \ud83d\udce6 7.1K \u00b7 \ud83d\udccb 120 - 5% open \u00b7 \u23f1\ufe0f 27.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ResidentMario\/missingno\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/missingno) (\ud83d\udce5 950K \/ month \u00b7 \ud83d\udce6 120 \u00b7 \u23f1\ufe0f 27.02.2022):\n\t```\n\tpip install missingno\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/missingno) (\ud83d\udce5 150K \u00b7 \u23f1\ufe0f 15.02.2020):\n\t```\n\tconda install -c conda-forge missingno\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/data-validation\">data-validation<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 630) - Library for exploring and validating machine learning.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/data-validation) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udce5 300 \u00b7 \ud83d\udce6 460 \u00b7 \ud83d\udccb 160 - 23% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/data-validation\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-data-validation) (\ud83d\udce5 1.3M \/ month \u00b7 \ud83d\udce6 27 \u00b7 \u23f1\ufe0f 02.03.2022):\n\t```\n\tpip install tensorflow-data-validation\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/holoviz\/hvplot\">hvPlot<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 550) - A high-level plotting API for pandas, dask, xarray, and networkx built on.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/holoviz\/hvplot) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 67 \u00b7 \ud83d\udce6 1.2K \u00b7 \ud83d\udccb 420 - 34% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/holoviz\/hvplot\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hvplot) (\ud83d\udce5 170K \/ month \u00b7 \ud83d\udce6 61 \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tpip install hvplot\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/hvplot) (\ud83d\udce5 170K \u00b7 \u23f1\ufe0f 23.07.2021):\n\t```\n\tconda install -c conda-forge hvplot\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PAIR-code\/facets\">Facets Overview<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 6.8K \u00b7 \ud83d\udca4) - Visualizations for machine learning datasets. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PAIR-code\/facets) (\ud83d\udc68\u200d\ud83d\udcbb 28 \u00b7 \ud83d\udd00 830 \u00b7 \ud83d\udce6 110 \u00b7 \ud83d\udccb 150 - 50% open \u00b7 \u23f1\ufe0f 06.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/pair-code\/facets\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/facets-overview) (\ud83d\udce5 220K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 24.07.2019):\n\t```\n\tpip install facets-overview\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ContextLab\/hypertools\">HyperTools<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 1.7K) - A Python toolbox for gaining geometric insights into high-dimensional.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ContextLab\/hypertools) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udce5 14 \u00b7 \ud83d\udce6 180 \u00b7 \ud83d\udccb 190 - 35% open \u00b7 \u23f1\ufe0f 12.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ContextLab\/hypertools\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hypertools) (\ud83d\udce5 690 \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 12.02.2022):\n\t```\n\tpip install hypertools\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jupyter-widgets\/pythreejs\">pythreejs<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 810) - A Jupyter - Three.js bridge. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jupyter-widgets\/pythreejs) (\ud83d\udc68\u200d\ud83d\udcbb 29 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce6 19 \u00b7 \ud83d\udccb 220 - 33% open \u00b7 \u23f1\ufe0f 06.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/jupyter-widgets\/pythreejs\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pythreejs) (\ud83d\udce5 50K \/ month \u00b7 \ud83d\udce6 38 \u00b7 \u23f1\ufe0f 26.02.2021):\n\t```\n\tpip install pythreejs\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pythreejs) (\ud83d\udce5 380K \u00b7 \u23f1\ufe0f 02.03.2021):\n\t```\n\tconda install -c conda-forge pythreejs\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/jupyter-threejs) (\ud83d\udce5 5.2K \/ month \u00b7 \ud83d\udce6 7 \u00b7 \u23f1\ufe0f 26.02.2021):\n\t```\n\tnpm install jupyter-threejs\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pavlin-policar\/openTSNE\">openTSNE<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 980) - Extensible, parallel implementations of t-SNE. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pavlin-policar\/openTSNE) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udce6 320 \u00b7 \ud83d\udccb 100 - 2% open \u00b7 \u23f1\ufe0f 18.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pavlin-policar\/openTSNE\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/opentsne) (\ud83d\udce5 17K \/ month \u00b7 \ud83d\udce6 9 \u00b7 \u23f1\ufe0f 18.03.2022):\n\t```\n\tpip install opentsne\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/opentsne) (\ud83d\udce5 130K \u00b7 \u23f1\ufe0f 13.11.2021):\n\t```\n\tconda install -c conda-forge opentsne\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/JetBrains\/lets-plot\">lets-plot<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 740) - An open-source plotting library for statistical data. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/JetBrains\/lets-plot) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 33 \u00b7 \ud83d\udce5 260 \u00b7 \ud83d\udce6 16 \u00b7 \ud83d\udccb 240 - 29% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/JetBrains\/lets-plot\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lets-plot) (\ud83d\udce5 1.3K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 21.03.2022):\n\t```\n\tpip install lets-plot\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/vega\/ipyvega\">vega<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 330) - IPython\/Jupyter notebook module for Vega and Vega-Lite. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/vega\/ipyvega) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 59 \u00b7 \ud83d\udccb 94 - 12% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/vega\/ipyvega\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vega) (\ud83d\udce5 6.4K \/ month \u00b7 \ud83d\udce6 84 \u00b7 \u23f1\ufe0f 10.02.2022):\n\t```\n\tpip install vega\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/vega) (\ud83d\udce5 480K \u00b7 \u23f1\ufe0f 10.02.2022):\n\t```\n\tconda install -c conda-forge vega\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/hiplot\">HiPlot<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 2.3K) - HiPlot makes understanding high dimensional data easy. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/hiplot) (\ud83d\udc68\u200d\ud83d\udcbb 8 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udce6 4 \u00b7 \ud83d\udccb 76 - 15% open \u00b7 \u23f1\ufe0f 26.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/hiplot\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hiplot) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 26.03.2022):\n\t```\n\tpip install hiplot\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/hiplot) (\ud83d\udce5 83K \u00b7 \u23f1\ufe0f 05.11.2021):\n\t```\n\tconda install -c conda-forge hiplot\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/fbdesignpro\/sweetviz\">Sweetviz<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 2K \u00b7 \ud83d\udca4) - Visualize and compare datasets, target values and associations, with.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/fbdesignpro\/sweetviz) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udccb 98 - 27% open \u00b7 \u23f1\ufe0f 08.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/fbdesignpro\/sweetviz\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sweetviz) (\ud83d\udce5 80K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 08.07.2021):\n\t```\n\tpip install sweetviz\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sweetviz) (\ud83d\udce5 9.5K \u00b7 \u23f1\ufe0f 09.07.2021):\n\t```\n\tconda install -c conda-forge sweetviz\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/AutoViML\/AutoViz\">AutoViz<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 700) - Automatically Visualize any dataset, any size with a single line of.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/AutoViML\/AutoViz) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 100 \u00b7 \ud83d\udce6 170 \u00b7 \ud83d\udccb 54 - 7% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/AutoViML\/AutoViz\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/autoviz) (\ud83d\udce5 53K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install autoviz\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/autoviz) (\ud83d\udce5 4K \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tconda install -c conda-forge autoviz\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PatrikHlobil\/Pandas-Bokeh\">Pandas-Bokeh<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 770) - Bokeh Plotting Backend for Pandas and GeoPandas. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PatrikHlobil\/Pandas-Bokeh) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 98 \u00b7 \ud83d\udccb 97 - 31% open \u00b7 \u23f1\ufe0f 25.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PatrikHlobil\/Pandas-Bokeh\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pandas-bokeh) (\ud83d\udce5 11K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 11.04.2021):\n\t```\n\tpip install pandas-bokeh\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/marcharper\/python-ternary\">python-ternary<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 550) - Ternary plotting library for python with matplotlib. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/marcharper\/python-ternary) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce5 17 \u00b7 \ud83d\udce6 91 \u00b7 \ud83d\udccb 120 - 22% open \u00b7 \u23f1\ufe0f 27.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/marcharper\/python-ternary\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/python-ternary) (\ud83d\udce5 20K \/ month \u00b7 \ud83d\udce6 21 \u00b7 \u23f1\ufe0f 17.02.2021):\n\t```\n\tpip install python-ternary\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/python-ternary) (\ud83d\udce5 62K \u00b7 \u23f1\ufe0f 17.02.2021):\n\t```\n\tconda install -c conda-forge python-ternary\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/leotac\/joypy\">joypy<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 420) - Joyplots in Python with matplotlib & pandas. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/leotac\/joypy) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 48 \u00b7 \ud83d\udce6 160 \u00b7 \ud83d\udccb 48 - 22% open \u00b7 \u23f1\ufe0f 19.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/leotac\/joypy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/joypy) (\ud83d\udce5 14K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 19.12.2021):\n\t```\n\tpip install joypy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/joypy) (\ud83d\udce5 13K \u00b7 \u23f1\ufe0f 28.12.2020):\n\t```\n\tconda install -c conda-forge joypy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/gyli\/PyWaffle\">PyWaffle<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 490) - Make Waffle Charts in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/gyli\/PyWaffle) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 88 \u00b7 \ud83d\udce6 130 \u00b7 \ud83d\udccb 16 - 25% open \u00b7 \u23f1\ufe0f 21.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/gyli\/PyWaffle\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pywaffle) (\ud83d\udce5 3.4K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 21.12.2021):\n\t```\n\tpip install pywaffle\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pywaffle) (\ud83d\udce5 4.4K \u00b7 \u23f1\ufe0f 22.12.2021):\n\t```\n\tconda install -c conda-forge pywaffle\n\t```\n<\/details>\n<details><summary>Show 13 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/SciTools\/cartopy\">cartopy<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 1K) - Cartopy - a cartographic python library with matplotlib support. <code><a href=\"http:\/\/bit.ly\/37RvQcA\">\u2757\ufe0fLGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/santosjorge\/cufflinks\">Cufflinks<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 2.6K \u00b7 \ud83d\udc80) - Productivity Tools for Plotly + Pandas. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/DmitryUlyanov\/Multicore-TSNE\">Multicore-TSNE<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 1.7K \u00b7 \ud83d\udc80) - Parallel t-SNE implementation with Python and Torch.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/spotify\/chartify\">Chartify<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 3.1K \u00b7 \ud83d\udc80) - Python library that makes it easy for data scientists to create.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/nicolaskruchten\/jupyter_pivottablejs\">pivottablejs<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 460 \u00b7 \ud83d\udc80) - Dragndrop Pivot Tables and Charts for Jupyter\/IPython.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/adamerose\/PandasGUI\">PandasGUI<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 2.6K) - A GUI for Pandas DataFrames. <code><a href=\"https:\/\/tldrlegal.com\/search?q=MIT-0\">\u2757\ufe0fMIT-0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/SauceCat\/PDPbox\">PDPbox<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 670 \u00b7 \ud83d\udc80) - python partial dependence plot toolbox. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/beringresearch\/ivis\">ivis<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 260) - Dimensionality reduction in very large datasets using Siamese.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/t-makaro\/animatplot\">animatplot<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 390 \u00b7 \ud83d\udc80) - A python package for animating plots build on matplotlib. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/altair-viz\/pdvega\">pdvega<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 340 \u00b7 \ud83d\udc80) - Interactive plotting for Pandas using Vega-Lite. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/data-describe\/data-describe\">data-describe<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 290) - datadescribe: Pythonic EDA Accelerator for Data Science. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Zsailer\/nx_altair\">nx-altair<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 190 \u00b7 \ud83d\udc80) - Draw interactive NetworkX graphs with Altair. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/biovault\/nptsne\">nptsne<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 28 \u00b7 \ud83d\udc80) - nptsne is a numpy compatible python binary package that offers a.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n<\/details>\n<br>\n\n## Text Data & NLP\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for processing, cleaning, manipulating, and analyzing text data as well as libraries for NLP tasks such as language detection, fuzzy matching, classification, seq2seq learning, conversational AI, keyword extraction, and translation._\n\n<details><summary><b><a href=\"https:\/\/github.com\/huggingface\/transformers\">transformers<\/a><\/b> (\ud83e\udd4749 \u00b7  \u2b50 62K) - Transformers: State-of-the-art Machine Learning for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/huggingface\/transformers) (\ud83d\udc68\u200d\ud83d\udcbb 1.3K \u00b7 \ud83d\udd00 14K \u00b7 \ud83d\udce5 1.5K \u00b7 \ud83d\udce6 27K \u00b7 \ud83d\udccb 9.3K - 5% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/huggingface\/transformers\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/transformers) (\ud83d\udce5 5.9M \/ month \u00b7 \ud83d\udce6 870 \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tpip install transformers\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/transformers) (\ud83d\udce5 180K \u00b7 \u23f1\ufe0f 31.01.2022):\n\t```\n\tconda install -c conda-forge transformers\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/explosion\/spaCy\">spaCy<\/a><\/b> (\ud83e\udd4744 \u00b7  \u2b50 23K) - Industrial-strength Natural Language Processing (NLP) in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/explosion\/spaCy) (\ud83d\udc68\u200d\ud83d\udcbb 680 \u00b7 \ud83d\udd00 3.8K \u00b7 \ud83d\udce5 3.1K \u00b7 \ud83d\udce6 38K \u00b7 \ud83d\udccb 5.1K - 1% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/explosion\/spaCy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/spacy) (\ud83d\udce5 4.5M \/ month \u00b7 \ud83d\udce6 2.3K \u00b7 \u23f1\ufe0f 05.04.2022):\n\t```\n\tpip install spacy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/spacy) (\ud83d\udce5 2.6M \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tconda install -c conda-forge spacy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nltk\/nltk\">nltk<\/a><\/b> (\ud83e\udd4744 \u00b7  \u2b50 11K) - Suite of libraries and programs for symbolic and statistical natural.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nltk\/nltk) (\ud83d\udc68\u200d\ud83d\udcbb 420 \u00b7 \ud83d\udd00 2.6K \u00b7 \ud83d\udce6 140K \u00b7 \ud83d\udccb 1.6K - 13% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nltk\/nltk\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nltk) (\ud83d\udce5 13M \/ month \u00b7 \ud83d\udce6 12K \u00b7 \u23f1\ufe0f 09.02.2022):\n\t```\n\tpip install nltk\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/nltk) (\ud83d\udce5 1.2M \u00b7 \u23f1\ufe0f 29.12.2021):\n\t```\n\tconda install -c conda-forge nltk\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/RaRe-Technologies\/gensim\">gensim<\/a><\/b> (\ud83e\udd4742 \u00b7  \u2b50 13K \u00b7 \ud83d\udcc8) - Topic Modelling for Humans. <code><a href=\"https:\/\/tldrlegal.com\/search?q=LGPL-2.1\">\u2757\ufe0fLGPL-2.1<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/RaRe-Technologies\/gensim) (\ud83d\udc68\u200d\ud83d\udcbb 430 \u00b7 \ud83d\udd00 4.2K \u00b7 \ud83d\udce5 3.5K \u00b7 \ud83d\udce6 33K \u00b7 \ud83d\udccb 1.8K - 21% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/RaRe-Technologies\/gensim\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gensim) (\ud83d\udce5 5.2M \/ month \u00b7 \ud83d\udce6 2.9K \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tpip install gensim\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/gensim) (\ud83d\udce5 790K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c conda-forge gensim\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/RasaHQ\/rasa\">Rasa<\/a><\/b> (\ud83e\udd4739 \u00b7  \u2b50 14K) - Open source machine learning framework to automate text- and voice-.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/RasaHQ\/rasa) (\ud83d\udc68\u200d\ud83d\udcbb 540 \u00b7 \ud83d\udd00 3.9K \u00b7 \ud83d\udccb 6.7K - 14% open \u00b7 \u23f1\ufe0f 26.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/RasaHQ\/rasa\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/rasa) (\ud83d\udce5 170K \/ month \u00b7 \ud83d\udce6 57 \u00b7 \u23f1\ufe0f 05.04.2022):\n\t```\n\tpip install rasa\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/flairNLP\/flair\">flair<\/a><\/b> (\ud83e\udd4738 \u00b7  \u2b50 12K) - A very simple framework for state-of-the-art Natural Language Processing.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/flairNLP\/flair) (\ud83d\udc68\u200d\ud83d\udcbb 220 \u00b7 \ud83d\udd00 1.8K \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 1.8K - 5% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/flairNLP\/flair\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/flair) (\ud83d\udce5 80K \/ month \u00b7 \ud83d\udce6 67 \u00b7 \u23f1\ufe0f 10.04.2022):\n\t```\n\tpip install flair\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/python-flair) (\ud83d\udce5 7.2K \u00b7 \u23f1\ufe0f 18.11.2021):\n\t```\n\tconda install -c conda-forge python-flair\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/fairseq\">fairseq<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 17K) - Facebook AI Research Sequence-to-Sequence Toolkit written in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/fairseq) (\ud83d\udc68\u200d\ud83d\udcbb 380 \u00b7 \ud83d\udd00 4.4K \u00b7 \ud83d\udce5 220 \u00b7 \ud83d\udce6 770 \u00b7 \ud83d\udccb 3.6K - 21% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/fairseq\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fairseq) (\ud83d\udce5 66K \/ month \u00b7 \ud83d\udce6 36 \u00b7 \u23f1\ufe0f 05.01.2021):\n\t```\n\tpip install fairseq\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/fairseq) (\ud83d\udce5 14K \u00b7 \u23f1\ufe0f 28.04.2021):\n\t```\n\tconda install -c conda-forge fairseq\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/allenai\/allennlp\">AllenNLP<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 11K) - An open-source NLP research library, built on PyTorch. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/allenai\/allennlp) (\ud83d\udc68\u200d\ud83d\udcbb 260 \u00b7 \ud83d\udd00 2.2K \u00b7 \ud83d\udce5 46 \u00b7 \ud83d\udce6 2.5K \u00b7 \ud83d\udccb 2.5K - 3% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/allenai\/allennlp\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/allennlp) (\ud83d\udce5 37K \/ month \u00b7 \ud83d\udce6 180 \u00b7 \u23f1\ufe0f 14.04.2022):\n\t```\n\tpip install allennlp\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/allennlp) (\ud83d\udce5 55K \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tconda install -c conda-forge allennlp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/gunthercox\/ChatterBot\">ChatterBot<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 12K \u00b7 \ud83d\udca4) - ChatterBot is a machine learning, conversational dialog engine.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/gunthercox\/ChatterBot) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 3.9K \u00b7 \ud83d\udce6 4.4K \u00b7 \ud83d\udccb 1.6K - 19% open \u00b7 \u23f1\ufe0f 01.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/gunthercox\/ChatterBot\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/chatterbot) (\ud83d\udce5 87K \/ month \u00b7 \ud83d\udce6 350 \u00b7 \u23f1\ufe0f 22.08.2020):\n\t```\n\tpip install chatterbot\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/ParlAI\">ParlAI<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 8.8K) - A framework for training and evaluating AI models on a variety of.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/ParlAI) (\ud83d\udc68\u200d\ud83d\udcbb 180 \u00b7 \ud83d\udd00 1.8K \u00b7 \ud83d\udce6 74 \u00b7 \ud83d\udccb 1.3K - 6% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/ParlAI\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/parlai) (\ud83d\udce5 1.8K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 30.03.2022):\n\t```\n\tpip install parlai\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/stanfordnlp\/stanza\">stanza<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 6.1K) - Official Stanford NLP Python Library for Many Human Languages. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/stanfordnlp\/stanza) (\ud83d\udc68\u200d\ud83d\udcbb 48 \u00b7 \ud83d\udd00 790 \u00b7 \ud83d\udce6 1K \u00b7 \ud83d\udccb 700 - 12% open \u00b7 \u23f1\ufe0f 23.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/stanfordnlp\/stanza\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/stanza) (\ud83d\udce5 390K \/ month \u00b7 \ud83d\udce6 67 \u00b7 \u23f1\ufe0f 23.04.2022):\n\t```\n\tpip install stanza\n\t```\n- [Conda](https:\/\/anaconda.org\/stanfordnlp\/stanza) (\ud83d\udce5 5K \u00b7 \u23f1\ufe0f 23.04.2022):\n\t```\n\tconda install -c stanfordnlp stanza\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/JohnSnowLabs\/spark-nlp\">spark-nlp<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 2.7K) - State of the Art Natural Language Processing. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/JohnSnowLabs\/spark-nlp) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 550 \u00b7 \ud83d\udccb 670 - 8% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/JohnSnowLabs\/spark-nlp\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/spark-nlp) (\ud83d\udce5 1.6M \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 12.04.2022):\n\t```\n\tpip install spark-nlp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/sloria\/TextBlob\">TextBlob<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 8.1K \u00b7 \ud83d\udca4) - Simple, Pythonic, text processing--Sentiment analysis, part-of-.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/sloria\/TextBlob) (\ud83d\udc68\u200d\ud83d\udcbb 35 \u00b7 \ud83d\udd00 1.1K \u00b7 \ud83d\udce5 98 \u00b7 \ud83d\udce6 20K \u00b7 \ud83d\udccb 260 - 39% open \u00b7 \u23f1\ufe0f 22.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/sloria\/TextBlob\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/textblob) (\ud83d\udce5 920K \/ month \u00b7 \ud83d\udce6 1.4K \u00b7 \u23f1\ufe0f 15.12.2021):\n\t```\n\tpip install textblob\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/textblob) (\ud83d\udce5 160K \u00b7 \u23f1\ufe0f 24.02.2019):\n\t```\n\tconda install -c conda-forge textblob\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/UKPLab\/sentence-transformers\">sentence-transformers<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 7.6K \u00b7 \ud83d\udcc9) - Multilingual Sentence & Image Embeddings with BERT. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/UKPLab\/sentence-transformers) (\ud83d\udc68\u200d\ud83d\udcbb 78 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce6 3.1K \u00b7 \ud83d\udccb 1.4K - 51% open \u00b7 \u23f1\ufe0f 21.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/UKPLab\/sentence-transformers\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sentence-transformers) (\ud83d\udce5 2M \/ month \u00b7 \ud83d\udce6 110 \u00b7 \u23f1\ufe0f 10.02.2022):\n\t```\n\tpip install sentence-transformers\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sentence-transformers) (\ud83d\udce5 20K \u00b7 \u23f1\ufe0f 20.02.2022):\n\t```\n\tconda install -c conda-forge sentence-transformers\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/sentencepiece\">sentencepiece<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 5.8K) - Unsupervised text tokenizer for Neural Network-based text.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/sentencepiece) (\ud83d\udc68\u200d\ud83d\udcbb 64 \u00b7 \ud83d\udd00 790 \u00b7 \ud83d\udce5 21K \u00b7 \ud83d\udce6 15K \u00b7 \ud83d\udccb 540 - 10% open \u00b7 \u23f1\ufe0f 12.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/sentencepiece\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sentencepiece) (\ud83d\udce5 5.6M \/ month \u00b7 \ud83d\udce6 350 \u00b7 \u23f1\ufe0f 18.06.2021):\n\t```\n\tpip install sentencepiece\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sentencepiece) (\ud83d\udce5 180K \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tconda install -c conda-forge sentencepiece\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/huggingface\/tokenizers\">Tokenizers<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 5.6K) - Fast State-of-the-Art Tokenizers optimized for Research and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/huggingface\/tokenizers) (\ud83d\udc68\u200d\ud83d\udcbb 56 \u00b7 \ud83d\udd00 470 \u00b7 \ud83d\udce6 48 \u00b7 \ud83d\udccb 620 - 30% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/huggingface\/tokenizers\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tokenizers) (\ud83d\udce5 6.8M \/ month \u00b7 \ud83d\udce6 120 \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tpip install tokenizers\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tokenizers) (\ud83d\udce5 140K \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tconda install -c conda-forge tokenizers\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/fastText\">fastText<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 24K) - Library for fast text representation and classification. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/fastText) (\ud83d\udc68\u200d\ud83d\udcbb 59 \u00b7 \ud83d\udd00 4.4K \u00b7 \ud83d\udce6 2.9K \u00b7 \ud83d\udccb 1.1K - 43% open \u00b7 \u23f1\ufe0f 04.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/fastText\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fasttext) (\ud83d\udce5 510K \/ month \u00b7 \ud83d\udce6 180 \u00b7 \u23f1\ufe0f 28.04.2020):\n\t```\n\tpip install fasttext\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/fasttext) (\ud83d\udce5 29K \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tconda install -c conda-forge fasttext\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dedupeio\/dedupe\">Dedupe<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 3.4K) - A python library for accurate and scalable fuzzy matching, record.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dedupeio\/dedupe) (\ud83d\udc68\u200d\ud83d\udcbb 63 \u00b7 \ud83d\udd00 470 \u00b7 \ud83d\udce6 220 \u00b7 \ud83d\udccb 720 - 5% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dedupeio\/dedupe\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dedupe) (\ud83d\udce5 320K \/ month \u00b7 \ud83d\udce6 48 \u00b7 \u23f1\ufe0f 20.04.2022):\n\t```\n\tpip install dedupe\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dedupe) (\ud83d\udce5 2.8K \u00b7 \u23f1\ufe0f 20.04.2022):\n\t```\n\tconda install -c conda-forge dedupe\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/text\">torchtext<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 3K) - Data loaders and abstractions for text and NLP. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/text) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 700 \u00b7 \ud83d\udccb 710 - 45% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/text\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchtext) (\ud83d\udce5 170K \/ month \u00b7 \ud83d\udce6 430 \u00b7 \u23f1\ufe0f 10.03.2022):\n\t```\n\tpip install torchtext\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/text\">TensorFlow Text<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 930) - Making text a first-class citizen in TensorFlow. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/text) (\ud83d\udc68\u200d\ud83d\udcbb 84 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce6 1.8K \u00b7 \ud83d\udccb 200 - 38% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/text\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-text) (\ud83d\udce5 2.7M \/ month \u00b7 \ud83d\udce6 76 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install tensorflow-text\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepmipt\/DeepPavlov\">DeepPavlov<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 5.7K) - An open source library for deep learning end-to-end dialog.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepmipt\/DeepPavlov) (\ud83d\udc68\u200d\ud83d\udcbb 67 \u00b7 \ud83d\udd00 1K \u00b7 \ud83d\udce6 260 \u00b7 \ud83d\udccb 630 - 12% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepmipt\/DeepPavlov\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deeppavlov) (\ud83d\udce5 11K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install deeppavlov\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/OpenNMT\/OpenNMT-py\">OpenNMT<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 5.6K) - Open Source Neural Machine Translation in PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/OpenNMT\/OpenNMT-py) (\ud83d\udc68\u200d\ud83d\udcbb 170 \u00b7 \ud83d\udd00 2K \u00b7 \ud83d\udce6 140 \u00b7 \ud83d\udccb 1.3K - 9% open \u00b7 \u23f1\ufe0f 09.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/OpenNMT\/OpenNMT-py\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/OpenNMT-py) (\ud83d\udce5 48K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 14.09.2021):\n\t```\n\tpip install OpenNMT-py\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jamesturk\/jellyfish\">jellyfish<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 1.6K) - a python library for doing approximate and phonetic matching of.. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jamesturk\/jellyfish) (\ud83d\udc68\u200d\ud83d\udcbb 25 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 3.6K \u00b7 \ud83d\udccb 110 - 8% open \u00b7 \u23f1\ufe0f 07.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/jamesturk\/jellyfish\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/jellyfish) (\ud83d\udce5 2.1M \/ month \u00b7 \ud83d\udce6 400 \u00b7 \u23f1\ufe0f 07.01.2022):\n\t```\n\tpip install jellyfish\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/jellyfish) (\ud83d\udce5 210K \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tconda install -c conda-forge jellyfish\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/snowballstem\/snowball\">snowballstemmer<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 560) - Snowball compiler and stemming algorithms. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/snowballstem\/snowball) (\ud83d\udc68\u200d\ud83d\udcbb 28 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 4 \u00b7 \ud83d\udccb 69 - 36% open \u00b7 \u23f1\ufe0f 17.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/snowballstem\/snowball\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/snowballstemmer) (\ud83d\udce5 6.6M \/ month \u00b7 \ud83d\udce6 6.7K \u00b7 \u23f1\ufe0f 16.11.2021):\n\t```\n\tpip install snowballstemmer\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/snowballstemmer) (\ud83d\udce5 4.2M \u00b7 \u23f1\ufe0f 17.11.2021):\n\t```\n\tconda install -c conda-forge snowballstemmer\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepset-ai\/haystack\">haystack<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 4.6K) - Haystack is an open source NLP framework that leverages Transformer.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepset-ai\/haystack) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 740 \u00b7 \ud83d\udce5 12 \u00b7 \ud83d\udccb 1.3K - 14% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepset-ai\/haystack\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/haystack) (\ud83d\udce5 910 \/ month \u00b7 \ud83d\udce6 85 \u00b7 \u23f1\ufe0f 15.12.2021):\n\t```\n\tpip install haystack\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/NVIDIA\/NeMo\">NeMo<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 4.2K) - NeMo: a toolkit for conversational AI. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/NVIDIA\/NeMo) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 940 \u00b7 \ud83d\udce5 2.2K \u00b7 \ud83d\udccb 1.1K - 6% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/NVIDIA\/NeMo\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nemo-toolkit) (\ud83d\udce5 16K \/ month \u00b7 \ud83d\udce6 7 \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tpip install nemo-toolkit\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rspeer\/python-ftfy\">ftfy<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 3.2K) - Fixes mojibake and other glitches in Unicode text, after the fact. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rspeer\/python-ftfy) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 5.6K \u00b7 \ud83d\udccb 130 - 8% open \u00b7 \u23f1\ufe0f 09.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rspeer\/python-ftfy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ftfy) (\ud83d\udce5 1.7M \/ month \u00b7 \ud83d\udce6 490 \u00b7 \u23f1\ufe0f 09.02.2022):\n\t```\n\tpip install ftfy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ftfy) (\ud83d\udce5 160K \u00b7 \u23f1\ufe0f 13.03.2022):\n\t```\n\tconda install -c conda-forge ftfy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/allenai\/scispacy\">SciSpacy<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 1.2K) - A full spaCy pipeline and models for scientific\/biomedical documents. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/allenai\/scispacy) (\ud83d\udc68\u200d\ud83d\udcbb 23 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 450 \u00b7 \ud83d\udccb 260 - 14% open \u00b7 \u23f1\ufe0f 10.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/allenai\/scispacy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scispacy) (\ud83d\udce5 28K \/ month \u00b7 \ud83d\udce6 17 \u00b7 \u23f1\ufe0f 10.03.2022):\n\t```\n\tpip install scispacy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/makcedward\/nlpaug\">nlpaug<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 3.2K) - Data augmentation for NLP. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/makcedward\/nlpaug) (\ud83d\udc68\u200d\ud83d\udcbb 30 \u00b7 \ud83d\udd00 360 \u00b7 \ud83d\udce6 320 \u00b7 \ud83d\udccb 180 - 19% open \u00b7 \u23f1\ufe0f 03.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/makcedward\/nlpaug\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nlpaug) (\ud83d\udce5 52K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 23.12.2021):\n\t```\n\tpip install nlpaug\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/nlpaug) (\ud83d\udce5 1.6K \u00b7 \u23f1\ufe0f 25.12.2021):\n\t```\n\tconda install -c conda-forge nlpaug\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/miso-belica\/sumy\">Sumy<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 2.8K) - Module for automatic summarization of text documents and HTML pages. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/miso-belica\/sumy) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 470 \u00b7 \ud83d\udce6 1.2K \u00b7 \ud83d\udccb 100 - 13% open \u00b7 \u23f1\ufe0f 21.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/miso-belica\/sumy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sumy) (\ud83d\udce5 21K \/ month \u00b7 \ud83d\udce6 100 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install sumy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sumy) (\ud83d\udce5 820 \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tconda install -c conda-forge sumy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/cltk\/cltk\">CLTK<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 720) - The Classical Language Toolkit. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/cltk\/cltk) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 310 \u00b7 \ud83d\udce5 25 \u00b7 \ud83d\udce6 200 \u00b7 \ud83d\udccb 520 - 4% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/cltk\/cltk\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cltk) (\ud83d\udce5 2.2K \/ month \u00b7 \ud83d\udce6 42 \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tpip install cltk\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/pytext\">PyText<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 6.3K) - A natural language modeling framework based on PyTorch. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/pytext) (\ud83d\udc68\u200d\ud83d\udcbb 230 \u00b7 \ud83d\udd00 800 \u00b7 \ud83d\udce5 290 \u00b7 \ud83d\udce6 110 \u00b7 \ud83d\udccb 220 - 66% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/pytext\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytext-nlp) (\ud83d\udce5 280 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 08.06.2020):\n\t```\n\tpip install pytext-nlp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/cjhutto\/vaderSentiment\">vaderSentiment<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 3.6K) - VADER Sentiment Analysis. VADER (Valence Aware Dictionary and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/cjhutto\/vaderSentiment) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 850 \u00b7 \ud83d\udce6 3.8K \u00b7 \ud83d\udccb 110 - 30% open \u00b7 \u23f1\ufe0f 01.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/cjhutto\/vaderSentiment\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vadersentiment) (\ud83d\udce5 210K \/ month \u00b7 \ud83d\udce6 170 \u00b7 \u23f1\ufe0f 22.05.2020):\n\t```\n\tpip install vadersentiment\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/vadersentiment) (\ud83d\udce5 8.5K \u00b7 \u23f1\ufe0f 22.03.2021):\n\t```\n\tconda install -c conda-forge vadersentiment\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/life4\/textdistance\">TextDistance<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.8K) - Compute distance between sequences. 30+ algorithms, pure python.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/life4\/textdistance) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udce5 650 \u00b7 \ud83d\udce6 2.1K \u00b7 \u23f1\ufe0f 29.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/life4\/textdistance\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/textdistance) (\ud83d\udce5 460K \/ month \u00b7 \ud83d\udce6 39 \u00b7 \u23f1\ufe0f 27.10.2021):\n\t```\n\tpip install textdistance\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/textdistance) (\ud83d\udce5 99K \u00b7 \u23f1\ufe0f 27.10.2021):\n\t```\n\tconda install -c conda-forge textdistance\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/huggingface\/neuralcoref\">neuralcoref<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.5K \u00b7 \ud83d\udca4) - Fast Coreference Resolution in spaCy with Neural Networks. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/huggingface\/neuralcoref) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 430 \u00b7 \ud83d\udce5 380 \u00b7 \ud83d\udce6 480 \u00b7 \ud83d\udccb 300 - 16% open \u00b7 \u23f1\ufe0f 22.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/huggingface\/neuralcoref\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/neuralcoref) (\ud83d\udce5 130K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 08.04.2019):\n\t```\n\tpip install neuralcoref\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/neuralcoref) (\ud83d\udce5 11K \u00b7 \u23f1\ufe0f 21.02.2020):\n\t```\n\tconda install -c conda-forge neuralcoref\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dmlc\/gluon-nlp\">GluonNLP<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.4K \u00b7 \ud83d\udca4) - Toolkit that enables easy text preprocessing, datasets.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dmlc\/gluon-nlp) (\ud83d\udc68\u200d\ud83d\udcbb 82 \u00b7 \ud83d\udd00 520 \u00b7 \ud83d\udce6 780 \u00b7 \ud83d\udccb 560 - 46% open \u00b7 \u23f1\ufe0f 24.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/dmlc\/gluon-nlp\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gluonnlp) (\ud83d\udce5 39K \/ month \u00b7 \ud83d\udce6 22 \u00b7 \u23f1\ufe0f 13.08.2020):\n\t```\n\tpip install gluonnlp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/chartbeat-labs\/textacy\">textacy<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 1.9K) - NLP, before and after spaCy. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/chartbeat-labs\/textacy) (\ud83d\udc68\u200d\ud83d\udcbb 32 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udccb 250 - 11% open \u00b7 \u23f1\ufe0f 06.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/chartbeat-labs\/textacy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/textacy) (\ud83d\udce5 36K \/ month \u00b7 \ud83d\udce6 100 \u00b7 \u23f1\ufe0f 06.12.2021):\n\t```\n\tpip install textacy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/textacy) (\ud83d\udce5 100K \u00b7 \u23f1\ufe0f 06.02.2022):\n\t```\n\tconda install -c conda-forge textacy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/DerwenAI\/pytextrank\">PyTextRank<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 1.8K) - Python implementation of TextRank algorithms (textgraphs) for phrase.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/DerwenAI\/pytextrank) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 320 \u00b7 \ud83d\udce6 250 \u00b7 \ud83d\udccb 85 - 28% open \u00b7 \u23f1\ufe0f 07.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/DerwenAI\/pytextrank\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytextrank) (\ud83d\udce5 22K \/ month \u00b7 \ud83d\udce6 12 \u00b7 \u23f1\ufe0f 06.03.2022):\n\t```\n\tpip install pytextrank\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Ciphey\/Ciphey\">Ciphey<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 9.8K) - Automatically decrypt encryptions without knowing the key or cipher,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Ciphey\/Ciphey) (\ud83d\udc68\u200d\ud83d\udcbb 46 \u00b7 \ud83d\udd00 600 \u00b7 \ud83d\udccb 290 - 17% open \u00b7 \u23f1\ufe0f 03.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/Ciphey\/Ciphey\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ciphey) (\ud83d\udce5 9.5K \/ month \u00b7 \u23f1\ufe0f 06.06.2021):\n\t```\n\tpip install ciphey\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/remnux\/ciphey) (\ud83d\udce5 15K \u00b7 \u2b50 6 \u00b7 \u23f1\ufe0f 14.04.2022):\n\t```\n\tdocker pull remnux\/ciphey\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google-research\/text-to-text-transfer-transformer\">T5<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 4.1K) - Code for the paper Exploring the Limits of Transfer Learning with a.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google-research\/text-to-text-transfer-transformer) (\ud83d\udc68\u200d\ud83d\udcbb 48 \u00b7 \ud83d\udd00 570 \u00b7 \ud83d\udce6 95 \u00b7 \ud83d\udccb 400 - 15% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google-research\/text-to-text-transfer-transformer\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/t5) (\ud83d\udce5 6.2K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 18.10.2021):\n\t```\n\tpip install t5\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/fastnlp\/fastNLP\">fastNLP<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 2.6K) - fastNLP: A Modularized and Extensible NLP Framework. Currently still.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/fastnlp\/fastNLP) (\ud83d\udc68\u200d\ud83d\udcbb 54 \u00b7 \ud83d\udd00 410 \u00b7 \ud83d\udce5 66 \u00b7 \ud83d\udce6 74 \u00b7 \ud83d\udccb 190 - 20% open \u00b7 \u23f1\ufe0f 06.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/fastnlp\/fastNLP\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fastnlp) (\ud83d\udce5 1.3K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 04.02.2019):\n\t```\n\tpip install fastnlp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/explosion\/spacy-transformers\">spacy-transformers<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 1.1K) - Use pretrained transformers like BERT, XLNet and GPT-2.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code>spacy<\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/explosion\/spacy-transformers) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 490 \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/explosion\/spacy-transformers\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/spacy-transformers) (\ud83d\udce5 99K \/ month \u00b7 \ud83d\udce6 17 \u00b7 \u23f1\ufe0f 15.03.2022):\n\t```\n\tpip install spacy-transformers\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/spacy-transformers) (\ud83d\udce5 1.1K \u00b7 \u23f1\ufe0f 15.03.2022):\n\t```\n\tconda install -c conda-forge spacy-transformers\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dwyl\/english-words\">english-words<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 7.1K) - A text file containing 479k English words for all your.. <code><a href=\"http:\/\/bit.ly\/3rvuUlR\">Unlicense<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dwyl\/english-words) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 1.4K \u00b7 \ud83d\udccb 89 - 67% open \u00b7 \u23f1\ufe0f 20.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dwyl\/english-words\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/english-words) (\ud83d\udce5 23K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 29.01.2022):\n\t```\n\tpip install english-words\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/snipsco\/snips-nlu\">Snips NLU<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 3.6K \u00b7 \ud83d\udca4) - Snips Python library to extract meaning from text. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/snipsco\/snips-nlu) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 500 \u00b7 \ud83d\udccb 260 - 23% open \u00b7 \u23f1\ufe0f 03.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/snipsco\/snips-nlu\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/snips-nlu) (\ud83d\udce5 3.4K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 15.01.2020):\n\t```\n\tpip install snips-nlu\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/JasonKessler\/scattertext\">scattertext<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 1.8K) - Beautiful visualizations of how language differs among document.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/JasonKessler\/scattertext) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 250 \u00b7 \ud83d\udce6 280 \u00b7 \ud83d\udccb 85 - 20% open \u00b7 \u23f1\ufe0f 26.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/JasonKessler\/scattertext\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scattertext) (\ud83d\udce5 3.5K \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 26.03.2022):\n\t```\n\tpip install scattertext\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/scattertext) (\ud83d\udce5 61K \u00b7 \u23f1\ufe0f 26.03.2022):\n\t```\n\tconda install -c conda-forge scattertext\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/NTMC-Community\/MatchZoo\">MatchZoo<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 3.6K \u00b7 \ud83d\udca4) - Facilitating the design, comparison and sharing of deep.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/NTMC-Community\/MatchZoo) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 910 \u00b7 \ud83d\udce6 10 \u00b7 \ud83d\udccb 460 - 6% open \u00b7 \u23f1\ufe0f 02.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/NTMC-Community\/MatchZoo\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/matchzoo) (\ud83d\udce5 95 \/ month \u00b7 \u23f1\ufe0f 24.10.2019):\n\t```\n\tpip install matchzoo\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PetrochukM\/PyTorch-NLP\">pytorch-nlp<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 2.1K \u00b7 \ud83d\udca4) - Basic Utilities for PyTorch Natural Language Processing.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PetrochukM\/PyTorch-NLP) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 250 \u00b7 \ud83d\udce6 370 \u00b7 \ud83d\udccb 67 - 26% open \u00b7 \u23f1\ufe0f 10.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/PetrochukM\/PyTorch-NLP\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch-nlp) (\ud83d\udce5 6.6K \/ month \u00b7 \ud83d\udce6 17 \u00b7 \u23f1\ufe0f 04.11.2019):\n\t```\n\tpip install pytorch-nlp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/utterworks\/fast-bert\">fast-bert<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.7K) - Super easy library for BERT based NLP models. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/utterworks\/fast-bert) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 330 \u00b7 \ud83d\udccb 250 - 61% open \u00b7 \u23f1\ufe0f 12.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/utterworks\/fast-bert\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fast-bert) (\ud83d\udce5 2K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 12.04.2022):\n\t```\n\tpip install fast-bert\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/BrikerMan\/Kashgari\">Kashgari<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 2.3K \u00b7 \ud83d\udca4) - Kashgari is a production-level NLP Transfer learning.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/BrikerMan\/Kashgari) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 430 \u00b7 \ud83d\udce6 52 \u00b7 \ud83d\udccb 370 - 10% open \u00b7 \u23f1\ufe0f 09.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/BrikerMan\/Kashgari\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/kashgari-tf) (\ud83d\udce5 43 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 18.10.2019):\n\t```\n\tpip install kashgari-tf\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/explosion\/sense2vec\">sense2vec<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.4K \u00b7 \ud83d\udca4) - Contextually-keyed word vectors. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/explosion\/sense2vec) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udce5 29K \u00b7 \ud83d\udce6 140 \u00b7 \ud83d\udccb 110 - 18% open \u00b7 \u23f1\ufe0f 16.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/explosion\/sense2vec\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sense2vec) (\ud83d\udce5 3.5K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 19.04.2021):\n\t```\n\tpip install sense2vec\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sense2vec) (\ud83d\udce5 25K \u00b7 \u23f1\ufe0f 14.07.2021):\n\t```\n\tconda install -c conda-forge sense2vec\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/awslabs\/sockeye\">Sockeye<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.1K) - Sequence-to-sequence framework with a focus on Neural Machine.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/awslabs\/sockeye) (\ud83d\udc68\u200d\ud83d\udcbb 56 \u00b7 \ud83d\udd00 310 \u00b7 \ud83d\udce5 14 \u00b7 \ud83d\udccb 280 - 2% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/awslabs\/sockeye\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sockeye) (\ud83d\udce5 560 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install sockeye\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/unitaryai\/detoxify\">detoxify<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 420) - Trained models & code to predict toxic comments on all 3 Jigsaw.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/unitaryai\/detoxify) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 49 \u00b7 \ud83d\udce5 41K \u00b7 \ud83d\udce6 81 \u00b7 \ud83d\udccb 34 - 50% open \u00b7 \u23f1\ufe0f 20.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/unitaryai\/detoxify\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/detoxify) (\ud83d\udce5 8.2K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 12.04.2022):\n\t```\n\tpip install detoxify\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jbesomi\/texthero\">Texthero<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 2.5K \u00b7 \ud83d\udca4) - Text preprocessing, representation and visualization from zero to.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jbesomi\/texthero) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 210 \u00b7 \ud83d\udce5 90 \u00b7 \ud83d\udccb 140 - 55% open \u00b7 \u23f1\ufe0f 19.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/jbesomi\/texthero\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/texthero) (\ud83d\udce5 14K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 01.07.2021):\n\t```\n\tpip install texthero\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepset-ai\/FARM\">FARM<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 1.5K) - Fast & easy transfer learning for NLP. Harvesting language models.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepset-ai\/FARM) (\ud83d\udc68\u200d\ud83d\udcbb 37 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udccb 440 - 8% open \u00b7 \u23f1\ufe0f 25.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepset-ai\/FARM\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/farm) (\ud83d\udce5 4.1K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 10.06.2021):\n\t```\n\tpip install farm\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/farm) (\ud83d\udce5 1K \u00b7 \u23f1\ufe0f 14.06.2021):\n\t```\n\tconda install -c conda-forge farm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/recognai\/rubrix\">rubrix<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 1K) - Rubrix, open-source framework for data-centric NLP. Data annotation and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/recognai\/rubrix) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 88 \u00b7 \ud83d\udccb 530 - 10% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/recognai\/rubrix\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/rubrix) (\ud83d\udce5 1.1K \/ month \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install rubrix\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/rubrix) (\ud83d\udce5 880 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tconda install -c conda-forge rubrix\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/anhaidgroup\/deepmatcher\">DeepMatcher<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 4.2K \u00b7 \ud83d\udca4) - Python package for performing Entity and Text Matching using.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/anhaidgroup\/deepmatcher) (\ud83d\udc68\u200d\ud83d\udcbb 7 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce6 18 \u00b7 \ud83d\udccb 81 - 72% open \u00b7 \u23f1\ufe0f 13.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/anhaidgroup\/deepmatcher\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deepmatcher) (\ud83d\udce5 950 \/ month \u00b7 \u23f1\ufe0f 13.06.2021):\n\t```\n\tpip install deepmatcher\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/IntelLabs\/nlp-architect\">NLP Architect<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 2.8K \u00b7 \ud83d\udca4) - A model library for exploring state-of-the-art deep.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/IntelLabs\/nlp-architect) (\ud83d\udc68\u200d\ud83d\udcbb 37 \u00b7 \ud83d\udd00 440 \u00b7 \ud83d\udce6 8 \u00b7 \ud83d\udccb 130 - 15% open \u00b7 \u23f1\ufe0f 12.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/IntelLabs\/nlp-architect\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nlp-architect) (\ud83d\udce5 140 \/ month \u00b7 \u23f1\ufe0f 12.04.2020):\n\t```\n\tpip install nlp-architect\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bytedance\/lightseq\">lightseq<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 2.1K) - LightSeq: A High Performance Library for Sequence Processing and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bytedance\/lightseq) (\ud83d\udc68\u200d\ud83d\udcbb 9 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce5 610 \u00b7 \ud83d\udccb 170 - 55% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bytedance\/lightseq\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lightseq) (\ud83d\udce5 700 \/ month \u00b7 \u23f1\ufe0f 26.01.2022):\n\t```\n\tpip install lightseq\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nyu-mll\/jiant\">jiant<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1.4K) - jiant is an nlp toolkit. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nyu-mll\/jiant) (\ud83d\udc68\u200d\ud83d\udcbb 57 \u00b7 \ud83d\udd00 270 \u00b7 \ud83d\udce6 2 \u00b7 \ud83d\udccb 550 - 11% open \u00b7 \u23f1\ufe0f 31.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nyu-mll\/jiant\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/jiant) (\ud83d\udce5 57 \/ month \u00b7 \u23f1\ufe0f 10.05.2021):\n\t```\n\tpip install jiant\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/thunlp\/OpenPrompt\">OpenPrompt<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1.4K) - An Open-Source Framework for Prompt-Learning. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/thunlp\/OpenPrompt) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 8 \u00b7 \ud83d\udccb 120 - 5% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/thunlp\/OpenPrompt\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/openprompt) (\ud83d\udce5 450 \/ month \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tpip install openprompt\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/minimaxir\/gpt-2-simple\">gpt-2-simple<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 2.9K \u00b7 \ud83d\udca4) - Python package to easily retrain OpenAIs GPT-2 text-.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/minimaxir\/gpt-2-simple) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 590 \u00b7 \ud83d\udce5 300 \u00b7 \ud83d\udccb 250 - 61% open \u00b7 \u23f1\ufe0f 18.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/minimaxir\/gpt-2-simple\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gpt-2-simple) (\ud83d\udce5 5K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 18.10.2021):\n\t```\n\tpip install gpt-2-simple\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/qdrant\/qdrant\">qdrant<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1.4K) - Qdrant - vector similarity search engine with extended filtering.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/qdrant\/qdrant) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 86 \u00b7 \ud83d\udccb 190 - 25% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/qdrant\/qdrant\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/IndicoDataSolutions\/finetune\">finetune<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 660) - Scikit-learn style model finetuning for NLP. <code><a href=\"http:\/\/bit.ly\/3postzC\">MPL-2.0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/IndicoDataSolutions\/finetune) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 70 \u00b7 \ud83d\udce6 9 \u00b7 \ud83d\udccb 140 - 15% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/IndicoDataSolutions\/finetune\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/finetune) (\ud83d\udce5 36 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 20.12.2021):\n\t```\n\tpip install finetune\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/RUCAIBox\/TextBox\">TextBox<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 350) - TextBox is an open-source library for building text generation system. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/RUCAIBox\/TextBox) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 63 \u00b7 \ud83d\udce6 5 \u00b7 \ud83d\udccb 20 - 15% open \u00b7 \u23f1\ufe0f 06.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/RUCAIBox\/TextBox\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/textbox) (\ud83d\udce5 51 \/ month \u00b7 \u23f1\ufe0f 15.04.2021):\n\t```\n\tpip install textbox\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/thunlp\/OpenNRE\">OpenNRE<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 3.6K) - An Open-Source Package for Neural Relation Extraction (NRE). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/thunlp\/OpenNRE) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 930 \u00b7 \ud83d\udccb 350 - 5% open \u00b7 \u23f1\ufe0f 06.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/thunlp\/OpenNRE\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PKSHATechnology-Research\/camphr\">Camphr<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 340 \u00b7 \ud83d\udca4) - Camphr - NLP libary for creating pipeline components. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code>spacy<\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PKSHATechnology-Research\/camphr) (\ud83d\udc68\u200d\ud83d\udcbb 7 \u00b7 \ud83d\udd00 17 \u00b7 \ud83d\udccb 28 - 7% open \u00b7 \u23f1\ufe0f 18.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/PKSHATechnology-Research\/camphr\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/camphr) (\ud83d\udce5 380 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 28.07.2021):\n\t```\n\tpip install camphr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/translate\">Translate<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 730 \u00b7 \ud83d\udca4) - Translate - a PyTorch Language Library. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/translate) (\ud83d\udc68\u200d\ud83d\udcbb 87 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udccb 93 - 70% open \u00b7 \u23f1\ufe0f 06.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/translate\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch-translate) (\ud83d\udce5 7 \/ month \u00b7 \u23f1\ufe0f 01.05.2018):\n\t```\n\tpip install pytorch-translate\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/vizseq\">VizSeq<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 390) - An Analysis Toolkit for Natural Language Generation (Translation,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/vizseq) (\ud83d\udc68\u200d\ud83d\udcbb 3 \u00b7 \ud83d\udd00 47 \u00b7 \ud83d\udce6 5 \u00b7 \ud83d\udccb 16 - 43% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/vizseq\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vizseq) (\ud83d\udce5 55 \/ month \u00b7 \u23f1\ufe0f 07.08.2020):\n\t```\n\tpip install vizseq\n\t```\n<\/details>\n<details><summary>Show 28 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/seatgeek\/fuzzywuzzy\">fuzzywuzzy<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 8.7K \u00b7 \ud83d\udca4) - Fuzzy String Matching in Python. <code><a href=\"http:\/\/bit.ly\/2KucAZR\">\u2757\ufe0fGPL-2.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/saffsd\/langid.py\">langid<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 2K \u00b7 \ud83d\udc80) - Stand-alone language identification system. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/vi3k6i5\/flashtext\">flashtext<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 5.2K \u00b7 \ud83d\udc80) - Extract Keywords from sentence or Replace keywords in sentences. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/aboSamoor\/polyglot\">polyglot<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 2K \u00b7 \ud83d\udc80) - Multilingual text (NLP) processing toolkit. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/minimaxir\/textgenrnn\">textgenrnn<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 4.7K \u00b7 \ud83d\udc80) - Easily train your own text-generating neural network of any.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/mchaput\/whoosh\">whoosh<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 220) - Pure-Python full-text search library. <code><a href=\"https:\/\/tldrlegal.com\/search?q=BSD-1-Clause\">\u2757\ufe0fBSD-1-Clause<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/VKCOM\/YouTokenToMe\">YouTokenToMe<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 800 \u00b7 \ud83d\udc80) - Unsupervised text tokenizer focused on computational efficiency. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/nipunsadvilkar\/pySBD\">pySBD<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 440 \u00b7 \ud83d\udc80) - pySBD (Python Sentence Boundary Disambiguation) is a rule-based sentence.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/asyml\/texar\">Texar<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 2.3K \u00b7 \ud83d\udc80) - Toolkit for Machine Learning, Natural Language Processing, and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/EricFillion\/happy-transformer\">happy-transformer<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 280) - A package built on top of Hugging Faces transformers.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code>huggingface<\/code>\n- <b><a href=\"https:\/\/github.com\/Delta-ML\/delta\">DELTA<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1.5K \u00b7 \ud83d\udc80) - DELTA is a deep learning based natural language and speech.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/Hironsan\/anago\">anaGo<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1.4K \u00b7 \ud83d\udc80) - Bidirectional LSTM-CRF and ELMo for Named-Entity Recognition,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/Alir3z4\/python-stop-words\">stop-words<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 140 \u00b7 \ud83d\udc80) - Get list of common stop words in various languages in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Ki6an\/fastT5\">fastT5<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 290) - boost inference speed of T5 models by 5x & reduce the model size by 3x. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/vrasneur\/pyfasttext\">pyfasttext<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 230 \u00b7 \ud83d\udc80) - Yet another Python binding for fastText. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/textpipe\/textpipe\">textpipe<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 300 \u00b7 \ud83d\udca4) - Textpipe: clean and extract metadata from text. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Franck-Dernoncourt\/NeuroNER\">NeuroNER<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 1.6K \u00b7 \ud83d\udc80) - Named-entity recognition using neural networks. Easy-to-use and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/koursaros-ai\/nboost\">nboost<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 620 \u00b7 \ud83d\udc80) - NBoost is a scalable, search-api-boosting platform for deploying.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/dsfsi\/textaugment\">textaugment<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 240 \u00b7 \ud83d\udca4) - TextAugment: Text Augmentation Library. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/shaypal5\/skift\">skift<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 230) - scikit-learn wrappers for Python fastText. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/facebookresearch\/BLINK\">BLINK<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 860 \u00b7 \ud83d\udc80) - Entity Linker solution. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/victordibia\/neuralqa\">NeuralQA<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 220 \u00b7 \ud83d\udc80) - NeuralQA: A Usable Library for Question Answering on Large Datasets.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/MartinoMensio\/spacy-dbpedia-spotlight\">spacy-dbpedia-spotlight<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 58) - A spaCy wrapper for DBpedia Spotlight. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code>spacy<\/code>\n- <b><a href=\"https:\/\/github.com\/as-ideas\/headliner\">Headliner<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 230 \u00b7 \ud83d\udc80) - Easy training and deployment of seq2seq models. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/textvec\/textvec\">textvec<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 180 \u00b7 \ud83d\udc80) - Text vectorization tool to outperform TFIDF for classification.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/jaidevd\/numerizer\">numerizer<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 140) - A Python module to convert natural language numerics into ints and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/feedly\/transfer-nlp\">TransferNLP<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 290 \u00b7 \ud83d\udc80) - NLP library designed for reproducible experimentation.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/abelriboulot\/onnxt5\">ONNX-T5<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 200 \u00b7 \ud83d\udc80) - Summarization, translation, sentiment-analysis, text-generation.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n<\/details>\n<br>\n\n## Image Data\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for image & video processing, manipulation, and augmentation as well as libraries for computer vision tasks such as facial recognition, object detection, and classification._\n\n<details><summary><b><a href=\"https:\/\/github.com\/python-pillow\/Pillow\">Pillow<\/a><\/b> (\ud83e\udd4745 \u00b7  \u2b50 9.7K) - The friendly PIL fork (Python Imaging Library). <code><a href=\"https:\/\/tldrlegal.com\/search?q=PIL\">\u2757\ufe0fPIL<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/python-pillow\/Pillow) (\ud83d\udc68\u200d\ud83d\udcbb 400 \u00b7 \ud83d\udd00 1.8K \u00b7 \ud83d\udccb 2.5K - 4% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/python-pillow\/Pillow\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/Pillow) (\ud83d\udce5 42M \/ month \u00b7 \ud83d\udce6 63K \u00b7 \u23f1\ufe0f 01.04.2022):\n\t```\n\tpip install Pillow\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pillow) (\ud83d\udce5 14M \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tconda install -c conda-forge pillow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-image\/scikit-image\">scikit-image<\/a><\/b> (\ud83e\udd4744 \u00b7  \u2b50 4.9K) - Image processing in Python. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-image\/scikit-image) (\ud83d\udc68\u200d\ud83d\udcbb 550 \u00b7 \ud83d\udd00 2K \u00b7 \ud83d\udce6 100K \u00b7 \ud83d\udccb 2.4K - 25% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-image\/scikit-image\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scikit-image) (\ud83d\udce5 5.8M \/ month \u00b7 \ud83d\udce6 9.2K \u00b7 \u23f1\ufe0f 17.02.2022):\n\t```\n\tpip install scikit-image\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/scikit-image) (\ud83d\udce5 3.4M \u00b7 \u23f1\ufe0f 18.02.2022):\n\t```\n\tconda install -c conda-forge scikit-image\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/vision\">torchvision<\/a><\/b> (\ud83e\udd4742 \u00b7  \u2b50 12K) - Datasets, Transforms and Models specific to Computer Vision. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/vision) (\ud83d\udc68\u200d\ud83d\udcbb 480 \u00b7 \ud83d\udd00 5.9K \u00b7 \ud83d\udce5 4.5K \u00b7 \ud83d\udccb 2.5K - 28% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/vision\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchvision) (\ud83d\udce5 3.8M \/ month \u00b7 \ud83d\udce6 3.6K \u00b7 \u23f1\ufe0f 10.03.2022):\n\t```\n\tpip install torchvision\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/torchvision) (\ud83d\udce5 220K \u00b7 \u23f1\ufe0f 29.03.2022):\n\t```\n\tconda install -c conda-forge torchvision\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/open-mmlab\/mmdetection\">MMDetection<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 20K) - OpenMMLab Detection Toolbox and Benchmark. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/open-mmlab\/mmdetection) (\ud83d\udc68\u200d\ud83d\udcbb 330 \u00b7 \ud83d\udd00 7.1K \u00b7 \ud83d\udce6 360 \u00b7 \ud83d\udccb 5.8K - 9% open \u00b7 \u23f1\ufe0f 30.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/open-mmlab\/mmdetection\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mmdet) (\ud83d\udce5 42K \/ month \u00b7 \ud83d\udce6 9 \u00b7 \u23f1\ufe0f 30.04.2022):\n\t```\n\tpip install mmdet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rwightman\/pytorch-image-models\">PyTorch Image Models<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 18K) - PyTorch image models, scripts, pretrained weights --.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rwightman\/pytorch-image-models) (\ud83d\udc68\u200d\ud83d\udcbb 73 \u00b7 \ud83d\udd00 3K \u00b7 \ud83d\udce5 1.2M \u00b7 \ud83d\udce6 2.8K \u00b7 \ud83d\udccb 490 - 11% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rwightman\/pytorch-image-models\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/timm) (\ud83d\udce5 1.8M \/ month \u00b7 \ud83d\udce6 90 \u00b7 \u23f1\ufe0f 17.01.2022):\n\t```\n\tpip install timm\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/timm) (\ud83d\udce5 16K \u00b7 \u23f1\ufe0f 30.06.2021):\n\t```\n\tconda install -c conda-forge timm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/imageio\/imageio\">imageio<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 1K) - Python library for reading and writing image data. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/imageio\/imageio) (\ud83d\udc68\u200d\ud83d\udcbb 88 \u00b7 \ud83d\udd00 210 \u00b7 \ud83d\udce5 210 \u00b7 \ud83d\udce6 61K \u00b7 \ud83d\udccb 450 - 13% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/imageio\/imageio\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/imageio) (\ud83d\udce5 15M \/ month \u00b7 \ud83d\udce6 2.6K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tpip install imageio\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/imageio) (\ud83d\udce5 2.8M \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge imageio\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Zulko\/moviepy\">MoviePy<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 9.2K) - Video editing with Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Zulko\/moviepy) (\ud83d\udc68\u200d\ud83d\udcbb 150 \u00b7 \ud83d\udd00 1.2K \u00b7 \ud83d\udce6 15K \u00b7 \ud83d\udccb 1.2K - 24% open \u00b7 \u23f1\ufe0f 26.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Zulko\/moviepy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/moviepy) (\ud83d\udce5 4.5M \/ month \u00b7 \ud83d\udce6 750 \u00b7 \u23f1\ufe0f 05.10.2020):\n\t```\n\tpip install moviepy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/moviepy) (\ud83d\udce5 110K \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tconda install -c conda-forge moviepy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/kornia\/kornia\">Kornia<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 6.3K) - Open Source Differentiable Computer Vision Library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/kornia\/kornia) (\ud83d\udc68\u200d\ud83d\udcbb 170 \u00b7 \ud83d\udd00 630 \u00b7 \ud83d\udce5 270 \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 580 - 27% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/kornia\/kornia\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/kornia) (\ud83d\udce5 250K \/ month \u00b7 \ud83d\udce6 46 \u00b7 \u23f1\ufe0f 21.03.2022):\n\t```\n\tpip install kornia\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/kornia) (\ud83d\udce5 15K \u00b7 \u23f1\ufe0f 21.03.2022):\n\t```\n\tconda install -c conda-forge kornia\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepinsight\/insightface\">InsightFace<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 12K) - State-of-the-art 2D and 3D Face Analysis Project. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepinsight\/insightface) (\ud83d\udc68\u200d\ud83d\udcbb 42 \u00b7 \ud83d\udd00 3.8K \u00b7 \ud83d\udce6 160 \u00b7 \ud83d\udccb 1.9K - 54% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepinsight\/insightface\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/insightface) (\ud83d\udce5 20K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 29.01.2022):\n\t```\n\tpip install insightface\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/opencv\/opencv-python\">opencv-python<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 2.7K) - Automated CI toolchain to produce precompiled opencv-python,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/opencv\/opencv-python) (\ud83d\udc68\u200d\ud83d\udcbb 38 \u00b7 \ud83d\udd00 530 \u00b7 \ud83d\udccb 540 - 7% open \u00b7 \u23f1\ufe0f 12.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/opencv\/opencv-python\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/opencv-python) (\ud83d\udce5 5.3M \/ month \u00b7 \ud83d\udce6 8.9K \u00b7 \u23f1\ufe0f 09.03.2022):\n\t```\n\tpip install opencv-python\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ageitgey\/face_recognition\">Face Recognition<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 44K \u00b7 \ud83d\udca4) - The worlds simplest facial recognition api for Python.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ageitgey\/face_recognition) (\ud83d\udc68\u200d\ud83d\udcbb 47 \u00b7 \ud83d\udd00 12K \u00b7 \ud83d\udce5 460 \u00b7 \ud83d\udccb 1.2K - 54% open \u00b7 \u23f1\ufe0f 14.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/ageitgey\/face_recognition\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/face_recognition) (\ud83d\udce5 51K \/ month \u00b7 \ud83d\udce6 210 \u00b7 \u23f1\ufe0f 21.08.2018):\n\t```\n\tpip install face_recognition\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/face_recognition) (\ud83d\udce5 6K \u00b7 \u23f1\ufe0f 30.04.2021):\n\t```\n\tconda install -c conda-forge face_recognition\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/detectron2\">detectron2<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 21K) - Detectron2 is a platform for object detection, segmentation.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/detectron2) (\ud83d\udc68\u200d\ud83d\udcbb 200 \u00b7 \ud83d\udd00 5.6K \u00b7 \ud83d\udce6 590 \u00b7 \ud83d\udccb 3K - 6% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/detectron2\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/detectron2) (\ud83d\udce6 3 \u00b7 \u23f1\ufe0f 06.02.2020):\n\t```\n\tpip install detectron2\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/detectron2) (\ud83d\udce5 48K \u00b7 \u23f1\ufe0f 25.04.2022):\n\t```\n\tconda install -c conda-forge detectron2\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dmlc\/gluon-cv\">GluonCV<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 5.2K) - Gluon CV Toolkit. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dmlc\/gluon-cv) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 1.1K \u00b7 \ud83d\udce6 720 \u00b7 \ud83d\udccb 820 - 7% open \u00b7 \u23f1\ufe0f 30.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dmlc\/gluon-cv\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gluoncv) (\ud83d\udce5 560K \/ month \u00b7 \ud83d\udce6 59 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install gluoncv\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/emcconville\/wand\">Wand<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 1.2K) - The ctypes-based simple ImageMagick binding for Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/emcconville\/wand) (\ud83d\udc68\u200d\ud83d\udcbb 97 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udce5 7.4K \u00b7 \ud83d\udce6 11K \u00b7 \ud83d\udccb 370 - 4% open \u00b7 \u23f1\ufe0f 14.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/emcconville\/wand\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/wand) (\ud83d\udce5 440K \/ month \u00b7 \ud83d\udce6 680 \u00b7 \u23f1\ufe0f 17.08.2021):\n\t```\n\tpip install wand\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/wand) (\ud83d\udce5 11K \u00b7 \u23f1\ufe0f 30.11.2020):\n\t```\n\tconda install -c conda-forge wand\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/albumentations-team\/albumentations\">Albumentations<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 10K) - Fast image augmentation library and an easy-to-use wrapper.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/albumentations-team\/albumentations) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udce6 7.7K \u00b7 \ud83d\udccb 590 - 41% open \u00b7 \u23f1\ufe0f 13.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/albumentations-team\/albumentations\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/albumentations) (\ud83d\udce5 390K \/ month \u00b7 \ud83d\udce6 180 \u00b7 \u23f1\ufe0f 04.10.2021):\n\t```\n\tpip install albumentations\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/albumentations) (\ud83d\udce5 36K \u00b7 \u23f1\ufe0f 15.07.2021):\n\t```\n\tconda install -c conda-forge albumentations\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PaddlePaddle\/PaddleDetection\">PaddleDetection<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 7.5K) - Object Detection toolkit based on PaddlePaddle. It.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PaddlePaddle\/PaddleDetection) (\ud83d\udc68\u200d\ud83d\udcbb 94 \u00b7 \ud83d\udd00 1.9K \u00b7 \ud83d\udce6 15 \u00b7 \ud83d\udccb 3.4K - 19% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PaddlePaddle\/PaddleDetection\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/paddledet) (\ud83d\udce5 370 \/ month \u00b7 \u23f1\ufe0f 24.04.2022):\n\t```\n\tpip install paddledet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PaddlePaddle\/PaddleSeg\">PaddleSeg<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 4.7K) - Easy-to-use image segmentation library with awesome pre-.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PaddlePaddle\/PaddleSeg) (\ud83d\udc68\u200d\ud83d\udcbb 78 \u00b7 \ud83d\udd00 1K \u00b7 \ud83d\udce6 540 \u00b7 \ud83d\udccb 1.1K - 51% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PaddlePaddle\/PaddleSeg\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/paddleseg) (\ud83d\udce5 1.5K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 20.04.2022):\n\t```\n\tpip install paddleseg\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PyImageSearch\/imutils\">imutils<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 4.1K) - A series of convenience functions to make basic image processing.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PyImageSearch\/imutils) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 960 \u00b7 \ud83d\udce6 25K \u00b7 \ud83d\udccb 220 - 65% open \u00b7 \u23f1\ufe0f 27.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PyImageSearch\/imutils\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/imutils) (\ud83d\udce5 280K \/ month \u00b7 \ud83d\udce6 760 \u00b7 \u23f1\ufe0f 15.01.2021):\n\t```\n\tpip install imutils\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/imutils) (\ud83d\udce5 82K \u00b7 \u23f1\ufe0f 09.12.2021):\n\t```\n\tconda install -c conda-forge imutils\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/OlafenwaMoses\/ImageAI\">imageai<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 7K \u00b7 \ud83d\udca4) - A python library built to empower developers to build applications and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/OlafenwaMoses\/ImageAI) (\ud83d\udc68\u200d\ud83d\udcbb 15 \u00b7 \ud83d\udd00 1.9K \u00b7 \ud83d\udce5 740K \u00b7 \ud83d\udce6 1.1K \u00b7 \ud83d\udccb 680 - 37% open \u00b7 \u23f1\ufe0f 08.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/OlafenwaMoses\/ImageAI\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/imageai) (\ud83d\udce5 20K \/ month \u00b7 \ud83d\udce6 16 \u00b7 \u23f1\ufe0f 05.01.2021):\n\t```\n\tpip install imageai\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/imageai) (\ud83d\udce5 2.6K \u00b7 \u23f1\ufe0f 30.04.2021):\n\t```\n\tconda install -c conda-forge imageai\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/serengil\/deepface\">deepface<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 3.7K) - A Lightweight Face Recognition and Facial Attribute Analysis (Age,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/serengil\/deepface) (\ud83d\udc68\u200d\ud83d\udcbb 23 \u00b7 \ud83d\udd00 850 \u00b7 \ud83d\udce6 530 \u00b7 \ud83d\udccb 440 - 1% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/serengil\/deepface\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deepface) (\ud83d\udce5 29K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 11.03.2022):\n\t```\n\tpip install deepface\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/JohannesBuchner\/imagehash\">ImageHash<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 2.4K \u00b7 \ud83d\udca4) - A Python Perceptual Image Hashing Module. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/JohannesBuchner\/imagehash) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 290 \u00b7 \ud83d\udce6 4.9K \u00b7 \ud83d\udccb 110 - 12% open \u00b7 \u23f1\ufe0f 07.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/JohannesBuchner\/imagehash\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ImageHash) (\ud83d\udce5 1.3M \/ month \u00b7 \ud83d\udce6 320 \u00b7 \u23f1\ufe0f 15.07.2021):\n\t```\n\tpip install ImageHash\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/imagehash) (\ud83d\udce5 170K \u00b7 \u23f1\ufe0f 15.07.2021):\n\t```\n\tconda install -c conda-forge imagehash\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lucidrains\/vit-pytorch\">vit-pytorch<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 9.8K) - Implementation of Vision Transformer, a simple way to achieve.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lucidrains\/vit-pytorch) (\ud83d\udc68\u200d\ud83d\udcbb 15 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 180 - 47% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lucidrains\/vit-pytorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vit-pytorch) (\ud83d\udce5 21K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install vit-pytorch\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mdbloice\/Augmentor\">Augmentor<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 4.7K) - Image augmentation library in Python for machine learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mdbloice\/Augmentor) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 850 \u00b7 \ud83d\udce6 450 \u00b7 \ud83d\udccb 200 - 64% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mdbloice\/Augmentor\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/Augmentor) (\ud83d\udce5 16K \/ month \u00b7 \ud83d\udce6 29 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install Augmentor\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/1adrianb\/face-alignment\">Face Alignment<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 5.7K \u00b7 \ud83d\udca4) - 2D and 3D Face alignment library build using pytorch. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/1adrianb\/face-alignment) (\ud83d\udc68\u200d\ud83d\udcbb 23 \u00b7 \ud83d\udd00 1.2K \u00b7 \ud83d\udccb 270 - 19% open \u00b7 \u23f1\ufe0f 04.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/1adrianb\/face-alignment\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/face-alignment) (\ud83d\udce5 8.4K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 14.09.2021):\n\t```\n\tpip install face-alignment\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/abhiTronix\/vidgear\">vidgear<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 2.2K) - A High-performance cross-platform Video Processing Python framework.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/abhiTronix\/vidgear) (\ud83d\udc68\u200d\ud83d\udcbb 9 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce5 570 \u00b7 \ud83d\udce6 190 \u00b7 \ud83d\udccb 210 - 0% open \u00b7 \u23f1\ufe0f 11.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/abhiTronix\/vidgear\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vidgear) (\ud83d\udce5 3.5K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 11.02.2022):\n\t```\n\tpip install vidgear\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/obss\/sahi\">sahi<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 1.6K) - A lightweight vision library for performing large scale object detection\/.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/obss\/sahi) (\ud83d\udc68\u200d\ud83d\udcbb 8 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udce5 1 \u00b7 \ud83d\udce6 47 \u00b7 \ud83d\udccb 150 - 13% open \u00b7 \u23f1\ufe0f 14.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/obss\/sahi\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sahi) (\ud83d\udce5 18K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 09.04.2022):\n\t```\n\tpip install sahi\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sahi) (\ud83d\udce5 2.2K \u00b7 \u23f1\ufe0f 09.04.2022):\n\t```\n\tconda install -c conda-forge sahi\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Layout-Parser\/layout-parser\">layout-parser<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 3K) - A Unified Toolkit for Deep Learning Based Document Image.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Layout-Parser\/layout-parser) (\ud83d\udc68\u200d\ud83d\udcbb 8 \u00b7 \ud83d\udd00 280 \u00b7 \ud83d\udce6 64 \u00b7 \ud83d\udccb 91 - 47% open \u00b7 \u23f1\ufe0f 05.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Layout-Parser\/layout-parser\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/layoutparser) (\ud83d\udce5 5.5K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tpip install layoutparser\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ipazc\/mtcnn\">mtcnn<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.8K \u00b7 \ud83d\udca4) - MTCNN face detection implementation for TensorFlow, as a PIP.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ipazc\/mtcnn) (\ud83d\udc68\u200d\ud83d\udcbb 15 \u00b7 \ud83d\udd00 450 \u00b7 \ud83d\udce6 2.2K \u00b7 \ud83d\udccb 100 - 62% open \u00b7 \u23f1\ufe0f 09.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/ipazc\/mtcnn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mtcnn) (\ud83d\udce5 26K \/ month \u00b7 \ud83d\udce6 43 \u00b7 \u23f1\ufe0f 09.07.2021):\n\t```\n\tpip install mtcnn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mtcnn) (\ud83d\udce5 4.6K \u00b7 \u23f1\ufe0f 17.08.2021):\n\t```\n\tconda install -c conda-forge mtcnn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lightly-ai\/lightly\">lightly<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.6K) - A python library for self-supervised learning on images. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lightly-ai\/lightly) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udce6 36 \u00b7 \ud83d\udccb 320 - 19% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lightly-ai\/lightly\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lightly) (\ud83d\udce5 2.5K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install lightly\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/luispedro\/mahotas\">mahotas<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 740) - Computer Vision in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/luispedro\/mahotas) (\ud83d\udc68\u200d\ud83d\udcbb 32 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 810 \u00b7 \ud83d\udccb 77 - 20% open \u00b7 \u23f1\ufe0f 07.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/luispedro\/mahotas\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mahotas) (\ud83d\udce5 11K \/ month \u00b7 \ud83d\udce6 110 \u00b7 \u23f1\ufe0f 14.10.2021):\n\t```\n\tpip install mahotas\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mahotas) (\ud83d\udce5 320K \u00b7 \u23f1\ufe0f 17.11.2021):\n\t```\n\tconda install -c conda-forge mahotas\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/CellProfiler\/CellProfiler\">CellProfiler<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 670) - An open-source application for biological image analysis. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/CellProfiler\/CellProfiler) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 310 \u00b7 \ud83d\udce5 2.6K \u00b7 \ud83d\udce6 8 \u00b7 \ud83d\udccb 3.1K - 6% open \u00b7 \u23f1\ufe0f 15.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/CellProfiler\/CellProfiler\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cellprofiler) (\ud83d\udce5 590 \/ month \u00b7 \u23f1\ufe0f 04.09.2017):\n\t```\n\tpip install cellprofiler\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/libvips\/pyvips\">pyvips<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 400) - python binding for libvips using cffi. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/libvips\/pyvips) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 39 \u00b7 \ud83d\udce6 320 \u00b7 \ud83d\udccb 280 - 38% open \u00b7 \u23f1\ufe0f 18.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/libvips\/pyvips\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyvips) (\ud83d\udce5 19K \/ month \u00b7 \ud83d\udce6 39 \u00b7 \u23f1\ufe0f 18.04.2022):\n\t```\n\tpip install pyvips\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyvips) (\ud83d\udce5 17K \u00b7 \u23f1\ufe0f 18.04.2022):\n\t```\n\tconda install -c conda-forge pyvips\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/mmf\">MMF<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 4.9K) - A modular framework for vision & language multimodal research from.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/mmf) (\ud83d\udc68\u200d\ud83d\udcbb 99 \u00b7 \ud83d\udd00 820 \u00b7 \ud83d\udce6 12 \u00b7 \ud83d\udccb 610 - 30% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/mmf\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mmf) (\ud83d\udce5 570 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 12.06.2020):\n\t```\n\tpip install mmf\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/timesler\/facenet-pytorch\">facenet-pytorch<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 2.8K) - Pretrained Pytorch face detection (MTCNN) and facial.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/timesler\/facenet-pytorch) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 630 \u00b7 \ud83d\udce5 280K \u00b7 \ud83d\udce6 750 \u00b7 \ud83d\udccb 150 - 40% open \u00b7 \u23f1\ufe0f 13.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/timesler\/facenet-pytorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/facenet-pytorch) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 10.03.2021):\n\t```\n\tpip install facenet-pytorch\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/pytorchvideo\">pytorchvideo<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 2.4K) - A deep learning library for video understanding research. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/pytorchvideo) (\ud83d\udc68\u200d\ud83d\udcbb 31 \u00b7 \ud83d\udd00 250 \u00b7 \ud83d\udccb 130 - 41% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/pytorchvideo\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorchvideo) (\ud83d\udce5 16K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 20.01.2022):\n\t```\n\tpip install pytorchvideo\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/airctic\/icevision\">icevision<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 680) - An Agnostic Computer Vision Framework - Pluggable to any Training.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/airctic\/icevision) (\ud83d\udc68\u200d\ud83d\udcbb 39 \u00b7 \ud83d\udd00 96 \u00b7 \ud83d\udccb 640 - 21% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/airctic\/icevision\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/icevision) (\ud83d\udce5 3.2K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 10.02.2022):\n\t```\n\tpip install icevision\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/graphics\">tensorflow-graphics<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 2.6K) - TensorFlow Graphics: Differentiable Graphics Layers.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/graphics) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 340 \u00b7 \ud83d\udccb 220 - 59% open \u00b7 \u23f1\ufe0f 04.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/graphics\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-graphics) (\ud83d\udce5 3K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 03.12.2021):\n\t```\n\tpip install tensorflow-graphics\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lucidrains\/deep-daze\">deep-daze<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 4.2K) - Simple command line tool for text to image generation using OpenAIs.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lucidrains\/deep-daze) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udce6 38 \u00b7 \ud83d\udccb 160 - 54% open \u00b7 \u23f1\ufe0f 13.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lucidrains\/deep-daze\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deep-daze) (\ud83d\udce5 2K \/ month \u00b7 \u23f1\ufe0f 13.03.2022):\n\t```\n\tpip install deep-daze\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/idealo\/image-super-resolution\">Image Super-Resolution<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 3.6K \u00b7 \ud83d\udca4) - Super-scale your images and run experiments with.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/idealo\/image-super-resolution) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 620 \u00b7 \ud83d\udce6 82 \u00b7 \ud83d\udccb 200 - 44% open \u00b7 \u23f1\ufe0f 02.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/idealo\/image-super-resolution\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ISR) (\ud83d\udce5 4.5K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 08.01.2020):\n\t```\n\tpip install ISR\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/idealo\/image-super-resolution-gpu) (\ud83d\udce5 210 \u00b7 \u23f1\ufe0f 01.04.2019):\n\t```\n\tdocker pull idealo\/image-super-resolution-gpu\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/vissl\">vissl<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 2.6K \u00b7 \ud83d\udcc9) - VISSL is FAIRs library of extensible, modular and scalable.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/vissl) (\ud83d\udc68\u200d\ud83d\udcbb 31 \u00b7 \ud83d\udd00 260 \u00b7 \ud83d\udce6 7 \u00b7 \ud83d\udccb 140 - 31% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/vissl\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vissl) (\ud83d\udce5 390 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 02.11.2021):\n\t```\n\tpip install vissl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/ClassyVision\">Classy Vision<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.4K) - An end-to-end PyTorch framework for image and video.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/ClassyVision) (\ud83d\udc68\u200d\ud83d\udcbb 75 \u00b7 \ud83d\udd00 250 \u00b7 \ud83d\udccb 110 - 46% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/ClassyVision\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/classy_vision) (\ud83d\udce5 2.2K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 09.07.2021):\n\t```\n\tpip install classy_vision\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/classy_vision) (\ud83d\udce5 12K \u00b7 \u23f1\ufe0f 22.03.2022):\n\t```\n\tconda install -c conda-forge classy_vision\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tryolabs\/norfair\">Norfair<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 1.4K) - Lightweight Python library for adding real-time object tracking to any.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tryolabs\/norfair) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udce5 68 \u00b7 \ud83d\udccb 57 - 31% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tryolabs\/norfair\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/norfair) (\ud83d\udce5 3.4K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 19.04.2022):\n\t```\n\tpip install norfair\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ProvenanceLabs\/image-match\">image-match<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 2.7K \u00b7 \ud83d\udca4) - Quickly search over billions of images. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ProvenanceLabs\/image-match) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 380 \u00b7 \ud83d\udccb 100 - 53% open \u00b7 \u23f1\ufe0f 21.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/ProvenanceLabs\/image-match\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/image_match) (\ud83d\udce5 550 \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 13.02.2017):\n\t```\n\tpip install image_match\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/detr\">DE\u2af6TR<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 8.8K) - End-to-End Object Detection with Transformers. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/detr) (\ud83d\udc68\u200d\ud83d\udcbb 25 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udccb 430 - 36% open \u00b7 \u23f1\ufe0f 07.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/detr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/pycls\">pycls<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 1.9K) - Codebase for Image Classification Research, written in PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/pycls) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 5 \u00b7 \ud83d\udccb 81 - 30% open \u00b7 \u23f1\ufe0f 14.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/pycls\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pycls) (\ud83d\udce5 430 \/ month \u00b7 \u23f1\ufe0f 05.09.2020):\n\t```\n\tpip install pycls\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/SlowFast\">PySlowFast<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 4.8K) - PySlowFast: video understanding codebase from FAIR for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/SlowFast) (\ud83d\udc68\u200d\ud83d\udcbb 26 \u00b7 \ud83d\udd00 920 \u00b7 \ud83d\udce6 7 \u00b7 \ud83d\udccb 500 - 50% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/SlowFast\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyslowfast) (\ud83d\udce5 14 \/ month \u00b7 \u23f1\ufe0f 15.01.2020):\n\t```\n\tpip install pyslowfast\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google-research\/scenic\">scenic<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 890) - Scenic: A Jax Library for Computer Vision Research and Beyond. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google-research\/scenic) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 16 \u00b7 \ud83d\udccb 37 - 45% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google-research\/scenic\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jasmcaus\/caer\">Caer<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 610 \u00b7 \ud83d\udca4) - A lightweight Computer Vision library. Scale your models, not boilerplate. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jasmcaus\/caer) (\ud83d\udc68\u200d\ud83d\udcbb 8 \u00b7 \ud83d\udd00 100 \u00b7 \ud83d\udce5 19 \u00b7 \ud83d\udccb 15 - 13% open \u00b7 \u23f1\ufe0f 13.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/jasmcaus\/caer\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/caer) (\ud83d\udce5 3.7K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 13.10.2021):\n\t```\n\tpip install caer\n\t```\n<\/details>\n<details><summary>Show 12 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/glfw\/glfw\">glfw<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 9K) - A multi-platform library for OpenGL, OpenGL ES, Vulkan, window and input. <code><a href=\"https:\/\/tldrlegal.com\/search?q=Zlib\">\u2757\ufe0fZlib<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/aleju\/imgaug\">imgaug<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 13K \u00b7 \ud83d\udc80) - Image augmentation for machine learning experiments. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/facebookresearch\/pytorch3d\">PyTorch3D<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 5.9K) - PyTorch3D is FAIRs library of reusable components for.. <code>\u2757Unlicensed<\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/uploadcare\/pillow-simd\">Pillow-SIMD<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 1.8K) - The friendly PIL fork. <code><a href=\"https:\/\/tldrlegal.com\/search?q=PIL\">\u2757\ufe0fPIL<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/chainer\/chainercv\">chainercv<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 1.5K \u00b7 \ud83d\udc80) - ChainerCV: a Library for Deep Learning in Computer Vision. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/qubvel\/segmentation_models\">segmentation_models<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 3.8K \u00b7 \ud83d\udc80) - Segmentation models with pretrained backbones. Keras.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/idealo\/imagededup\">Image Deduplicator<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 4K \u00b7 \ud83d\udc80) - Finding duplicate images made easy!. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/tryolabs\/luminoth\">Luminoth<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 2.4K \u00b7 \ud83d\udc80) - Deep Learning toolkit for Computer Vision. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/hhatto\/nude.py\">nude.py<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 850 \u00b7 \ud83d\udc80) - Nudity detection with Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Oulu-IMEDS\/solt\">solt<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 250 \u00b7 \ud83d\udc80) - Streaming over lightweight data transformations. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/qanastek\/HugsVision\">HugsVision<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 160) - HugsVision is a easy to use huggingface wrapper for state-of-the-.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code>huggingface<\/code>\n- <b><a href=\"https:\/\/github.com\/nicolas-chaulet\/torch-points3d\">Torch Points 3D<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 51 \u00b7 \ud83d\udc23) - Pytorch framework for doing deep learning on point.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Graph Data\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for graph processing, clustering, embedding, and machine learning tasks._\n\n<details><summary><b><a href=\"https:\/\/github.com\/networkx\/networkx\">networkx<\/a><\/b> (\ud83e\udd4743 \u00b7  \u2b50 11K) - Network Analysis in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/networkx\/networkx) (\ud83d\udc68\u200d\ud83d\udcbb 590 \u00b7 \ud83d\udd00 2.6K \u00b7 \ud83d\udce5 59 \u00b7 \ud83d\udce6 110K \u00b7 \ud83d\udccb 2.9K - 12% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/networkx\/networkx\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/networkx) (\ud83d\udce5 18M \/ month \u00b7 \ud83d\udce6 13K \u00b7 \u23f1\ufe0f 09.04.2022):\n\t```\n\tpip install networkx\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/networkx) (\ud83d\udce5 6.2M \u00b7 \u23f1\ufe0f 10.04.2022):\n\t```\n\tconda install -c conda-forge networkx\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pyg-team\/pytorch_geometric\">PyTorch Geometric<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 15K) - Graph Neural Network Library for PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pyg-team\/pytorch_geometric) (\ud83d\udc68\u200d\ud83d\udcbb 260 \u00b7 \ud83d\udd00 2.6K \u00b7 \ud83d\udccb 2.5K - 37% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pyg-team\/pytorch_geometric\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torch-geometric) (\ud83d\udce5 74K \/ month \u00b7 \ud83d\udce6 39 \u00b7 \u23f1\ufe0f 12.03.2022):\n\t```\n\tpip install torch-geometric\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pytorch_geometric) (\ud83d\udce5 6.1K \u00b7 \u23f1\ufe0f 19.01.2022):\n\t```\n\tconda install -c conda-forge pytorch_geometric\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dmlc\/dgl\">dgl<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 9.5K) - Python package built to ease deep learning on graph, on top of existing.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dmlc\/dgl) (\ud83d\udc68\u200d\ud83d\udcbb 200 \u00b7 \ud83d\udd00 2.2K \u00b7 \ud83d\udce6 3 \u00b7 \ud83d\udccb 1.5K - 16% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dmlc\/dgl\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dgl) (\ud83d\udce5 29K \/ month \u00b7 \ud83d\udce6 42 \u00b7 \u23f1\ufe0f 16.03.2022):\n\t```\n\tpip install dgl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/stellargraph\/stellargraph\">StellarGraph<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.4K \u00b7 \ud83d\udca4) - StellarGraph - Machine Learning on Graphs. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/stellargraph\/stellargraph) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 360 \u00b7 \ud83d\udce6 130 \u00b7 \ud83d\udccb 1K - 27% open \u00b7 \u23f1\ufe0f 29.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/stellargraph\/stellargraph\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/stellargraph) (\ud83d\udce5 14K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 30.06.2020):\n\t```\n\tpip install stellargraph\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/snap-stanford\/ogb\">ogb<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 1.3K) - Benchmark datasets, data loaders, and evaluators for graph machine learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/snap-stanford\/ogb) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 280 \u00b7 \ud83d\udce6 290 \u00b7 \ud83d\udccb 210 - 1% open \u00b7 \u23f1\ufe0f 20.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/snap-stanford\/ogb\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ogb) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 13 \u00b7 \u23f1\ufe0f 23.02.2022):\n\t```\n\tpip install ogb\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ogb) (\ud83d\udce5 7.5K \u00b7 \u23f1\ufe0f 23.02.2022):\n\t```\n\tconda install -c conda-forge ogb\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PaddlePaddle\/PGL\">Paddle Graph Learning<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 1.3K) - Paddle Graph Learning (PGL) is an efficient and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PaddlePaddle\/PGL) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 26 \u00b7 \ud83d\udccb 120 - 39% open \u00b7 \u23f1\ufe0f 21.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PaddlePaddle\/PGL\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pgl) (\ud83d\udce5 8.6K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install pgl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/danielegrattarola\/spektral\">Spektral<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 2.1K) - Graph Neural Networks with Keras and Tensorflow 2. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/danielegrattarola\/spektral) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 280 \u00b7 \ud83d\udce6 120 \u00b7 \ud83d\udccb 220 - 19% open \u00b7 \u23f1\ufe0f 09.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/danielegrattarola\/spektral\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/spektral) (\ud83d\udce5 5.4K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 09.04.2022):\n\t```\n\tpip install spektral\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/graphistry\/pygraphistry\">pygraphistry<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 1.6K) - PyGraphistry is a Python library to quickly load, shape,.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/graphistry\/pygraphistry) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udce6 69 \u00b7 \ud83d\udccb 210 - 42% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/graphistry\/pygraphistry\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/graphistry) (\ud83d\udce5 2.6K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tpip install graphistry\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/PyTorch-BigGraph\">PyTorch-BigGraph<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 3.1K) - Generate embeddings from large-scale graph-structured.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/PyTorch-BigGraph) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 410 \u00b7 \ud83d\udce5 130 \u00b7 \ud83d\udccb 190 - 28% open \u00b7 \u23f1\ufe0f 11.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/PyTorch-BigGraph\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchbiggraph) (\ud83d\udce5 180K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 01.05.2019):\n\t```\n\tpip install torchbiggraph\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/benedekrozemberczki\/pytorch_geometric_temporal\">pytorch_geometric_temporal<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 1.5K) - PyTorch Geometric Temporal: Spatiotemporal Signal.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/benedekrozemberczki\/pytorch_geometric_temporal) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udccb 100 - 7% open \u00b7 \u23f1\ufe0f 24.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/benedekrozemberczki\/pytorch_geometric_temporal\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torch-geometric-temporal) (\ud83d\udce5 1.8K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 04.04.2022):\n\t```\n\tpip install torch-geometric-temporal\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pykeen\/pykeen\">PyKEEN<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 830) - A Python library for learning and evaluating knowledge graph embeddings. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pykeen\/pykeen) (\ud83d\udc68\u200d\ud83d\udcbb 26 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce5 140 \u00b7 \ud83d\udccb 400 - 31% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pykeen\/pykeen\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pykeen) (\ud83d\udce5 1K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 20.04.2022):\n\t```\n\tpip install pykeen\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Accenture\/AmpliGraph\">AmpliGraph<\/a><\/b> (\ud83e\udd4823 \u00b7  \u2b50 1.7K \u00b7 \ud83d\udca4) - Python library for Representation Learning on Knowledge.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Accenture\/AmpliGraph) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce6 20 \u00b7 \ud83d\udccb 200 - 10% open \u00b7 \u23f1\ufe0f 25.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/Accenture\/AmpliGraph\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ampligraph) (\ud83d\udce5 950 \/ month \u00b7 \u23f1\ufe0f 25.05.2021):\n\t```\n\tpip install ampligraph\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/graph4ai\/graph4nlp\">graph4nlp<\/a><\/b> (\ud83e\udd4823 \u00b7  \u2b50 1.3K) - Graph4nlp is the library for the easy use of Graph Neural.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/graph4ai\/graph4nlp) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udccb 150 - 10% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/graph4ai\/graph4nlp\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/graph4nlp) (\ud83d\udce5 75 \/ month \u00b7 \u23f1\ufe0f 20.01.2022):\n\t```\n\tpip install graph4nlp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/eliorc\/node2vec\">Node2Vec<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 900) - Implementation of the node2vec algorithm. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/eliorc\/node2vec) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 210 \u00b7 \u23f1\ufe0f 30.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/eliorc\/node2vec\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/node2vec) (\ud83d\udce5 58K \/ month \u00b7 \ud83d\udce6 15 \u00b7 \u23f1\ufe0f 30.04.2022):\n\t```\n\tpip install node2vec\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/node2vec) (\ud83d\udce5 20K \u00b7 \u23f1\ufe0f 25.04.2020):\n\t```\n\tconda install -c conda-forge node2vec\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rusty1s\/pytorch_cluster\">torch-cluster<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 510) - PyTorch Extension Library of Optimized Graph Cluster.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rusty1s\/pytorch_cluster) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 95 \u00b7 \ud83d\udccb 100 - 14% open \u00b7 \u23f1\ufe0f 20.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rusty1s\/pytorch_cluster\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torch-cluster) (\ud83d\udce5 13K \/ month \u00b7 \ud83d\udce6 27 \u00b7 \u23f1\ufe0f 11.03.2022):\n\t```\n\tpip install torch-cluster\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pytorch_cluster) (\ud83d\udce5 29K \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tconda install -c conda-forge pytorch_cluster\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/THUMNLab\/AutoGL\">AutoGL<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 790) - An autoML framework & toolkit for machine learning on graphs. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/THUMNLab\/AutoGL) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 87 \u00b7 \ud83d\udccb 20 - 30% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/THUMNLab\/AutoGL\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/auto-graph-learning) (\ud83d\udce5 26 \/ month \u00b7 \u23f1\ufe0f 23.12.2020):\n\t```\n\tpip install auto-graph-learning\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/snap-stanford\/deepsnap\">deepsnap<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 410 \u00b7 \ud83d\udca4) - Python library assists deep learning on graphs. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/snap-stanford\/deepsnap) (\ud83d\udc68\u200d\ud83d\udcbb 15 \u00b7 \ud83d\udd00 45 \u00b7 \ud83d\udce5 8 \u00b7 \ud83d\udce6 20 \u00b7 \ud83d\udccb 39 - 38% open \u00b7 \u23f1\ufe0f 19.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/snap-stanford\/deepsnap\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deepsnap) (\ud83d\udce5 380 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 05.09.2021):\n\t```\n\tpip install deepsnap\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepmind\/jraph\">jraph<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 880) - A Graph Neural Network Library in Jax. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepmind\/jraph) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 57 \u00b7 \ud83d\udce6 16 \u00b7 \ud83d\udccb 20 - 55% open \u00b7 \u23f1\ufe0f 17.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepmind\/jraph\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/jraph) (\ud83d\udce5 1.6K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 19.11.2021):\n\t```\n\tpip install jraph\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/jraph) (\ud83d\udce5 360 \u00b7 \u23f1\ufe0f 31.10.2021):\n\t```\n\tconda install -c conda-forge jraph\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/snap-stanford\/GraphGym\">GraphGym<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 1K) - Platform for designing and evaluating Graph Neural Networks (GNN). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/snap-stanford\/GraphGym) (\ud83d\udc68\u200d\ud83d\udcbb 5 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce5 13 \u00b7 \ud83d\udce6 2 \u00b7 \ud83d\udccb 26 - 15% open \u00b7 \u23f1\ufe0f 25.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/snap-stanford\/GraphGym\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/graphgym) (\ud83d\udce5 39 \/ month \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tpip install graphgym\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/vaticle\/kglib\">kglib<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 500 \u00b7 \ud83d\udca4) - Grakn Knowledge Graph Library (ML R&D). <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/vaticle\/kglib) (\ud83d\udc68\u200d\ud83d\udcbb 7 \u00b7 \ud83d\udd00 89 \u00b7 \ud83d\udce5 210 \u00b7 \ud83d\udccb 62 - 19% open \u00b7 \u23f1\ufe0f 22.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/vaticle\/kglib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/grakn-kglib) (\ud83d\udce5 51 \/ month \u00b7 \u23f1\ufe0f 19.08.2020):\n\t```\n\tpip install grakn-kglib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/ptgnn\">ptgnn<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 320) - A PyTorch Graph Neural Network Library. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/ptgnn) (\ud83d\udc68\u200d\ud83d\udcbb 7 \u00b7 \ud83d\udd00 38 \u00b7 \ud83d\udce6 1 \u00b7 \ud83d\udccb 7 - 28% open \u00b7 \u23f1\ufe0f 01.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/ptgnn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ptgnn) (\ud83d\udce5 80 \/ month \u00b7 \u23f1\ufe0f 21.10.2021):\n\t```\n\tpip install ptgnn\n\t```\n<\/details>\n<details><summary>Show 15 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/igraph\/python-igraph\">igraph<\/a><\/b> (\ud83e\udd4731 \u00b7  \u2b50 960) - Python interface for igraph. <code><a href=\"http:\/\/bit.ly\/2KucAZR\">\u2757\ufe0fGPL-2.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Kozea\/pygal\">pygal<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.5K) - PYthon svg GrAph plotting Library. <code><a href=\"http:\/\/bit.ly\/37RvQcA\">\u2757\ufe0fLGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/benedekrozemberczki\/karateclub\">Karate Club<\/a><\/b> (\ud83e\udd4823 \u00b7  \u2b50 1.6K) - Karate Club: An API Oriented Open-source Python Framework for.. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/phanein\/deepwalk\">DeepWalk<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 2.4K \u00b7 \ud83d\udc80) - DeepWalk - Deep Learning for Graphs. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/divelab\/DIG\">DIG<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1.1K) - A library for graph deep learning research. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/deepmind\/graph_nets\">graph-nets<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 5.1K \u00b7 \ud83d\udc80) - Build Graph Nets in Tensorflow. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/deepgraph\/deepgraph\">DeepGraph<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 250 \u00b7 \ud83d\udca4) - Analyze Data with Pandas-based Networks. Documentation:. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/IBCNServices\/pyRDF2Vec\">pyRDF2Vec<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 150) - Python Implementation and Extension of RDF2Vec. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/shenweichen\/GraphEmbedding\">GraphEmbedding<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 2.7K \u00b7 \ud83d\udc80) - Implementation and experiments of graph embedding.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/gsi-upm\/sematch\">Sematch<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 380 \u00b7 \ud83d\udc80) - semantic similarity framework for knowledge graph. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/thunlp\/OpenKE\">OpenKE<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 3K \u00b7 \ud83d\udc80) - An Open-Source Package for Knowledge Embedding (KE). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/alibaba\/euler\">Euler<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 2.8K \u00b7 \ud83d\udc80) - A distributed graph deep learning framework. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/williamleif\/GraphSAGE\">GraphSAGE<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 2.7K \u00b7 \ud83d\udc80) - Representation learning on large graphs using stochastic.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/thunlp\/OpenNE\">OpenNE<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 1.6K \u00b7 \ud83d\udc80) - An Open-Source Package for Network Embedding (NE). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/DeepGraphLearning\/graphvite\">GraphVite<\/a><\/b> (\ud83e\udd4912 \u00b7  \u2b50 1K \u00b7 \ud83d\udc80) - GraphVite: A General and High-performance Graph Embedding System. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n<\/details>\n<br>\n\n## Audio Data\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for audio analysis, manipulation, transformation, and extraction, as well as speech recognition and music generation tasks._\n\n<details><summary><b><a href=\"https:\/\/github.com\/espnet\/espnet\">espnet<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 5K) - End-to-End Speech Processing Toolkit. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/espnet\/espnet) (\ud83d\udc68\u200d\ud83d\udcbb 270 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udce5 76 \u00b7 \ud83d\udce6 48 \u00b7 \ud83d\udccb 1.8K - 17% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/espnet\/espnet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/espnet) (\ud83d\udce5 7.4K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 12.04.2022):\n\t```\n\tpip install espnet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mozilla\/DeepSpeech\">DeepSpeech<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 19K) - DeepSpeech is an open source embedded (offline, on-device).. <code><a href=\"http:\/\/bit.ly\/3postzC\">MPL-2.0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mozilla\/DeepSpeech) (\ud83d\udc68\u200d\ud83d\udcbb 160 \u00b7 \ud83d\udd00 3.5K \u00b7 \ud83d\udce5 830K \u00b7 \ud83d\udce6 730 \u00b7 \ud83d\udccb 2.1K - 5% open \u00b7 \u23f1\ufe0f 17.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/mozilla\/DeepSpeech\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deepspeech) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 37 \u00b7 \u23f1\ufe0f 19.12.2020):\n\t```\n\tpip install deepspeech\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/deepspeech) (\ud83d\udce5 460 \u00b7 \u23f1\ufe0f 29.07.2021):\n\t```\n\tconda install -c conda-forge deepspeech\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/librosa\/librosa\">librosa<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 5.2K) - Python library for audio and music analysis. <code><a href=\"http:\/\/bit.ly\/3hkKRql\">ISC<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/librosa\/librosa) (\ud83d\udc68\u200d\ud83d\udcbb 95 \u00b7 \ud83d\udd00 790 \u00b7 \ud83d\udccb 960 - 3% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/librosa\/librosa\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/librosa) (\ud83d\udce5 900K \/ month \u00b7 \ud83d\udce6 1.2K \u00b7 \u23f1\ufe0f 15.02.2022):\n\t```\n\tpip install librosa\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/librosa) (\ud83d\udce5 450K \u00b7 \u23f1\ufe0f 15.02.2022):\n\t```\n\tconda install -c conda-forge librosa\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/magenta\/magenta\">Magenta<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 18K) - Magenta: Music and Art Generation with Machine Intelligence. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/magenta\/magenta) (\ud83d\udc68\u200d\ud83d\udcbb 150 \u00b7 \ud83d\udd00 3.5K \u00b7 \ud83d\udce6 360 \u00b7 \ud83d\udccb 900 - 35% open \u00b7 \u23f1\ufe0f 16.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/magenta\/magenta\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/magenta) (\ud83d\udce5 4.9K \/ month \u00b7 \ud83d\udce6 36 \u00b7 \u23f1\ufe0f 12.11.2020):\n\t```\n\tpip install magenta\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/audio\">torchaudio<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 1.7K) - Data manipulation and transformation for audio signal.. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/audio) (\ud83d\udc68\u200d\ud83d\udcbb 150 \u00b7 \ud83d\udd00 400 \u00b7 \ud83d\udccb 620 - 25% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/audio\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchaudio) (\ud83d\udce5 370K \/ month \u00b7 \ud83d\udce6 120 \u00b7 \u23f1\ufe0f 10.03.2022):\n\t```\n\tpip install torchaudio\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jiaaro\/pydub\">Pydub<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 6.1K \u00b7 \ud83d\udca4) - Manipulate audio with a simple and easy high level interface. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jiaaro\/pydub) (\ud83d\udc68\u200d\ud83d\udcbb 91 \u00b7 \ud83d\udd00 800 \u00b7 \ud83d\udce6 12K \u00b7 \ud83d\udccb 470 - 45% open \u00b7 \u23f1\ufe0f 08.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/jiaaro\/pydub\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pydub) (\ud83d\udce5 1.9M \/ month \u00b7 \ud83d\udce6 900 \u00b7 \u23f1\ufe0f 10.03.2021):\n\t```\n\tpip install pydub\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pydub) (\ud83d\udce5 23K \u00b7 \u23f1\ufe0f 13.03.2021):\n\t```\n\tconda install -c conda-forge pydub\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/speechbrain\/speechbrain\">speechbrain<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 4K) - A PyTorch-based Speech Toolkit. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/speechbrain\/speechbrain) (\ud83d\udc68\u200d\ud83d\udcbb 150 \u00b7 \ud83d\udd00 750 \u00b7 \ud83d\udce6 170 \u00b7 \ud83d\udccb 650 - 20% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/speechbrain\/speechbrain\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/speechbrain) (\ud83d\udce5 8.2K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 20.12.2021):\n\t```\n\tpip install speechbrain\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Uberi\/speech_recognition\">SpeechRecognition<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 6.2K) - Speech recognition module for Python, supporting several.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Uberi\/speech_recognition) (\ud83d\udc68\u200d\ud83d\udcbb 43 \u00b7 \ud83d\udd00 2K \u00b7 \ud83d\udccb 520 - 45% open \u00b7 \u23f1\ufe0f 27.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Uberi\/speech_recognition\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/SpeechRecognition) (\ud83d\udce5 260K \/ month \u00b7 \ud83d\udce6 700 \u00b7 \u23f1\ufe0f 05.12.2017):\n\t```\n\tpip install SpeechRecognition\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/speechrecognition) (\ud83d\udce5 130K \u00b7 \u23f1\ufe0f 13.12.2021):\n\t```\n\tconda install -c conda-forge speechrecognition\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tyiannak\/pyAudioAnalysis\">pyAudioAnalysis<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 4.7K) - Python Audio Analysis Library: Feature Extraction,.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tyiannak\/pyAudioAnalysis) (\ud83d\udc68\u200d\ud83d\udcbb 26 \u00b7 \ud83d\udd00 1.1K \u00b7 \ud83d\udce6 260 \u00b7 \ud83d\udccb 290 - 60% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tyiannak\/pyAudioAnalysis\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyAudioAnalysis) (\ud83d\udce5 16K \/ month \u00b7 \ud83d\udce6 19 \u00b7 \u23f1\ufe0f 07.02.2022):\n\t```\n\tpip install pyAudioAnalysis\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/coqui-ai\/TTS\">Coqui TTS<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 4.7K) - - a deep learning toolkit for Text-to-Speech, battle-.. <code><a href=\"http:\/\/bit.ly\/3postzC\">MPL-2.0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/coqui-ai\/TTS) (\ud83d\udc68\u200d\ud83d\udcbb 85 \u00b7 \ud83d\udd00 470 \u00b7 \ud83d\udce5 130K \u00b7 \ud83d\udccb 320 - 8% open \u00b7 \u23f1\ufe0f 20.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/coqui-ai\/TTS\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tts) (\ud83d\udce5 5K \/ month \u00b7 \u23f1\ufe0f 14.07.2017):\n\t```\n\tpip install tts\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tts) (\ud83d\udce5 1.4K \u00b7 \u23f1\ufe0f 15.12.2021):\n\t```\n\tconda install -c conda-forge tts\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deezer\/spleeter\">spleeter<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 19K) - Deezer source separation library including pretrained models. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deezer\/spleeter) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 2.1K \u00b7 \ud83d\udce5 1.6M \u00b7 \ud83d\udccb 680 - 21% open \u00b7 \u23f1\ufe0f 01.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deezer\/spleeter\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/spleeter) (\ud83d\udce5 13K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 03.09.2021):\n\t```\n\tpip install spleeter\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/spleeter) (\ud83d\udce5 63K \u00b7 \u23f1\ufe0f 30.06.2020):\n\t```\n\tconda install -c conda-forge spleeter\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/devsnd\/tinytag\">tinytag<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 520) - Read audio and music meta data and duration of MP3, OGG, OPUS, MP4, M4A,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/devsnd\/tinytag) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 89 \u00b7 \ud83d\udce6 520 \u00b7 \ud83d\udccb 89 - 12% open \u00b7 \u23f1\ufe0f 15.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/devsnd\/tinytag\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tinytag) (\ud83d\udce5 93K \/ month \u00b7 \ud83d\udce6 70 \u00b7 \u23f1\ufe0f 12.03.2022):\n\t```\n\tpip install tinytag\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/beetbox\/audioread\">audioread<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 400) - cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/beetbox\/audioread) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 93 \u00b7 \ud83d\udce6 8.2K \u00b7 \ud83d\udccb 79 - 39% open \u00b7 \u23f1\ufe0f 03.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/beetbox\/audioread\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/audioread) (\ud83d\udce5 870K \/ month \u00b7 \ud83d\udce6 320 \u00b7 \u23f1\ufe0f 20.10.2020):\n\t```\n\tpip install audioread\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/audioread) (\ud83d\udce5 420K \u00b7 \u23f1\ufe0f 10.04.2022):\n\t```\n\tconda install -c conda-forge audioread\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Picovoice\/porcupine\">Porcupine<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 2.7K) - On-device wake word detection powered by deep learning. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Picovoice\/porcupine) (\ud83d\udc68\u200d\ud83d\udcbb 31 \u00b7 \ud83d\udd00 390 \u00b7 \ud83d\udce6 9 \u00b7 \ud83d\udccb 380 - 0% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Picovoice\/Porcupine\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pvporcupine) (\ud83d\udce5 1.1K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 11.03.2022):\n\t```\n\tpip install pvporcupine\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/magenta\/ddsp\">DDSP<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 2.1K) - DDSP: Differentiable Digital Signal Processing. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/magenta\/ddsp) (\ud83d\udc68\u200d\ud83d\udcbb 31 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udce6 24 \u00b7 \ud83d\udccb 130 - 16% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/magenta\/ddsp\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ddsp) (\ud83d\udce5 2.9K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install ddsp\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ddsp) (\ud83d\udce5 10K \u00b7 \u23f1\ufe0f 08.06.2020):\n\t```\n\tconda install -c conda-forge ddsp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/iver56\/audiomentations\">audiomentations<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 960) - A Python library for audio data augmentation. Inspired by.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/iver56\/audiomentations) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udce6 130 \u00b7 \ud83d\udccb 110 - 24% open \u00b7 \u23f1\ufe0f 05.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/iver56\/audiomentations\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/audiomentations) (\ud83d\udce5 4.2K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 18.03.2022):\n\t```\n\tpip install audiomentations\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/CPJKU\/madmom\">Madmom<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 890) - Python audio and music signal processing library. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/CPJKU\/madmom) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 190 \u00b7 \ud83d\udccb 260 - 21% open \u00b7 \u23f1\ufe0f 06.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/CPJKU\/madmom\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/madmom) (\ud83d\udce5 2.8K \/ month \u00b7 \ud83d\udce6 27 \u00b7 \u23f1\ufe0f 14.11.2018):\n\t```\n\tpip install madmom\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bastibe\/python-soundfile\">python-soundfile<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 450) - SoundFile is an audio library based on libsndfile, CFFI, and.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bastibe\/python-soundfile) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 68 \u00b7 \ud83d\udce5 3.1K \u00b7 \ud83d\udccb 170 - 39% open \u00b7 \u23f1\ufe0f 23.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bastibe\/python-soundfile\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/soundfile) (\ud83d\udce5 1M \/ month \u00b7 \ud83d\udce6 540 \u00b7 \u23f1\ufe0f 27.11.2019):\n\t```\n\tpip install soundfile\n\t```\n- [Conda](https:\/\/anaconda.org\/anaconda\/pysoundfile):\n\t```\n\tconda install -c anaconda pysoundfile\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/keunwoochoi\/kapre\">kapre<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 820) - kapre: Keras Audio Preprocessors. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/keunwoochoi\/kapre) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce5 22 \u00b7 \ud83d\udce6 1.5K \u00b7 \ud83d\udccb 94 - 12% open \u00b7 \u23f1\ufe0f 21.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/keunwoochoi\/kapre\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/kapre) (\ud83d\udce5 3.5K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 21.01.2022):\n\t```\n\tpip install kapre\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/KinWaiCheuk\/nnAudio\">nnAudio<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 680) - Audio processing by using pytorch 1D convolution network. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/KinWaiCheuk\/nnAudio) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 67 \u00b7 \ud83d\udce6 46 \u00b7 \ud83d\udccb 49 - 24% open \u00b7 \u23f1\ufe0f 24.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/KinWaiCheuk\/nnAudio\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nnAudio) (\ud83d\udce5 1.5K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 24.12.2021):\n\t```\n\tpip install nnAudio\n\t```\n<\/details>\n<details><summary>Show 8 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/aubio\/aubio\">aubio<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.7K) - a library for audio and music analysis. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/MTG\/essentia\">Essentia<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.1K) - C++ library for audio and music analysis, description and.. <code><a href=\"http:\/\/bit.ly\/3pwmjO5\">\u2757\ufe0fAGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/jameslyons\/python_speech_features\">python_speech_features<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 2.1K \u00b7 \ud83d\udc80) - This library provides common speech features for ASR.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/mozilla\/TTS\">TTS<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 5.9K \u00b7 \ud83d\udc80) - Deep learning for Text to Speech (Discussion forum:.. <code><a href=\"http:\/\/bit.ly\/3postzC\">MPL-2.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/worldveil\/dejavu\">Dejavu<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 5.7K \u00b7 \ud83d\udc80) - Audio fingerprinting and recognition in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Parisson\/TimeSide\">TimeSide<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 320) - Scalable audio processing framework written in Python with a.. <code><a href=\"http:\/\/bit.ly\/3pwmjO5\">\u2757\ufe0fAGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/bmcfee\/muda\">Muda<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 210 \u00b7 \ud83d\udca4) - A library for augmenting annotated audio data. <code><a href=\"http:\/\/bit.ly\/3hkKRql\">ISC<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/adefossez\/julius\">Julius<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 260) - Fast PyTorch based DSP for audio and 1D signals. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Geospatial Data\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries to load, process, analyze, and write geographic data as well as libraries for spatial analysis, map visualization, and geocoding._\n\n<details><summary><b><a href=\"https:\/\/github.com\/visgl\/deck.gl\">pydeck<\/a><\/b> (\ud83e\udd4742 \u00b7  \u2b50 9.8K) - WebGL2 powered visualization framework. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/visgl\/deck.gl) (\ud83d\udc68\u200d\ud83d\udcbb 190 \u00b7 \ud83d\udd00 1.8K \u00b7 \ud83d\udce6 4K \u00b7 \ud83d\udccb 2.4K - 6% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/visgl\/deck.gl\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pydeck) (\ud83d\udce5 990K \/ month \u00b7 \ud83d\udce6 23 \u00b7 \u23f1\ufe0f 25.10.2021):\n\t```\n\tpip install pydeck\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pydeck) (\ud83d\udce5 94K \u00b7 \u23f1\ufe0f 26.10.2021):\n\t```\n\tconda install -c conda-forge pydeck\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/deck.gl) (\ud83d\udce5 290K \/ month \u00b7 \ud83d\udce6 380 \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tnpm install deck.gl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/shapely\/shapely\">Shapely<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 2.8K \u00b7 \ud83d\udcc8) - Manipulation and analysis of geometric objects. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/shapely\/shapely) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 460 \u00b7 \ud83d\udce5 29 \u00b7 \ud83d\udce6 29K \u00b7 \ud83d\udccb 880 - 18% open \u00b7 \u23f1\ufe0f 30.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/shapely\/shapely\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/shapely) (\ud83d\udce5 7M \/ month \u00b7 \ud83d\udce6 3.8K \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install shapely\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/shapely) (\ud83d\udce5 3.6M \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tconda install -c conda-forge shapely\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rasterio\/rasterio\">Rasterio<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 1.7K) - Rasterio reads and writes geospatial raster datasets. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rasterio\/rasterio) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 460 \u00b7 \ud83d\udce5 760 \u00b7 \ud83d\udce6 4.8K \u00b7 \ud83d\udccb 1.5K - 9% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rasterio\/rasterio\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/rasterio) (\ud83d\udce5 730K \/ month \u00b7 \ud83d\udce6 760 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install rasterio\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/rasterio) (\ud83d\udce5 1.5M \u00b7 \u23f1\ufe0f 11.03.2022):\n\t```\n\tconda install -c conda-forge rasterio\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/geopandas\/geopandas\">GeoPandas<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 3.1K) - Python tools for geographic data. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/geopandas\/geopandas) (\ud83d\udc68\u200d\ud83d\udcbb 170 \u00b7 \ud83d\udd00 680 \u00b7 \ud83d\udce5 1.5K \u00b7 \ud83d\udce6 13K \u00b7 \ud83d\udccb 1.3K - 30% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/geopandas\/geopandas\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/geopandas) (\ud83d\udce5 2.3M \/ month \u00b7 \ud83d\udce6 1.1K \u00b7 \u23f1\ufe0f 16.10.2021):\n\t```\n\tpip install geopandas\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/geopandas) (\ud83d\udce5 1.5M \u00b7 \u23f1\ufe0f 01.12.2021):\n\t```\n\tconda install -c conda-forge geopandas\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pyproj4\/pyproj\">pyproj<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 750) - Python interface to PROJ (cartographic projections and coordinate.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pyproj4\/pyproj) (\ud83d\udc68\u200d\ud83d\udcbb 49 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce6 14K \u00b7 \ud83d\udccb 490 - 3% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pyproj4\/pyproj\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyproj) (\ud83d\udce5 4.7M \/ month \u00b7 \ud83d\udce6 1.7K \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tpip install pyproj\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyproj) (\ud83d\udce5 3.3M \u00b7 \u23f1\ufe0f 24.04.2022):\n\t```\n\tconda install -c conda-forge pyproj\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/python-visualization\/folium\">folium<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 5.7K) - Python Data. Leaflet.js Maps. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/python-visualization\/folium) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 2K \u00b7 \ud83d\udce6 16K \u00b7 \ud83d\udccb 920 - 22% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/python-visualization\/folium\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/folium) (\ud83d\udce5 1M \/ month \u00b7 \ud83d\udce6 630 \u00b7 \u23f1\ufe0f 19.11.2021):\n\t```\n\tpip install folium\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/folium) (\ud83d\udce5 730K \u00b7 \u23f1\ufe0f 03.12.2021):\n\t```\n\tconda install -c conda-forge folium\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/geopy\/geopy\">geopy<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 3.6K \u00b7 \ud83d\udca4) - Geocoding library for Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/geopy\/geopy) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 560 \u00b7 \ud83d\udce6 37K \u00b7 \ud83d\udccb 260 - 10% open \u00b7 \u23f1\ufe0f 26.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/geopy\/geopy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/geopy) (\ud83d\udce5 3.6M \/ month \u00b7 \ud83d\udce6 3.9K \u00b7 \u23f1\ufe0f 11.07.2021):\n\t```\n\tpip install geopy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/geopy) (\ud83d\udce5 690K \u00b7 \u23f1\ufe0f 12.07.2021):\n\t```\n\tconda install -c conda-forge geopy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Toblerity\/Fiona\">Fiona<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 910) - Fiona reads and writes geographic data files. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Toblerity\/Fiona) (\ud83d\udc68\u200d\ud83d\udcbb 66 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce6 8.5K \u00b7 \ud83d\udccb 680 - 11% open \u00b7 \u23f1\ufe0f 01.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Toblerity\/Fiona\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fiona) (\ud83d\udce5 2.7M \/ month \u00b7 \ud83d\udce6 770 \u00b7 \u23f1\ufe0f 07.02.2022):\n\t```\n\tpip install fiona\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/fiona) (\ud83d\udce5 2.8M \u00b7 \u23f1\ufe0f 26.03.2022):\n\t```\n\tconda install -c conda-forge fiona\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\">ipyleaflet<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 1.3K) - A Jupyter - Leaflet.js bridge. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jupyter-widgets\/ipyleaflet) (\ud83d\udc68\u200d\ud83d\udcbb 78 \u00b7 \ud83d\udd00 330 \u00b7 \ud83d\udce6 1.7K \u00b7 \ud83d\udccb 490 - 38% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/jupyter-widgets\/ipyleaflet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ipyleaflet) (\ud83d\udce5 91K \/ month \u00b7 \ud83d\udce6 110 \u00b7 \u23f1\ufe0f 14.04.2022):\n\t```\n\tpip install ipyleaflet\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ipyleaflet) (\ud83d\udce5 810K \u00b7 \u23f1\ufe0f 14.04.2022):\n\t```\n\tconda install -c conda-forge ipyleaflet\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/jupyter-leaflet) (\ud83d\udce5 48K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 14.04.2022):\n\t```\n\tnpm install jupyter-leaflet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Esri\/arcgis-python-api\">ArcGIS API<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 1.3K) - Documentation and samples for ArcGIS API for Python. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Esri\/arcgis-python-api) (\ud83d\udc68\u200d\ud83d\udcbb 78 \u00b7 \ud83d\udd00 880 \u00b7 \ud83d\udce5 3.1K \u00b7 \ud83d\udccb 500 - 24% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Esri\/arcgis-python-api\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/arcgis) (\ud83d\udce5 55K \/ month \u00b7 \ud83d\udce6 22 \u00b7 \u23f1\ufe0f 03.02.2022):\n\t```\n\tpip install arcgis\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/esridocker\/arcgis-api-python-notebook) (\ud83d\udce5 6.9K \u00b7 \u2b50 33 \u00b7 \u23f1\ufe0f 04.02.2022):\n\t```\n\tdocker pull esridocker\/arcgis-api-python-notebook\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jazzband\/geojson\">geojson<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 710) - Python bindings and utilities for GeoJSON. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jazzband\/geojson) (\ud83d\udc68\u200d\ud83d\udcbb 46 \u00b7 \ud83d\udd00 90 \u00b7 \ud83d\udce6 9.2K \u00b7 \ud83d\udccb 82 - 26% open \u00b7 \u23f1\ufe0f 03.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/jazzband\/geojson\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/geojson) (\ud83d\udce5 900K \/ month \u00b7 \ud83d\udce6 1.1K \u00b7 \u23f1\ufe0f 09.08.2019):\n\t```\n\tpip install geojson\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/geojson) (\ud83d\udce5 500K \u00b7 \u23f1\ufe0f 11.08.2019):\n\t```\n\tconda install -c conda-forge geojson\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pysal\/pysal\">PySAL<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1K) - PySAL: Python Spatial Analysis Library Meta-Package. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pysal\/pysal) (\ud83d\udc68\u200d\ud83d\udcbb 75 \u00b7 \ud83d\udd00 270 \u00b7 \ud83d\udccb 610 - 1% open \u00b7 \u23f1\ufe0f 30.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pysal\/pysal\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pysal) (\ud83d\udce5 18K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 30.01.2022):\n\t```\n\tpip install pysal\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pysal) (\ud83d\udce5 440K \u00b7 \u23f1\ufe0f 31.01.2022):\n\t```\n\tconda install -c conda-forge pysal\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/holoviz\/geoviews\">GeoViews<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 410) - Simple, concise geographical visualization in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/holoviz\/geoviews) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 68 \u00b7 \ud83d\udccb 300 - 34% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/holoviz\/geoviews\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/geoviews) (\ud83d\udce5 9.6K \/ month \u00b7 \ud83d\udce6 26 \u00b7 \u23f1\ufe0f 08.03.2022):\n\t```\n\tpip install geoviews\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/geoviews) (\ud83d\udce5 100K \u00b7 \u23f1\ufe0f 08.03.2022):\n\t```\n\tconda install -c conda-forge geoviews\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/earthlab\/earthpy\">EarthPy<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 350) - A package built to support working with spatial data using open source.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/earthlab\/earthpy) (\ud83d\udc68\u200d\ud83d\udcbb 40 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 140 \u00b7 \ud83d\udccb 220 - 8% open \u00b7 \u23f1\ufe0f 20.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/earthlab\/earthpy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/earthpy) (\ud83d\udce5 8.2K \/ month \u00b7 \ud83d\udce6 7 \u00b7 \u23f1\ufe0f 01.10.2021):\n\t```\n\tpip install earthpy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/earthpy) (\ud83d\udce5 43K \u00b7 \u23f1\ufe0f 04.10.2021):\n\t```\n\tconda install -c conda-forge earthpy\n\t```\n<\/details>\n<details><summary>Show 8 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/DenisCarriere\/geocoder\">Geocoder<\/a><\/b> (\ud83e\udd4931 \u00b7  \u2b50 1.4K \u00b7 \ud83d\udc80) - Python Geocoder. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/pytroll\/satpy\">Satpy<\/a><\/b> (\ud83e\udd4930 \u00b7  \u2b50 820) - Python package for earth-observing satellite data processing. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/sentinelsat\/sentinelsat\">Sentinelsat<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 750) - Search and download Copernicus Sentinel satellite images. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/pbugnion\/gmaps\">gmaps<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 740 \u00b7 \ud83d\udc80) - Google maps for Jupyter notebooks. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/mapbox\/mapboxgl-jupyter\">Mapbox GL<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 600 \u00b7 \ud83d\udc80) - Use Mapbox GL JS to visualize data in a Python Jupyter notebook. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/geospace-code\/pymap3d\">pymap3d<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 240) - pure-Python (Numpy optional) 3D coordinate conversions for geospace ecef.. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/andrea-cuttone\/geoplotlib\">geoplotlib<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 960 \u00b7 \ud83d\udc80) - python toolbox for visualizing geographical data and making maps. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/marceloprates\/prettymaps\">prettymaps<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 8K) - A small set of Python functions to draw pretty maps from.. <code><a href=\"http:\/\/bit.ly\/3pwmjO5\">\u2757\ufe0fAGPL-3.0<\/a><\/code>\n<\/details>\n<br>\n\n## Financial Data\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for algorithmic stock\/crypto trading, risk analytics, backtesting, technical analysis, and other tasks on financial data._\n\n<details><summary><b><a href=\"https:\/\/github.com\/ranaroussi\/yfinance\">yfinance<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 7K \u00b7 \ud83d\udcc8) - Download market data from Yahoo! Finances API. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ranaroussi\/yfinance) (\ud83d\udc68\u200d\ud83d\udcbb 53 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce6 11K \u00b7 \ud83d\udccb 770 - 56% open \u00b7 \u23f1\ufe0f 30.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ranaroussi\/yfinance\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/yfinance) (\ud83d\udce5 330K \/ month \u00b7 \ud83d\udce6 120 \u00b7 \u23f1\ufe0f 30.01.2022):\n\t```\n\tpip install yfinance\n\t```\n- [Conda](https:\/\/anaconda.org\/ranaroussi\/yfinance) (\ud83d\udce5 40K \u00b7 \u23f1\ufe0f 10.07.2021):\n\t```\n\tconda install -c ranaroussi yfinance\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/qlib\">Qlib<\/a><\/b> (\ud83e\udd4732 \u00b7  \u2b50 8.5K) - Qlib is an AI-oriented quantitative investment platform, which aims to.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/qlib) (\ud83d\udc68\u200d\ud83d\udcbb 93 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce5 280 \u00b7 \ud83d\udce6 15 \u00b7 \ud83d\udccb 540 - 29% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/qlib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyqlib) (\ud83d\udce5 1.9K \/ month \u00b7 \u23f1\ufe0f 24.04.2022):\n\t```\n\tpip install pyqlib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bukosabino\/ta\">ta<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 3K) - Technical Analysis Library using Pandas and Numpy. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bukosabino\/ta) (\ud83d\udc68\u200d\ud83d\udcbb 28 \u00b7 \ud83d\udd00 700 \u00b7 \ud83d\udce6 1.2K \u00b7 \ud83d\udccb 210 - 53% open \u00b7 \u23f1\ufe0f 24.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bukosabino\/ta\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ta) (\ud83d\udce5 73K \/ month \u00b7 \ud83d\udce6 33 \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tpip install ta\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ta) (\ud83d\udce5 2.8K \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tconda install -c conda-forge ta\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensortrade-org\/tensortrade\">TensorTrade<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 3.8K) - An open source reinforcement learning framework for training,.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensortrade-org\/tensortrade) (\ud83d\udc68\u200d\ud83d\udcbb 61 \u00b7 \ud83d\udd00 870 \u00b7 \ud83d\udce6 34 \u00b7 \ud83d\udccb 230 - 14% open \u00b7 \u23f1\ufe0f 02.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensortrade-org\/tensortrade\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensortrade) (\ud83d\udce5 470 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 10.05.2021):\n\t```\n\tpip install tensortrade\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensortrade) (\ud83d\udce5 1K \u00b7 \u23f1\ufe0f 10.05.2021):\n\t```\n\tconda install -c conda-forge tensortrade\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/RomelTorres\/alpha_vantage\">Alpha Vantage<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 3.6K \u00b7 \ud83d\udca4) - A python wrapper for Alpha Vantage API for financial data. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/RomelTorres\/alpha_vantage) (\ud83d\udc68\u200d\ud83d\udcbb 39 \u00b7 \ud83d\udd00 640 \u00b7 \ud83d\udccb 260 - 5% open \u00b7 \u23f1\ufe0f 14.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/RomelTorres\/alpha_vantage\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/alpha_vantage) (\ud83d\udce5 25K \/ month \u00b7 \ud83d\udce6 100 \u00b7 \u23f1\ufe0f 26.08.2018):\n\t```\n\tpip install alpha_vantage\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/alpha_vantage) (\ud83d\udce5 1.1K \u00b7 \u23f1\ufe0f 14.01.2021):\n\t```\n\tconda install -c conda-forge alpha_vantage\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/erdewit\/ib_insync\">IB-insync<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 1.8K) - Python sync\/async framework for Interactive Brokers API. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/erdewit\/ib_insync) (\ud83d\udc68\u200d\ud83d\udcbb 30 \u00b7 \ud83d\udd00 480 \u00b7 \ud83d\udccb 400 - 2% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/erdewit\/ib_insync\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ib_insync) (\ud83d\udce5 6.5K \/ month \u00b7 \ud83d\udce6 19 \u00b7 \u23f1\ufe0f 28.11.2021):\n\t```\n\tpip install ib_insync\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ib-insync) (\ud83d\udce5 15K \u00b7 \u23f1\ufe0f 29.11.2021):\n\t```\n\tconda install -c conda-forge ib-insync\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pmorissette\/bt\">bt<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 1.4K) - bt - flexible backtesting for Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pmorissette\/bt) (\ud83d\udc68\u200d\ud83d\udcbb 25 \u00b7 \ud83d\udd00 310 \u00b7 \ud83d\udce6 110 \u00b7 \ud83d\udccb 280 - 19% open \u00b7 \u23f1\ufe0f 03.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pmorissette\/bt\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/bt) (\ud83d\udce5 4.8K \/ month \u00b7 \ud83d\udce6 21 \u00b7 \u23f1\ufe0f 21.04.2021):\n\t```\n\tpip install bt\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/bt) (\ud83d\udce5 3.9K \u00b7 \u23f1\ufe0f 18.01.2022):\n\t```\n\tconda install -c conda-forge bt\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pmorissette\/ffn\">ffn<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.1K) - ffn - a financial function library for Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pmorissette\/ffn) (\ud83d\udc68\u200d\ud83d\udcbb 26 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce6 190 \u00b7 \ud83d\udccb 96 - 16% open \u00b7 \u23f1\ufe0f 25.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pmorissette\/ffn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ffn) (\ud83d\udce5 29K \/ month \u00b7 \ud83d\udce6 25 \u00b7 \u23f1\ufe0f 21.04.2021):\n\t```\n\tpip install ffn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ffn) (\ud83d\udce5 790 \u00b7 \u23f1\ufe0f 22.04.2021):\n\t```\n\tconda install -c conda-forge ffn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scrtlabs\/catalyst\">Enigma Catalyst<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 2.3K \u00b7 \ud83d\udca4) - An Algorithmic Trading Library for Crypto-Assets in.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scrtlabs\/catalyst) (\ud83d\udc68\u200d\ud83d\udcbb 150 \u00b7 \ud83d\udd00 700 \u00b7 \ud83d\udce6 24 \u00b7 \ud83d\udccb 490 - 27% open \u00b7 \u23f1\ufe0f 22.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/scrtlabs\/catalyst\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/enigma-catalyst) (\ud83d\udce5 470 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 11.11.2018):\n\t```\n\tpip install enigma-catalyst\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jealous\/stockstats\">stockstats<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 990) - Supply a wrapper ``StockDataFrame`` based on the.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jealous\/stockstats) (\ud83d\udc68\u200d\ud83d\udcbb 8 \u00b7 \ud83d\udd00 260 \u00b7 \ud83d\udce6 450 \u00b7 \ud83d\udccb 85 - 9% open \u00b7 \u23f1\ufe0f 07.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/jealous\/stockstats\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/stockstats) (\ud83d\udce5 8.3K \/ month \u00b7 \ud83d\udce6 29 \u00b7 \u23f1\ufe0f 07.01.2022):\n\t```\n\tpip install stockstats\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/CryptoSignal\/Crypto-Signal\">Crypto Signals<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 4K \u00b7 \ud83d\udca4) - Github.com\/CryptoSignal - #1 Quant Trading & Technical.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/CryptoSignal\/Crypto-Signal) (\ud83d\udc68\u200d\ud83d\udcbb 28 \u00b7 \ud83d\udd00 1K \u00b7 \ud83d\udccb 260 - 20% open \u00b7 \u23f1\ufe0f 28.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/CryptoSignal\/crypto-signal\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/shadowreaver\/crypto-signal) (\ud83d\udce5 140K \u00b7 \u2b50 7 \u00b7 \u23f1\ufe0f 03.09.2020):\n\t```\n\tdocker pull shadowreaver\/crypto-signal\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/tf-quant-finance\">tf-quant-finance<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 3.1K) - High-performance TensorFlow library for quantitative.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/tf-quant-finance) (\ud83d\udc68\u200d\ud83d\udcbb 39 \u00b7 \ud83d\udd00 420 \u00b7 \ud83d\udccb 42 - 50% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/tf-quant-finance\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tf-quant-finance) (\ud83d\udce5 900 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install tf-quant-finance\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/cuemacro\/finmarketpy\">finmarketpy<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 2.9K) - Python library for backtesting trading strategies & analyzing.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/cuemacro\/finmarketpy) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 450 \u00b7 \ud83d\udce5 40 \u00b7 \ud83d\udce6 5 \u00b7 \ud83d\udccb 26 - 88% open \u00b7 \u23f1\ufe0f 05.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/cuemacro\/finmarketpy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/finmarketpy) (\ud83d\udce5 59 \/ month \u00b7 \u23f1\ufe0f 07.10.2021):\n\t```\n\tpip install finmarketpy\n\t```\n<\/details>\n<details><summary>Show 12 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/quantopian\/zipline\">zipline<\/a><\/b> (\ud83e\udd4732 \u00b7  \u2b50 15K \u00b7 \ud83d\udc80) - Zipline, a Pythonic Algorithmic Trading Library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/mementum\/backtrader\">backtrader<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 8.7K \u00b7 \ud83d\udca4) - Python Backtesting library for trading strategies. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/quantopian\/pyfolio\">pyfolio<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 4.4K \u00b7 \ud83d\udc80) - Portfolio and risk analytics in Python. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/bashtage\/arch\">arch<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 910) - ARCH models in Python. <code><a href=\"https:\/\/tldrlegal.com\/search?q=NCSA\">\u2757\ufe0fNCSA<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/quantopian\/alphalens\">Alphalens<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 2.3K \u00b7 \ud83d\udc80) - Performance analysis of predictive (alpha) stock factors. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/quantopian\/empyrical\">empyrical<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 920 \u00b7 \ud83d\udc80) - Common financial risk and performance metrics. Used by zipline.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/gbeced\/pyalgotrade\">PyAlgoTrade<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 3.7K \u00b7 \ud83d\udc80) - Python Algorithmic Trading Library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/peerchemist\/finta\">FinTA<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.6K \u00b7 \ud83d\udca4) - Common financial technical indicators implemented in Pandas. <code><a href=\"http:\/\/bit.ly\/37RvQcA\">\u2757\ufe0fLGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/kernc\/backtesting.py\">Backtesting.py<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 2.4K) - Backtest trading strategies in Python. <code><a href=\"http:\/\/bit.ly\/3pwmjO5\">\u2757\ufe0fAGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/fmilthaler\/FinQuant\">FinQuant<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 760 \u00b7 \ud83d\udc80) - A program for financial portfolio management, analysis and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/tradytics\/surpriver\">surpriver<\/a><\/b> (\ud83e\udd4912 \u00b7  \u2b50 1.4K \u00b7 \ud83d\udc80) - Find big moving stocks before they move using machine.. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/alvarobartt\/pyrtfolio\">pyrtfolio<\/a><\/b> (\ud83e\udd497 \u00b7  \u2b50 110 \u00b7 \ud83d\udc80) - Python package to generate stock portfolios. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n<\/details>\n<br>\n\n## Time Series Data\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for forecasting, anomaly detection, feature extraction, and machine learning on time-series and sequential data._\n\n<details><summary><b><a href=\"https:\/\/github.com\/alan-turing-institute\/sktime\">sktime<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 5.3K) - A unified framework for machine learning with time series. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/alan-turing-institute\/sktime) (\ud83d\udc68\u200d\ud83d\udcbb 170 \u00b7 \ud83d\udd00 850 \u00b7 \ud83d\udce5 76 \u00b7 \ud83d\udce6 420 \u00b7 \ud83d\udccb 1.1K - 35% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/alan-turing-institute\/sktime\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sktime) (\ud83d\udce5 130K \/ month \u00b7 \ud83d\udce6 23 \u00b7 \u23f1\ufe0f 30.04.2022):\n\t```\n\tpip install sktime\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sktime-all-extras) (\ud83d\udce5 3.3K \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tconda install -c conda-forge sktime-all-extras\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebook\/prophet\">Prophet<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 14K) - Tool for producing high quality forecasts for time series data that has.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebook\/prophet) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 4.2K \u00b7 \ud83d\udce5 650 \u00b7 \ud83d\udccb 1.8K - 12% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebook\/prophet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fbprophet) (\ud83d\udce5 1.3M \/ month \u00b7 \ud83d\udce6 130 \u00b7 \u23f1\ufe0f 05.09.2020):\n\t```\n\tpip install fbprophet\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/prophet) (\ud83d\udce5 42K \u00b7 \u23f1\ufe0f 23.08.2021):\n\t```\n\tconda install -c conda-forge prophet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/alkaline-ml\/pmdarima\">pmdarima<\/a><\/b> (\ud83e\udd4732 \u00b7  \u2b50 1.2K) - A statistical library designed to fill the void in Pythons time series.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/alkaline-ml\/pmdarima) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce6 2K \u00b7 \ud83d\udccb 280 - 9% open \u00b7 \u23f1\ufe0f 25.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/alkaline-ml\/pmdarima\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pmdarima) (\ud83d\udce5 1.2M \/ month \u00b7 \ud83d\udce6 51 \u00b7 \u23f1\ufe0f 22.02.2022):\n\t```\n\tpip install pmdarima\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pmdarima) (\ud83d\udce5 36K \u00b7 \u23f1\ufe0f 24.02.2022):\n\t```\n\tconda install -c conda-forge pmdarima\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/unit8co\/darts\">Darts<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 4K) - A python library for easy manipulation and forecasting of time series. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/unit8co\/darts) (\ud83d\udc68\u200d\ud83d\udcbb 53 \u00b7 \ud83d\udd00 400 \u00b7 \ud83d\udce6 53 \u00b7 \ud83d\udccb 440 - 36% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/unit8co\/darts\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/u8darts) (\ud83d\udce5 8.6K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tpip install u8darts\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/u8darts-all) (\ud83d\udce5 3.6K \u00b7 \u23f1\ufe0f 14.04.2022):\n\t```\n\tconda install -c conda-forge u8darts-all\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/unit8\/darts) (\ud83d\udce5 330 \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tdocker pull unit8\/darts\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ourownstory\/neural_prophet\">NeuralProphet<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 2.2K \u00b7 \u2795) - NeuralProphet: A simple forecasting package. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ourownstory\/neural_prophet) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 280 \u00b7 \ud83d\udce6 74 \u00b7 \ud83d\udccb 260 - 27% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ourownstory\/neural_prophet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/neuralprophet) (\ud83d\udce5 72K \/ month \u00b7 \u23f1\ufe0f 22.03.2022):\n\t```\n\tpip install neuralprophet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tslearn-team\/tslearn\">tslearn<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 2.1K) - A machine learning toolkit dedicated to time-series data. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tslearn-team\/tslearn) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 270 \u00b7 \ud83d\udce6 480 \u00b7 \ud83d\udccb 270 - 32% open \u00b7 \u23f1\ufe0f 06.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/tslearn-team\/tslearn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tslearn) (\ud83d\udce5 110K \/ month \u00b7 \ud83d\udce6 19 \u00b7 \u23f1\ufe0f 16.08.2021):\n\t```\n\tpip install tslearn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tslearn) (\ud83d\udce5 250K \u00b7 \u23f1\ufe0f 15.01.2022):\n\t```\n\tconda install -c conda-forge tslearn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/blue-yonder\/tsfresh\">tsfresh<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 6.4K) - Automatic extraction of relevant features from time series:. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/blue-yonder\/tsfresh) (\ud83d\udc68\u200d\ud83d\udcbb 82 \u00b7 \ud83d\udd00 970 \u00b7 \ud83d\udccb 480 - 8% open \u00b7 \u23f1\ufe0f 21.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/blue-yonder\/tsfresh\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tsfresh) (\ud83d\udce5 380K \/ month \u00b7 \ud83d\udce6 55 \u00b7 \u23f1\ufe0f 21.12.2021):\n\t```\n\tpip install tsfresh\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tsfresh) (\ud83d\udce5 150K \u00b7 \u23f1\ufe0f 21.12.2021):\n\t```\n\tconda install -c conda-forge tsfresh\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/TDAmeritrade\/stumpy\">STUMPY<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 2.3K) - STUMPY is a powerful and scalable Python library for modern time series.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/TDAmeritrade\/stumpy) (\ud83d\udc68\u200d\ud83d\udcbb 29 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udccb 310 - 9% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/TDAmeritrade\/stumpy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/stumpy) (\ud83d\udce5 260K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 31.03.2022):\n\t```\n\tpip install stumpy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/stumpy) (\ud83d\udce5 38K \u00b7 \u23f1\ufe0f 31.03.2022):\n\t```\n\tconda install -c conda-forge stumpy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jdb78\/pytorch-forecasting\">pytorch-forecasting<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 1.9K) - Time series forecasting with PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jdb78\/pytorch-forecasting) (\ud83d\udc68\u200d\ud83d\udcbb 29 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udccb 450 - 45% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/jdb78\/pytorch-forecasting\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch-forecasting) (\ud83d\udce5 55K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tpip install pytorch-forecasting\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pytorch-forecasting) (\ud83d\udce5 18K \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tconda install -c conda-forge pytorch-forecasting\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/awslabs\/gluon-ts\">GluonTS<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.7K) - Probabilistic time series modeling in Python. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/awslabs\/gluon-ts) (\ud83d\udc68\u200d\ud83d\udcbb 85 \u00b7 \ud83d\udd00 560 \u00b7 \ud83d\udccb 730 - 40% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/awslabs\/gluon-ts\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gluonts) (\ud83d\udce5 71K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install gluonts\n\t```\n- [Conda](https:\/\/anaconda.org\/anaconda\/gluonts) (\ud83d\udce5 22 \u00b7 \u23f1\ufe0f 14.10.2021):\n\t```\n\tconda install -c anaconda gluonts\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/johannfaouzi\/pyts\">pyts<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 1.3K) - A Python package for time series classification. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/johannfaouzi\/pyts) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 130 \u00b7 \ud83d\udce6 190 \u00b7 \ud83d\udccb 60 - 58% open \u00b7 \u23f1\ufe0f 02.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/johannfaouzi\/pyts\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyts) (\ud83d\udce5 130K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 31.10.2021):\n\t```\n\tpip install pyts\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyts) (\ud83d\udce5 11K \u00b7 \u23f1\ufe0f 31.10.2021):\n\t```\n\tconda install -c conda-forge pyts\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Nixtla\/statsforecast\">StatsForecast<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 560 \u00b7 \ud83d\udc23) - Lightning fast forecasting with statistical and econometric.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Nixtla\/statsforecast) (\ud83d\udc68\u200d\ud83d\udcbb 7 \u00b7 \ud83d\udd00 33 \u00b7 \ud83d\udccb 37 - 29% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Nixtla\/statsforecast\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/statsforecast) (\ud83d\udce5 170K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tpip install statsforecast\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/statsforecast) (\ud83d\udce5 2.1K \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tconda install -c conda-forge statsforecast\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/python-streamz\/streamz\">Streamz<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.1K) - Real-time stream processing for python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/python-streamz\/streamz) (\ud83d\udc68\u200d\ud83d\udcbb 45 \u00b7 \ud83d\udd00 130 \u00b7 \ud83d\udce6 290 \u00b7 \ud83d\udccb 250 - 41% open \u00b7 \u23f1\ufe0f 24.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/python-streamz\/streamz\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/streamz) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 29 \u00b7 \u23f1\ufe0f 04.10.2021):\n\t```\n\tpip install streamz\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/streamz) (\ud83d\udce5 280K \u00b7 \u23f1\ufe0f 04.10.2021):\n\t```\n\tconda install -c conda-forge streamz\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/linkedin\/greykite\">greykite<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1.5K) - A flexible, intuitive and fast forecasting library. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/linkedin\/greykite) (\ud83d\udc68\u200d\ud83d\udcbb 7 \u00b7 \ud83d\udd00 68 \u00b7 \ud83d\udce6 8 \u00b7 \ud83d\udccb 60 - 13% open \u00b7 \u23f1\ufe0f 15.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/linkedin\/greykite\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/greykite) (\ud83d\udce5 52K \/ month \u00b7 \u23f1\ufe0f 15.12.2021):\n\t```\n\tpip install greykite\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/fraunhoferportugal\/tsfel\">TSFEL<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 500) - An intuitive library to extract features from time series. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/fraunhoferportugal\/tsfel) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 62 \u00b7 \ud83d\udce6 34 \u00b7 \ud83d\udccb 49 - 12% open \u00b7 \u23f1\ufe0f 16.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/fraunhoferportugal\/tsfel\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tsfel) (\ud83d\udce5 5.1K \/ month \u00b7 \u23f1\ufe0f 14.02.2021):\n\t```\n\tpip install tsfel\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/AutoViML\/Auto_TS\">Auto TS<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 430) - Automatically build ARIMA, SARIMAX, VAR, FB Prophet and XGBoost.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/AutoViML\/Auto_TS) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 78 \u00b7 \ud83d\udccb 71 - 12% open \u00b7 \u23f1\ufe0f 13.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/AutoViML\/Auto_TS\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/auto-ts) (\ud83d\udce5 1.5K \/ month \u00b7 \u23f1\ufe0f 31.01.2022):\n\t```\n\tpip install auto-ts\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/firmai\/atspy\">atspy<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 440) - AtsPy: Automated Time Series Models in Python (by @firmai). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/firmai\/atspy) (\ud83d\udc68\u200d\ud83d\udcbb 5 \u00b7 \ud83d\udd00 84 \u00b7 \ud83d\udce6 5 \u00b7 \ud83d\udccb 20 - 90% open \u00b7 \u23f1\ufe0f 18.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/firmai\/atspy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/atspy) (\ud83d\udce5 1.7K \/ month \u00b7 \u23f1\ufe0f 24.04.2020):\n\t```\n\tpip install atspy\n\t```\n<\/details>\n<details><summary>Show 9 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/RJT1990\/pyflux\">PyFlux<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 2K \u00b7 \ud83d\udc80) - Open source time series library for Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/linkedin\/luminol\">luminol<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1K \u00b7 \ud83d\udc80) - Anomaly Detection and Correlation library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Nixtla\/neuralforecast\">NeuralForecast<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 580) - Scalable and user friendly neural forecasting algorithms.. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/dmbee\/seglearn\">seglearn<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 510 \u00b7 \ud83d\udc80) - Python module for machine learning time series:. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/X-DataInitiative\/tick\">tick<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 380 \u00b7 \ud83d\udc80) - Module for statistical learning, with a particular emphasis on time-.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/wwrechard\/pydlm\">pydlm<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 420 \u00b7 \ud83d\udc80) - A python library for Bayesian time series modeling. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/arundo\/adtk\">ADTK<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 810 \u00b7 \ud83d\udc80) - A Python toolkit for rule-based\/unsupervised anomaly detection in time.. <code><a href=\"http:\/\/bit.ly\/3postzC\">MPL-2.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/target\/matrixprofile-ts\">matrixprofile-ts<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 680 \u00b7 \ud83d\udc80) - A Python library for detecting patterns and anomalies.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/arundo\/tsaug\">tsaug<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 240 \u00b7 \ud83d\udc80) - A Python package for time series augmentation. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n<\/details>\n<br>\n\n## Medical Data\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for processing and analyzing medical data such as MRIs, EEGs, genomic data, and other medical imaging formats._\n\n<details><summary><b><a href=\"https:\/\/github.com\/mne-tools\/mne-python\">MNE<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 1.9K \u00b7 \ud83d\udcc9) - MNE: Magnetoencephalography (MEG) and Electroencephalography (EEG) in.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mne-tools\/mne-python) (\ud83d\udc68\u200d\ud83d\udcbb 290 \u00b7 \ud83d\udd00 1K \u00b7 \ud83d\udce6 1.6K \u00b7 \ud83d\udccb 4K - 9% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mne-tools\/mne-python\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mne) (\ud83d\udce5 53K \/ month \u00b7 \ud83d\udce6 210 \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tpip install mne\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mne) (\ud83d\udce5 190K \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tconda install -c conda-forge mne\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nilearn\/nilearn\">Nilearn<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 830) - Machine learning for NeuroImaging in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nilearn\/nilearn) (\ud83d\udc68\u200d\ud83d\udcbb 180 \u00b7 \ud83d\udd00 440 \u00b7 \ud83d\udce5 40 \u00b7 \ud83d\udce6 1.5K \u00b7 \ud83d\udccb 1.6K - 16% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nilearn\/nilearn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nilearn) (\ud83d\udce5 20K \/ month \u00b7 \ud83d\udce6 240 \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tpip install nilearn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/nilearn) (\ud83d\udce5 150K \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tconda install -c conda-forge nilearn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nipy\/nipype\">NIPYPE<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 630) - Workflows and interfaces for neuroimaging packages. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nipy\/nipype) (\ud83d\udc68\u200d\ud83d\udcbb 240 \u00b7 \ud83d\udd00 490 \u00b7 \ud83d\udce6 910 \u00b7 \ud83d\udccb 1.3K - 29% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nipy\/nipype\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nipype) (\ud83d\udce5 53K \/ month \u00b7 \ud83d\udce6 150 \u00b7 \u23f1\ufe0f 05.04.2022):\n\t```\n\tpip install nipype\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/nipype) (\ud83d\udce5 470K \u00b7 \u23f1\ufe0f 05.04.2022):\n\t```\n\tconda install -c conda-forge nipype\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nipy\/nibabel\">NiBabel<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 470) - Python package to access a cacophony of neuro-imaging file formats. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nipy\/nibabel) (\ud83d\udc68\u200d\ud83d\udcbb 93 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udce6 7K \u00b7 \ud83d\udccb 460 - 30% open \u00b7 \u23f1\ufe0f 19.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nipy\/nibabel\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nibabel) (\ud83d\udce5 180K \/ month \u00b7 \ud83d\udce6 970 \u00b7 \u23f1\ufe0f 07.02.2022):\n\t```\n\tpip install nibabel\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/nibabel) (\ud83d\udce5 430K \u00b7 \u23f1\ufe0f 07.02.2022):\n\t```\n\tconda install -c conda-forge nibabel\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Project-MONAI\/MONAI\">MONAI<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 3K) - AI Toolkit for Healthcare Imaging. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Project-MONAI\/MONAI) (\ud83d\udc68\u200d\ud83d\udcbb 94 \u00b7 \ud83d\udd00 570 \u00b7 \ud83d\udce6 290 \u00b7 \ud83d\udccb 1.6K - 12% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Project-MONAI\/MONAI\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/monai) (\ud83d\udce5 49K \/ month \u00b7 \ud83d\udce6 16 \u00b7 \u23f1\ufe0f 16.02.2022):\n\t```\n\tpip install monai\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/monai) (\ud83d\udce5 440 \u00b7 \u23f1\ufe0f 09.01.2022):\n\t```\n\tconda install -c conda-forge monai\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/CamDavidsonPilon\/lifelines\">Lifelines<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 1.9K) - Survival analysis in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/CamDavidsonPilon\/lifelines) (\ud83d\udc68\u200d\ud83d\udcbb 99 \u00b7 \ud83d\udd00 470 \u00b7 \ud83d\udce6 870 \u00b7 \ud83d\udccb 860 - 25% open \u00b7 \u23f1\ufe0f 16.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/CamDavidsonPilon\/lifelines\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lifelines) (\ud83d\udce5 310K \/ month \u00b7 \ud83d\udce6 100 \u00b7 \u23f1\ufe0f 15.03.2022):\n\t```\n\tpip install lifelines\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/lifelines) (\ud83d\udce5 190K \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tconda install -c conda-forge lifelines\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/hail-is\/hail\">Hail<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 790) - Scalable genomic data analysis. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/hail-is\/hail) (\ud83d\udc68\u200d\ud83d\udcbb 79 \u00b7 \ud83d\udd00 210 \u00b7 \ud83d\udce6 62 \u00b7 \ud83d\udccb 2K - 2% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/hail-is\/hail\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hail) (\ud83d\udce5 33K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install hail\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dipy\/dipy\">DIPY<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 510) - DIPY is the paragon 3D\/4D+ imaging library in Python. Contains generic.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dipy\/dipy) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 340 \u00b7 \ud83d\udce6 550 \u00b7 \ud83d\udccb 780 - 15% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dipy\/dipy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dipy) (\ud83d\udce5 13K \/ month \u00b7 \ud83d\udce6 80 \u00b7 \u23f1\ufe0f 11.03.2022):\n\t```\n\tpip install dipy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dipy) (\ud83d\udce5 290K \u00b7 \u23f1\ufe0f 15.03.2022):\n\t```\n\tconda install -c conda-forge dipy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/deepvariant\">DeepVariant<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 2.5K) - DeepVariant is an analysis pipeline that uses a deep neural.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/deepvariant) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 600 \u00b7 \ud83d\udce5 3.8K \u00b7 \ud83d\udccb 480 - 0% open \u00b7 \u23f1\ufe0f 28.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/deepvariant\n\t```\n- [Conda](https:\/\/anaconda.org\/bioconda\/deepvariant) (\ud83d\udce5 40K \u00b7 \u23f1\ufe0f 16.12.2021):\n\t```\n\tconda install -c bioconda deepvariant\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/MIC-DKFZ\/medicaldetectiontoolkit\">Medical Detection Toolkit<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 1.1K) - The Medical Detection Toolkit contains 2D + 3D.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/MIC-DKFZ\/medicaldetectiontoolkit) (\ud83d\udc68\u200d\ud83d\udcbb 3 \u00b7 \ud83d\udd00 280 \u00b7 \ud83d\udccb 120 - 32% open \u00b7 \u23f1\ufe0f 04.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/MIC-DKFZ\/medicaldetectiontoolkit\n\t```\n<\/details>\n<details><summary>Show 9 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/NifTK\/NiftyNet\">NiftyNet<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 1.3K \u00b7 \ud83d\udc80) - [unmaintained] An open-source convolutional neural.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/nipy\/nipy\">NIPY<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 320 \u00b7 \ud83d\udc80) - Neuroimaging in Python FMRI analysis package. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/DLTK\/DLTK\">DLTK<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 1.3K \u00b7 \ud83d\udc80) - Deep Learning Toolkit for Medical Image Analysis. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/loli\/medpy\">MedPy<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 400 \u00b7 \ud83d\udc80) - Medical image processing in Python. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/projectglow\/glow\">Glow<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 200) - An open-source toolkit for large-scale genomic analysis. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/brainiak\/brainiak\">Brainiak<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 270 \u00b7 \ud83d\udca4) - Brain Imaging Analysis Kit. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/perone\/medicaltorch\">MedicalTorch<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 770 \u00b7 \ud83d\udc80) - A medical imaging framework for Pytorch. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/QTIM-Lab\/DeepNeuro\">DeepNeuro<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 110 \u00b7 \ud83d\udc80) - A deep learning python package for neuroimaging data. Made by:. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Tencent\/MedicalNet\">MedicalNet<\/a><\/b> (\ud83e\udd4912 \u00b7  \u2b50 1.4K \u00b7 \ud83d\udc80) - Many studies have shown that the performance on deep learning is.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n<\/details>\n<br>\n\n## Tabular Data\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for processing tabular and structured data._\n\n<details><summary><b><a href=\"https:\/\/github.com\/carefree0910\/carefree-learn\">carefree-learn<\/a><\/b> (\ud83e\udd4818 \u00b7  \u2b50 360) - Deep Learning PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/carefree0910\/carefree-learn) (\ud83d\udc68\u200d\ud83d\udcbb 1 \u00b7 \ud83d\udd00 33 \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/carefree0910\/carefree-learn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/carefree-learn) (\ud83d\udce5 120 \/ month \u00b7 \u23f1\ufe0f 29.04.2022):\n\t```\n\tpip install carefree-learn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/manujosephv\/pytorch_tabular\">pytorch_tabular<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 590) - A standard framework for modelling Deep Learning Models.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/manujosephv\/pytorch_tabular) (\ud83d\udc68\u200d\ud83d\udcbb 9 \u00b7 \ud83d\udd00 61 \u00b7 \ud83d\udccb 58 - 27% open \u00b7 \u23f1\ufe0f 30.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/manujosephv\/pytorch_tabular\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch_tabular) (\ud83d\udce5 2.3K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 01.09.2021):\n\t```\n\tpip install pytorch_tabular\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/firmai\/deltapy\">deltapy<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 420) - DeltaPy - Tabular Data Augmentation (by @firmai). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/firmai\/deltapy) (\ud83d\udc68\u200d\ud83d\udcbb 4 \u00b7 \ud83d\udd00 42 \u00b7 \ud83d\udce6 2 \u00b7 \ud83d\udccb 3 - 66% open \u00b7 \u23f1\ufe0f 01.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/firmai\/deltapy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deltapy) (\ud83d\udce5 110 \/ month \u00b7 \u23f1\ufe0f 09.04.2020):\n\t```\n\tpip install deltapy\n\t```\n<\/details>\n<details><summary>Show 2 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/AnotherSamWilson\/miceforest\">miceforest<\/a><\/b> (\ud83e\udd4723 \u00b7  \u2b50 150) - Multiple Imputation with Random Forests in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/upgini\/upgini\">upgini<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 23 \u00b7 \ud83d\udc23) - Features search library for supervised machine learning searches.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n<\/details>\n<br>\n\n## Optical Character Recognition\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for optical character recognition (OCR) and text extraction from images or videos._\n\n<details><summary><b><a href=\"https:\/\/github.com\/PaddlePaddle\/PaddleOCR\">PaddleOCR<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 20K) - Awesome multilingual OCR toolkits based on PaddlePaddle.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PaddlePaddle\/PaddleOCR) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 4.3K \u00b7 \ud83d\udce6 620 \u00b7 \ud83d\udccb 4.4K - 23% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PaddlePaddle\/PaddleOCR\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/paddleocr) (\ud83d\udce5 51K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 25.04.2022):\n\t```\n\tpip install paddleocr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/JaidedAI\/EasyOCR\">EasyOCR<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 15K) - Ready-to-use OCR with 80+ supported languages and all popular writing.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/JaidedAI\/EasyOCR) (\ud83d\udc68\u200d\ud83d\udcbb 98 \u00b7 \ud83d\udd00 2K \u00b7 \ud83d\udce5 1.5M \u00b7 \ud83d\udce6 1.1K \u00b7 \ud83d\udccb 550 - 18% open \u00b7 \u23f1\ufe0f 09.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/JaidedAI\/EasyOCR\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/easyocr) (\ud83d\udce5 110K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 09.04.2022):\n\t```\n\tpip install easyocr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/madmaze\/pytesseract\">Tesseract<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 4.2K) - Python-tesseract is an optical character recognition (OCR) tool.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/madmaze\/pytesseract) (\ud83d\udc68\u200d\ud83d\udcbb 41 \u00b7 \ud83d\udd00 590 \u00b7 \ud83d\udccb 300 - 4% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/madmaze\/pytesseract\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytesseract) (\ud83d\udce5 570K \/ month \u00b7 \ud83d\udce6 940 \u00b7 \u23f1\ufe0f 19.02.2022):\n\t```\n\tpip install pytesseract\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pytesseract) (\ud83d\udce5 500K \u00b7 \u23f1\ufe0f 15.03.2022):\n\t```\n\tconda install -c conda-forge pytesseract\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ocrmypdf\/OCRmyPDF\">OCRmyPDF<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 6.3K) - OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them.. <code><a href=\"http:\/\/bit.ly\/3postzC\">MPL-2.0<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ocrmypdf\/OCRmyPDF) (\ud83d\udc68\u200d\ud83d\udcbb 64 \u00b7 \ud83d\udd00 560 \u00b7 \ud83d\udccb 850 - 10% open \u00b7 \u23f1\ufe0f 15.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ocrmypdf\/OCRmyPDF\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ocrmypdf) (\ud83d\udce5 23K \/ month \u00b7 \ud83d\udce6 12 \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tpip install ocrmypdf\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ocrmypdf) (\ud83d\udce5 9.7K \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tconda install -c conda-forge ocrmypdf\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/sirfz\/tesserocr\">tesserocr<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 1.6K) - A Python wrapper for the tesseract-ocr API. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/sirfz\/tesserocr) (\ud83d\udc68\u200d\ud83d\udcbb 26 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 650 \u00b7 \ud83d\udccb 250 - 32% open \u00b7 \u23f1\ufe0f 09.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/sirfz\/tesserocr\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tesserocr) (\ud83d\udce5 59K \/ month \u00b7 \ud83d\udce6 67 \u00b7 \u23f1\ufe0f 19.06.2021):\n\t```\n\tpip install tesserocr\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tesserocr) (\ud83d\udce5 67K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge tesserocr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/open-mmlab\/mmocr\">MMOCR<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 2.4K) - OpenMMLab Text Detection, Recognition and Understanding Toolbox. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/open-mmlab\/mmocr) (\ud83d\udc68\u200d\ud83d\udcbb 49 \u00b7 \ud83d\udd00 420 \u00b7 \ud83d\udce6 11 \u00b7 \ud83d\udccb 540 - 18% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/open-mmlab\/mmocr\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mmocr) (\ud83d\udce5 3.2K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 31.03.2022):\n\t```\n\tpip install mmocr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Calamari-OCR\/calamari\">calamari<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 910) - Line based ATR Engine based on OCRopy. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Calamari-OCR\/calamari) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udccb 250 - 19% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Calamari-OCR\/calamari\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/calamari_ocr) (\ud83d\udce5 1K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 13.11.2018):\n\t```\n\tpip install calamari_ocr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/faustomorales\/keras-ocr\">keras-ocr<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1K) - A packaged and flexible version of the CRAFT text detector and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/faustomorales\/keras-ocr) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 250 \u00b7 \ud83d\udce5 250K \u00b7 \ud83d\udccb 170 - 37% open \u00b7 \u23f1\ufe0f 28.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/faustomorales\/keras-ocr\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/keras-ocr) (\ud83d\udce5 5.6K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 24.11.2021):\n\t```\n\tpip install keras-ocr\n\t```\n- [Conda](https:\/\/anaconda.org\/anaconda\/keras-ocr) (\ud83d\udce5 31 \u00b7 \u23f1\ufe0f 14.01.2022):\n\t```\n\tconda install -c anaconda keras-ocr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/emedvedev\/attention-ocr\">attention-ocr<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 910 \u00b7 \ud83d\udca4) - A Tensorflow model for text recognition (CNN + seq2seq.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/emedvedev\/attention-ocr) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udce6 19 \u00b7 \ud83d\udccb 150 - 16% open \u00b7 \u23f1\ufe0f 29.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/emedvedev\/attention-ocr\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/aocr) (\ud83d\udce5 250 \/ month \u00b7 \u23f1\ufe0f 19.04.2019):\n\t```\n\tpip install aocr\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/WZBSocialScienceCenter\/pdftabextract\">pdftabextract<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 2K) - A set of tools for extracting tables from PDF files helping to.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/WZBSocialScienceCenter\/pdftabextract) (\ud83d\udc68\u200d\ud83d\udcbb 2 \u00b7 \ud83d\udd00 350 \u00b7 \ud83d\udce6 39 \u00b7 \ud83d\udccb 21 - 14% open \u00b7 \u23f1\ufe0f 07.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/WZBSocialScienceCenter\/pdftabextract\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pdftabextract) (\ud83d\udce5 570 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 09.01.2018):\n\t```\n\tpip install pdftabextract\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/aashrafh\/Mozart\">Mozart<\/a><\/b> (\ud83e\udd4910 \u00b7  \u2b50 370 \u00b7 \ud83d\udca4) - An optical music recognition (OMR) system. Converts sheet.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/aashrafh\/Mozart) (\ud83d\udc68\u200d\ud83d\udcbb 5 \u00b7 \ud83d\udd00 56 \u00b7 \ud83d\udccb 11 - 27% open \u00b7 \u23f1\ufe0f 05.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/aashrafh\/Mozart\n\t```\n<\/details>\n<details><summary>Show 1 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/jlsutherland\/doc2text\">doc2text<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 1.3K \u00b7 \ud83d\udc80) - Detect text blocks and OCR poorly scanned PDFs in bulk. Python.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n<\/details>\n<br>\n\n## Data Containers & Structures\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_General-purpose data containers & structures as well as utilities & extensions for pandas._\n\n<br>\n\n## Data Loading & Extraction\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for loading, collecting, and extracting data from a variety of data sources and formats._\n\n\ud83d\udd17&nbsp;<b><a href=\"https:\/\/github.com\/ml-tooling\/best-of-python#data-loading--extraction\">best-of-python - Data Extraction<\/a><\/b> ( \u2b50 2.1K)  - Collection of data-loading and -extraction libraries.\n\n\ud83d\udd17&nbsp;<b><a href=\"https:\/\/github.com\/ml-tooling\/best-of-python#data-containers--dataframes\">best-of-python - Data Containers<\/a><\/b> ( \u2b50 2.1K)  - Collection of data-container, dataframe, and pandas-..\n\n<br>\n\n## Web Scraping & Crawling\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for web scraping, crawling, downloading, and mining as well as libraries._\n\n\ud83d\udd17&nbsp;<b><a href=\"https:\/\/github.com\/ml-tooling\/best-of-web-python#web-scraping--crawling\">best-of-web-python - Web Scraping<\/a><\/b> ( \u2b50 1.5K)  - Collection of web-scraping and crawling libraries.\n\n<br>\n\n## Data Pipelines & Streaming\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for data batch- and stream-processing, workflow automation, job scheduling, and other data pipeline tasks._\n\n<details><summary><b><a href=\"https:\/\/github.com\/celery\/celery\">Celery<\/a><\/b> (\ud83e\udd4746 \u00b7  \u2b50 19K) - Asynchronous task queue\/job queue based on distributed message passing. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/celery\/celery) (\ud83d\udc68\u200d\ud83d\udcbb 1.2K \u00b7 \ud83d\udd00 4.3K \u00b7 \ud83d\udce6 69K \u00b7 \ud83d\udccb 4.7K - 11% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/celery\/celery\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/celery) (\ud83d\udce5 4.9M \/ month \u00b7 \ud83d\udce6 15K \u00b7 \u23f1\ufe0f 05.04.2022):\n\t```\n\tpip install celery\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/celery) (\ud83d\udce5 820K \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tconda install -c conda-forge celery\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/apache\/airflow\">Airflow<\/a><\/b> (\ud83e\udd4745 \u00b7  \u2b50 26K) - Platform to programmatically author, schedule, and monitor workflows. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/apache\/airflow) (\ud83d\udc68\u200d\ud83d\udcbb 2.4K \u00b7 \ud83d\udd00 10K \u00b7 \ud83d\udce5 280K \u00b7 \ud83d\udccb 5.5K - 16% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/apache\/airflow\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/apache-airflow) (\ud83d\udce5 5.6M \/ month \u00b7 \ud83d\udce6 460 \u00b7 \u23f1\ufe0f 30.04.2022):\n\t```\n\tpip install apache-airflow\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/airflow) (\ud83d\udce5 590K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c conda-forge airflow\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/apache\/airflow) (\ud83d\udce5 71M \u00b7 \u2b50 330 \u00b7 \u23f1\ufe0f 30.04.2022):\n\t```\n\tdocker pull apache\/airflow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/apache\/beam\">Beam<\/a><\/b> (\ud83e\udd4739 \u00b7  \u2b50 5.5K) - Unified programming model to define and execute data processing.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/apache\/beam) (\ud83d\udc68\u200d\ud83d\udcbb 1.3K \u00b7 \ud83d\udd00 3.5K \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/apache\/beam\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/apache-beam) (\ud83d\udce5 7.1M \/ month \u00b7 \ud83d\udce6 150 \u00b7 \u23f1\ufe0f 20.04.2022):\n\t```\n\tpip install apache-beam\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/apache-beam-with-aws) (\ud83d\udce5 8.2K \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tconda install -c conda-forge apache-beam-with-aws\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rq\/rq\">rq<\/a><\/b> (\ud83e\udd4738 \u00b7  \u2b50 8.3K) - Simple job queues for Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rq\/rq) (\ud83d\udc68\u200d\ud83d\udcbb 260 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udce6 10K \u00b7 \ud83d\udccb 960 - 18% open \u00b7 \u23f1\ufe0f 02.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rq\/rq\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/rq) (\ud83d\udce5 510K \/ month \u00b7 \ud83d\udce6 1.7K \u00b7 \u23f1\ufe0f 07.12.2021):\n\t```\n\tpip install rq\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/rq) (\ud83d\udce5 70K \u00b7 \u23f1\ufe0f 30.06.2021):\n\t```\n\tconda install -c conda-forge rq\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/spotify\/luigi\">luigi<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 16K \u00b7 \ud83d\udcc9) - Luigi is a Python module that helps you build complex pipelines of.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/spotify\/luigi) (\ud83d\udc68\u200d\ud83d\udcbb 590 \u00b7 \ud83d\udd00 2.3K \u00b7 \ud83d\udce6 1.7K \u00b7 \ud83d\udccb 970 - 10% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/spotify\/luigi\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/luigi) (\ud83d\udce5 560K \/ month \u00b7 \ud83d\udce6 400 \u00b7 \u23f1\ufe0f 23.09.2020):\n\t```\n\tpip install luigi\n\t```\n- [Conda](https:\/\/anaconda.org\/anaconda\/luigi) (\ud83d\udce5 9.8K \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c anaconda luigi\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PrefectHQ\/prefect\">Prefect<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 8.8K) - The easiest way to automate your data. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PrefectHQ\/prefect) (\ud83d\udc68\u200d\ud83d\udcbb 320 \u00b7 \ud83d\udd00 870 \u00b7 \ud83d\udce6 740 \u00b7 \ud83d\udccb 2.2K - 22% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PrefectHQ\/prefect\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/prefect) (\ud83d\udce5 230K \/ month \u00b7 \ud83d\udce6 59 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install prefect\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/prefect) (\ud83d\udce5 260K \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tconda install -c conda-forge prefect\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dagster-io\/dagster\">Dagster<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 4.7K) - An orchestration platform for the development, production, and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dagster-io\/dagster) (\ud83d\udc68\u200d\ud83d\udcbb 200 \u00b7 \ud83d\udd00 590 \u00b7 \ud83d\udce6 360 \u00b7 \ud83d\udccb 4.1K - 24% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dagster-io\/dagster\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dagster) (\ud83d\udce5 220K \/ month \u00b7 \ud83d\udce6 87 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install dagster\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dagster) (\ud83d\udce5 490K \u00b7 \u23f1\ufe0f 29.04.2022):\n\t```\n\tconda install -c conda-forge dagster\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/joblib\/joblib\">joblib<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 2.8K) - Computing with Python functions. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/joblib\/joblib) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 320 \u00b7 \ud83d\udce6 180K \u00b7 \ud83d\udccb 730 - 45% open \u00b7 \u23f1\ufe0f 30.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/joblib\/joblib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/joblib) (\ud83d\udce5 24M \/ month \u00b7 \ud83d\udce6 4.9K \u00b7 \u23f1\ufe0f 07.10.2021):\n\t```\n\tpip install joblib\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/joblib) (\ud83d\udce5 8.4M \u00b7 \u23f1\ufe0f 07.10.2021):\n\t```\n\tconda install -c conda-forge joblib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/great-expectations\/great_expectations\">Great Expectations<\/a><\/b> (\ud83e\udd4836 \u00b7  \u2b50 6.5K) - Always know what to expect from your data. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/great-expectations\/great_expectations) (\ud83d\udc68\u200d\ud83d\udcbb 290 \u00b7 \ud83d\udd00 940 \u00b7 \ud83d\udccb 1.3K - 15% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/great-expectations\/great_expectations\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/great_expectations) (\ud83d\udce5 4.7M \/ month \u00b7 \ud83d\udce6 31 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install great_expectations\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/great-expectations) (\ud83d\udce5 370K \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tconda install -c conda-forge great-expectations\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dbt-labs\/dbt-core\">dbt<\/a><\/b> (\ud83e\udd4836 \u00b7  \u2b50 4.7K) - dbt enables data analysts and engineers to transform their data using the.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dbt-labs\/dbt-core) (\ud83d\udc68\u200d\ud83d\udcbb 210 \u00b7 \ud83d\udd00 860 \u00b7 \ud83d\udce5 310 \u00b7 \ud83d\udce6 430 \u00b7 \ud83d\udccb 2.8K - 12% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dbt-labs\/dbt-core\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dbt) (\ud83d\udce5 590K \/ month \u00b7 \ud83d\udce6 29 \u00b7 \u23f1\ufe0f 06.12.2021):\n\t```\n\tpip install dbt\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dbt) (\ud83d\udce5 200K \u00b7 \u23f1\ufe0f 09.12.2021):\n\t```\n\tconda install -c conda-forge dbt\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/kedro-org\/kedro\">Kedro<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 7.2K) - A Python framework for creating reproducible, maintainable and modular.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/kedro-org\/kedro) (\ud83d\udc68\u200d\ud83d\udcbb 150 \u00b7 \ud83d\udd00 640 \u00b7 \ud83d\udce6 850 \u00b7 \ud83d\udccb 730 - 13% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/kedro-org\/kedro\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/kedro) (\ud83d\udce5 310K \/ month \u00b7 \ud83d\udce6 36 \u00b7 \u23f1\ufe0f 31.03.2022):\n\t```\n\tpip install kedro\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/petl-developers\/petl\">petl<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 1K) - Python Extract Transform and Load Tables of Data. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/petl-developers\/petl) (\ud83d\udc68\u200d\ud83d\udcbb 54 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 700 \u00b7 \ud83d\udccb 440 - 17% open \u00b7 \u23f1\ufe0f 25.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/petl-developers\/petl\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/petl) (\ud83d\udce5 170K \/ month \u00b7 \ud83d\udce6 74 \u00b7 \u23f1\ufe0f 25.04.2022):\n\t```\n\tpip install petl\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/petl) (\ud83d\udce5 73K \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tconda install -c conda-forge petl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/activeloopai\/Hub\">Activeloop<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 4.5K) - Dataset format for AI. Build, manage, query & visualize datasets.. <code><a href=\"http:\/\/bit.ly\/3postzC\">MPL-2.0<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/activeloopai\/Hub) (\ud83d\udc68\u200d\ud83d\udcbb 95 \u00b7 \ud83d\udd00 380 \u00b7 \ud83d\udccb 370 - 16% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/activeloopai\/Hub\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hub) (\ud83d\udce5 3.6K \/ month \u00b7 \ud83d\udce6 53 \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tpip install hub\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/combust\/mleap\">mleap<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 1.4K) - MLeap: Deploy ML Pipelines to Production. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/combust\/mleap) (\ud83d\udc68\u200d\ud83d\udcbb 73 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udce6 180 \u00b7 \ud83d\udccb 440 - 20% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/combust\/mleap\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mleap) (\ud83d\udce5 210K \/ month \u00b7 \ud83d\udce6 26 \u00b7 \u23f1\ufe0f 04.04.2022):\n\t```\n\tpip install mleap\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mleap) (\ud83d\udce5 46K \u00b7 \u23f1\ufe0f 05.04.2022):\n\t```\n\tconda install -c conda-forge mleap\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/coleifer\/huey\">huey<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 3.9K) - a little task queue for python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/coleifer\/huey) (\ud83d\udc68\u200d\ud83d\udcbb 66 \u00b7 \ud83d\udd00 330 \u00b7 \ud83d\udce6 910 \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/coleifer\/huey\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/huey) (\ud83d\udce5 170K \/ month \u00b7 \ud83d\udce6 160 \u00b7 \u23f1\ufe0f 28.12.2021):\n\t```\n\tpip install huey\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/huey) (\ud83d\udce5 23K \u00b7 \u23f1\ufe0f 16.10.2019):\n\t```\n\tconda install -c conda-forge huey\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ploomber\/ploomber\">ploomber<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 2.4K) - The fastest way to build data pipelines. Develop iteratively,.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ploomber\/ploomber) (\ud83d\udc68\u200d\ud83d\udcbb 40 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 39 \u00b7 \ud83d\udccb 640 - 27% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ploomber\/ploomber\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ploomber) (\ud83d\udce5 4.4K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tpip install ploomber\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ploomber) (\ud83d\udce5 6.1K \u00b7 \u23f1\ufe0f 23.04.2022):\n\t```\n\tconda install -c conda-forge ploomber\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/tfx\">TFX<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 1.7K) - TFX is an end-to-end platform for deploying production ML pipelines. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/tfx) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 550 \u00b7 \ud83d\udccb 760 - 32% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/tfx\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tfx) (\ud83d\udce5 340K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 11.04.2022):\n\t```\n\tpip install tfx\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/hi-primus\/optimus\">Optimus<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 1.2K) - Agile Data Preparation Workflows madeeasy with Pandas, Dask,.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/hi-primus\/optimus) (\ud83d\udc68\u200d\ud83d\udcbb 25 \u00b7 \ud83d\udd00 210 \u00b7 \ud83d\udccb 230 - 13% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/hi-primus\/optimus\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/optimuspyspark) (\ud83d\udce5 110K \/ month \u00b7 \u23f1\ufe0f 30.05.2019):\n\t```\n\tpip install optimuspyspark\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/EntilZha\/PyFunctional\">PyFunctional<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 2K) - Python library for creating data pipelines with chain functional.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/EntilZha\/PyFunctional) (\ud83d\udc68\u200d\ud83d\udcbb 25 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 430 \u00b7 \ud83d\udccb 130 - 5% open \u00b7 \u23f1\ufe0f 05.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/EntilZha\/PyFunctional\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyfunctional) (\ud83d\udce5 92K \/ month \u00b7 \ud83d\udce6 12 \u00b7 \u23f1\ufe0f 12.01.2021):\n\t```\n\tpip install pyfunctional\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Parsely\/streamparse\">streamparse<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.5K) - Run Python in Apache Storm topologies. Pythonic API, CLI.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Parsely\/streamparse) (\ud83d\udc68\u200d\ud83d\udcbb 42 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 54 \u00b7 \ud83d\udccb 330 - 21% open \u00b7 \u23f1\ufe0f 10.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Parsely\/streamparse\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/streamparse) (\ud83d\udce5 3.8K \/ month \u00b7 \ud83d\udce6 27 \u00b7 \u23f1\ufe0f 10.01.2022):\n\t```\n\tpip install streamparse\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/samuelcolvin\/arq\">arq<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.2K) - Fast job queuing and RPC in python with asyncio and redis. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/samuelcolvin\/arq) (\ud83d\udc68\u200d\ud83d\udcbb 39 \u00b7 \ud83d\udd00 98 \u00b7 \ud83d\udce6 220 \u00b7 \ud83d\udccb 140 - 26% open \u00b7 \u23f1\ufe0f 28.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/samuelcolvin\/arq\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/arq) (\ud83d\udce5 21K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 09.03.2022):\n\t```\n\tpip install arq\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/arq) (\ud83d\udce5 2.1K \u00b7 \u23f1\ufe0f 03.09.2021):\n\t```\n\tconda install -c conda-forge arq\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/zenml-io\/zenml\">zenml<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 1.9K) - ZenML : MLOps framework to create reproducible pipelines... <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/zenml-io\/zenml) (\ud83d\udc68\u200d\ud83d\udcbb 32 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udccb 72 - 13% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/zenml-io\/zenml\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/zenml) (\ud83d\udce5 1.1K \/ month \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install zenml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/whylabs\/whylogs\">whylogs<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1K) - Open standard for end-to-end data and ML monitoring for any scale in.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/whylabs\/whylogs) (\ud83d\udc68\u200d\ud83d\udcbb 31 \u00b7 \ud83d\udd00 53 \u00b7 \ud83d\udce5 50 \u00b7 \ud83d\udccb 150 - 39% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/whylabs\/whylogs\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/whylogs) (\ud83d\udce5 10K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install whylogs\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/cgarciae\/pypeln\">Pypeline<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.3K) - Concurrent data pipelines in Python . <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/cgarciae\/pypeln) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 80 \u00b7 \ud83d\udccb 58 - 25% open \u00b7 \u23f1\ufe0f 06.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/cgarciae\/pypeln\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pypeln) (\ud83d\udce5 8.2K \/ month \u00b7 \ud83d\udce6 9 \u00b7 \u23f1\ufe0f 06.01.2022):\n\t```\n\tpip install pypeln\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pypeln) (\ud83d\udce5 6K \u00b7 \u23f1\ufe0f 06.01.2022):\n\t```\n\tconda install -c conda-forge pypeln\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/closeio\/tasktiger\">TaskTiger<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.1K) - Python task queue using Redis. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/closeio\/tasktiger) (\ud83d\udc68\u200d\ud83d\udcbb 25 \u00b7 \ud83d\udd00 62 \u00b7 \ud83d\udce6 22 \u00b7 \ud83d\udccb 68 - 48% open \u00b7 \u23f1\ufe0f 25.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/closeio\/tasktiger\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tasktiger) (\ud83d\udce5 1.1K \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 02.12.2021):\n\t```\n\tpip install tasktiger\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pdpipe\/pdpipe\">pdpipe<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 670) - Easy pipelines for pandas DataFrames. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pdpipe\/pdpipe) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 34 \u00b7 \ud83d\udce6 40 \u00b7 \ud83d\udccb 43 - 30% open \u00b7 \u23f1\ufe0f 13.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pdpipe\/pdpipe\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pdpipe) (\ud83d\udce5 1.5K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 13.03.2022):\n\t```\n\tpip install pdpipe\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pdpipe) (\ud83d\udce5 3.8K \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tconda install -c conda-forge pdpipe\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/databricks\/spark-deep-learning\">spark-deep-learning<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 1.9K) - Deep Learning Pipelines for Apache Spark. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/databricks\/spark-deep-learning) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 460 \u00b7 \ud83d\udce6 21 \u00b7 \ud83d\udccb 100 - 74% open \u00b7 \u23f1\ufe0f 21.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/databricks\/spark-deep-learning\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nerevu\/riko\">riko<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 1.6K) - A Python stream processing engine modeled after Yahoo! Pipes. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nerevu\/riko) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 75 \u00b7 \ud83d\udccb 30 - 73% open \u00b7 \u23f1\ufe0f 28.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/nerevu\/riko\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/riko) (\ud83d\udce5 35 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 28.12.2021):\n\t```\n\tpip install riko\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/d6t\/d6tflow\">Databolt Flow<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 940 \u00b7 \ud83d\udca4) - Python library for building highly effective data science.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/d6t\/d6tflow) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 70 \u00b7 \ud83d\udce6 20 \u00b7 \ud83d\udccb 23 - 43% open \u00b7 \u23f1\ufe0f 28.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/d6t\/d6tflow\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/d6tflow) (\ud83d\udce5 320 \/ month \u00b7 \u23f1\ufe0f 06.10.2021):\n\t```\n\tpip install d6tflow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/kubeflow-kale\/kale\">kale<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 530 \u00b7 \ud83d\udca4) - Kubeflows superfood for Data Scientists. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/kubeflow-kale\/kale) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 100 \u00b7 \ud83d\udccb 160 - 52% open \u00b7 \u23f1\ufe0f 20.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/kubeflow-kale\/kale\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/kubeflow-kale) (\ud83d\udce5 1.6K \/ month \u00b7 \u23f1\ufe0f 19.05.2021):\n\t```\n\tpip install kubeflow-kale\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mara\/mara-pipelines\">Mara Pipelines<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 1.9K) - A lightweight opinionated ETL framework, halfway between plain.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mara\/mara-pipelines) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 88 \u00b7 \ud83d\udce6 9 \u00b7 \ud83d\udccb 25 - 44% open \u00b7 \u23f1\ufe0f 30.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mara\/mara-pipelines\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mara-pipelines) (\ud83d\udce5 240 \/ month \u00b7 \u23f1\ufe0f 23.01.2021):\n\t```\n\tpip install mara-pipelines\n\t```\n<\/details>\n<details><summary>Show 12 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/Yelp\/mrjob\">mrjob<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 2.6K \u00b7 \ud83d\udc80) - Run MapReduce jobs on Hadoop or Amazon Web Services. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/robinhood\/faust\">faust<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 6.1K \u00b7 \ud83d\udc80) - Python Stream Processing. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/databand-ai\/dbnd\">dbnd<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 220) - DBND is an agile pipeline framework that helps data engineering teams.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/python-bonobo\/bonobo\">bonobo<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.5K \u00b7 \ud83d\udc80) - Extract Transform Load for Python 3.5+. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/douban\/dpark\">dpark<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 2.7K \u00b7 \ud83d\udc80) - Python clone of Spark, a MapReduce alike framework in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/svenkreiss\/pysparkling\">pysparkling<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 250 \u00b7 \ud83d\udc80) - A pure Python implementation of Apache Sparks RDD and DStream.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/analysiscenter\/batchflow\">BatchFlow<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 180) - BatchFlow helps you conveniently work with random or sequential.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/pricingassistant\/mrq\">mrq<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 860 \u00b7 \ud83d\udc80) - Mr. Queue - A distributed worker task queue in Python using Redis & gevent. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/bodywork-ml\/bodywork-core\">bodywork-core<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 330) - ML pipeline orchestration and model deployments on.. <code><a href=\"http:\/\/bit.ly\/3pwmjO5\">\u2757\ufe0fAGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/olirice\/flupy\">flupy<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 170) - Fluent data pipelines for python and your shell. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/kkyon\/botflow\">Botflow<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 1.2K \u00b7 \ud83d\udc80) - Python Fast Dataflow programming framework for Data pipeline work(.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/vincentclaes\/datajob\">datajob<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 88) - Build and deploy a serverless data pipeline on AWS with no effort. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n<\/details>\n<br>\n\n## Distributed Machine Learning\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries that provide capabilities to distribute and parallelize machine learning tasks across large-scale compute infrastructure._\n\n<details><summary><b><a href=\"https:\/\/github.com\/ray-project\/ray\">Ray<\/a><\/b> (\ud83e\udd4744 \u00b7  \u2b50 20K) - An open source framework that provides a simple, universal API for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ray-project\/ray) (\ud83d\udc68\u200d\ud83d\udcbb 680 \u00b7 \ud83d\udd00 3.5K \u00b7 \ud83d\udce6 4.7K \u00b7 \ud83d\udccb 10K - 22% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ray-project\/ray\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ray) (\ud83d\udce5 1M \/ month \u00b7 \ud83d\udce6 270 \u00b7 \u23f1\ufe0f 29.04.2022):\n\t```\n\tpip install ray\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ray-tune) (\ud83d\udce5 26K \u00b7 \u23f1\ufe0f 07.04.2022):\n\t```\n\tconda install -c conda-forge ray-tune\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dask\/dask\">dask<\/a><\/b> (\ud83e\udd4743 \u00b7  \u2b50 9.8K \u00b7 \ud83d\udcc9) - Parallel computing with task scheduling. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dask\/dask) (\ud83d\udc68\u200d\ud83d\udcbb 530 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce6 36K \u00b7 \ud83d\udccb 4.3K - 16% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dask\/dask\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dask) (\ud83d\udce5 5.8M \/ month \u00b7 \ud83d\udce6 2.6K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tpip install dask\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dask) (\ud83d\udce5 5.4M \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c conda-forge dask\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dask\/distributed\">dask.distributed<\/a><\/b> (\ud83e\udd4741 \u00b7  \u2b50 1.3K) - A distributed task scheduler for Dask. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dask\/distributed) (\ud83d\udc68\u200d\ud83d\udcbb 270 \u00b7 \ud83d\udd00 620 \u00b7 \ud83d\udce6 23K \u00b7 \ud83d\udccb 2.8K - 37% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dask\/distributed\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/distributed) (\ud83d\udce5 4.8M \/ month \u00b7 \ud83d\udce6 1.2K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tpip install distributed\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/distributed) (\ud83d\udce5 6.7M \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c conda-forge distributed\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/horovod\/horovod\">horovod<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 12K) - Distributed training framework for TensorFlow, Keras, PyTorch, and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/horovod\/horovod) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 2K \u00b7 \ud83d\udce6 570 \u00b7 \ud83d\udccb 2K - 15% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/horovod\/horovod\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/horovod) (\ud83d\udce5 54K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install horovod\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/h2oai\/h2o-3\">H2O-3<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 5.8K) - H2O is an Open Source, Distributed, Fast & Scalable Machine Learning.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/h2oai\/h2o-3) (\ud83d\udc68\u200d\ud83d\udcbb 220 \u00b7 \ud83d\udd00 1.9K \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/h2oai\/h2o-3\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/h2o) (\ud83d\udce5 380K \/ month \u00b7 \ud83d\udce6 75 \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tpip install h2o\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/intel-analytics\/BigDL\">BigDL<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 3.9K) - Building Large-Scale AI Applications for Distributed Big Data. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/intel-analytics\/BigDL) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 990 \u00b7 \ud83d\udce6 36 \u00b7 \ud83d\udccb 1.3K - 34% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/intel-analytics\/BigDL\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/bigdl) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install bigdl\n\t```\n- [Maven](https:\/\/search.maven.org\/artifact\/com.intel.analytics.bigdl\/bigdl-SPARK_2.4) (\ud83d\udce6 4 \u00b7 \u23f1\ufe0f 20.04.2021):\n\t```\n\t<dependency>\n\t\t<groupId>com.intel.analytics.bigdl<\/groupId>\n\t\t<artifactId>bigdl-SPARK_2.4<\/artifactId>\n\t\t<version>[VERSION]<\/version>\n\t<\/dependency>\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ipython\/ipyparallel\">ipyparallel<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 2.2K) - IPython Parallel: Interactive Parallel Computing in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ipython\/ipyparallel) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 880 \u00b7 \ud83d\udce6 1.9K \u00b7 \ud83d\udccb 330 - 16% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ipython\/ipyparallel\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ipyparallel) (\ud83d\udce5 51K \/ month \u00b7 \ud83d\udce6 290 \u00b7 \u23f1\ufe0f 01.04.2022):\n\t```\n\tpip install ipyparallel\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ipyparallel) (\ud83d\udce5 590K \u00b7 \u23f1\ufe0f 01.04.2022):\n\t```\n\tconda install -c conda-forge ipyparallel\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/DeepSpeed\">DeepSpeed<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 6.7K) - DeepSpeed is a deep learning optimization library that makes.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/DeepSpeed) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 780 \u00b7 \ud83d\udce6 240 \u00b7 \ud83d\udccb 900 - 52% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/DeepSpeed\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deepspeed) (\ud83d\udce5 180K \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install deepspeed\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/deepspeed\/deepspeed) (\ud83d\udce5 14K \u00b7 \u2b50 3 \u00b7 \u23f1\ufe0f 09.03.2022):\n\t```\n\tdocker pull deepspeed\/deepspeed\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/fairscale\">FairScale<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 1.7K) - PyTorch extensions for high performance and large scale training. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/fairscale) (\ud83d\udc68\u200d\ud83d\udcbb 59 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 260 \u00b7 \ud83d\udccb 300 - 21% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/fairscale\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fairscale) (\ud83d\udce5 100K \/ month \u00b7 \ud83d\udce6 15 \u00b7 \u23f1\ufe0f 09.03.2022):\n\t```\n\tpip install fairscale\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/fairscale) (\ud83d\udce5 13K \u00b7 \u23f1\ufe0f 22.03.2022):\n\t```\n\tconda install -c conda-forge fairscale\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PyTorchLightning\/metrics\">metrics<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 840) - Machine learning metrics for distributed, scalable PyTorch.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PyTorchLightning\/metrics) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce5 610 \u00b7 \ud83d\udce6 2.8K \u00b7 \ud83d\udccb 360 - 17% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PyTorchLightning\/metrics\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/metrics) (\ud83d\udce5 2.6K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 28.04.2018):\n\t```\n\tpip install metrics\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/torchmetrics) (\ud83d\udce5 320K \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tconda install -c conda-forge torchmetrics\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/yahoo\/TensorFlowOnSpark\">TensorFlowOnSpark<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 3.8K) - TensorFlowOnSpark brings TensorFlow programs to.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/yahoo\/TensorFlowOnSpark) (\ud83d\udc68\u200d\ud83d\udcbb 34 \u00b7 \ud83d\udd00 970 \u00b7 \ud83d\udccb 360 - 1% open \u00b7 \u23f1\ufe0f 21.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/yahoo\/TensorFlowOnSpark\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflowonspark) (\ud83d\udce5 450K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install tensorflowonspark\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorflowonspark) (\ud83d\udce5 10K \u00b7 \u23f1\ufe0f 27.03.2022):\n\t```\n\tconda install -c conda-forge tensorflowonspark\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/SynapseML\">SynapseML<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 3.3K) - Simple and Distributed Machine Learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/SynapseML) (\ud83d\udc68\u200d\ud83d\udcbb 89 \u00b7 \ud83d\udd00 640 \u00b7 \ud83d\udccb 560 - 41% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/SynapseML\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/synapseml) (\ud83d\udce5 15K \/ month \u00b7 \u23f1\ufe0f 12.01.2022):\n\t```\n\tpip install synapseml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/uber\/petastorm\">petastorm<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 1.4K) - Petastorm library enables single machine or distributed training.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/uber\/petastorm) (\ud83d\udc68\u200d\ud83d\udcbb 44 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udce5 310 \u00b7 \ud83d\udce6 66 \u00b7 \ud83d\udccb 280 - 50% open \u00b7 \u23f1\ufe0f 21.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/uber\/petastorm\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/petastorm) (\ud83d\udce5 92K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 19.02.2022):\n\t```\n\tpip install petastorm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dask\/dask-ml\">dask-ml<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 800) - Scalable Machine Learning with Dask. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dask\/dask-ml) (\ud83d\udc68\u200d\ud83d\udcbb 72 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 610 \u00b7 \ud83d\udccb 470 - 48% open \u00b7 \u23f1\ufe0f 16.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dask\/dask-ml\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dask-ml) (\ud83d\udce5 81K \/ month \u00b7 \ud83d\udce6 55 \u00b7 \u23f1\ufe0f 22.01.2022):\n\t```\n\tpip install dask-ml\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dask-ml) (\ud83d\udce5 300K \u00b7 \u23f1\ufe0f 22.01.2022):\n\t```\n\tconda install -c conda-forge dask-ml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/intel-analytics\/analytics-zoo\">analytics-zoo<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 2.5K) - Distributed Tensorflow, Keras and PyTorch on Apache.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/intel-analytics\/analytics-zoo) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 710 \u00b7 \ud83d\udce6 3 \u00b7 \ud83d\udccb 1.4K - 39% open \u00b7 \u23f1\ufe0f 29.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/intel-analytics\/analytics-zoo\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/analytics-zoo) (\ud83d\udce5 10K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install analytics-zoo\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mpi4py\/mpi4py\">mpi4py<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 530) - Python bindings for MPI. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mpi4py\/mpi4py) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 77 \u00b7 \ud83d\udce5 4.6K \u00b7 \ud83d\udccb 75 - 17% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mpi4py\/mpi4py\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mpi4py) (\ud83d\udce5 150K \/ month \u00b7 \ud83d\udce6 580 \u00b7 \u23f1\ufe0f 15.12.2021):\n\t```\n\tpip install mpi4py\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mpi4py) (\ud83d\udce5 1M \u00b7 \u23f1\ufe0f 04.04.2022):\n\t```\n\tconda install -c conda-forge mpi4py\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/maxpumperla\/elephas\">Elephas<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.5K) - Distributed Deep learning with Keras & Spark. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code>keras<\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/maxpumperla\/elephas) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udce6 55 \u00b7 \ud83d\udccb 160 - 11% open \u00b7 \u23f1\ufe0f 30.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/maxpumperla\/elephas\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/elephas) (\ud83d\udce5 53K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 30.03.2022):\n\t```\n\tpip install elephas\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/elephas) (\ud83d\udce5 8K \u00b7 \u23f1\ufe0f 02.06.2021):\n\t```\n\tconda install -c conda-forge elephas\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/mesh\">Mesh<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.2K) - Mesh TensorFlow: Model Parallelism Made Easier. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/mesh) (\ud83d\udc68\u200d\ud83d\udcbb 47 \u00b7 \ud83d\udd00 210 \u00b7 \ud83d\udce6 670 \u00b7 \ud83d\udccb 100 - 86% open \u00b7 \u23f1\ufe0f 25.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/mesh\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mesh-tensorflow) (\ud83d\udce5 19K \/ month \u00b7 \ud83d\udce6 32 \u00b7 \u23f1\ufe0f 24.03.2021):\n\t```\n\tpip install mesh-tensorflow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/SynapseML\">MMLSpark<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 3.3K) - Simple and Distributed Machine Learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/SynapseML) (\ud83d\udc68\u200d\ud83d\udcbb 89 \u00b7 \ud83d\udd00 640 \u00b7 \ud83d\udccb 560 - 41% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/SynapseML\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mmlspark) (\ud83d\udce5 10 \/ month \u00b7 \u23f1\ufe0f 18.03.2020):\n\t```\n\tpip install mmlspark\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookincubator\/submitit\">Submit it<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 620) - Python 3.6+ toolbox for submitting jobs to Slurm. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookincubator\/submitit) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 62 \u00b7 \ud83d\udccb 63 - 30% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookincubator\/submitit\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/submitit) (\ud83d\udce5 21K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 07.04.2022):\n\t```\n\tpip install submitit\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/submitit) (\ud83d\udce5 5.8K \u00b7 \u23f1\ufe0f 10.02.2021):\n\t```\n\tconda install -c conda-forge submitit\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/learning-at-home\/hivemind\">Hivemind<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1K) - Decentralized deep learning in PyTorch. Built to train models on thousands.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/learning-at-home\/hivemind) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 60 \u00b7 \ud83d\udce6 5 \u00b7 \ud83d\udccb 120 - 35% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/learning-at-home\/hivemind\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hivemind) (\ud83d\udce5 1.4K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 20.12.2021):\n\t```\n\tpip install hivemind\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/apache\/singa\">Apache Singa<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 2.6K \u00b7 \ud83d\udca4) - a distributed deep learning platform. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/apache\/singa) (\ud83d\udc68\u200d\ud83d\udcbb 76 \u00b7 \ud83d\udd00 790 \u00b7 \ud83d\udce6 1 \u00b7 \ud83d\udccb 97 - 41% open \u00b7 \u23f1\ufe0f 10.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/apache\/singa\n\t```\n- [Conda](https:\/\/anaconda.org\/nusdbsystem\/singa) (\ud83d\udce5 450 \u00b7 \u23f1\ufe0f 09.08.2021):\n\t```\n\tconda install -c nusdbsystem singa\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/apache\/singa) (\ud83d\udce5 280 \u00b7 \u2b50 4 \u00b7 \u23f1\ufe0f 04.06.2019):\n\t```\n\tdocker pull apache\/singa\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bytedance\/byteps\">BytePS<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 3.2K) - A high performance and generic framework for distributed DNN training. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bytedance\/byteps) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 440 \u00b7 \ud83d\udccb 260 - 38% open \u00b7 \u23f1\ufe0f 10.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bytedance\/byteps\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/byteps) (\ud83d\udce5 45 \/ month \u00b7 \u23f1\ufe0f 02.08.2021):\n\t```\n\tpip install byteps\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/bytepsimage\/tensorflow) (\ud83d\udce5 1.3K \u00b7 \u23f1\ufe0f 03.03.2020):\n\t```\n\tdocker pull bytepsimage\/tensorflow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/kingoflolz\/mesh-transformer-jax\">mesh-transformer-jax<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 4.1K) - Model parallel transformers in JAX and Haiku. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/kingoflolz\/mesh-transformer-jax) (\ud83d\udc68\u200d\ud83d\udcbb 23 \u00b7 \ud83d\udd00 510 \u00b7 \ud83d\udccb 170 - 10% open \u00b7 \u23f1\ufe0f 28.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/kingoflolz\/mesh-transformer-jax\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tunib-ai\/parallelformers\">parallelformers<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 460) - Parallelformers: An Efficient Model Parallelization.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tunib-ai\/parallelformers) (\ud83d\udc68\u200d\ud83d\udcbb 4 \u00b7 \ud83d\udd00 28 \u00b7 \ud83d\udce6 6 \u00b7 \ud83d\udccb 18 - 38% open \u00b7 \u23f1\ufe0f 02.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tunib-ai\/parallelformers\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/parallelformers) (\ud83d\udce5 200 \/ month \u00b7 \u23f1\ufe0f 29.12.2021):\n\t```\n\tpip install parallelformers\n\t```\n<\/details>\n<details><summary>Show 8 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/DEAP\/deap\">DEAP<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 4.7K) - Distributed Evolutionary Algorithms in Python. <code><a href=\"http:\/\/bit.ly\/37RvQcA\">\u2757\ufe0fLGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/deepmind\/launchpad\">launchpad<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 270) - Launchpad is a library that simplifies writing distributed.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/databricks\/tensorframes\">TensorFrames<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 760 \u00b7 \ud83d\udc80) - [DEPRECATED] Tensorflow wrapper for DataFrames on.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/Ibotta\/sk-dist\">sk-dist<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 280 \u00b7 \ud83d\udca4) - Distributed scikit-learn meta-estimators in PySpark. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/peterwittek\/somoclu\">somoclu<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 240 \u00b7 \ud83d\udca4) - Massively parallel self-organizing maps: accelerate training on.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/uber\/fiber\">Fiber<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 970 \u00b7 \ud83d\udc80) - Distributed Computing for AI Made Simple. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/ml-tooling\/lazycluster\">LazyCluster<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 43 \u00b7 \ud83d\udca4) - Distributed machine learning made simple. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/petuum\/autodist\">autodist<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 120 \u00b7 \ud83d\udc80) - Simple Distributed Deep Learning on TensorFlow. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Hyperparameter Optimization & AutoML\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for hyperparameter optimization, automl and neural architecture search._\n\n<details><summary><b><a href=\"https:\/\/github.com\/optuna\/optuna\">Optuna<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 6.3K \u00b7 \ud83d\udcc9) - A hyperparameter optimization framework. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/optuna\/optuna) (\ud83d\udc68\u200d\ud83d\udcbb 190 \u00b7 \ud83d\udd00 700 \u00b7 \ud83d\udce6 3.2K \u00b7 \ud83d\udccb 1.2K - 12% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/optuna\/optuna\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/optuna) (\ud83d\udce5 2.2M \/ month \u00b7 \ud83d\udce6 200 \u00b7 \u23f1\ufe0f 12.04.2022):\n\t```\n\tpip install optuna\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/optuna) (\ud83d\udce5 270K \u00b7 \u23f1\ufe0f 04.10.2021):\n\t```\n\tconda install -c conda-forge optuna\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/nni\">NNI<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 11K) - An open source AutoML toolkit for automate machine learning lifecycle,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/nni) (\ud83d\udc68\u200d\ud83d\udcbb 170 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udce6 220 \u00b7 \ud83d\udccb 1.6K - 17% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/nni\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nni) (\ud83d\udce5 11K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 18.04.2022):\n\t```\n\tpip install nni\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/keras-team\/autokeras\">AutoKeras<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 8.5K) - AutoML library for deep learning. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/keras-team\/autokeras) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 1.4K \u00b7 \ud83d\udce5 3.9K \u00b7 \ud83d\udce6 300 \u00b7 \ud83d\udccb 820 - 9% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/keras-team\/autokeras\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/autokeras) (\ud83d\udce5 59K \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 30.04.2022):\n\t```\n\tpip install autokeras\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/automl\/auto-sklearn\">auto-sklearn<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 6.2K) - Automated Machine Learning with scikit-learn. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/automl\/auto-sklearn) (\ud83d\udc68\u200d\ud83d\udcbb 79 \u00b7 \ud83d\udd00 1.2K \u00b7 \ud83d\udce5 35 \u00b7 \ud83d\udce6 280 \u00b7 \ud83d\udccb 880 - 12% open \u00b7 \u23f1\ufe0f 24.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/automl\/auto-sklearn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/auto-sklearn) (\ud83d\udce5 38K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install auto-sklearn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/auto-sklearn) (\ud83d\udce5 4.1K \u00b7 \u23f1\ufe0f 18.02.2022):\n\t```\n\tconda install -c conda-forge auto-sklearn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/hyperopt\/hyperopt\">Hyperopt<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 6.2K) - Distributed Asynchronous Hyperparameter Optimization in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/hyperopt\/hyperopt) (\ud83d\udc68\u200d\ud83d\udcbb 93 \u00b7 \ud83d\udd00 960 \u00b7 \ud83d\udce6 6.4K \u00b7 \ud83d\udccb 610 - 61% open \u00b7 \u23f1\ufe0f 29.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/hyperopt\/hyperopt\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hyperopt) (\ud83d\udce5 2.2M \/ month \u00b7 \ud83d\udce6 420 \u00b7 \u23f1\ufe0f 17.11.2021):\n\t```\n\tpip install hyperopt\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/hyperopt) (\ud83d\udce5 390K \u00b7 \u23f1\ufe0f 30.04.2022):\n\t```\n\tconda install -c conda-forge hyperopt\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/keras-team\/keras-tuner\">Keras Tuner<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 2.5K) - Hyperparameter tuning for humans. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/keras-team\/keras-tuner) (\ud83d\udc68\u200d\ud83d\udcbb 47 \u00b7 \ud83d\udd00 320 \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 380 - 44% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/keras-team\/keras-tuner\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/keras-tuner) (\ud83d\udce5 660K \/ month \u00b7 \ud83d\udce6 38 \u00b7 \u23f1\ufe0f 25.03.2022):\n\t```\n\tpip install keras-tuner\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/keras-tuner) (\ud83d\udce5 6.1K \u00b7 \u23f1\ufe0f 12.01.2022):\n\t```\n\tconda install -c conda-forge keras-tuner\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/botorch\">BoTorch<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 2.3K) - Bayesian optimization in PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/botorch) (\ud83d\udc68\u200d\ud83d\udcbb 73 \u00b7 \ud83d\udd00 260 \u00b7 \ud83d\udce6 240 \u00b7 \ud83d\udccb 260 - 22% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/botorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/botorch) (\ud83d\udce5 170K \/ month \u00b7 \ud83d\udce6 13 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install botorch\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/botorch) (\ud83d\udce5 24K \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tconda install -c conda-forge botorch\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/alteryx\/featuretools\">featuretools<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 6.1K) - An open source python library for automated feature engineering. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/alteryx\/featuretools) (\ud83d\udc68\u200d\ud83d\udcbb 64 \u00b7 \ud83d\udd00 810 \u00b7 \ud83d\udccb 780 - 19% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/alteryx\/featuretools\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/featuretools) (\ud83d\udce5 130K \/ month \u00b7 \ud83d\udce6 63 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install featuretools\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/featuretools) (\ud83d\udce5 83K \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tconda install -c conda-forge featuretools\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebook\/Ax\">Ax<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 1.8K) - Adaptive Experimentation Platform. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebook\/Ax) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce6 270 \u00b7 \ud83d\udccb 420 - 11% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebook\/Ax\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ax-platform) (\ud83d\udce5 140K \/ month \u00b7 \ud83d\udce6 15 \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tpip install ax-platform\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ax-platform) (\ud83d\udce5 1.1K \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tconda install -c conda-forge ax-platform\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/awslabs\/autogluon\">AutoGluon<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 4.4K) - AutoGluon: AutoML for Image, Text, and Tabular Data. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/awslabs\/autogluon) (\ud83d\udc68\u200d\ud83d\udcbb 70 \u00b7 \ud83d\udd00 580 \u00b7 \ud83d\udce6 120 \u00b7 \ud83d\udccb 670 - 22% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/awslabs\/autogluon\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/autogluon) (\ud83d\udce5 67K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install autogluon\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/nevergrad\">nevergrad<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 3.3K) - A Python toolbox for performing gradient-free optimization. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/nevergrad) (\ud83d\udc68\u200d\ud83d\udcbb 48 \u00b7 \ud83d\udd00 310 \u00b7 \ud83d\udce6 320 \u00b7 \ud83d\udccb 260 - 41% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/nevergrad\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nevergrad) (\ud83d\udce5 34K \/ month \u00b7 \ud83d\udce6 18 \u00b7 \u23f1\ufe0f 08.03.2022):\n\t```\n\tpip install nevergrad\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/nevergrad) (\ud83d\udce5 25K \u00b7 \u23f1\ufe0f 14.06.2021):\n\t```\n\tconda install -c conda-forge nevergrad\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-optimize\/scikit-optimize\">scikit-optimize<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 2.3K \u00b7 \ud83d\udca4) - Sequential model-based optimization with a.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-optimize\/scikit-optimize) (\ud83d\udc68\u200d\ud83d\udcbb 76 \u00b7 \ud83d\udd00 420 \u00b7 \ud83d\udce6 2.6K \u00b7 \ud83d\udccb 620 - 37% open \u00b7 \u23f1\ufe0f 12.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-optimize\/scikit-optimize\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scikit-optimize) (\ud83d\udce5 840K \/ month \u00b7 \ud83d\udce6 170 \u00b7 \u23f1\ufe0f 12.10.2021):\n\t```\n\tpip install scikit-optimize\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/scikit-optimize) (\ud83d\udce5 540K \u00b7 \u23f1\ufe0f 15.12.2021):\n\t```\n\tconda install -c conda-forge scikit-optimize\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/autonomio\/talos\">Talos<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 1.5K) - Hyperparameter Optimization for TensorFlow, Keras and PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/autonomio\/talos) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 260 \u00b7 \ud83d\udce6 140 \u00b7 \ud83d\udccb 390 - 5% open \u00b7 \u23f1\ufe0f 23.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/autonomio\/talos\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/talos) (\ud83d\udce5 1.7K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tpip install talos\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mljar\/mljar-supervised\">mljar-supervised<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 1.9K) - Python package for AutoML on Tabular Data with Feature.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mljar\/mljar-supervised) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 270 \u00b7 \ud83d\udce6 45 \u00b7 \ud83d\udccb 470 - 17% open \u00b7 \u23f1\ufe0f 14.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mljar\/mljar-supervised\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mljar-supervised) (\ud83d\udce5 10K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 02.03.2022):\n\t```\n\tpip install mljar-supervised\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mljar-supervised) (\ud83d\udce5 1.4K \u00b7 \u23f1\ufe0f 02.03.2022):\n\t```\n\tconda install -c conda-forge mljar-supervised\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/maxpumperla\/hyperas\">Hyperas<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 2.1K) - Keras + Hyperopt: A very simple wrapper for convenient.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/maxpumperla\/hyperas) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udce6 230 \u00b7 \ud83d\udccb 250 - 37% open \u00b7 \u23f1\ufe0f 19.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/maxpumperla\/hyperas\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hyperas) (\ud83d\udce5 15K \/ month \u00b7 \ud83d\udce6 24 \u00b7 \u23f1\ufe0f 28.02.2019):\n\t```\n\tpip install hyperas\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/adanet\">AdaNet<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 3.4K \u00b7 \ud83d\udca4) - Fast and flexible AutoML with learning guarantees. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/adanet) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 520 \u00b7 \ud83d\udce6 43 \u00b7 \ud83d\udccb 110 - 57% open \u00b7 \u23f1\ufe0f 30.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/adanet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/adanet) (\ud83d\udce5 1.3K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 09.07.2020):\n\t```\n\tpip install adanet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Neuraxio\/Neuraxle\">Neuraxle<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 520) - The worlds cleanest AutoML framework - Do hyperparameter tuning with.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Neuraxio\/Neuraxle) (\ud83d\udc68\u200d\ud83d\udcbb 7 \u00b7 \ud83d\udd00 53 \u00b7 \ud83d\udce6 31 \u00b7 \ud83d\udccb 320 - 25% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Neuraxio\/Neuraxle\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/neuraxle) (\ud83d\udce5 460 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tpip install neuraxle\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/automl\/HpBandSter\">HpBandSter<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 540) - a distributed Hyperband implementation on Steroids. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/automl\/HpBandSter) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 220 \u00b7 \ud83d\udccb 92 - 61% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/automl\/HpBandSter\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hpbandster) (\ud83d\udce5 16K \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 06.11.2018):\n\t```\n\tpip install hpbandster\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/hpbandster) (\ud83d\udce5 1.3K \u00b7 \u23f1\ufe0f 11.12.2020):\n\t```\n\tconda install -c conda-forge hpbandster\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/SimonBlanke\/Hyperactive\">Hyperactive<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 380) - An optimization and data collection toolbox for convenient and fast.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/SimonBlanke\/Hyperactive) (\ud83d\udc68\u200d\ud83d\udcbb 4 \u00b7 \ud83d\udd00 32 \u00b7 \ud83d\udce5 100 \u00b7 \ud83d\udce6 13 \u00b7 \ud83d\udccb 44 - 11% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/SimonBlanke\/Hyperactive\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hyperactive) (\ud83d\udce5 410 \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install hyperactive\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/shankarpandala\/lazypredict\">lazypredict<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 350) - Lazy Predict help build a lot of basic models without much code.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/shankarpandala\/lazypredict) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 60 \u00b7 \ud83d\udce6 280 \u00b7 \ud83d\udccb 63 - 46% open \u00b7 \u23f1\ufe0f 29.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/shankarpandala\/lazypredict\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lazypredict) (\ud83d\udce5 7.9K \/ month \u00b7 \u23f1\ufe0f 17.02.2021):\n\t```\n\tpip install lazypredict\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/lazypredict) (\ud83d\udce5 360 \u00b7 \u23f1\ufe0f 24.08.2021):\n\t```\n\tconda install -c conda-forge lazypredict\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/AutoViML\/Auto_ViML\">Auto ViML<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 340) - Automatically Build Multiple ML Models with a Single Line of Code... <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/AutoViML\/Auto_ViML) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 77 \u00b7 \ud83d\udce6 16 \u00b7 \ud83d\udccb 20 - 25% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/AutoViML\/Auto_ViML\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/autoviml) (\ud83d\udce5 1.2K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install autoviml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rsteca\/sklearn-deap\">sklearn-deap<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 690 \u00b7 \ud83d\udca4) - Use evolutionary algorithms instead of gridsearch in.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rsteca\/sklearn-deap) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 31 \u00b7 \ud83d\udccb 55 - 38% open \u00b7 \u23f1\ufe0f 30.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/rsteca\/sklearn-deap\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sklearn-deap) (\ud83d\udce5 1.1K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 30.07.2021):\n\t```\n\tpip install sklearn-deap\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ScottfreeLLC\/AlphaPy\">AlphaPy<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 770) - Automated Machine Learning [AutoML] with Python, scikit-learn, Keras,.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ScottfreeLLC\/AlphaPy) (\ud83d\udc68\u200d\ud83d\udcbb 3 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udce6 3 \u00b7 \ud83d\udccb 41 - 29% open \u00b7 \u23f1\ufe0f 23.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ScottfreeLLC\/AlphaPy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/alphapy) (\ud83d\udce5 130 \/ month \u00b7 \u23f1\ufe0f 29.08.2020):\n\t```\n\tpip install alphapy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/model_search\">model_search<\/a><\/b> (\ud83e\udd4910 \u00b7  \u2b50 3.2K) - AutoML algorithms for model architecture search at scale. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/model_search) (\ud83d\udc68\u200d\ud83d\udcbb 1 \u00b7 \ud83d\udd00 360 \u00b7 \ud83d\udccb 50 - 70% open \u00b7 \u23f1\ufe0f 09.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/model_search\n\t```\n<\/details>\n<details><summary>Show 23 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/EpistasisLab\/tpot\">TPOT<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 8.6K \u00b7 \ud83d\udc80) - A Python Automated Machine Learning tool that optimizes.. <code><a href=\"http:\/\/bit.ly\/37RvQcA\">\u2757\ufe0fLGPL-3.0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/fmfn\/BayesianOptimization\">Bayesian Optimization<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 5.9K \u00b7 \ud83d\udc80) - A Python implementation of global optimization with.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Epistimio\/orion\">Orion<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 230) - Asynchronous Distributed Hyperparameter Optimization. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/SheffieldML\/GPyOpt\">GPyOpt<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 810 \u00b7 \ud83d\udc80) - Gaussian Process Optimization using GPy. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/automl\/SMAC3\">SMAC3<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 690) - Sequential Model-based Algorithm Configuration. <code><a href=\"https:\/\/tldrlegal.com\/search?q=BSD-1-Clause\">\u2757\ufe0fBSD-1-Clause<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/ClimbsRocks\/auto_ml\">auto_ml<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.6K \u00b7 \ud83d\udc80) - [UNMAINTAINED] Automated machine learning for analytics & production. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/AutoViML\/featurewiz\">featurewiz<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 230) - Use advanced feature engineering strategies and select best.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/AxeldeRomblay\/MLBox\">MLBox<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 1.3K \u00b7 \ud83d\udc80) - MLBox is a powerful Automated Machine Learning python library. <code><a href=\"https:\/\/tldrlegal.com\/search?q=BSD-1-Clause\">\u2757\ufe0fBSD-1-Clause<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/claesenm\/optunity\">optunity<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 380 \u00b7 \ud83d\udc80) - optimization routines for hyperparameter tuning. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/williamFalcon\/test-tube\">Test Tube<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 710 \u00b7 \ud83d\udc80) - Python library to easily log experiments and parallelize.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/dragonfly\/dragonfly\">Dragonfly<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 640 \u00b7 \ud83d\udc80) - An open source python library for scalable Bayesian optimisation. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/HDI-Project\/ATM\">Auto Tune Models<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 520 \u00b7 \ud83d\udc80) - Auto Tune Models - A multi-tenant, multi-data system for.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/sherpa-ai\/sherpa\">Sherpa<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 310 \u00b7 \ud83d\udc80) - Hyperparameter optimization that enables researchers to.. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/tobegit3hub\/advisor\">Advisor<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 1.4K \u00b7 \ud83d\udc80) - Open-source implementation of Google Vizier for hyper parameters.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/minimaxir\/automl-gs\">automl-gs<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 1.8K \u00b7 \ud83d\udc80) - Provide an input CSV and a target field to predict, generate a.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/reiinakano\/xcessiv\">Xcessiv<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 1.3K \u00b7 \ud83d\udc80) - A web-based application for quick, scalable, and automated.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/HunterMcGushion\/hyperparameter_hunter\">HyperparameterHunter<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 690 \u00b7 \ud83d\udc80) - Easy hyperparameter optimization and automatic result.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/jmcarpenter2\/parfit\">Parfit<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 200 \u00b7 \ud83d\udc80) - A package for parallelizing the fit and flexibly scoring of.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/carpedm20\/ENAS-pytorch\">ENAS<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 2.5K \u00b7 \ud83d\udc80) - PyTorch implementation of Efficient Neural Architecture Search via.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/LGE-ARC-AdvancedAI\/auptimizer\">Auptimizer<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 180 \u00b7 \ud83d\udc80) - An automatic ML model optimization tool. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/joeddav\/devol\">Devol<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 940 \u00b7 \ud83d\udc80) - Genetic neural architecture search with Keras. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/electricbrainio\/hypermax\">Hypermax<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 100 \u00b7 \ud83d\udc80) - Better, faster hyper-parameter optimization. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/gdikov\/hypertunity\">Hypertunity<\/a><\/b> (\ud83e\udd499 \u00b7  \u2b50 120 \u00b7 \ud83d\udc80) - A toolset for black-box hyperparameter optimisation. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n<\/details>\n<br>\n\n## Reinforcement Learning\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for building and evaluating reinforcement learning & agent-based systems._\n\n<details><summary><b><a href=\"https:\/\/github.com\/openai\/gym\">OpenAI Gym<\/a><\/b> (\ud83e\udd4741 \u00b7  \u2b50 27K) - A toolkit for developing and comparing reinforcement learning.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/openai\/gym) (\ud83d\udc68\u200d\ud83d\udcbb 360 \u00b7 \ud83d\udd00 7.8K \u00b7 \ud83d\udce6 29K \u00b7 \ud83d\udccb 1.6K - 6% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/openai\/gym\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gym) (\ud83d\udce5 430K \/ month \u00b7 \ud83d\udce6 2.4K \u00b7 \u23f1\ufe0f 14.03.2022):\n\t```\n\tpip install gym\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/gym) (\ud83d\udce5 100K \u00b7 \u23f1\ufe0f 08.02.2022):\n\t```\n\tconda install -c conda-forge gym\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/agents\">TF-Agents<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 2.3K) - TF-Agents: A reliable, scalable and easy to use TensorFlow.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/agents) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 620 \u00b7 \ud83d\udce6 790 \u00b7 \ud83d\udccb 560 - 23% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/agents\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tf-agents) (\ud83d\udce5 220K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 20.04.2022):\n\t```\n\tpip install tf-agents\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/AI4Finance-Foundation\/FinRL\">FinRL<\/a><\/b> (\ud83e\udd4730 \u00b7  \u2b50 4.8K) - FinRL: The first open-source project for financial reinforcement learning... <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/AI4Finance-Foundation\/FinRL) (\ud83d\udc68\u200d\ud83d\udcbb 62 \u00b7 \ud83d\udd00 1.1K \u00b7 \ud83d\udce6 11 \u00b7 \ud83d\udccb 400 - 12% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/AI4Finance-Foundation\/FinRL\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/finrl) (\ud83d\udce5 280 \/ month \u00b7 \u23f1\ufe0f 08.01.2022):\n\t```\n\tpip install finrl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/dopamine\">Dopamine<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 9.8K) - Dopamine is a research framework for fast prototyping of.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/dopamine) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udccb 160 - 49% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/dopamine\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dopamine-rl) (\ud83d\udce5 43K \/ month \u00b7 \ud83d\udce6 37 \u00b7 \u23f1\ufe0f 13.12.2021):\n\t```\n\tpip install dopamine-rl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepmind\/acme\">Acme<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 2.6K) - A library of reinforcement learning components and agents. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepmind\/acme) (\ud83d\udc68\u200d\ud83d\udcbb 66 \u00b7 \ud83d\udd00 320 \u00b7 \ud83d\udce6 74 \u00b7 \ud83d\udccb 200 - 20% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepmind\/acme\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dm-acme) (\ud83d\udce5 4.6K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 10.02.2022):\n\t```\n\tpip install dm-acme\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dm-acme) (\ud83d\udce5 2.2K \u00b7 \u23f1\ufe0f 09.12.2021):\n\t```\n\tconda install -c conda-forge dm-acme\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mwydmuch\/ViZDoom\">ViZDoom<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 1.4K) - Doom-based AI Research Platform for Reinforcement Learning from Raw.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mwydmuch\/ViZDoom) (\ud83d\udc68\u200d\ud83d\udcbb 47 \u00b7 \ud83d\udd00 340 \u00b7 \ud83d\udce5 12K \u00b7 \ud83d\udce6 130 \u00b7 \ud83d\udccb 430 - 19% open \u00b7 \u23f1\ufe0f 17.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mwydmuch\/ViZDoom\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/vizdoom) (\ud83d\udce5 1.3K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 18.04.2022):\n\t```\n\tpip install vizdoom\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorlayer\/TensorLayer\">TensorLayer<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 7K) - Deep Learning and Reinforcement Learning Library for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorlayer\/TensorLayer) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udce5 1.4K \u00b7 \ud83d\udccb 470 - 5% open \u00b7 \u23f1\ufe0f 23.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorlayer\/tensorlayer\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorlayer) (\ud83d\udce5 2.8K \/ month \u00b7 \ud83d\udce6 40 \u00b7 \u23f1\ufe0f 15.02.2022):\n\t```\n\tpip install tensorlayer\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PaddlePaddle\/PARL\">PARL<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 2.6K) - A high-performance distributed training framework for Reinforcement.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PaddlePaddle\/PARL) (\ud83d\udc68\u200d\ud83d\udcbb 30 \u00b7 \ud83d\udd00 690 \u00b7 \ud83d\udce6 89 \u00b7 \ud83d\udccb 360 - 20% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PaddlePaddle\/PARL\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/parl) (\ud83d\udce5 820 \/ month \u00b7 \u23f1\ufe0f 30.12.2021):\n\t```\n\tpip install parl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/hill-a\/stable-baselines\">Stable Baselines<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 3.5K \u00b7 \ud83d\udca4) - A fork of OpenAI Baselines, implementations of.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/hill-a\/stable-baselines) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 690 \u00b7 \ud83d\udccb 940 - 13% open \u00b7 \u23f1\ufe0f 25.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/hill-a\/stable-baselines\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/stable-baselines) (\ud83d\udce5 9.6K \/ month \u00b7 \ud83d\udce6 34 \u00b7 \u23f1\ufe0f 06.04.2021):\n\t```\n\tpip install stable-baselines\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorforce\/tensorforce\">TensorForce<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 3.1K) - Tensorforce: a TensorFlow library for applied.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorforce\/tensorforce) (\ud83d\udc68\u200d\ud83d\udcbb 82 \u00b7 \ud83d\udd00 510 \u00b7 \ud83d\udccb 640 - 2% open \u00b7 \u23f1\ufe0f 10.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorforce\/tensorforce\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorforce) (\ud83d\udce5 1.3K \/ month \u00b7 \ud83d\udce6 26 \u00b7 \u23f1\ufe0f 07.09.2019):\n\t```\n\tpip install tensorforce\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rlworkgroup\/garage\">garage<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.4K) - A toolkit for reproducible reinforcement learning research. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rlworkgroup\/garage) (\ud83d\udc68\u200d\ud83d\udcbb 78 \u00b7 \ud83d\udd00 260 \u00b7 \ud83d\udce6 39 \u00b7 \ud83d\udccb 1K - 20% open \u00b7 \u23f1\ufe0f 17.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rlworkgroup\/garage\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/garage) (\ud83d\udce5 380 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 23.03.2021):\n\t```\n\tpip install garage\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepmind\/rlax\">RLax<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 790) - A library of reinforcement learning building blocks in JAX. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepmind\/rlax) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 61 \u00b7 \ud83d\udce6 47 \u00b7 \ud83d\udccb 24 - 50% open \u00b7 \u23f1\ufe0f 12.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepmind\/rlax\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/rlax) (\ud83d\udce5 4.7K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 24.02.2022):\n\t```\n\tpip install rlax\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepmind\/trfl\">TRFL<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 3.1K \u00b7 \ud83d\udca4) - TensorFlow Reinforcement Learning. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepmind\/trfl) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 380 \u00b7 \ud83d\udce6 78 \u00b7 \ud83d\udccb 22 - 27% open \u00b7 \u23f1\ufe0f 16.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/deepmind\/trfl\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/trfl) (\ud83d\udce5 5.5K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 16.08.2021):\n\t```\n\tpip install trfl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pfnet\/pfrl\">PFRL<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 830) - PFRL: a PyTorch-based deep reinforcement learning library. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pfnet\/pfrl) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 41 \u00b7 \ud83d\udccb 66 - 40% open \u00b7 \u23f1\ufe0f 14.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pfnet\/pfrl\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pfrl) (\ud83d\udce5 3.2K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 07.07.2021):\n\t```\n\tpip install pfrl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/ReAgent\">ReAgent<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 3.2K) - A platform for Reasoning systems (Reinforcement Learning,.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/ReAgent) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 440 \u00b7 \ud83d\udccb 120 - 35% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/ReAgent\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/reagent) (\ud83d\udce5 20 \/ month \u00b7 \u23f1\ufe0f 27.05.2020):\n\t```\n\tpip install reagent\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/IntelLabs\/coach\">Coach<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 2.1K \u00b7 \ud83d\udca4) - Reinforcement Learning Coach by Intel AI Lab enables easy.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/IntelLabs\/coach) (\ud83d\udc68\u200d\ud83d\udcbb 35 \u00b7 \ud83d\udd00 420 \u00b7 \ud83d\udccb 270 - 32% open \u00b7 \u23f1\ufe0f 28.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/IntelLabs\/coach\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/rl_coach) (\ud83d\udce5 120 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 10.10.2019):\n\t```\n\tpip install rl_coach\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google-research\/rliable\">rliable<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 400) - [NeurIPS21 Outstanding Paper] Library for reliable evaluation on RL.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google-research\/rliable) (\ud83d\udc68\u200d\ud83d\udcbb 2 \u00b7 \ud83d\udd00 22 \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 18.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google-research\/rliable\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/rliable`):\n\t```\n\tpip install rliable`\n\t```\n<\/details>\n<details><summary>Show 6 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/openai\/baselines\">baselines<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 13K \u00b7 \ud83d\udc80) - OpenAI Baselines: high-quality implementations of reinforcement.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/keras-rl\/keras-rl\">keras-rl<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 5.3K \u00b7 \ud83d\udc80) - Deep Reinforcement Learning for Keras. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/chainer\/chainerrl\">ChainerRL<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1K \u00b7 \ud83d\udc80) - ChainerRL is a deep reinforcement learning library built on top of.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/deepmind\/lab\">DeepMind Lab<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 6.7K) - A customisable 3D platform for agent-based AI research. <code><a href=\"http:\/\/bit.ly\/2KucAZR\">\u2757\ufe0fGPL-2.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/SerpentAI\/SerpentAI\">SerpentAI<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 6.2K \u00b7 \ud83d\udc80) - Game Agent Framework. Helping you create AIs \/ Bots that learn to.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/enlite-ai\/maze\">Maze<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 210) - Maze Applied Reinforcement Learning Framework. <code><a href=\"https:\/\/tldrlegal.com\/search?q=Custom\">\u2757\ufe0fCustom<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Recommender Systems\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for building and evaluating recommendation systems._\n\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/recommenders\">Recommenders<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 13K) - Best Practices on Recommendation Systems. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/recommenders) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 2.3K \u00b7 \ud83d\udce5 160 \u00b7 \ud83d\udce6 23 \u00b7 \ud83d\udccb 680 - 20% open \u00b7 \u23f1\ufe0f 31.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/recommenders\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/recommenders) (\ud83d\udce5 96K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 01.04.2022):\n\t```\n\tpip install recommenders\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/benfred\/implicit\">implicit<\/a><\/b> (\ud83e\udd4730 \u00b7  \u2b50 2.8K) - Fast Python Collaborative Filtering for Implicit Feedback Datasets. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/benfred\/implicit) (\ud83d\udc68\u200d\ud83d\udcbb 30 \u00b7 \ud83d\udd00 530 \u00b7 \ud83d\udce6 590 \u00b7 \ud83d\udccb 400 - 17% open \u00b7 \u23f1\ufe0f 12.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/benfred\/implicit\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/implicit) (\ud83d\udce5 140K \/ month \u00b7 \ud83d\udce6 31 \u00b7 \u23f1\ufe0f 29.01.2022):\n\t```\n\tpip install implicit\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/implicit) (\ud83d\udce5 340K \u00b7 \u23f1\ufe0f 29.01.2022):\n\t```\n\tconda install -c conda-forge implicit\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lyst\/lightfm\">lightfm<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 4K) - A Python implementation of LightFM, a hybrid recommendation algorithm. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lyst\/lightfm) (\ud83d\udc68\u200d\ud83d\udcbb 44 \u00b7 \ud83d\udd00 620 \u00b7 \ud83d\udce6 720 \u00b7 \ud83d\udccb 440 - 22% open \u00b7 \u23f1\ufe0f 09.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lyst\/lightfm\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lightfm) (\ud83d\udce5 390K \/ month \u00b7 \ud83d\udce6 45 \u00b7 \u23f1\ufe0f 27.11.2020):\n\t```\n\tpip install lightfm\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/lightfm) (\ud83d\udce5 120K \u00b7 \u23f1\ufe0f 09.03.2022):\n\t```\n\tconda install -c conda-forge lightfm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/recommenders\">TF Recommenders<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 1.3K) - TensorFlow Recommenders is a library for building.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/recommenders) (\ud83d\udc68\u200d\ud83d\udcbb 32 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 270 - 55% open \u00b7 \u23f1\ufe0f 30.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/recommenders\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-recommenders) (\ud83d\udce5 1M \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 23.08.2021):\n\t```\n\tpip install tensorflow-recommenders\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/ranking\">TF Ranking<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 2.5K) - Learning to Rank in TensorFlow. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/ranking) (\ud83d\udc68\u200d\ud83d\udcbb 28 \u00b7 \ud83d\udd00 430 \u00b7 \ud83d\udccb 290 - 17% open \u00b7 \u23f1\ufe0f 26.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/ranking\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow_ranking) (\ud83d\udce5 47K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 16.11.2021):\n\t```\n\tpip install tensorflow_ranking\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/RUCAIBox\/RecBole\">RecBole<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 1.8K) - A unified, comprehensive and efficient recommendation library. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/RUCAIBox\/RecBole) (\ud83d\udc68\u200d\ud83d\udcbb 45 \u00b7 \ud83d\udd00 340 \u00b7 \ud83d\udccb 390 - 14% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/RUCAIBox\/RecBole\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/recbole) (\ud83d\udce5 3K \/ month \u00b7 \u23f1\ufe0f 25.02.2022):\n\t```\n\tpip install recbole\n\t```\n- [Conda](https:\/\/anaconda.org\/aibox\/recbole) (\ud83d\udce5 1.5K \u00b7 \u23f1\ufe0f 25.02.2022):\n\t```\n\tconda install -c aibox recbole\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PreferredAI\/cornac\">Cornac<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 570) - A Comparative Framework for Multimodal Recommender Systems. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PreferredAI\/cornac) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 92 \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 92 - 4% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PreferredAI\/cornac\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cornac) (\ud83d\udce5 100K \/ month \u00b7 \ud83d\udce6 15 \u00b7 \u23f1\ufe0f 19.02.2022):\n\t```\n\tpip install cornac\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/cornac) (\ud83d\udce5 210K \u00b7 \u23f1\ufe0f 19.02.2022):\n\t```\n\tconda install -c conda-forge cornac\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/statisticianinstilettos\/recmetrics\">recmetrics<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 390) - A library of metrics for evaluating recommender systems. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/statisticianinstilettos\/recmetrics) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 85 \u00b7 \ud83d\udce6 25 \u00b7 \ud83d\udccb 21 - 42% open \u00b7 \u23f1\ufe0f 17.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/statisticianinstilettos\/recmetrics\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/recmetrics) (\ud83d\udce5 1.1K \/ month \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tpip install recmetrics\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/caserec\/CaseRecommender\">Case Recommender<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 400) - Case Recommender: A Flexible and Extensible Python.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/caserec\/CaseRecommender) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 78 \u00b7 \ud83d\udce6 10 \u00b7 \ud83d\udccb 27 - 25% open \u00b7 \u23f1\ufe0f 25.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/caserec\/CaseRecommender\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/caserecommender) (\ud83d\udce5 340 \/ month \u00b7 \u23f1\ufe0f 25.11.2021):\n\t```\n\tpip install caserecommender\n\t```\n<\/details>\n<details><summary>Show 7 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/NicolasHug\/Surprise\">scikit-surprise<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 5.4K \u00b7 \ud83d\udc80) - A Python scikit for building and analyzing recommender.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/jfkirk\/tensorrec\">tensorrec<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 1.2K \u00b7 \ud83d\udc80) - A TensorFlow recommendation algorithm and framework in.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/lenskit\/lkpy\">lkpy<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 200) - Python recommendation toolkit. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/ibayer\/fastFM\">fastFM<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 980 \u00b7 \ud83d\udc80) - fastFM: A Library for Factorization Machines. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/maciejkula\/spotlight\">Spotlight<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 2.7K \u00b7 \ud83d\udc80) - Deep recommender models using PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/ShopRunner\/collie\">Collie<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 88) - A library for preparing, training, and evaluating scalable deep.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/ylongqi\/openrec\">OpenRec<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 390 \u00b7 \ud83d\udc80) - OpenRec is an open-source and modular library for neural network-.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n<\/details>\n<br>\n\n## Privacy Machine Learning\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for encrypted and privacy-preserving machine learning using methods like federated learning & differential privacy._\n\n<details><summary><b><a href=\"https:\/\/github.com\/OpenMined\/PySyft\">PySyft<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 8.1K) - A library for answering questions using data you cannot see. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/OpenMined\/PySyft) (\ud83d\udc68\u200d\ud83d\udcbb 440 \u00b7 \ud83d\udd00 1.8K \u00b7 \ud83d\udccb 3.1K - 10% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/OpenMined\/PySyft\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/syft) (\ud83d\udce5 4.6K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tpip install syft\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/opacus\">Opacus<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 1.1K) - Training PyTorch models with differential privacy. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/opacus) (\ud83d\udc68\u200d\ud83d\udcbb 48 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udce5 45 \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 180 - 23% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/opacus\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/opacus) (\ud83d\udce5 4.7K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tpip install opacus\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/opacus) (\ud83d\udce5 490 \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tconda install -c conda-forge opacus\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/FederatedAI\/FATE\">FATE<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 4.2K) - An Industrial Grade Federated Learning Framework. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/FederatedAI\/FATE) (\ud83d\udc68\u200d\ud83d\udcbb 74 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udccb 1.2K - 32% open \u00b7 \u23f1\ufe0f 15.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/FederatedAI\/FATE\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ETAF) (\ud83d\udce5 2 \/ month \u00b7 \u23f1\ufe0f 06.05.2020):\n\t```\n\tpip install ETAF\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/privacy\">TensorFlow Privacy<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 1.6K) - Library for training machine learning models with.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/privacy) (\ud83d\udc68\u200d\ud83d\udcbb 46 \u00b7 \ud83d\udd00 350 \u00b7 \ud83d\udce5 70 \u00b7 \ud83d\udccb 150 - 43% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/privacy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-privacy) (\ud83d\udce5 32K \/ month \u00b7 \ud83d\udce6 7 \u00b7 \u23f1\ufe0f 22.02.2022):\n\t```\n\tpip install tensorflow-privacy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tf-encrypted\/tf-encrypted\">TFEncrypted<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 1K) - A Framework for Encrypted Machine Learning in TensorFlow. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tf-encrypted\/tf-encrypted) (\ud83d\udc68\u200d\ud83d\udcbb 28 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce6 61 \u00b7 \ud83d\udccb 410 - 40% open \u00b7 \u23f1\ufe0f 10.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tf-encrypted\/tf-encrypted\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tf-encrypted) (\ud83d\udce5 740 \/ month \u00b7 \ud83d\udce6 9 \u00b7 \u23f1\ufe0f 07.03.2022):\n\t```\n\tpip install tf-encrypted\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/CrypTen\">CrypTen<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.1K) - A framework for Privacy Preserving Machine Learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/CrypTen) (\ud83d\udc68\u200d\ud83d\udcbb 28 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 18 \u00b7 \ud83d\udccb 140 - 16% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/CrypTen\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/crypten) (\ud83d\udce5 250 \/ month \u00b7 \u23f1\ufe0f 09.09.2021):\n\t```\n\tpip install crypten\n\t```\n<\/details>\n<br>\n\n## Workflow & Experiment Tracking\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries to organize, track, and visualize machine learning experiments._\n\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/tensorboard\">Tensorboard<\/a><\/b> (\ud83e\udd4743 \u00b7  \u2b50 5.8K) - TensorFlows Visualization Toolkit. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/tensorboard) (\ud83d\udc68\u200d\ud83d\udcbb 280 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce6 110K \u00b7 \ud83d\udccb 1.7K - 33% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/tensorboard\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorboard) (\ud83d\udce5 14M \/ month \u00b7 \ud83d\udce6 2.4K \u00b7 \u23f1\ufe0f 20.01.2022):\n\t```\n\tpip install tensorboard\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorboard) (\ud83d\udce5 2.9M \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c conda-forge tensorboard\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mlflow\/mlflow\">mlflow<\/a><\/b> (\ud83e\udd4741 \u00b7  \u2b50 12K) - Open source platform for the machine learning lifecycle. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mlflow\/mlflow) (\ud83d\udc68\u200d\ud83d\udcbb 400 \u00b7 \ud83d\udd00 2.6K \u00b7 \ud83d\udccb 2.4K - 43% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mlflow\/mlflow\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mlflow) (\ud83d\udce5 10M \/ month \u00b7 \ud83d\udce6 300 \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tpip install mlflow\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mlflow) (\ud83d\udce5 550K \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tconda install -c conda-forge mlflow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pycaret\/pycaret\">PyCaret<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 5.6K) - An open-source, low-code machine learning library in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pycaret\/pycaret) (\ud83d\udc68\u200d\ud83d\udcbb 78 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udce5 560 \u00b7 \ud83d\udce6 2.1K \u00b7 \ud83d\udccb 1.5K - 16% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pycaret\/pycaret\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pycaret) (\ud83d\udce5 480K \/ month \u00b7 \ud83d\udce6 12 \u00b7 \u23f1\ufe0f 10.04.2022):\n\t```\n\tpip install pycaret\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pycaret) (\ud83d\udce5 9.1K \u00b7 \u23f1\ufe0f 18.04.2022):\n\t```\n\tconda install -c conda-forge pycaret\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\">SageMaker SDK<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 1.6K) - A library for training and deploying machine learning.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/aws\/sagemaker-python-sdk) (\ud83d\udc68\u200d\ud83d\udcbb 260 \u00b7 \ud83d\udd00 770 \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 1K - 34% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/aws\/sagemaker-python-sdk\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sagemaker) (\ud83d\udce5 5.9M \/ month \u00b7 \ud83d\udce6 48 \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tpip install sagemaker\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sagemaker-python-sdk) (\ud83d\udce5 250K \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tconda install -c conda-forge sagemaker-python-sdk\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/iterative\/dvc\">DVC<\/a><\/b> (\ud83e\udd4836 \u00b7  \u2b50 9.7K) - Data Version Control | Git for Data & Models | ML Experiments Management. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/iterative\/dvc) (\ud83d\udc68\u200d\ud83d\udcbb 270 \u00b7 \ud83d\udd00 930 \u00b7 \ud83d\udce5 78K \u00b7 \ud83d\udccb 3.7K - 17% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/iterative\/dvc\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dvc) (\ud83d\udce5 480K \/ month \u00b7 \ud83d\udce6 47 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install dvc\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dvc) (\ud83d\udce5 1M \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tconda install -c conda-forge dvc\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/wandb\/client\">wandb client<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 3.9K) - A tool for visualizing and tracking your machine learning.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/wandb\/client) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 310 \u00b7 \ud83d\udccb 1.8K - 24% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/wandb\/client\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/wandb) (\ud83d\udce5 1.7M \/ month \u00b7 \ud83d\udce6 240 \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install wandb\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/wandb) (\ud83d\udce5 52K \u00b7 \u23f1\ufe0f 11.04.2022):\n\t```\n\tconda install -c conda-forge wandb\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\">AzureML SDK<\/a><\/b> (\ud83e\udd4835 \u00b7  \u2b50 3K) - Python notebooks with ML and deep learning examples with Azure Machine.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Azure\/MachineLearningNotebooks) (\ud83d\udc68\u200d\ud83d\udcbb 60 \u00b7 \ud83d\udd00 2.1K \u00b7 \ud83d\udce5 460 \u00b7 \ud83d\udccb 1.3K - 21% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Azure\/MachineLearningNotebooks\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/azureml-sdk) (\ud83d\udce5 780K \/ month \u00b7 \ud83d\udce6 45 \u00b7 \u23f1\ufe0f 25.04.2022):\n\t```\n\tpip install azureml-sdk\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/snakemake\/snakemake\">snakemake<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 1.3K \u00b7 \ud83d\udcc8) - This is the development home of the workflow management system.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/snakemake\/snakemake) (\ud83d\udc68\u200d\ud83d\udcbb 250 \u00b7 \ud83d\udd00 320 \u00b7 \ud83d\udce6 1.1K \u00b7 \ud83d\udccb 980 - 59% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/snakemake\/snakemake\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/snakemake) (\ud83d\udce5 40K \/ month \u00b7 \ud83d\udce6 210 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install snakemake\n\t```\n- [Conda](https:\/\/anaconda.org\/bioconda\/snakemake) (\ud83d\udce5 420K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c bioconda snakemake\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lanpa\/tensorboardX\">tensorboardX<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 7.3K) - tensorboard for pytorch (and chainer, mxnet, numpy, ...). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lanpa\/tensorboardX) (\ud83d\udc68\u200d\ud83d\udcbb 70 \u00b7 \ud83d\udd00 850 \u00b7 \ud83d\udce5 350 \u00b7 \ud83d\udce6 19K \u00b7 \ud83d\udccb 440 - 16% open \u00b7 \u23f1\ufe0f 19.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lanpa\/tensorboardX\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorboardX) (\ud83d\udce5 1.6M \/ month \u00b7 \ud83d\udce6 880 \u00b7 \u23f1\ufe0f 22.02.2022):\n\t```\n\tpip install tensorboardX\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorboardx) (\ud83d\udce5 680K \u00b7 \u23f1\ufe0f 23.02.2022):\n\t```\n\tconda install -c conda-forge tensorboardx\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/allegroai\/clearml\">ClearML<\/a><\/b> (\ud83e\udd4833 \u00b7  \u2b50 3.1K) - ClearML - Auto-Magical CI\/CD to streamline your ML workflow... <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/allegroai\/clearml) (\ud83d\udc68\u200d\ud83d\udcbb 49 \u00b7 \ud83d\udd00 430 \u00b7 \ud83d\udce5 430 \u00b7 \ud83d\udce6 240 \u00b7 \ud83d\udccb 520 - 40% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/allegroai\/clearml\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/clearml) (\ud83d\udce5 110K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tpip install clearml\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/allegroai\/trains) (\ud83d\udce5 30K \u00b7 \u23f1\ufe0f 05.10.2020):\n\t```\n\tdocker pull allegroai\/trains\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/catalyst-team\/catalyst\">Catalyst<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 2.9K) - Accelerated deep learning R&D. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/catalyst-team\/catalyst) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 360 \u00b7 \ud83d\udce6 540 \u00b7 \ud83d\udccb 330 - 0% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/catalyst-team\/catalyst\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/catalyst) (\ud83d\udce5 910K \/ month \u00b7 \ud83d\udce6 29 \u00b7 \u23f1\ufe0f 29.04.2022):\n\t```\n\tpip install catalyst\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Netflix\/metaflow\">Metaflow<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 5.6K) - Build and manage real-life data science projects with ease!. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Netflix\/metaflow) (\ud83d\udc68\u200d\ud83d\udcbb 52 \u00b7 \ud83d\udd00 500 \u00b7 \ud83d\udce6 280 \u00b7 \ud83d\udccb 440 - 49% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Netflix\/metaflow\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/metaflow) (\ud83d\udce5 50K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 25.04.2022):\n\t```\n\tpip install metaflow\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/metaflow) (\ud83d\udce5 36K \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tconda install -c conda-forge metaflow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/IDSIA\/sacred\">sacred<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 3.8K) - Sacred is a tool to help you configure, organize, log and reproduce.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/IDSIA\/sacred) (\ud83d\udc68\u200d\ud83d\udcbb 97 \u00b7 \ud83d\udd00 340 \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 540 - 16% open \u00b7 \u23f1\ufe0f 28.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/IDSIA\/sacred\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sacred) (\ud83d\udce5 24K \/ month \u00b7 \ud83d\udce6 100 \u00b7 \u23f1\ufe0f 14.12.2020):\n\t```\n\tpip install sacred\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sacred) (\ud83d\udce5 320 \u00b7 \u23f1\ufe0f 14.11.2021):\n\t```\n\tconda install -c conda-forge sacred\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/aimhubio\/aim\">aim<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 2.4K) - Aim easy-to-use and performant open-source ML experiment tracker. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/aimhubio\/aim) (\ud83d\udc68\u200d\ud83d\udcbb 32 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 75 \u00b7 \ud83d\udccb 520 - 34% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/aimhubio\/aim\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/aim) (\ud83d\udce5 35K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install aim\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/aim) (\ud83d\udce5 11K \u00b7 \u23f1\ufe0f 15.10.2021):\n\t```\n\tconda install -c conda-forge aim\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PaddlePaddle\/VisualDL\">VisualDL<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 4.3K) - Deep Learning Visualization Toolkit. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PaddlePaddle\/VisualDL) (\ud83d\udc68\u200d\ud83d\udcbb 31 \u00b7 \ud83d\udd00 580 \u00b7 \ud83d\udce5 180 \u00b7 \ud83d\udce6 1.1K \u00b7 \ud83d\udccb 410 - 18% open \u00b7 \u23f1\ufe0f 01.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PaddlePaddle\/VisualDL\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/visualdl) (\ud83d\udce5 74K \/ month \u00b7 \ud83d\udce6 23 \u00b7 \u23f1\ufe0f 06.01.2022):\n\t```\n\tpip install visualdl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/ml-metadata\">ml-metadata<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 470) - For recording and retrieving metadata associated with ML.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/ml-metadata) (\ud83d\udc68\u200d\ud83d\udcbb 15 \u00b7 \ud83d\udd00 90 \u00b7 \ud83d\udce5 1.7K \u00b7 \ud83d\udce6 210 \u00b7 \ud83d\udccb 85 - 27% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/ml-metadata\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ml-metadata) (\ud83d\udce5 670K \/ month \u00b7 \ud83d\udce6 18 \u00b7 \u23f1\ufe0f 28.02.2022):\n\t```\n\tpip install ml-metadata\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/stared\/livelossplot\">livelossplot<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.2K) - Live training loss plot in Jupyter Notebook for Keras,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/stared\/livelossplot) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 780 \u00b7 \ud83d\udccb 74 - 5% open \u00b7 \u23f1\ufe0f 04.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/stared\/livelossplot\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/livelossplot) (\ud83d\udce5 67K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 04.04.2022):\n\t```\n\tpip install livelossplot\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/guildai\/guildai\">Guild AI<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 700) - Experiment tracking, ML developer tools. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/guildai\/guildai) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 62 \u00b7 \ud83d\udce6 51 \u00b7 \ud83d\udccb 350 - 44% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/guildai\/guildai\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/guildai) (\ud83d\udce5 3.4K \/ month \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install guildai\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/labmlai\/labml\">Labml<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.1K) - Monitor deep learning model training and hardware usage from your mobile.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/labmlai\/labml) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 75 \u00b7 \ud83d\udce6 46 \u00b7 \ud83d\udccb 28 - 57% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/labmlai\/labml\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/labml) (\ud83d\udce5 3.9K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tpip install labml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/studioml\/studio\">Studio.ml<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 370 \u00b7 \ud83d\udca4) - Studio: Simplify and expedite model building process. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/studioml\/studio) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 52 \u00b7 \ud83d\udce6 5 \u00b7 \ud83d\udccb 250 - 22% open \u00b7 \u23f1\ufe0f 14.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/studioml\/studio\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/studioml) (\ud83d\udce5 440 \/ month \u00b7 \u23f1\ufe0f 14.09.2021):\n\t```\n\tpip install studioml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/instacart\/lore\">lore<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1.5K \u00b7 \ud83d\udcc9) - Lore makes machine learning approachable for Software Engineers and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/instacart\/lore) (\ud83d\udc68\u200d\ud83d\udcbb 26 \u00b7 \ud83d\udd00 130 \u00b7 \ud83d\udce6 19 \u00b7 \ud83d\udccb 45 - 57% open \u00b7 \u23f1\ufe0f 11.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/instacart\/lore\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lore) (\ud83d\udce5 130 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 02.02.2022):\n\t```\n\tpip install lore\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/replicate\/keepsake\">keepsake<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 1.6K) - Version control for machine learning. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/replicate\/keepsake) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 62 \u00b7 \ud83d\udccb 190 - 65% open \u00b7 \u23f1\ufe0f 15.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/replicate\/keepsake\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/keepsake) (\ud83d\udce5 970 \/ month \u00b7 \u23f1\ufe0f 11.03.2021):\n\t```\n\tpip install keepsake\n\t```\n<\/details>\n<details><summary>Show 17 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/neptune-ai\/neptune-client\">Neptune.ai<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 270) - Experiment tracking tool and model registry. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Kaggle\/kaggle-api\">kaggle<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 4.7K \u00b7 \ud83d\udc80) - Official Kaggle API. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/huggingface\/knockknock\">knockknock<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 2.4K \u00b7 \ud83d\udc80) - Knock Knock: Get notified when your training ends with only two.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/pytorch\/tnt\">TNT<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 1.4K \u00b7 \ud83d\udc80) - Simple tools for logging and visualizing, loading and training. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/EducationalTestingService\/skll\">SKLL<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 530) - SciKit-Learn Laboratory (SKLL) makes it easy to run machine.. <code><a href=\"https:\/\/tldrlegal.com\/search?q=BSD-1-Clause\">\u2757\ufe0fBSD-1-Clause<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/m3dev\/gokart\">gokart<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 250) - Gokart solves reproducibility, task dependencies, constraints of good code,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/microsoft\/tensorwatch\">TensorWatch<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 3.2K \u00b7 \ud83d\udc80) - Debugging, monitoring and visualization for Python Machine.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/waleedka\/hiddenlayer\">hiddenlayer<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 1.6K \u00b7 \ud83d\udc80) - Neural network graphs and training metrics for.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/MrPowers\/quinn\">quinn<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 330 \u00b7 \ud83d\udc80) - pyspark methods to enhance developer productivity. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/TeamHG-Memex\/tensorboard_logger\">TensorBoard Logger<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 620 \u00b7 \ud83d\udc80) - Log TensorBoard events without touching TensorFlow. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/awslabs\/mxboard\">MXBoard<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 330 \u00b7 \ud83d\udc80) - Logging MXNet data for visualization in TensorBoard. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1X\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/datmo\/datmo\">datmo<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 340 \u00b7 \ud83d\udc80) - Open source production model management tool for data scientists. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/gradsflow\/chitra\">chitra<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 190) - A multi-functional library for full-stack Deep Learning. Simplifies.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/minerva-ml\/steppy\">steppy<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 130 \u00b7 \ud83d\udc80) - Lightweight, Python library for fast and reproducible experimentation. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/google\/caliban\">caliban<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 420 \u00b7 \ud83d\udc80) - Research workflows made easy, locally and in the Cloud. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/ModelChimp\/modelchimp\">ModelChimp<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 120 \u00b7 \ud83d\udca4) - Experiment tracking for machine and deep learning projects. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/jrieke\/traintool\">traintool<\/a><\/b> (\ud83e\udd498 \u00b7  \u2b50 9 \u00b7 \ud83d\udc80) - Train off-the-shelf machine learning models in one.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Model Serialization & Deployment\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries to serialize models to files, convert between a variety of model formats, and optimize models for deployment._\n\n<details><summary><b><a href=\"https:\/\/github.com\/onnx\/onnx\">onnx<\/a><\/b> (\ud83e\udd4741 \u00b7  \u2b50 12K) - Open standard for machine learning interoperability. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/onnx\/onnx) (\ud83d\udc68\u200d\ud83d\udcbb 230 \u00b7 \ud83d\udd00 2.7K \u00b7 \ud83d\udce5 18K \u00b7 \ud83d\udce6 6.6K \u00b7 \ud83d\udccb 2K - 13% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/onnx\/onnx\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/onnx) (\ud83d\udce5 990K \/ month \u00b7 \ud83d\udce6 370 \u00b7 \u23f1\ufe0f 17.02.2022):\n\t```\n\tpip install onnx\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/onnx) (\ud83d\udce5 390K \u00b7 \u23f1\ufe0f 24.04.2022):\n\t```\n\tconda install -c conda-forge onnx\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/apple\/coremltools\">Core ML Tools<\/a><\/b> (\ud83e\udd4732 \u00b7  \u2b50 2.6K) - Core ML tools contain supporting tools for Core ML model.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/apple\/coremltools) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 400 \u00b7 \ud83d\udce5 4.2K \u00b7 \ud83d\udce6 890 \u00b7 \ud83d\udccb 1K - 33% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/apple\/coremltools\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/coremltools) (\ud83d\udce5 160K \/ month \u00b7 \ud83d\udce6 140 \u00b7 \u23f1\ufe0f 22.02.2022):\n\t```\n\tpip install coremltools\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/coremltools) (\ud83d\udce5 30K \u00b7 \u23f1\ufe0f 15.10.2021):\n\t```\n\tconda install -c conda-forge coremltools\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/huggingface\/huggingface_hub\">huggingface_hub<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 410) - All the open source things related to the Hugging Face Hub. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/huggingface\/huggingface_hub) (\ud83d\udc68\u200d\ud83d\udcbb 61 \u00b7 \ud83d\udd00 92 \u00b7 \ud83d\udccb 230 - 32% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/huggingface\/huggingface_hub\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/huggingface_hub) (\ud83d\udce5 4.2M \/ month \u00b7 \ud83d\udce6 66 \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tpip install huggingface_hub\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/huggingface_hub) (\ud83d\udce5 110K \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tconda install -c conda-forge huggingface_hub\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/serve\">TorchServe<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 2.6K) - Serve, optimize and scale PyTorch models in production. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/serve) (\ud83d\udc68\u200d\ud83d\udcbb 110 \u00b7 \ud83d\udd00 500 \u00b7 \ud83d\udce5 1.4K \u00b7 \ud83d\udccb 910 - 16% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/serve\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchserve) (\ud83d\udce5 14K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 01.03.2022):\n\t```\n\tpip install torchserve\n\t```\n- [Conda](https:\/\/anaconda.org\/pytorch\/torchserve) (\ud83d\udce5 22K \u00b7 \u23f1\ufe0f 01.03.2022):\n\t```\n\tconda install -c pytorch torchserve\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/pytorch\/torchserve) (\ud83d\udce5 1M \u00b7 \u2b50 11 \u00b7 \u23f1\ufe0f 01.03.2022):\n\t```\n\tdocker pull pytorch\/torchserve\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/openai\/triton\">triton<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 3.6K) - Development repository for the Triton language and compiler. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/openai\/triton) (\ud83d\udc68\u200d\ud83d\udcbb 38 \u00b7 \ud83d\udd00 280 \u00b7 \ud83d\udce6 96 \u00b7 \ud83d\udccb 190 - 36% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/openai\/triton\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/triton) (\ud83d\udce5 58K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tpip install triton\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bentoml\/BentoML\">BentoML<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 3.5K) - The Unified Model Serving Framework. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bentoml\/BentoML) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 390 \u00b7 \ud83d\udce5 1.3K \u00b7 \ud83d\udccb 590 - 10% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bentoml\/BentoML\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/bentoml) (\ud83d\udce5 16K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tpip install bentoml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/hummingbird\">Hummingbird<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 2.8K) - Hummingbird compiles trained ML models into tensor computation for.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/hummingbird) (\ud83d\udc68\u200d\ud83d\udcbb 29 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce5 160 \u00b7 \ud83d\udce6 29 \u00b7 \ud83d\udccb 240 - 20% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/hummingbird\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hummingbird-ml) (\ud83d\udce5 1.9K \/ month \u00b7 \u23f1\ufe0f 25.04.2022):\n\t```\n\tpip install hummingbird-ml\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/hummingbird-ml) (\ud83d\udce5 6.8K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c conda-forge hummingbird-ml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/cortexlabs\/cortex\">cortex<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 7.7K) - Production infrastructure for machine learning at scale. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/cortexlabs\/cortex) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 590 \u00b7 \ud83d\udccb 1.1K - 9% open \u00b7 \u23f1\ufe0f 23.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/cortexlabs\/cortex\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cortex) (\ud83d\udce5 1.9K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 10.01.2022):\n\t```\n\tpip install cortex\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/BayesWitnesses\/m2cgen\">m2cgen<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 2.1K) - Transform ML models into a native code (Java, C, Python, Go, JavaScript,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/BayesWitnesses\/m2cgen) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udce5 10 \u00b7 \ud83d\udce6 31 \u00b7 \ud83d\udccb 90 - 26% open \u00b7 \u23f1\ufe0f 26.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/BayesWitnesses\/m2cgen\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/m2cgen) (\ud83d\udce5 40K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 26.04.2022):\n\t```\n\tpip install m2cgen\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/gmalivenko\/pytorch2keras\">pytorch2keras<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 790 \u00b7 \ud83d\udca4) - PyTorch to Keras model convertor. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/gmalivenko\/pytorch2keras) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 130 \u00b7 \ud83d\udce6 45 \u00b7 \ud83d\udccb 120 - 43% open \u00b7 \u23f1\ufe0f 06.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/gmalivenko\/pytorch2keras\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch2keras) (\ud83d\udce5 750 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 14.05.2020):\n\t```\n\tpip install pytorch2keras\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nebuly-ai\/nebullvm\">nebullvm<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 1K \u00b7 \ud83d\udc23) - Easy-to-use library to boost AI inference leveraging multiple DL.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nebuly-ai\/nebullvm) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 50 \u00b7 \ud83d\udce6 1 \u00b7 \ud83d\udccb 31 - 70% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nebuly-ai\/nebullvm\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nebullvm) (\ud83d\udce5 460 \/ month \u00b7 \u23f1\ufe0f 30.04.2022):\n\t```\n\tpip install nebullvm\n\t```\n<\/details>\n<details><summary>Show 5 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/microsoft\/MMdnn\">mmdnn<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 5.6K \u00b7 \ud83d\udc80) - MMdnn is a set of tools to help users inter-operate among different deep.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/larq\/compute-engine\">Larq Compute Engine<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 190) - Highly optimized inference engine for Binarized.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/nok\/sklearn-porter\">sklearn-porter<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 1.1K \u00b7 \ud83d\udc80) - Transpile trained scikit-learn estimators to C, Java,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/riga\/tfdeploy\">tfdeploy<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 350 \u00b7 \ud83d\udc80) - Deploy tensorflow graphs for fast evaluation and export to.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/backprop-ai\/backprop\">backprop<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 230 \u00b7 \ud83d\udca4) - Backprop makes it simple to use, finetune, and deploy state-of-.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n<\/details>\n<br>\n\n## Model Interpretability\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries to visualize, explain, debug, evaluate, and interpret machine learning models._\n\n<details><summary><b><a href=\"https:\/\/github.com\/slundberg\/shap\">shap<\/a><\/b> (\ud83e\udd4740 \u00b7  \u2b50 16K) - A game theoretic approach to explain the output of any machine learning model. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/slundberg\/shap) (\ud83d\udc68\u200d\ud83d\udcbb 180 \u00b7 \ud83d\udd00 2.4K \u00b7 \ud83d\udce6 5.3K \u00b7 \ud83d\udccb 1.9K - 69% open \u00b7 \u23f1\ufe0f 01.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/slundberg\/shap\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/shap) (\ud83d\udce5 4.1M \/ month \u00b7 \ud83d\udce6 220 \u00b7 \u23f1\ufe0f 20.10.2021):\n\t```\n\tpip install shap\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/shap) (\ud83d\udce5 1.1M \u00b7 \u23f1\ufe0f 23.01.2022):\n\t```\n\tconda install -c conda-forge shap\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/arviz-devs\/arviz\">arviz<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 1.2K) - Exploratory analysis of Bayesian models with Python. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/arviz-devs\/arviz) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 270 \u00b7 \ud83d\udce5 110 \u00b7 \ud83d\udce6 2.2K \u00b7 \ud83d\udccb 720 - 19% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/arviz-devs\/arviz\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/arviz) (\ud83d\udce5 390K \/ month \u00b7 \ud83d\udce6 96 \u00b7 \u23f1\ufe0f 23.03.2022):\n\t```\n\tpip install arviz\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/arviz) (\ud83d\udce5 700K \u00b7 \u23f1\ufe0f 23.03.2022):\n\t```\n\tconda install -c conda-forge arviz\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/marcotcr\/lime\">Lime<\/a><\/b> (\ud83e\udd4731 \u00b7  \u2b50 9.8K \u00b7 \ud83d\udca4) - Lime: Explaining the predictions of any machine learning classifier. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/marcotcr\/lime) (\ud83d\udc68\u200d\ud83d\udcbb 61 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce6 2.2K \u00b7 \ud83d\udccb 560 - 6% open \u00b7 \u23f1\ufe0f 29.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/marcotcr\/lime\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lime) (\ud83d\udce5 360K \/ month \u00b7 \ud83d\udce6 110 \u00b7 \u23f1\ufe0f 26.06.2020):\n\t```\n\tpip install lime\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/lime) (\ud83d\udce5 98K \u00b7 \u23f1\ufe0f 28.06.2020):\n\t```\n\tconda install -c conda-forge lime\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/interpretml\/interpret\">InterpretML<\/a><\/b> (\ud83e\udd4731 \u00b7  \u2b50 4.7K) - Fit interpretable models. Explain blackbox machine learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/interpretml\/interpret) (\ud83d\udc68\u200d\ud83d\udcbb 28 \u00b7 \ud83d\udd00 580 \u00b7 \ud83d\udce6 190 \u00b7 \ud83d\udccb 280 - 31% open \u00b7 \u23f1\ufe0f 30.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/interpretml\/interpret\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/interpret) (\ud83d\udce5 96K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 23.09.2021):\n\t```\n\tpip install interpret\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pytorch\/captum\">Captum<\/a><\/b> (\ud83e\udd4731 \u00b7  \u2b50 3.1K) - Model interpretability and understanding for PyTorch. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pytorch\/captum) (\ud83d\udc68\u200d\ud83d\udcbb 82 \u00b7 \ud83d\udd00 330 \u00b7 \ud83d\udce6 520 \u00b7 \ud83d\udccb 360 - 24% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pytorch\/captum\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/captum) (\ud83d\udce5 53K \/ month \u00b7 \ud83d\udce6 19 \u00b7 \u23f1\ufe0f 03.03.2022):\n\t```\n\tpip install captum\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/captum) (\ud83d\udce5 760 \u00b7 \u23f1\ufe0f 04.03.2022):\n\t```\n\tconda install -c conda-forge captum\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/model-analysis\">Model Analysis<\/a><\/b> (\ud83e\udd4731 \u00b7  \u2b50 1.2K) - Model analysis tools for TensorFlow. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/model-analysis) (\ud83d\udc68\u200d\ud83d\udcbb 41 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udccb 77 - 36% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/model-analysis\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-model-analysis) (\ud83d\udce5 950K \/ month \u00b7 \ud83d\udce6 21 \u00b7 \u23f1\ufe0f 05.03.2022):\n\t```\n\tpip install tensorflow-model-analysis\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/DistrictDataLabs\/yellowbrick\">yellowbrick<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 3.6K) - Visual analysis and diagnostic tools to facilitate machine.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/DistrictDataLabs\/yellowbrick) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 520 \u00b7 \ud83d\udccb 660 - 13% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/DistrictDataLabs\/yellowbrick\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/yellowbrick) (\ud83d\udce5 600K \/ month \u00b7 \ud83d\udce6 62 \u00b7 \u23f1\ufe0f 19.02.2022):\n\t```\n\tpip install yellowbrick\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/yellowbrick) (\ud83d\udce5 25K \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tconda install -c conda-forge yellowbrick\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/MAIF\/shapash\">shapash<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 1.7K) - Shapash makes Machine Learning models transparent and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/MAIF\/shapash) (\ud83d\udc68\u200d\ud83d\udcbb 32 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 72 \u00b7 \ud83d\udccb 120 - 15% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/MAIF\/shapash\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/shapash) (\ud83d\udce5 19K \/ month \u00b7 \u23f1\ufe0f 19.04.2022):\n\t```\n\tpip install shapash\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/dowhy\">DoWhy<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 3.8K) - DoWhy is a Python library for causal inference that supports explicit.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/dowhy) (\ud83d\udc68\u200d\ud83d\udcbb 48 \u00b7 \ud83d\udd00 600 \u00b7 \ud83d\udce5 27 \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 190 - 25% open \u00b7 \u23f1\ufe0f 20.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Microsoft\/dowhy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dowhy) (\ud83d\udce5 99K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 20.03.2022):\n\t```\n\tpip install dowhy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dowhy) (\ud83d\udce5 5.1K \u00b7 \u23f1\ufe0f 21.03.2022):\n\t```\n\tconda install -c conda-forge dowhy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/parrt\/dtreeviz\">dtreeviz<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 2.1K) - A python library for decision tree visualization and model interpretation. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/parrt\/dtreeviz) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 270 \u00b7 \ud83d\udce6 370 \u00b7 \ud83d\udccb 120 - 20% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/parrt\/dtreeviz\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dtreeviz) (\ud83d\udce5 95K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 29.04.2022):\n\t```\n\tpip install dtreeviz\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/dtreeviz) (\ud83d\udce5 9.7K \u00b7 \u23f1\ufe0f 29.04.2022):\n\t```\n\tconda install -c conda-forge dtreeviz\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/SeldonIO\/alibi\">Alibi<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 1.6K) - Algorithms for explaining machine learning models. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/SeldonIO\/alibi) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce6 160 \u00b7 \ud83d\udccb 290 - 41% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/SeldonIO\/alibi\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/alibi) (\ud83d\udce5 27K \/ month \u00b7 \ud83d\udce6 23 \u00b7 \u23f1\ufe0f 18.03.2022):\n\t```\n\tpip install alibi\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/fairlearn\/fairlearn\">fairlearn<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 1.3K) - A Python package to assess and improve fairness of machine.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/fairlearn\/fairlearn) (\ud83d\udc68\u200d\ud83d\udcbb 66 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udccb 350 - 40% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/fairlearn\/fairlearn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fairlearn) (\ud83d\udce5 71K \/ month \u00b7 \ud83d\udce6 9 \u00b7 \u23f1\ufe0f 07.07.2021):\n\t```\n\tpip install fairlearn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/fairlearn) (\ud83d\udce5 18K \u00b7 \u23f1\ufe0f 07.07.2021):\n\t```\n\tconda install -c conda-forge fairlearn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/oegedijk\/explainerdashboard\">explainerdashboard<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 1.2K) - Quickly build Explainable AI dashboards that show the inner.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/oegedijk\/explainerdashboard) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 180 - 11% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/oegedijk\/explainerdashboard\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/explainerdashboard) (\ud83d\udce5 32K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 02.04.2022):\n\t```\n\tpip install explainerdashboard\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/explainerdashboard) (\ud83d\udce5 15K \u00b7 \u23f1\ufe0f 15.02.2022):\n\t```\n\tconda install -c conda-forge explainerdashboard\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/microsoft\/responsible-ai-toolbox\">responsible-ai-widgets<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 480) - This project provides responsible AI user interfaces.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/microsoft\/responsible-ai-toolbox) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 27 \u00b7 \ud83d\udccb 260 - 20% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/microsoft\/responsible-ai-toolbox\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/raiwidgets) (\ud83d\udce5 11K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 20.04.2022):\n\t```\n\tpip install raiwidgets\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/marcotcr\/checklist\">checklist<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 1.6K \u00b7 \ud83d\udca4) - Beyond Accuracy: Behavioral Testing of NLP models with.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/marcotcr\/checklist) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 120 \u00b7 \ud83d\udccb 92 - 13% open \u00b7 \u23f1\ufe0f 28.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/marcotcr\/checklist\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/checklist) (\ud83d\udce5 20K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 24.05.2021):\n\t```\n\tpip install checklist\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/checklist) (\ud83d\udce5 2.8K \u00b7 \u23f1\ufe0f 15.07.2021):\n\t```\n\tconda install -c conda-forge checklist\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Trusted-AI\/AIF360\">Fairness 360<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.7K) - A comprehensive set of fairness metrics for datasets and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Trusted-AI\/AIF360) (\ud83d\udc68\u200d\ud83d\udcbb 48 \u00b7 \ud83d\udd00 550 \u00b7 \ud83d\udce6 150 \u00b7 \ud83d\udccb 140 - 54% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Trusted-AI\/AIF360\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/aif360) (\ud83d\udce5 6.8K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 04.03.2021):\n\t```\n\tpip install aif360\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/aif360) (\ud83d\udce5 1.9K \u00b7 \u23f1\ufe0f 29.09.2021):\n\t```\n\tconda install -c conda-forge aif360\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/quantumblacklabs\/causalnex\">CausalNex<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.5K) - A Python library that helps data scientists to infer.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/quantumblacklabs\/causalnex) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 39 \u00b7 \ud83d\udccb 110 - 19% open \u00b7 \u23f1\ufe0f 11.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/quantumblacklabs\/causalnex\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/causalnex) (\ud83d\udce5 1.3K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 11.11.2021):\n\t```\n\tpip install causalnex\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/Trusted-AI\/AIX360\">Explainability 360<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.1K) - Interpretability and explainability of data and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Trusted-AI\/AIX360) (\ud83d\udc68\u200d\ud83d\udcbb 29 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 45 \u00b7 \ud83d\udccb 64 - 59% open \u00b7 \u23f1\ufe0f 18.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Trusted-AI\/AIX360\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/aix360) (\ud83d\udce5 1.3K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 28.10.2020):\n\t```\n\tpip install aix360\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/csinva\/imodels\">imodels<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 740) - Interpretable ML package for concise, transparent, and accurate predictive.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/csinva\/imodels) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 72 \u00b7 \ud83d\udce6 12 \u00b7 \ud83d\udccb 31 - 29% open \u00b7 \u23f1\ufe0f 26.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/csinva\/imodels\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/imodels) (\ud83d\udce5 1.8K \/ month \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tpip install imodels\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PAIR-code\/lit\">LIT<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 2.9K) - The Language Interpretability Tool: Interactively analyze NLP models for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PAIR-code\/lit) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udce6 10 \u00b7 \ud83d\udccb 130 - 48% open \u00b7 \u23f1\ufe0f 15.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PAIR-code\/lit\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lit-nlp) (\ud83d\udce5 1.2K \/ month \u00b7 \u23f1\ufe0f 21.12.2021):\n\t```\n\tpip install lit-nlp\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/lit-nlp) (\ud83d\udce5 34K \u00b7 \u23f1\ufe0f 09.11.2021):\n\t```\n\tconda install -c conda-forge lit-nlp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/philipperemy\/keract\">keract<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 980) - Layers Outputs and Gradients in Keras. Made easy. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/philipperemy\/keract) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce6 130 \u00b7 \ud83d\udccb 84 - 3% open \u00b7 \u23f1\ufe0f 24.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/philipperemy\/keract\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/keract) (\ud83d\udce5 1.2K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 19.06.2021):\n\t```\n\tpip install keract\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/sicara\/tf-explain\">tf-explain<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 920) - Interpretability Methods for tf.keras models with Tensorflow 2.x. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/sicara\/tf-explain) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 96 \u00b7 \ud83d\udce6 110 \u00b7 \ud83d\udccb 90 - 43% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/sicara\/tf-explain\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tf-explain) (\ud83d\udce5 1.6K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 18.11.2021):\n\t```\n\tpip install tf-explain\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/interpretml\/DiCE\">DiCE<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 820) - Generate Diverse Counterfactual Explanations for any machine.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/interpretml\/DiCE) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udccb 110 - 43% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/interpretml\/DiCE\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/dice-ml) (\ud83d\udce5 31K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 27.09.2021):\n\t```\n\tpip install dice-ml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PAIR-code\/what-if-tool\">What-If Tool<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 670) - Source code\/webpage\/demos for the What-If Tool. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PAIR-code\/what-if-tool) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 130 \u00b7 \ud83d\udccb 100 - 54% open \u00b7 \u23f1\ufe0f 05.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PAIR-code\/what-if-tool\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/witwidget) (\ud83d\udce5 7.6K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 12.10.2021):\n\t```\n\tpip install witwidget\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorboard-plugin-wit) (\ud83d\udce5 950K \u00b7 \u23f1\ufe0f 06.01.2022):\n\t```\n\tconda install -c conda-forge tensorboard-plugin-wit\n\t```\n- [npm](https:\/\/www.npmjs.com\/package\/wit-widget) (\ud83d\udce5 4.4K \/ month \u00b7 \u23f1\ufe0f 12.10.2021):\n\t```\n\tnpm install wit-widget\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/kundajelab\/deeplift\">deeplift<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 630) - Public facing deeplift repo. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/kundajelab\/deeplift) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 56 \u00b7 \ud83d\udccb 84 - 42% open \u00b7 \u23f1\ufe0f 11.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/kundajelab\/deeplift\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deeplift) (\ud83d\udce5 720 \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 11.11.2020):\n\t```\n\tpip install deeplift\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/edublancas\/sklearn-evaluation\">sklearn-evaluation<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 320) - Machine learning model evaluation made easy: plots,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/edublancas\/sklearn-evaluation) (\ud83d\udc68\u200d\ud83d\udcbb 8 \u00b7 \ud83d\udd00 31 \u00b7 \ud83d\udce6 45 \u00b7 \ud83d\udccb 39 - 23% open \u00b7 \u23f1\ufe0f 16.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/edublancas\/sklearn-evaluation\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sklearn-evaluation) (\ud83d\udce5 1.7K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tpip install sklearn-evaluation\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jalammar\/ecco\">ecco<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1.4K) - Explain, analyze, and visualize NLP language models. Ecco creates.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jalammar\/ecco) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 92 \u00b7 \ud83d\udce5 16 \u00b7 \ud83d\udce6 7 \u00b7 \ud83d\udccb 36 - 38% open \u00b7 \u23f1\ufe0f 18.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/jalammar\/ecco\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ecco) (\ud83d\udce5 280 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 09.01.2022):\n\t```\n\tpip install ecco\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/ecco) (\ud83d\udce5 320 \u00b7 \u23f1\ufe0f 10.01.2022):\n\t```\n\tconda install -c conda-forge ecco\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/albermax\/innvestigate\">iNNvestigate<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 970 \u00b7 \ud83d\udca4) - A toolbox to iNNvestigate neural networks predictions!. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/albermax\/innvestigate) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udccb 230 - 30% open \u00b7 \u23f1\ufe0f 03.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/albermax\/innvestigate\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/innvestigate) (\ud83d\udce5 380 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 14.11.2020):\n\t```\n\tpip install innvestigate\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dssg\/aequitas\">aequitas<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 470 \u00b7 \ud83d\udca4) - Bias and Fairness Audit Toolkit. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dssg\/aequitas) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 88 \u00b7 \ud83d\udce6 99 \u00b7 \ud83d\udccb 58 - 63% open \u00b7 \u23f1\ufe0f 27.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/dssg\/aequitas\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/aequitas) (\ud83d\udce5 730 \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 16.12.2020):\n\t```\n\tpip install aequitas\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/tcav\">tcav<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 520 \u00b7 \ud83d\udca4) - Code for the TCAV ML interpretability project. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/tcav) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 130 \u00b7 \ud83d\udce6 11 \u00b7 \ud83d\udccb 61 - 11% open \u00b7 \u23f1\ufe0f 16.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/tcav\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tcav) (\ud83d\udce5 77 \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 23.02.2021):\n\t```\n\tpip install tcav\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/aerdem4\/lofo-importance\">LOFO<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 460) - Leave One Feature Out Importance. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/aerdem4\/lofo-importance) (\ud83d\udc68\u200d\ud83d\udcbb 3 \u00b7 \ud83d\udd00 53 \u00b7 \ud83d\udce6 16 \u00b7 \ud83d\udccb 20 - 20% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/aerdem4\/lofo-importance\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lofo-importance) (\ud83d\udce5 380 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install lofo-importance\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/EthicalML\/xai\">XAI<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 810 \u00b7 \ud83d\udca4) - XAI - An eXplainability toolbox for machine learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/EthicalML\/xai) (\ud83d\udc68\u200d\ud83d\udcbb 3 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udce6 15 \u00b7 \ud83d\udccb 9 - 22% open \u00b7 \u23f1\ufe0f 30.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/EthicalML\/xai\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/xai) (\ud83d\udce5 110 \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 30.10.2021):\n\t```\n\tpip install xai\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/marcotcr\/anchor\">Anchor<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 700) - Code for High-Precision Model-Agnostic Explanations paper. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/marcotcr\/anchor) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 96 \u00b7 \ud83d\udccb 70 - 27% open \u00b7 \u23f1\ufe0f 17.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/marcotcr\/anchor\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/anchor_exp) (\ud83d\udce5 920 \/ month \u00b7 \u23f1\ufe0f 26.06.2020):\n\t```\n\tpip install anchor_exp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/interpretml\/interpret-text\">interpret-text<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 310) - A library that incorporates state-of-the-art explainers for.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/interpretml\/interpret-text) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 57 \u00b7 \ud83d\udccb 74 - 78% open \u00b7 \u23f1\ufe0f 06.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/interpretml\/interpret-text\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/interpret-text) (\ud83d\udce5 85 \/ month \u00b7 \u23f1\ufe0f 07.12.2021):\n\t```\n\tpip install interpret-text\n\t```\n<\/details>\n<details><summary>Show 16 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/bmabey\/pyLDAvis\">pyLDAvis<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 1.6K \u00b7 \ud83d\udc80) - Python library for interactive topic model visualization... <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/TeamHG-Memex\/eli5\">eli5<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.5K \u00b7 \ud83d\udc80) - A library for debugging\/inspecting machine learning classifiers and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/tensorflow\/lucid\">Lucid<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 4.4K \u00b7 \ud83d\udc80) - A collection of infrastructure and tools for research in.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/reiinakano\/scikit-plot\">scikit-plot<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 2.2K \u00b7 \ud83d\udc80) - An intuitive library to add plotting functionality to.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/raghakot\/keras-vis\">keras-vis<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 2.9K \u00b7 \ud83d\udc80) - Neural network visualization toolkit for keras. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/ModelOriented\/DALEX\">DALEX<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1K) - moDel Agnostic Language for Exploration and eXplanation. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/andosa\/treeinterpreter\">TreeInterpreter<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 710 \u00b7 \ud83d\udc80) - Package for interpreting scikit-learns decision tree.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/parrt\/random-forest-importances\">random-forest-importances<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 500 \u00b7 \ud83d\udc80) - Code to compute permutation and drop-column.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/oracle\/Skater\">Skater<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1K) - Python Library for Model Interpretation\/Explanations. <code><a href=\"https:\/\/tldrlegal.com\/search?q=UPL-1.0\">\u2757\ufe0fUPL-1.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/tensorflow\/model-card-toolkit\">model-card-toolkit<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 270) - a tool that leverages rich metadata and lineage.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/tensorflow\/fairness-indicators\">fairness-indicators<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 250) - Tensorflows Fairness Evaluation and Visualization.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/MisaOgura\/flashtorch\">FlashTorch<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 660 \u00b7 \ud83d\udc80) - Visualization toolkit for neural networks in PyTorch! Demo --. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/explainX\/explainx\">ExplainX.ai<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 310 \u00b7 \ud83d\udc80) - Explainable AI framework for data scientists. Explain & debug any.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/SAP\/contextual-ai\">contextual-ai<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 80) - Contextual AI adds explainability to different stages of.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/suinleelab\/attributionpriors\">Attribution Priors<\/a><\/b> (\ud83e\udd4912 \u00b7  \u2b50 96 \u00b7 \ud83d\udc80) - Tools for training explainable models using.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/intuit\/bias-detector\">bias-detector<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 37) - Bias Detector is a python package for detecting bias in machine.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n<\/details>\n<br>\n\n## Vector Similarity Search (ANN)\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for Approximate Nearest Neighbor Search and Vector Indexing\/Similarity Search._\n\n\ud83d\udd17&nbsp;<b><a href=\"https:\/\/github.com\/erikbern\/ann-benchmarks\">ANN Benchmarks<\/a><\/b> ( \u2b50 2.9K)  - Benchmarks of approximate nearest neighbor libraries in Python.\n\n<details><summary><b><a href=\"https:\/\/github.com\/milvus-io\/milvus\">Milvus<\/a><\/b> (\ud83e\udd4738 \u00b7  \u2b50 10K) - An open-source vector database for scalable similarity search and AI.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/milvus-io\/milvus) (\ud83d\udc68\u200d\ud83d\udcbb 200 \u00b7 \ud83d\udd00 1.5K \u00b7 \ud83d\udce5 23K \u00b7 \ud83d\udccb 5K - 6% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/milvus-io\/milvus\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pymilvus) (\ud83d\udce5 37K \/ month \u00b7 \ud83d\udce6 16 \u00b7 \u23f1\ufe0f 02.04.2022):\n\t```\n\tpip install pymilvus\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/milvusdb\/milvus) (\ud83d\udce5 930K \u00b7 \u2b50 18 \u00b7 \u23f1\ufe0f 02.04.2022):\n\t```\n\tdocker pull milvusdb\/milvus\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/faiss\">Faiss<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 17K) - A library for efficient similarity search and clustering of dense vectors. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/faiss) (\ud83d\udc68\u200d\ud83d\udcbb 97 \u00b7 \ud83d\udd00 2.6K \u00b7 \ud83d\udce6 630 \u00b7 \ud83d\udccb 1.8K - 10% open \u00b7 \u23f1\ufe0f 30.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/faiss\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pymilvus) (\ud83d\udce5 37K \/ month \u00b7 \ud83d\udce6 16 \u00b7 \u23f1\ufe0f 02.04.2022):\n\t```\n\tpip install pymilvus\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/faiss) (\ud83d\udce5 300K \u00b7 \u23f1\ufe0f 09.02.2022):\n\t```\n\tconda install -c conda-forge faiss\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/spotify\/annoy\">Annoy<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 9.8K) - Approximate Nearest Neighbors in C++\/Python optimized for memory usage.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/spotify\/annoy) (\ud83d\udc68\u200d\ud83d\udcbb 81 \u00b7 \ud83d\udd00 1K \u00b7 \ud83d\udce6 2.1K \u00b7 \ud83d\udccb 340 - 11% open \u00b7 \u23f1\ufe0f 25.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/spotify\/annoy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/annoy) (\ud83d\udce5 1.2M \/ month \u00b7 \ud83d\udce6 240 \u00b7 \u23f1\ufe0f 18.09.2020):\n\t```\n\tpip install annoy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/python-annoy) (\ud83d\udce5 230K \u00b7 \u23f1\ufe0f 23.04.2022):\n\t```\n\tconda install -c conda-forge python-annoy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nmslib\/nmslib\">NMSLIB<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 2.8K) - Non-Metric Space Library (NMSLIB): An efficient similarity search.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nmslib\/nmslib) (\ud83d\udc68\u200d\ud83d\udcbb 48 \u00b7 \ud83d\udd00 380 \u00b7 \ud83d\udce6 600 \u00b7 \ud83d\udccb 400 - 15% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nmslib\/nmslib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/nmslib) (\ud83d\udce5 100K \/ month \u00b7 \ud83d\udce6 47 \u00b7 \u23f1\ufe0f 03.02.2021):\n\t```\n\tpip install nmslib\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/nmslib) (\ud83d\udce5 50K \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tconda install -c conda-forge nmslib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nmslib\/hnswlib\">hnswlib<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 2K) - Header-only C++\/python library for fast approximate nearest neighbors. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nmslib\/hnswlib) (\ud83d\udc68\u200d\ud83d\udcbb 56 \u00b7 \ud83d\udd00 360 \u00b7 \ud83d\udce6 220 \u00b7 \ud83d\udccb 250 - 51% open \u00b7 \u23f1\ufe0f 16.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nmslib\/hnswlib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hnswlib) (\ud83d\udce5 240K \/ month \u00b7 \ud83d\udce6 24 \u00b7 \u23f1\ufe0f 14.02.2022):\n\t```\n\tpip install hnswlib\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/hnswlib) (\ud83d\udce5 38K \u00b7 \u23f1\ufe0f 16.04.2022):\n\t```\n\tconda install -c conda-forge hnswlib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lmcinnes\/pynndescent\">PyNNDescent<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 610) - A Python nearest neighbor descent for approximate nearest neighbors. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lmcinnes\/pynndescent) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 82 \u00b7 \ud83d\udce6 1.5K \u00b7 \ud83d\udccb 100 - 49% open \u00b7 \u23f1\ufe0f 25.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lmcinnes\/pynndescent\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pynndescent) (\ud83d\udce5 1.5M \/ month \u00b7 \ud83d\udce6 25 \u00b7 \u23f1\ufe0f 21.01.2022):\n\t```\n\tpip install pynndescent\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pynndescent) (\ud83d\udce5 570K \u00b7 \u23f1\ufe0f 22.01.2022):\n\t```\n\tconda install -c conda-forge pynndescent\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/yahoojapan\/NGT\">NGT<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 880) - Nearest Neighbor Search with Neighborhood Graph and Tree for High-.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/yahoojapan\/NGT) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 89 \u00b7 \ud83d\udccb 98 - 15% open \u00b7 \u23f1\ufe0f 06.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/yahoojapan\/NGT\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/ngt) (\ud83d\udce5 16K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tpip install ngt\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/kakao\/n2\">N2<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 510 \u00b7 \ud83d\udca4) - TOROS N2 - lightweight approximate Nearest Neighbor library which runs.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/kakao\/n2) (\ud83d\udc68\u200d\ud83d\udcbb 18 \u00b7 \ud83d\udd00 64 \u00b7 \ud83d\udce6 23 \u00b7 \ud83d\udccb 37 - 45% open \u00b7 \u23f1\ufe0f 20.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/kakao\/n2\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/n2) (\ud83d\udce5 630 \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 16.10.2020):\n\t```\n\tpip install n2\n\t```\n<\/details>\n<details><summary>Show 3 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/plasticityai\/magnitude\">Magnitude<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.5K \u00b7 \ud83d\udc80) - A fast, efficient universal vector embedding utility package. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/pixelogik\/NearPy\">NearPy<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 710 \u00b7 \ud83d\udc80) - Python framework for fast (approximated) nearest neighbour search in.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/facebookresearch\/pysparnn\">PySparNN<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 900 \u00b7 \ud83d\udc80) - Approximate Nearest Neighbor Search for Sparse Data in Python!. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n<\/details>\n<br>\n\n## Probabilistics & Statistics\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries providing capabilities for probabilistic programming\/reasoning, bayesian inference, gaussian processes, or statistics._\n\n<details><summary><b><a href=\"https:\/\/github.com\/pymc-devs\/pymc\">PyMC3<\/a><\/b> (\ud83e\udd4739 \u00b7  \u2b50 6.5K) - Probabilistic Programming in Python: Bayesian Modeling and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pymc-devs\/pymc) (\ud83d\udc68\u200d\ud83d\udcbb 380 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udce5 1.9K \u00b7 \ud83d\udce6 620 \u00b7 \ud83d\udccb 2.7K - 7% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pymc-devs\/pymc\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pymc3) (\ud83d\udce5 360K \/ month \u00b7 \ud83d\udce6 240 \u00b7 \u23f1\ufe0f 15.03.2022):\n\t```\n\tpip install pymc3\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pymc3) (\ud83d\udce5 400K \u00b7 \u23f1\ufe0f 12.10.2021):\n\t```\n\tconda install -c conda-forge pymc3\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/probability\">tensorflow-probability<\/a><\/b> (\ud83e\udd4737 \u00b7  \u2b50 3.7K) - Probabilistic reasoning and statistical analysis in.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/probability) (\ud83d\udc68\u200d\ud83d\udcbb 440 \u00b7 \ud83d\udd00 970 \u00b7 \ud83d\udccb 1.2K - 45% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/probability\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-probability) (\ud83d\udce5 840K \/ month \u00b7 \ud83d\udce6 310 \u00b7 \u23f1\ufe0f 14.02.2022):\n\t```\n\tpip install tensorflow-probability\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorflow-probability) (\ud83d\udce5 55K \u00b7 \u23f1\ufe0f 26.01.2022):\n\t```\n\tconda install -c conda-forge tensorflow-probability\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pyro-ppl\/pyro\">Pyro<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 7.4K) - Deep universal probabilistic programming with Python and PyTorch. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pyro-ppl\/pyro) (\ud83d\udc68\u200d\ud83d\udcbb 120 \u00b7 \ud83d\udd00 900 \u00b7 \ud83d\udce6 700 \u00b7 \ud83d\udccb 960 - 21% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pyro-ppl\/pyro\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyro-ppl) (\ud83d\udce5 120K \/ month \u00b7 \ud83d\udce6 56 \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tpip install pyro-ppl\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyro-ppl) (\ud83d\udce5 7.2K \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tconda install -c conda-forge pyro-ppl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/cornellius-gp\/gpytorch\">GPyTorch<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 2.7K) - A highly efficient and modular implementation of Gaussian Processes.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/cornellius-gp\/gpytorch) (\ud83d\udc68\u200d\ud83d\udcbb 93 \u00b7 \ud83d\udd00 400 \u00b7 \ud83d\udce6 560 \u00b7 \ud83d\udccb 1.1K - 24% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/cornellius-gp\/gpytorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gpytorch) (\ud83d\udce5 180K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 04.12.2021):\n\t```\n\tpip install gpytorch\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/gpytorch) (\ud83d\udce5 32K \u00b7 \u23f1\ufe0f 14.12.2021):\n\t```\n\tconda install -c conda-forge gpytorch\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/hmmlearn\/hmmlearn\">hmmlearn<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 2.5K) - Hidden Markov Models in Python, with scikit-learn like API. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/hmmlearn\/hmmlearn) (\ud83d\udc68\u200d\ud83d\udcbb 39 \u00b7 \ud83d\udd00 700 \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 380 - 14% open \u00b7 \u23f1\ufe0f 23.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/hmmlearn\/hmmlearn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hmmlearn) (\ud83d\udce5 460K \/ month \u00b7 \ud83d\udce6 130 \u00b7 \u23f1\ufe0f 10.02.2022):\n\t```\n\tpip install hmmlearn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/hmmlearn) (\ud83d\udce5 120K \u00b7 \u23f1\ufe0f 12.02.2022):\n\t```\n\tconda install -c conda-forge hmmlearn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rlabbe\/filterpy\">filterpy<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 2.2K \u00b7 \ud83d\udca4) - Python Kalman filtering and optimal estimation library. Implements.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rlabbe\/filterpy) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 510 \u00b7 \ud83d\udce6 1.4K \u00b7 \ud83d\udccb 210 - 26% open \u00b7 \u23f1\ufe0f 04.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/rlabbe\/filterpy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/filterpy) (\ud83d\udce5 690K \/ month \u00b7 \ud83d\udce6 130 \u00b7 \u23f1\ufe0f 10.10.2018):\n\t```\n\tpip install filterpy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/filterpy) (\ud83d\udce5 86K \u00b7 \u23f1\ufe0f 05.05.2020):\n\t```\n\tconda install -c conda-forge filterpy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/GPflow\/GPflow\">GPflow<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 1.6K) - Gaussian processes in TensorFlow. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/GPflow\/GPflow) (\ud83d\udc68\u200d\ud83d\udcbb 78 \u00b7 \ud83d\udd00 420 \u00b7 \ud83d\udce6 360 \u00b7 \ud83d\udccb 770 - 16% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/GPflow\/GPflow\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gpflow) (\ud83d\udce5 15K \/ month \u00b7 \ud83d\udce6 29 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install gpflow\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/gpflow) (\ud83d\udce5 11K \u00b7 \u23f1\ufe0f 01.03.2022):\n\t```\n\tconda install -c conda-forge gpflow\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pgmpy\/pgmpy\">pgmpy<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 2K) - Python Library for learning (Structure and Parameter), inference.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pgmpy\/pgmpy) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 630 \u00b7 \ud83d\udce5 150 \u00b7 \ud83d\udce6 350 \u00b7 \ud83d\udccb 780 - 28% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pgmpy\/pgmpy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pgmpy) (\ud83d\udce5 68K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 30.03.2022):\n\t```\n\tpip install pgmpy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/twopirllc\/pandas-ta\">pandas-ta<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 2.4K) - Technical Analysis Indicators - Pandas TA is an easy to use.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/twopirllc\/pandas-ta) (\ud83d\udc68\u200d\ud83d\udcbb 44 \u00b7 \ud83d\udd00 550 \u00b7 \ud83d\udce6 630 \u00b7 \ud83d\udccb 390 - 14% open \u00b7 \u23f1\ufe0f 31.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/twopirllc\/pandas-ta\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pandas-ta) (\ud83d\udce5 78K \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 28.07.2021):\n\t```\n\tpip install pandas-ta\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pandas-ta) (\ud83d\udce5 660 \u00b7 \u23f1\ufe0f 05.10.2021):\n\t```\n\tconda install -c conda-forge pandas-ta\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pydata\/patsy\">patsy<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 830 \u00b7 \ud83d\udca4) - Describing statistical models in Python using symbolic formulas. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pydata\/patsy) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 94 \u00b7 \ud83d\udce6 51K \u00b7 \ud83d\udccb 140 - 51% open \u00b7 \u23f1\ufe0f 26.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/pydata\/patsy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/patsy) (\ud83d\udce5 6.8M \/ month \u00b7 \ud83d\udce6 2.6K \u00b7 \u23f1\ufe0f 26.09.2021):\n\t```\n\tpip install patsy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/patsy) (\ud83d\udce5 4.6M \u00b7 \u23f1\ufe0f 26.09.2021):\n\t```\n\tconda install -c conda-forge patsy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jmschrei\/pomegranate\">pomegranate<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 2.9K) - Fast, flexible and easy to use probabilistic modelling in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jmschrei\/pomegranate) (\ud83d\udc68\u200d\ud83d\udcbb 65 \u00b7 \ud83d\udd00 520 \u00b7 \ud83d\udce6 680 \u00b7 \ud83d\udccb 670 - 8% open \u00b7 \u23f1\ufe0f 21.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/jmschrei\/pomegranate\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pomegranate) (\ud83d\udce5 31K \/ month \u00b7 \ud83d\udce6 46 \u00b7 \u23f1\ufe0f 21.02.2022):\n\t```\n\tpip install pomegranate\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pomegranate) (\ud83d\udce5 84K \u00b7 \u23f1\ufe0f 16.11.2021):\n\t```\n\tconda install -c conda-forge pomegranate\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/SALib\/SALib\">SALib<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 590) - Sensitivity Analysis Library in Python. Contains Sobol, Morris, FAST, and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/SALib\/SALib) (\ud83d\udc68\u200d\ud83d\udcbb 36 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udccb 280 - 16% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/SALib\/SALib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/salib) (\ud83d\udce5 140K \/ month \u00b7 \ud83d\udce6 58 \u00b7 \u23f1\ufe0f 06.02.2022):\n\t```\n\tpip install salib\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/salib) (\ud83d\udce5 79K \u00b7 \u23f1\ufe0f 04.09.2021):\n\t```\n\tconda install -c conda-forge salib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/uber\/orbit\">Orbit<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.4K) - A Python package for Bayesian forecasting with object-oriented design.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/uber\/orbit) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 94 \u00b7 \ud83d\udce6 6 \u00b7 \ud83d\udccb 370 - 16% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/uber\/orbit\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/orbit-ml) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tpip install orbit-ml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bambinos\/bambi\">bambi<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 770) - BAyesian Model-Building Interface (Bambi) in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bambinos\/bambi) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 80 \u00b7 \ud83d\udce6 27 \u00b7 \ud83d\udccb 240 - 15% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bambinos\/bambi\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/bambi) (\ud83d\udce5 3.8K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 15.01.2022):\n\t```\n\tpip install bambi\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/bambi) (\ud83d\udce5 7.5K \u00b7 \u23f1\ufe0f 18.01.2022):\n\t```\n\tconda install -c conda-forge bambi\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ElementAI\/baal\">Baal<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 580) - Library to enable Bayesian active learning in your research or labeling.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ElementAI\/baal) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 52 \u00b7 \ud83d\udccb 72 - 22% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ElementAI\/baal\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/baal) (\ud83d\udce5 1.4K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install baal\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/baal) (\ud83d\udce5 1.4K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge baal\n\t```\n<\/details>\n<details><summary>Show 7 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/raphaelvallat\/pingouin\">pingouin<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 1K) - Statistical package in Python based on Pandas. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/blei-lab\/edward\">Edward<\/a><\/b> (\ud83e\udd4928 \u00b7  \u2b50 4.7K \u00b7 \ud83d\udc80) - A probabilistic programming language in TensorFlow. Deep.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/stan-dev\/pystan\">PyStan<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 180) - PyStan, a Python interface to Stan, a platform for statistical modeling... <code><a href=\"http:\/\/bit.ly\/3hkKRql\">ISC<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/mattjj\/pyhsmm\">pyhsmm<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 510 \u00b7 \ud83d\udc80) - Bayesian inference in HSMMs and HMMs. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/maximtrp\/scikit-posthocs\">scikit-posthocs<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 240) - Multiple Pairwise Comparisons (Post Hoc) Tests in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/pyro-ppl\/funsor\">Funsor<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 190) - Functional tensors for probabilistic programming. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/thu-ml\/zhusuan\">ZhuSuan<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 2.1K \u00b7 \ud83d\udc80) - A probabilistic programming library for Bayesian deep learning,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Adversarial Robustness\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for testing the robustness of machine learning models against attacks with adversarial\/malicious examples._\n\n<details><summary><b><a href=\"https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\">ART<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 3K) - Adversarial Robustness Toolbox (ART) - Python Library for Machine Learning.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox) (\ud83d\udc68\u200d\ud83d\udcbb 97 \u00b7 \ud83d\udd00 810 \u00b7 \ud83d\udce6 210 \u00b7 \ud83d\udccb 670 - 12% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/adversarial-robustness-toolbox) (\ud83d\udce5 7.9K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tpip install adversarial-robustness-toolbox\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/adversarial-robustness-toolbox) (\ud83d\udce5 8.9K \u00b7 \u23f1\ufe0f 10.01.2022):\n\t```\n\tconda install -c conda-forge adversarial-robustness-toolbox\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/bethgelab\/foolbox\">Foolbox<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 2.2K) - A Python toolbox to create adversarial examples that fool neural networks.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/bethgelab\/foolbox) (\ud83d\udc68\u200d\ud83d\udcbb 32 \u00b7 \ud83d\udd00 400 \u00b7 \ud83d\udce6 280 \u00b7 \ud83d\udccb 350 - 6% open \u00b7 \u23f1\ufe0f 02.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/bethgelab\/foolbox\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/foolbox) (\ud83d\udce5 3.2K \/ month \u00b7 \ud83d\udce6 13 \u00b7 \u23f1\ufe0f 02.04.2022):\n\t```\n\tpip install foolbox\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/foolbox) (\ud83d\udce5 5.7K \u00b7 \u23f1\ufe0f 30.04.2021):\n\t```\n\tconda install -c conda-forge foolbox\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/cleverhans-lab\/cleverhans\">CleverHans<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 5.5K \u00b7 \ud83d\udca4) - An adversarial example library for constructing attacks,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/cleverhans-lab\/cleverhans) (\ud83d\udc68\u200d\ud83d\udcbb 130 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udce6 320 \u00b7 \ud83d\udccb 450 - 5% open \u00b7 \u23f1\ufe0f 23.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/cleverhans-lab\/cleverhans\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cleverhans) (\ud83d\udce5 1.6K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 24.07.2021):\n\t```\n\tpip install cleverhans\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/cleverhans) (\ud83d\udce5 2.9K \u00b7 \u23f1\ufe0f 29.07.2021):\n\t```\n\tconda install -c conda-forge cleverhans\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/QData\/TextAttack\">TextAttack<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 2K) - TextAttack is a Python framework for adversarial attacks, data.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/QData\/TextAttack) (\ud83d\udc68\u200d\ud83d\udcbb 51 \u00b7 \ud83d\udd00 250 \u00b7 \ud83d\udce6 75 \u00b7 \ud83d\udccb 200 - 20% open \u00b7 \u23f1\ufe0f 04.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/QData\/TextAttack\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/textattack) (\ud83d\udce5 6K \/ month \u00b7 \ud83d\udce6 7 \u00b7 \u23f1\ufe0f 10.11.2021):\n\t```\n\tpip install textattack\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/textattack) (\ud83d\udce5 2.7K \u00b7 \u23f1\ufe0f 29.06.2021):\n\t```\n\tconda install -c conda-forge textattack\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/advboxes\/AdvBox\">AdvBox<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 1.2K \u00b7 \ud83d\udca4) - Advbox is a toolbox to generate adversarial examples that fool.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/advboxes\/AdvBox) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udccb 38 - 21% open \u00b7 \u23f1\ufe0f 03.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/advboxes\/AdvBox\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/advbox) (\ud83d\udce5 38 \/ month \u00b7 \u23f1\ufe0f 05.12.2018):\n\t```\n\tpip install advbox\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/MadryLab\/robustness\">robustness<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 690) - A library for experimenting with, training and evaluating neural.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/MadryLab\/robustness) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 75 \u00b7 \ud83d\udccb 73 - 23% open \u00b7 \u23f1\ufe0f 14.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/MadryLab\/robustness\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/robustness) (\ud83d\udce5 630 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 01.12.2020):\n\t```\n\tpip install robustness\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/robustness) (\ud83d\udce5 3.4K \u00b7 \u23f1\ufe0f 30.04.2021):\n\t```\n\tconda install -c conda-forge robustness\n\t```\n<\/details>\n<details><summary>Show 3 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/BorealisAI\/advertorch\">advertorch<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.1K) - A Toolbox for Adversarial Robustness Research. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/textflint\/textflint\">textflint<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 550) - Unified Multilingual Robustness Evaluation Toolkit for Natural.. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/airbnb\/artificial-adversary\">Adversary<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 360 \u00b7 \ud83d\udc80) - Tool to generate adversarial text examples and test machine.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n<\/details>\n<br>\n\n## GPU Utilities\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries that require and make use of CUDA\/GPU system capabilities to optimize data handling and machine learning tasks._\n\n<details><summary><b><a href=\"https:\/\/github.com\/cupy\/cupy\">CuPy<\/a><\/b> (\ud83e\udd4738 \u00b7  \u2b50 6K) - NumPy & SciPy for GPU. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/cupy\/cupy) (\ud83d\udc68\u200d\ud83d\udcbb 300 \u00b7 \ud83d\udd00 590 \u00b7 \ud83d\udce5 30K \u00b7 \ud83d\udce6 1K \u00b7 \ud83d\udccb 1.7K - 22% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/cupy\/cupy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cupy) (\ud83d\udce5 26K \/ month \u00b7 \ud83d\udce6 160 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install cupy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/cupy) (\ud83d\udce5 1.3M \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge cupy\n\t```\n- [Docker Hub](https:\/\/hub.docker.com\/r\/cupy\/cupy) (\ud83d\udce5 55K \u00b7 \u2b50 7 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tdocker pull cupy\/cupy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rapidsai\/cudf\">cuDF<\/a><\/b> (\ud83e\udd4731 \u00b7  \u2b50 4.7K) - cuDF - GPU DataFrame Library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rapidsai\/cudf) (\ud83d\udc68\u200d\ud83d\udcbb 240 \u00b7 \ud83d\udd00 600 \u00b7 \ud83d\udccb 4.5K - 16% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rapidsai\/cudf\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cudf) (\ud83d\udce5 1.4K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 01.06.2020):\n\t```\n\tpip install cudf\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/arrayfire\/arrayfire\">ArrayFire<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 3.8K) - ArrayFire: a general purpose GPU library. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/arrayfire\/arrayfire) (\ud83d\udc68\u200d\ud83d\udcbb 81 \u00b7 \ud83d\udd00 490 \u00b7 \ud83d\udce5 2.3K \u00b7 \ud83d\udccb 1.5K - 15% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/arrayfire\/arrayfire\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/arrayfire) (\ud83d\udce5 27K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 22.02.2022):\n\t```\n\tpip install arrayfire\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/inducer\/pycuda\">PyCUDA<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 1.3K) - CUDA integration for Python, plus shiny features. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/inducer\/pycuda) (\ud83d\udc68\u200d\ud83d\udcbb 75 \u00b7 \ud83d\udd00 250 \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 220 - 29% open \u00b7 \u23f1\ufe0f 03.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/inducer\/pycuda\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pycuda) (\ud83d\udce5 28K \/ month \u00b7 \ud83d\udce6 190 \u00b7 \u23f1\ufe0f 03.04.2021):\n\t```\n\tpip install pycuda\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pycuda) (\ud83d\udce5 62K \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tconda install -c conda-forge pycuda\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/wookayin\/gpustat\">gpustat<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 2.8K) - A simple command-line utility for querying and monitoring GPU status. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/wookayin\/gpustat) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 1.8K \u00b7 \ud83d\udccb 81 - 27% open \u00b7 \u23f1\ufe0f 07.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/wookayin\/gpustat\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gpustat) (\ud83d\udce5 550K \/ month \u00b7 \ud83d\udce6 99 \u00b7 \u23f1\ufe0f 02.01.2021):\n\t```\n\tpip install gpustat\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/gpustat) (\ud83d\udce5 130K \u00b7 \u23f1\ufe0f 24.11.2020):\n\t```\n\tconda install -c conda-forge gpustat\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rapidsai\/cuml\">cuML<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 2.7K) - cuML - RAPIDS Machine Learning Library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rapidsai\/cuml) (\ud83d\udc68\u200d\ud83d\udcbb 150 \u00b7 \ud83d\udd00 400 \u00b7 \ud83d\udccb 2K - 33% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rapidsai\/cuml\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cuml) (\ud83d\udce5 710 \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 01.06.2020):\n\t```\n\tpip install cuml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/NVIDIA\/apex\">Apex<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 6.3K) - A PyTorch Extension: Tools for easy mixed precision and distributed.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/NVIDIA\/apex) (\ud83d\udc68\u200d\ud83d\udcbb 92 \u00b7 \ud83d\udd00 990 \u00b7 \ud83d\udce6 1K \u00b7 \ud83d\udccb 990 - 58% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/NVIDIA\/apex\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/nvidia-apex) (\ud83d\udce5 80K \u00b7 \u23f1\ufe0f 06.04.2022):\n\t```\n\tconda install -c conda-forge nvidia-apex\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/NVIDIA\/DALI\">DALI<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 3.8K) - A GPU-accelerated library containing highly optimized building blocks.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/NVIDIA\/DALI) (\ud83d\udc68\u200d\ud83d\udcbb 74 \u00b7 \ud83d\udd00 480 \u00b7 \ud83d\udccb 1.1K - 14% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/NVIDIA\/DALI\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rapidsai\/cugraph\">cuGraph<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1K) - cuGraph - RAPIDS Graph Analytics Library. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rapidsai\/cugraph) (\ud83d\udc68\u200d\ud83d\udcbb 83 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udccb 810 - 12% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rapidsai\/cugraph\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/cugraph) (\ud83d\udce5 240 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 01.06.2020):\n\t```\n\tpip install cugraph\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/libcugraph) (\ud83d\udce5 6.6K \u00b7 \u23f1\ufe0f 29.04.2021):\n\t```\n\tconda install -c conda-forge libcugraph\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lebedov\/scikit-cuda\">scikit-cuda<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 890) - Python interface to GPU-powered libraries. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lebedov\/scikit-cuda) (\ud83d\udc68\u200d\ud83d\udcbb 46 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 180 \u00b7 \ud83d\udccb 220 - 22% open \u00b7 \u23f1\ufe0f 31.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lebedov\/scikit-cuda\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scikit-cuda) (\ud83d\udce5 620 \/ month \u00b7 \ud83d\udce6 44 \u00b7 \u23f1\ufe0f 27.05.2019):\n\t```\n\tpip install scikit-cuda\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/BlazingDB\/blazingsql\">BlazingSQL<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 1.7K \u00b7 \ud83d\udca4) - BlazingSQL is a lightweight, GPU accelerated, SQL engine for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/BlazingDB\/blazingsql) (\ud83d\udc68\u200d\ud83d\udcbb 49 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udccb 720 - 18% open \u00b7 \u23f1\ufe0f 30.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/BlazingDB\/blazingsql\n\t```\n- [Conda](https:\/\/anaconda.org\/blazingsql\/blazingsql-protocol) (\ud83d\udce5 940 \u00b7 \u23f1\ufe0f 11.11.2019):\n\t```\n\tconda install -c blazingsql blazingsql-protocol\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/KomputeProject\/kompute\">Vulkan Kompute<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 850) - General purpose GPU compute framework built on Vulkan to.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/KomputeProject\/kompute) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 60 \u00b7 \ud83d\udce5 150 \u00b7 \ud83d\udce6 3 \u00b7 \ud83d\udccb 170 - 32% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/KomputeProject\/kompute\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/kp) (\ud83d\udce5 130 \/ month \u00b7 \u23f1\ufe0f 13.04.2022):\n\t```\n\tpip install kp\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rapidsai\/cusignal\">cuSignal<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 590) - GPU accelerated signal processing. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rapidsai\/cusignal) (\ud83d\udc68\u200d\ud83d\udcbb 38 \u00b7 \ud83d\udd00 91 \u00b7 \ud83d\udccb 130 - 11% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rapidsai\/cusignal\n\t```\n<\/details>\n<details><summary>Show 5 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/anderskm\/gputil\">GPUtil<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 850 \u00b7 \ud83d\udc80) - A Python module for getting the GPU status from NVIDA GPUs using.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/fbcotter\/py3nvml\">py3nvml<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 200) - Python 3 Bindings for NVML library. Get NVIDIA GPU status inside your.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/nicolargo\/nvidia-ml-py3\">nvidia-ml-py3<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 77 \u00b7 \ud83d\udc80) - Python 3 Bindings for the NVIDIA Management Library. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Santosh-Gupta\/SpeedTorch\">SpeedTorch<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 650 \u00b7 \ud83d\udc80) - Library for faster pinned CPU - GPU transfer in Pytorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/stas00\/ipyexperiments\">ipyexperiments<\/a><\/b> (\ud83e\udd4914 \u00b7  \u2b50 150) - jupyter\/ipython experiment containers for GPU and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1E\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Tensorflow Utilities\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries that extend TensorFlow with additional capabilities._\n\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/addons\">TF Addons<\/a><\/b> (\ud83e\udd4736 \u00b7  \u2b50 1.5K) - Useful extra functionality for TensorFlow 2.x maintained by.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/addons) (\ud83d\udc68\u200d\ud83d\udcbb 190 \u00b7 \ud83d\udd00 500 \u00b7 \ud83d\udce6 6.2K \u00b7 \ud83d\udccb 930 - 23% open \u00b7 \u23f1\ufe0f 19.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/addons\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-addons) (\ud83d\udce5 1.9M \/ month \u00b7 \ud83d\udce6 160 \u00b7 \u23f1\ufe0f 15.02.2022):\n\t```\n\tpip install tensorflow-addons\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/datasets\">TensorFlow Datasets<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 3.2K) - TFDS is a collection of datasets ready to use with.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/datasets) (\ud83d\udc68\u200d\ud83d\udcbb 250 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udccb 1.2K - 47% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/datasets\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-datasets) (\ud83d\udce5 1.2M \/ month \u00b7 \ud83d\udce6 160 \u00b7 \u23f1\ufe0f 31.01.2022):\n\t```\n\tpip install tensorflow-datasets\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorflow-datasets) (\ud83d\udce5 4.2K \u00b7 \u23f1\ufe0f 17.08.2021):\n\t```\n\tconda install -c conda-forge tensorflow-datasets\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/hub\">tensorflow-hub<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 3.1K) - A library for transfer learning by reusing parts of.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/hub) (\ud83d\udc68\u200d\ud83d\udcbb 90 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udce6 12K \u00b7 \ud83d\udccb 640 - 2% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/hub\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-hub) (\ud83d\udce5 3.7M \/ month \u00b7 \ud83d\udce6 280 \u00b7 \u23f1\ufe0f 14.04.2021):\n\t```\n\tpip install tensorflow-hub\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorflow-hub) (\ud83d\udce5 62K \u00b7 \u23f1\ufe0f 18.04.2021):\n\t```\n\tconda install -c conda-forge tensorflow-hub\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/tensor2tensor\">tensor2tensor<\/a><\/b> (\ud83e\udd4834 \u00b7  \u2b50 12K) - Library of deep learning models and datasets designed to.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/tensor2tensor) (\ud83d\udc68\u200d\ud83d\udcbb 240 \u00b7 \ud83d\udd00 3K \u00b7 \ud83d\udce6 1.2K \u00b7 \ud83d\udccb 1.2K - 45% open \u00b7 \u23f1\ufe0f 15.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/tensor2tensor\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensor2tensor) (\ud83d\udce5 13K \/ month \u00b7 \ud83d\udce6 93 \u00b7 \u23f1\ufe0f 17.06.2020):\n\t```\n\tpip install tensor2tensor\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/model-optimization\">TF Model Optimization<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 1.2K) - A toolkit to optimize ML models for deployment for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/model-optimization) (\ud83d\udc68\u200d\ud83d\udcbb 66 \u00b7 \ud83d\udd00 280 \u00b7 \ud83d\udce6 1.8K \u00b7 \ud83d\udccb 310 - 51% open \u00b7 \u23f1\ufe0f 06.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/model-optimization\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-model-optimization) (\ud83d\udce5 160K \/ month \u00b7 \ud83d\udce6 19 \u00b7 \u23f1\ufe0f 18.03.2022):\n\t```\n\tpip install tensorflow-model-optimization\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/transform\">TensorFlow Transform<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 920) - Input pipeline framework. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/transform) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 180 \u00b7 \ud83d\udce6 850 \u00b7 \ud83d\udccb 180 - 14% open \u00b7 \u23f1\ufe0f 12.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/transform\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-transform) (\ud83d\udce5 3.2M \/ month \u00b7 \ud83d\udce6 56 \u00b7 \u23f1\ufe0f 30.03.2022):\n\t```\n\tpip install tensorflow-transform\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/keras-team\/keras-preprocessing\">Keras-Preprocessing<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 1K) - Utilities for working with image data, text data, and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/keras-team\/keras-preprocessing) (\ud83d\udc68\u200d\ud83d\udcbb 52 \u00b7 \ud83d\udd00 440 \u00b7 \ud83d\udccb 200 - 47% open \u00b7 \u23f1\ufe0f 17.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/keras-team\/keras-preprocessing\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/keras-preprocessing) (\ud83d\udce5 8.9M \/ month \u00b7 \ud83d\udce6 1.5K \u00b7 \u23f1\ufe0f 14.05.2020):\n\t```\n\tpip install keras-preprocessing\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/keras-preprocessing) (\ud83d\udce5 1.3M \u00b7 \u23f1\ufe0f 15.01.2021):\n\t```\n\tconda install -c conda-forge keras-preprocessing\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/io\">TensorFlow I\/O<\/a><\/b> (\ud83e\udd4929 \u00b7  \u2b50 550) - Dataset, streaming, and file system extensions.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/io) (\ud83d\udc68\u200d\ud83d\udcbb 92 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udccb 520 - 36% open \u00b7 \u23f1\ufe0f 18.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/io\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-io) (\ud83d\udce5 180K \/ month \u00b7 \ud83d\udce6 23 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install tensorflow-io\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/qubvel\/efficientnet\">efficientnet<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 2K \u00b7 \ud83d\udca4) - Implementation of EfficientNet model. Keras and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/qubvel\/efficientnet) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 450 \u00b7 \ud83d\udce5 230K \u00b7 \ud83d\udce6 980 \u00b7 \ud83d\udccb 120 - 50% open \u00b7 \u23f1\ufe0f 16.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/qubvel\/efficientnet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/efficientnet) (\ud83d\udce5 100K \/ month \u00b7 \ud83d\udce6 7 \u00b7 \u23f1\ufe0f 15.09.2020):\n\t```\n\tpip install efficientnet\n\t```\n- [Conda](https:\/\/anaconda.org\/anaconda\/efficientnet) (\ud83d\udce5 38 \u00b7 \u23f1\ufe0f 14.01.2022):\n\t```\n\tconda install -c anaconda efficientnet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/neural-structured-learning\">Neural Structured Learning<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 920) - Training neural models with structured signals. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/neural-structured-learning) (\ud83d\udc68\u200d\ud83d\udcbb 33 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 220 \u00b7 \ud83d\udccb 61 - 4% open \u00b7 \u23f1\ufe0f 24.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/neural-structured-learning\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/neural-structured-learning) (\ud83d\udce5 15K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 18.08.2020):\n\t```\n\tpip install neural-structured-learning\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/cloud\">TensorFlow Cloud<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 320) - The TensorFlow Cloud repository provides APIs that.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/cloud) (\ud83d\udc68\u200d\ud83d\udcbb 27 \u00b7 \ud83d\udd00 67 \u00b7 \ud83d\udce6 140 \u00b7 \ud83d\udccb 81 - 67% open \u00b7 \u23f1\ufe0f 24.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/cloud\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-cloud) (\ud83d\udce5 180K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 17.06.2021):\n\t```\n\tpip install tensorflow-cloud\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/geffy\/tffm\">tffm<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 780) - TensorFlow implementation of an arbitrary order Factorization Machine. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/geffy\/tffm) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udce6 11 \u00b7 \ud83d\udccb 40 - 45% open \u00b7 \u23f1\ufe0f 17.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/geffy\/tffm\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tffm) (\ud83d\udce5 2K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 17.01.2022):\n\t```\n\tpip install tffm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorflow\/compression\">TF Compression<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 600) - Data compression in TensorFlow. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorflow\/compression) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 210 \u00b7 \ud83d\udccb 82 - 2% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorflow\/compression\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorflow-compression) (\ud83d\udce5 860 \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 02.03.2022):\n\t```\n\tpip install tensorflow-compression\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PAIR-code\/saliency\">Saliency<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 790 \u00b7 \ud83d\udca4) - Framework-agnostic implementation for state-of-the-art.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PAIR-code\/saliency) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udce6 26 \u00b7 \ud83d\udccb 40 - 35% open \u00b7 \u23f1\ufe0f 28.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/PAIR-code\/saliency\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/saliency) (\ud83d\udce5 810 \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 03.05.2021):\n\t```\n\tpip install saliency\n\t```\n<\/details>\n<details><summary>Show 1 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/taehoonlee\/tensornets\">TensorNets<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1K \u00b7 \ud83d\udc80) - High level network definitions with pre-trained weights in.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Jax Utilities\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries that extend Jax with additional capabilities._\n\n<details><summary><b><a href=\"https:\/\/github.com\/patrick-kidger\/equinox\">equinox<\/a><\/b> (\ud83e\udd4723 \u00b7  \u2b50 490) - Callable PyTrees and filtered JIT\/grad transformations = neural.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/patrick-kidger\/equinox) (\ud83d\udc68\u200d\ud83d\udcbb 5 \u00b7 \ud83d\udd00 25 \u00b7 \ud83d\udce6 17 \u00b7 \ud83d\udccb 35 - 28% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/patrick-kidger\/equinox\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/equinox) (\ud83d\udce5 1.7K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tpip install equinox\n\t```\n<\/details>\n<details><summary>Show 1 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/ucl-bug\/jaxdf\">jaxdf<\/a><\/b> (\ud83e\udd498 \u00b7  \u2b50 48) - A JAX-based research framework for writing differentiable.. <code><a href=\"http:\/\/bit.ly\/37RvQcA\">\u2757\ufe0fLGPL-3.0<\/a><\/code> <code><img src=\"https:\/\/jax.readthedocs.io\/en\/latest\/_static\/favicon.png\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Sklearn Utilities\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries that extend scikit-learn with additional capabilities._\n\n<details><summary><b><a href=\"https:\/\/github.com\/rasbt\/mlxtend\">MLxtend<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 3.9K) - A library of extension and helper modules for Pythons data.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rasbt\/mlxtend) (\ud83d\udc68\u200d\ud83d\udcbb 88 \u00b7 \ud83d\udd00 750 \u00b7 \ud83d\udce6 5.8K \u00b7 \ud83d\udccb 410 - 25% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rasbt\/mlxtend\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/mlxtend) (\ud83d\udce5 1.5M \/ month \u00b7 \ud83d\udce6 140 \u00b7 \u23f1\ufe0f 03.09.2021):\n\t```\n\tpip install mlxtend\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/mlxtend) (\ud83d\udce5 200K \u00b7 \u23f1\ufe0f 03.09.2021):\n\t```\n\tconda install -c conda-forge mlxtend\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn\">imbalanced-learn<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 5.8K) - A Python Package to Tackle the Curse of Imbalanced.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn) (\ud83d\udc68\u200d\ud83d\udcbb 63 \u00b7 \ud83d\udd00 1.2K \u00b7 \ud83d\udce6 11K \u00b7 \ud83d\udccb 520 - 11% open \u00b7 \u23f1\ufe0f 18.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/imbalanced-learn) (\ud83d\udce5 2.5M \/ month \u00b7 \ud83d\udce6 240 \u00b7 \u23f1\ufe0f 11.01.2022):\n\t```\n\tpip install imbalanced-learn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/imbalanced-learn) (\ud83d\udce5 200K \u00b7 \u23f1\ufe0f 11.01.2022):\n\t```\n\tconda install -c conda-forge imbalanced-learn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-learn-contrib\/category_encoders\">category_encoders<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 1.9K) - A library of sklearn compatible categorical variable.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-learn-contrib\/category_encoders) (\ud83d\udc68\u200d\ud83d\udcbb 52 \u00b7 \ud83d\udd00 350 \u00b7 \ud83d\udce6 3.4K \u00b7 \ud83d\udccb 240 - 29% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-learn-contrib\/category_encoders\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/category_encoders) (\ud83d\udce5 1.9M \/ month \u00b7 \ud83d\udce6 23 \u00b7 \u23f1\ufe0f 14.10.2018):\n\t```\n\tpip install category_encoders\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/category_encoders) (\ud83d\udce5 150K \u00b7 \u23f1\ufe0f 09.03.2022):\n\t```\n\tconda install -c conda-forge category_encoders\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/guofei9987\/scikit-opt\">scikit-opt<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 3.2K) - Genetic Algorithm, Particle Swarm Optimization, Simulated.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/guofei9987\/scikit-opt) (\ud83d\udc68\u200d\ud83d\udcbb 14 \u00b7 \ud83d\udd00 740 \u00b7 \ud83d\udce6 74 \u00b7 \ud83d\udccb 140 - 27% open \u00b7 \u23f1\ufe0f 08.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/guofei9987\/scikit-opt\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scikit-opt) (\ud83d\udce5 3.2K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 14.01.2022):\n\t```\n\tpip install scikit-opt\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/iskandr\/fancyimpute\">fancyimpute<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 1.1K \u00b7 \ud83d\udca4) - Multivariate imputation and matrix completion.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/iskandr\/fancyimpute) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 1.2K \u00b7 \ud83d\udccb 110 - 0% open \u00b7 \u23f1\ufe0f 21.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/iskandr\/fancyimpute\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/fancyimpute) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 29 \u00b7 \u23f1\ufe0f 21.10.2021):\n\t```\n\tpip install fancyimpute\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/koaning\/scikit-lego\">scikit-lego<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 810) - Extra blocks for scikit-learn pipelines. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/koaning\/scikit-lego) (\ud83d\udc68\u200d\ud83d\udcbb 50 \u00b7 \ud83d\udd00 85 \u00b7 \ud83d\udce6 45 \u00b7 \ud83d\udccb 240 - 9% open \u00b7 \u23f1\ufe0f 20.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/koaning\/scikit-lego\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scikit-lego) (\ud83d\udce5 13K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 20.04.2022):\n\t```\n\tpip install scikit-lego\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/scikit-lego) (\ud83d\udce5 18K \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tconda install -c conda-forge scikit-lego\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-learn-contrib\/lightning\">sklearn-contrib-lightning<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 1.6K) - Large-scale linear classification, regression and.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-learn-contrib\/lightning) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce5 230 \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 93 - 54% open \u00b7 \u23f1\ufe0f 30.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-learn-contrib\/lightning\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sklearn-contrib-lightning) (\ud83d\udce5 1.8K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 30.01.2022):\n\t```\n\tpip install sklearn-contrib-lightning\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sklearn-contrib-lightning) (\ud83d\udce5 160K \u00b7 \u23f1\ufe0f 13.11.2021):\n\t```\n\tconda install -c conda-forge sklearn-contrib-lightning\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/yzhao062\/combo\">combo<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 580 \u00b7 \ud83d\udca4) - (AAAI 20) A Python Toolbox for Machine Learning Model.. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code>xgboost<\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/yzhao062\/combo) (\ud83d\udc68\u200d\ud83d\udcbb 1 \u00b7 \ud83d\udd00 99 \u00b7 \ud83d\udce6 460 \u00b7 \ud83d\udccb 14 - 78% open \u00b7 \u23f1\ufe0f 02.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/yzhao062\/combo\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/combo) (\ud83d\udce5 29K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 02.04.2022):\n\t```\n\tpip install combo\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/trent-b\/iterative-stratification\">iterative-stratification<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 650) - scikit-learn cross validators for iterative.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/trent-b\/iterative-stratification) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 59 \u00b7 \ud83d\udce6 200 \u00b7 \ud83d\udccb 19 - 21% open \u00b7 \u23f1\ufe0f 11.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/trent-b\/iterative-stratification\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/iterative-stratification) (\ud83d\udce5 900K \/ month \u00b7 \ud83d\udce6 8 \u00b7 \u23f1\ufe0f 03.10.2021):\n\t```\n\tpip install iterative-stratification\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-learn-contrib\/DESlib\">DESlib<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 400 \u00b7 \ud83d\udca4) - A Python library for dynamic classifier and ensemble selection. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-learn-contrib\/DESlib) (\ud83d\udc68\u200d\ud83d\udcbb 13 \u00b7 \ud83d\udd00 73 \u00b7 \ud83d\udce6 24 \u00b7 \ud83d\udccb 140 - 9% open \u00b7 \u23f1\ufe0f 10.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-learn-contrib\/DESlib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deslib) (\ud83d\udce5 510 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 08.02.2021):\n\t```\n\tpip install deslib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-tda\/scikit-tda\">scikit-tda<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 340) - Topological Data Analysis for Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-tda\/scikit-tda) (\ud83d\udc68\u200d\ud83d\udcbb 4 \u00b7 \ud83d\udd00 43 \u00b7 \ud83d\udce6 29 \u00b7 \ud83d\udccb 19 - 78% open \u00b7 \u23f1\ufe0f 13.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-tda\/scikit-tda\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scikit-tda) (\ud83d\udce5 940 \/ month \u00b7 \u23f1\ufe0f 03.08.2021):\n\t```\n\tpip install scikit-tda\n\t```\n<\/details>\n<details><summary>Show 6 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/TeamHG-Memex\/sklearn-crfsuite\">sklearn-crfsuite<\/a><\/b> (\ud83e\udd4826 \u00b7  \u2b50 400 \u00b7 \ud83d\udc80) - scikit-learn inspired API for CRFsuite. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/scikit-multilearn\/scikit-multilearn\">scikit-multilearn<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 740 \u00b7 \ud83d\udc80) - A scikit-learn based module for multi-label et. al... <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/scikit-learn-contrib\/skope-rules\">skope-rules<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 460 \u00b7 \ud83d\udc80) - machine learning with logical rules in Python. <code><a href=\"https:\/\/tldrlegal.com\/search?q=BSD-1-Clause\">\u2757\ufe0fBSD-1-Clause<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/mathurinm\/celer\">celer<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 140) - Fast solver for L1-type problems: Lasso, sparse Logisitic regression,.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/skggm\/skggm\">skggm<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 200) - Scikit-learn compatible estimation of general graphical models. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/amueller\/dabl\">dabl<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 110 \u00b7 \ud83d\udca4) - Data Analysis Baseline Library. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Pytorch Utilities\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries that extend Pytorch with additional capabilities._\n\n<details><summary><b><a href=\"https:\/\/github.com\/KevinMusgrave\/pytorch-metric-learning\">PML<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 4.4K) - The easiest way to use deep metric learning in your application. Modular,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/KevinMusgrave\/pytorch-metric-learning) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 540 \u00b7 \ud83d\udce6 260 \u00b7 \ud83d\udccb 350 - 13% open \u00b7 \u23f1\ufe0f 30.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/KevinMusgrave\/pytorch-metric-learning\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch-metric-learning) (\ud83d\udce5 910K \/ month \u00b7 \ud83d\udce6 9 \u00b7 \u23f1\ufe0f 02.04.2022):\n\t```\n\tpip install pytorch-metric-learning\n\t```\n- [Conda](https:\/\/anaconda.org\/metric-learning\/pytorch-metric-learning) (\ud83d\udce5 6.7K \u00b7 \u23f1\ufe0f 30.03.2022):\n\t```\n\tconda install -c metric-learning pytorch-metric-learning\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/huggingface\/accelerate\">accelerate<\/a><\/b> (\ud83e\udd4732 \u00b7  \u2b50 2.4K) - A simple way to train and use PyTorch models with multi-.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/huggingface\/accelerate) (\ud83d\udc68\u200d\ud83d\udcbb 43 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udce6 480 \u00b7 \ud83d\udccb 200 - 34% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/huggingface\/accelerate\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/accelerate) (\ud83d\udce5 970K \/ month \u00b7 \ud83d\udce6 13 \u00b7 \u23f1\ufe0f 29.04.2022):\n\t```\n\tpip install accelerate\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/accelerate) (\ud83d\udce5 1.5K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge accelerate\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PyTorchLightning\/lightning-flash\">lightning-flash<\/a><\/b> (\ud83e\udd4729 \u00b7  \u2b50 1.5K) - Your PyTorch AI Factory - Flash enables you to easily.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PyTorchLightning\/lightning-flash) (\ud83d\udc68\u200d\ud83d\udcbb 69 \u00b7 \ud83d\udd00 160 \u00b7 \ud83d\udce6 70 \u00b7 \ud83d\udccb 460 - 7% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PyTorchLightning\/lightning-flash\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/lightning-flash) (\ud83d\udce5 5.1K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install lightning-flash\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/lightning-flash) (\ud83d\udce5 1.3K \u00b7 \u23f1\ufe0f 28.04.2022):\n\t```\n\tconda install -c conda-forge lightning-flash\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/sksq96\/pytorch-summary\">pytorch-summary<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 3.5K \u00b7 \ud83d\udca4) - Model summary in PyTorch similar to `model.summary()`.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/sksq96\/pytorch-summary) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 390 \u00b7 \ud83d\udce6 4.9K \u00b7 \ud83d\udccb 160 - 74% open \u00b7 \u23f1\ufe0f 10.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/sksq96\/pytorch-summary\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchsummary) (\ud83d\udce5 93K \/ month \u00b7 \ud83d\udce6 71 \u00b7 \u23f1\ufe0f 26.09.2018):\n\t```\n\tpip install torchsummary\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jettify\/pytorch-optimizer\">pytorch-optimizer<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 2.4K) - torch-optimizer -- collection of optimizers for.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jettify\/pytorch-optimizer) (\ud83d\udc68\u200d\ud83d\udcbb 25 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udce6 570 \u00b7 \ud83d\udccb 44 - 36% open \u00b7 \u23f1\ufe0f 11.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/jettify\/pytorch-optimizer\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torch_optimizer) (\ud83d\udce5 60K \/ month \u00b7 \ud83d\udce6 23 \u00b7 \u23f1\ufe0f 31.10.2021):\n\t```\n\tpip install torch_optimizer\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/torch-optimizer) (\ud83d\udce5 1.2K \u00b7 \u23f1\ufe0f 31.10.2021):\n\t```\n\tconda install -c conda-forge torch-optimizer\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rtqichen\/torchdiffeq\">torchdiffeq<\/a><\/b> (\ud83e\udd4825 \u00b7  \u2b50 4.1K) - Differentiable ODE solvers with full GPU support and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rtqichen\/torchdiffeq) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 730 \u00b7 \ud83d\udce6 240 \u00b7 \ud83d\udccb 180 - 22% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rtqichen\/torchdiffeq\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchdiffeq) (\ud83d\udce5 6.7K \/ month \u00b7 \ud83d\udce6 20 \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tpip install torchdiffeq\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/torchdiffeq) (\ud83d\udce5 4.9K \u00b7 \u23f1\ufe0f 03.06.2021):\n\t```\n\tconda install -c conda-forge torchdiffeq\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/dreamquark-ai\/tabnet\">TabNet<\/a><\/b> (\ud83e\udd4824 \u00b7  \u2b50 1.7K) - PyTorch implementation of TabNet paper :.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/dreamquark-ai\/tabnet) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 340 \u00b7 \ud83d\udccb 240 - 15% open \u00b7 \u23f1\ufe0f 23.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/dreamquark-ai\/tabnet\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch-tabnet) (\ud83d\udce5 17K \/ month \u00b7 \ud83d\udce6 7 \u00b7 \u23f1\ufe0f 02.02.2021):\n\t```\n\tpip install pytorch-tabnet\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pytorch-tabnet) (\ud83d\udce5 370 \u00b7 \u23f1\ufe0f 30.12.2021):\n\t```\n\tconda install -c conda-forge pytorch-tabnet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rusty1s\/pytorch_scatter\">torch-scatter<\/a><\/b> (\ud83e\udd4824 \u00b7  \u2b50 970) - PyTorch Extension Library of Optimized Scatter Operations. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rusty1s\/pytorch_scatter) (\ud83d\udc68\u200d\ud83d\udcbb 19 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udccb 260 - 8% open \u00b7 \u23f1\ufe0f 25.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rusty1s\/pytorch_scatter\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torch-scatter) (\ud83d\udce5 30K \/ month \u00b7 \ud83d\udce6 43 \u00b7 \u23f1\ufe0f 22.10.2021):\n\t```\n\tpip install torch-scatter\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pytorch_scatter) (\ud83d\udce5 73K \u00b7 \u23f1\ufe0f 21.03.2022):\n\t```\n\tconda install -c conda-forge pytorch_scatter\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rusty1s\/pytorch_sparse\">PyTorch Sparse<\/a><\/b> (\ud83e\udd4824 \u00b7  \u2b50 620) - PyTorch Extension Library of Optimized Autograd Sparse.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rusty1s\/pytorch_sparse) (\ud83d\udc68\u200d\ud83d\udcbb 25 \u00b7 \ud83d\udd00 86 \u00b7 \ud83d\udccb 180 - 17% open \u00b7 \u23f1\ufe0f 26.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rusty1s\/pytorch_sparse\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torch-sparse) (\ud83d\udce5 24K \/ month \u00b7 \ud83d\udce6 37 \u00b7 \u23f1\ufe0f 11.03.2022):\n\t```\n\tpip install torch-sparse\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pytorch_sparse) (\ud83d\udce5 83K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c conda-forge pytorch_sparse\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/asappresearch\/sru\">SRU<\/a><\/b> (\ud83e\udd4823 \u00b7  \u2b50 2K \u00b7 \ud83d\udca4) - Training RNNs as Fast as CNNs (https:\/\/arxiv.org\/abs\/1709.02755). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/asappresearch\/sru) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udce6 18 \u00b7 \ud83d\udccb 120 - 45% open \u00b7 \u23f1\ufe0f 19.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/asappresearch\/sru\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sru) (\ud83d\udce5 1.8K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 17.06.2021):\n\t```\n\tpip install sru\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tristandeleu\/pytorch-meta\">Torchmeta<\/a><\/b> (\ud83e\udd4823 \u00b7  \u2b50 1.6K \u00b7 \ud83d\udca4) - A collection of extensions and data-loaders for few-shot.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tristandeleu\/pytorch-meta) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce6 89 \u00b7 \ud83d\udccb 130 - 32% open \u00b7 \u23f1\ufe0f 20.09.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/tristandeleu\/pytorch-meta\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchmeta) (\ud83d\udce5 1.8K \/ month \u00b7 \u23f1\ufe0f 20.09.2021):\n\t```\n\tpip install torchmeta\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rwightman\/gen-efficientnet-pytorch\">EfficientNets<\/a><\/b> (\ud83e\udd4823 \u00b7  \u2b50 1.5K \u00b7 \ud83d\udca4) - Pretrained EfficientNet, EfficientNet-Lite, MixNet,.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rwightman\/gen-efficientnet-pytorch) (\ud83d\udc68\u200d\ud83d\udcbb 5 \u00b7 \ud83d\udd00 190 \u00b7 \ud83d\udce6 100 \u00b7 \ud83d\udccb 53 - 3% open \u00b7 \u23f1\ufe0f 08.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/rwightman\/gen-efficientnet-pytorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/geffnet) (\ud83d\udce5 11K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 08.07.2021):\n\t```\n\tpip install geffnet\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/higher\">Higher<\/a><\/b> (\ud83e\udd4823 \u00b7  \u2b50 1.4K \u00b7 \ud83d\udca4) - higher is a pytorch library allowing users to obtain higher.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/higher) (\ud83d\udc68\u200d\ud83d\udcbb 9 \u00b7 \ud83d\udd00 98 \u00b7 \ud83d\udce6 130 \u00b7 \ud83d\udccb 99 - 49% open \u00b7 \u23f1\ufe0f 26.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/higher\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/higher) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 14.07.2020):\n\t```\n\tpip install higher\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/BloodAxe\/pytorch-toolbelt\">Pytorch Toolbelt<\/a><\/b> (\ud83e\udd4823 \u00b7  \u2b50 1.2K) - PyTorch extensions for fast R&D prototyping and Kaggle.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/BloodAxe\/pytorch-toolbelt) (\ud83d\udc68\u200d\ud83d\udcbb 7 \u00b7 \ud83d\udd00 94 \u00b7 \ud83d\udccb 27 - 22% open \u00b7 \u23f1\ufe0f 24.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/BloodAxe\/pytorch-toolbelt\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pytorch_toolbelt) (\ud83d\udce5 7.3K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 10.03.2022):\n\t```\n\tpip install pytorch_toolbelt\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lucidrains\/performer-pytorch\">Performer Pytorch<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 820) - An implementation of Performer, a linear attention-based.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lucidrains\/performer-pytorch) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 46 \u00b7 \ud83d\udccb 76 - 44% open \u00b7 \u23f1\ufe0f 02.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/lucidrains\/performer-pytorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/performer-pytorch) (\ud83d\udce5 9K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 02.02.2022):\n\t```\n\tpip install performer-pytorch\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/lucidrains\/reformer-pytorch\">reformer-pytorch<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1.7K \u00b7 \ud83d\udcc9) - Reformer, the efficient Transformer, in Pytorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/lucidrains\/reformer-pytorch) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udccb 120 - 11% open \u00b7 \u23f1\ufe0f 06.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/lucidrains\/reformer-pytorch\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/reformer-pytorch) (\ud83d\udce5 4.7K \/ month \u00b7 \u23f1\ufe0f 06.11.2021):\n\t```\n\tpip install reformer-pytorch\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google-research\/torchsde\">torchsde<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 970 \u00b7 \ud83d\udca4) - Differentiable SDE solvers with GPU support and efficient.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google-research\/torchsde) (\ud83d\udc68\u200d\ud83d\udcbb 5 \u00b7 \ud83d\udd00 100 \u00b7 \ud83d\udce6 14 \u00b7 \ud83d\udccb 48 - 14% open \u00b7 \u23f1\ufe0f 26.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/google-research\/torchsde\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torchsde) (\ud83d\udce5 1K \/ month \u00b7 \u23f1\ufe0f 20.07.2021):\n\t```\n\tpip install torchsde\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/torchsde) (\ud83d\udce5 8.9K \u00b7 \u23f1\ufe0f 12.07.2021):\n\t```\n\tconda install -c conda-forge torchsde\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/harvardnlp\/pytorch-struct\">Torch-Struct<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 1K) - Fast, general, and tested differentiable structured prediction.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/harvardnlp\/pytorch-struct) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 81 \u00b7 \ud83d\udccb 54 - 44% open \u00b7 \u23f1\ufe0f 30.01.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/harvardnlp\/pytorch-struct\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/torch-struct) (\ud83d\udce5 33K \/ month \u00b7 \u23f1\ufe0f 14.02.2021):\n\t```\n\tpip install torch-struct\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/szagoruyko\/pytorchviz\">pytorchviz<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 2.2K \u00b7 \ud83d\udca4) - A small package to create visualizations of PyTorch execution.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/szagoruyko\/pytorchviz) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 650 \u00b7 \ud83d\udccb 54 - 35% open \u00b7 \u23f1\ufe0f 15.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/szagoruyko\/pytorchviz\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/abhishekkrthakur\/tez\">Tez<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 1K) - Tez is a super-simple and lightweight Trainer for PyTorch. It also.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/abhishekkrthakur\/tez) (\ud83d\udc68\u200d\ud83d\udcbb 1 \u00b7 \ud83d\udd00 130 \u00b7 \ud83d\udce6 24 \u00b7 \ud83d\udccb 32 - 50% open \u00b7 \u23f1\ufe0f 15.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/abhishekkrthakur\/tez\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tez) (\ud83d\udce5 920 \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tpip install tez\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/geohot\/tinygrad\">tinygrad<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 6K) - You like pytorch? You like micrograd? You love tinygrad!. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/geohot\/tinygrad) (\ud83d\udc68\u200d\ud83d\udcbb 58 \u00b7 \ud83d\udd00 600 \u00b7 \ud83d\udce6 2 \u00b7 \ud83d\udccb 110 - 19% open \u00b7 \u23f1\ufe0f 05.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/geohot\/tinygrad\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/madgrad\">madgrad<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 760) - MADGRAD Optimization Method. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/madgrad) (\ud83d\udc68\u200d\ud83d\udcbb 2 \u00b7 \ud83d\udd00 55 \u00b7 \ud83d\udce6 25 \u00b7 \ud83d\udccb 8 - 12% open \u00b7 \u23f1\ufe0f 10.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/madgrad\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/madgrad) (\ud83d\udce5 7.8K \/ month \u00b7 \u23f1\ufe0f 08.03.2022):\n\t```\n\tpip install madgrad\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/parrt\/tensor-sensor\">Tensor Sensor<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 640) - The goal of this library is to generate more helpful.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/parrt\/tensor-sensor) (\ud83d\udc68\u200d\ud83d\udcbb 4 \u00b7 \ud83d\udd00 33 \u00b7 \ud83d\udce6 7 \u00b7 \ud83d\udccb 23 - 34% open \u00b7 \u23f1\ufe0f 07.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/parrt\/tensor-sensor\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensor-sensor) (\ud83d\udce5 6.4K \/ month \u00b7 \u23f1\ufe0f 11.12.2021):\n\t```\n\tpip install tensor-sensor\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensor-sensor) (\ud83d\udce5 220 \u00b7 \u23f1\ufe0f 11.12.2021):\n\t```\n\tconda install -c conda-forge tensor-sensor\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/achaiah\/pywick\">Pywick<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 370 \u00b7 \ud83d\udca4) - High-level batteries-included neural network training library for.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/achaiah\/pywick) (\ud83d\udc68\u200d\ud83d\udcbb 4 \u00b7 \ud83d\udd00 38 \u00b7 \ud83d\udce6 6 \u00b7 \ud83d\udccb 14 - 14% open \u00b7 \u23f1\ufe0f 22.10.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/achaiah\/pywick\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pywick) (\ud83d\udce5 27 \/ month \u00b7 \u23f1\ufe0f 22.10.2021):\n\t```\n\tpip install pywick\n\t```\n<\/details>\n<details><summary>Show 8 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/Cadene\/pretrained-models.pytorch\">pretrainedmodels<\/a><\/b> (\ud83e\udd4732 \u00b7  \u2b50 8.5K \u00b7 \ud83d\udc80) - Pretrained ConvNets for pytorch: NASNet, ResNeXt,.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch\">EfficientNet-PyTorch<\/a><\/b> (\ud83e\udd4827 \u00b7  \u2b50 6.9K \u00b7 \ud83d\udc80) - A PyTorch implementation of EfficientNet and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/GRAAL-Research\/poutyne\">Poutyne<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 520) - A simplified framework and utilities for PyTorch. <code><a href=\"http:\/\/bit.ly\/37RvQcA\">\u2757\ufe0fLGPL-3.0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/Luolc\/AdaBound\">AdaBound<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 2.9K \u00b7 \ud83d\udc80) - An optimizer that trains as fast as Adam and as good as SGD. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/adobe\/antialiased-cnns\">Antialiased CNNs<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 1.5K \u00b7 \ud83d\udca4) - pip install antialiased-cnns to improve stability and.. <code><a href=\"https:\/\/tldrlegal.com\/search?q=CC%20BY-NC-SA%204.0\">\u2757\ufe0fCC BY-NC-SA 4.0<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/lucidrains\/lambda-networks\">Lambda Networks<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 1.5K \u00b7 \ud83d\udc80) - Implementation of LambdaNetworks, a new approach to.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/karpathy\/micrograd\">micrograd<\/a><\/b> (\ud83e\udd4916 \u00b7  \u2b50 2K \u00b7 \ud83d\udc80) - A tiny scalar-valued autograd engine and a neural net library.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/TorchDrift\/TorchDrift\">TorchDrift<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 210 \u00b7 \ud83d\udca4) - Drift Detection for your PyTorch Models. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n<\/details>\n<br>\n\n## Database Clients\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n_Libraries for connecting to, operating, and querying databases._\n\n\ud83d\udd17&nbsp;<b><a href=\"https:\/\/github.com\/ml-tooling\/best-of-python#database-clients\">best-of-python - DB Clients<\/a><\/b> ( \u2b50 2.1K)  - Collection of database clients for python.\n\n<br>\n\n## Others\n\n<a href=\"#contents\"><img align=\"right\" width=\"15\" height=\"15\" src=\"https:\/\/git.io\/JtehR\" alt=\"Back to top\"><\/a>\n\n<details><summary><b><a href=\"https:\/\/github.com\/scipy\/scipy\">scipy<\/a><\/b> (\ud83e\udd4749 \u00b7  \u2b50 9.5K) - Ecosystem of open-source software for mathematics, science, and engineering. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scipy\/scipy) (\ud83d\udc68\u200d\ud83d\udcbb 1.3K \u00b7 \ud83d\udd00 4.1K \u00b7 \ud83d\udce5 350K \u00b7 \ud83d\udce6 500K \u00b7 \ud83d\udccb 8.5K - 21% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scipy\/scipy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/scipy) (\ud83d\udce5 42M \/ month \u00b7 \ud83d\udce6 57K \u00b7 \u23f1\ufe0f 05.02.2022):\n\t```\n\tpip install scipy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/scipy) (\ud83d\udce5 23M \u00b7 \u23f1\ufe0f 09.02.2022):\n\t```\n\tconda install -c conda-forge scipy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/sympy\/sympy\">SymPy<\/a><\/b> (\ud83e\udd4744 \u00b7  \u2b50 9.2K) - A computer algebra system written in pure Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/sympy\/sympy) (\ud83d\udc68\u200d\ud83d\udcbb 1.1K \u00b7 \ud83d\udd00 3.7K \u00b7 \ud83d\udce5 450K \u00b7 \ud83d\udce6 42K \u00b7 \ud83d\udccb 12K - 36% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/sympy\/sympy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/sympy) (\ud83d\udce5 1.8M \/ month \u00b7 \ud83d\udce6 4.1K \u00b7 \u23f1\ufe0f 20.03.2022):\n\t```\n\tpip install sympy\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/sympy) (\ud83d\udce5 2M \u00b7 \u23f1\ufe0f 20.03.2022):\n\t```\n\tconda install -c conda-forge sympy\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/streamlit\/streamlit\">Streamlit<\/a><\/b> (\ud83e\udd4738 \u00b7  \u2b50 19K) - Streamlit The fastest way to build data apps in Python. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/streamlit\/streamlit) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 1.7K \u00b7 \ud83d\udce6 280 \u00b7 \ud83d\udccb 2.4K - 24% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/streamlit\/streamlit\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/streamlit) (\ud83d\udce5 1M \/ month \u00b7 \ud83d\udce6 370 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install streamlit\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/gradio-app\/gradio\">Gradio<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 6.7K) - Wrap UIs around any model, share with anyone. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/gradio-app\/gradio) (\ud83d\udc68\u200d\ud83d\udcbb 72 \u00b7 \ud83d\udd00 420 \u00b7 \ud83d\udce6 750 \u00b7 \ud83d\udccb 590 - 20% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/gradio-app\/gradio\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gradio) (\ud83d\udce5 150K \/ month \u00b7 \ud83d\udce6 18 \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tpip install gradio\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/yzhao062\/pyod\">PyOD<\/a><\/b> (\ud83e\udd4735 \u00b7  \u2b50 5.6K) - A Comprehensive and Scalable Python Library for Outlier Detection (Anomaly.. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/yzhao062\/pyod) (\ud83d\udc68\u200d\ud83d\udcbb 38 \u00b7 \ud83d\udd00 1.1K \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 250 - 49% open \u00b7 \u23f1\ufe0f 23.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/yzhao062\/pyod\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyod) (\ud83d\udce5 490K \/ month \u00b7 \ud83d\udce6 32 \u00b7 \u23f1\ufe0f 23.04.2022):\n\t```\n\tpip install pyod\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyod) (\ud83d\udce5 20K \u00b7 \u23f1\ufe0f 24.04.2022):\n\t```\n\tconda install -c conda-forge pyod\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/deepchem\/deepchem\">DeepChem<\/a><\/b> (\ud83e\udd4734 \u00b7  \u2b50 3.6K) - Democratizing Deep-Learning for Drug Discovery, Quantum Chemistry,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1A\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/deepchem\/deepchem) (\ud83d\udc68\u200d\ud83d\udcbb 190 \u00b7 \ud83d\udd00 1.3K \u00b7 \ud83d\udce6 84 \u00b7 \ud83d\udccb 1.5K - 31% open \u00b7 \u23f1\ufe0f 22.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/deepchem\/deepchem\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/deepchem) (\ud83d\udce5 8K \/ month \u00b7 \ud83d\udce6 4 \u00b7 \u23f1\ufe0f 22.04.2022):\n\t```\n\tpip install deepchem\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/deepchem) (\ud83d\udce5 9.4K \u00b7 \u23f1\ufe0f 19.01.2022):\n\t```\n\tconda install -c conda-forge deepchem\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PaddlePaddle\/PaddleHub\">PaddleHub<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 7.9K) - Awesome pre-trained models toolkit based on.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1M\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PaddlePaddle\/PaddleHub) (\ud83d\udc68\u200d\ud83d\udcbb 61 \u00b7 \ud83d\udd00 1.6K \u00b7 \ud83d\udce5 570 \u00b7 \ud83d\udce6 770 \u00b7 \ud83d\udccb 1.1K - 39% open \u00b7 \u23f1\ufe0f 15.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PaddlePaddle\/PaddleHub\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/paddlehub) (\ud83d\udce5 12K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 28.12.2021):\n\t```\n\tpip install paddlehub\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/simonw\/datasette\">Datasette<\/a><\/b> (\ud83e\udd4733 \u00b7  \u2b50 6K) - An open source multi-tool for exploring and publishing data. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/simonw\/datasette) (\ud83d\udc68\u200d\ud83d\udcbb 63 \u00b7 \ud83d\udd00 410 \u00b7 \ud83d\udce5 36 \u00b7 \ud83d\udce6 650 \u00b7 \ud83d\udccb 1.3K - 26% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/simonw\/datasette\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/datasette) (\ud83d\udce5 260K \/ month \u00b7 \ud83d\udce6 160 \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tpip install datasette\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/datasette) (\ud83d\udce5 3.6K \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tconda install -c conda-forge datasette\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/HIPS\/autograd\">Autograd<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 5.8K) - Efficiently computes derivatives of numpy code. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/HIPS\/autograd) (\ud83d\udc68\u200d\ud83d\udcbb 51 \u00b7 \ud83d\udd00 820 \u00b7 \ud83d\udce6 3.3K \u00b7 \ud83d\udccb 390 - 42% open \u00b7 \u23f1\ufe0f 08.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/HIPS\/autograd\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/autograd) (\ud83d\udce5 1.1M \/ month \u00b7 \ud83d\udce6 280 \u00b7 \u23f1\ufe0f 08.04.2022):\n\t```\n\tpip install autograd\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/autograd) (\ud83d\udce5 210K \u00b7 \u23f1\ufe0f 25.07.2019):\n\t```\n\tconda install -c conda-forge autograd\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/datalad\/datalad\">datalad<\/a><\/b> (\ud83e\udd4832 \u00b7  \u2b50 310) - Keep code, data, containers under control with git and git-annex. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/datalad\/datalad) (\ud83d\udc68\u200d\ud83d\udcbb 46 \u00b7 \ud83d\udd00 89 \u00b7 \ud83d\udccb 3.5K - 13% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/datalad\/datalad\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/datalad) (\ud83d\udce5 6.8K \/ month \u00b7 \ud83d\udce6 52 \u00b7 \u23f1\ufe0f 21.04.2022):\n\t```\n\tpip install datalad\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/datalad) (\ud83d\udce5 200K \u00b7 \u23f1\ufe0f 19.04.2022):\n\t```\n\tconda install -c conda-forge datalad\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/carla-simulator\/carla\">carla<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 7.7K) - Open-source simulator for autonomous driving research. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/carla-simulator\/carla) (\ud83d\udc68\u200d\ud83d\udcbb 140 \u00b7 \ud83d\udd00 2.3K \u00b7 \ud83d\udce6 170 \u00b7 \ud83d\udccb 3.9K - 15% open \u00b7 \u23f1\ufe0f 19.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/carla-simulator\/carla\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/carla) (\ud83d\udce5 11K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 17.11.2021):\n\t```\n\tpip install carla\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/serge-sans-paille\/pythran\">Pythran<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 1.7K) - Ahead of Time compiler for numeric kernels. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/serge-sans-paille\/pythran) (\ud83d\udc68\u200d\ud83d\udcbb 65 \u00b7 \ud83d\udd00 170 \u00b7 \ud83d\udce6 130 \u00b7 \ud83d\udccb 760 - 14% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/serge-sans-paille\/pythran\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pythran) (\ud83d\udce5 270K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 14.12.2021):\n\t```\n\tpip install pythran\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pythran) (\ud83d\udce5 230K \u00b7 \u23f1\ufe0f 10.04.2022):\n\t```\n\tconda install -c conda-forge pythran\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/inducer\/pyopencl\">pyopencl<\/a><\/b> (\ud83e\udd4831 \u00b7  \u2b50 880) - OpenCL integration for Python, plus shiny features. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/inducer\/pyopencl) (\ud83d\udc68\u200d\ud83d\udcbb 90 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udce6 720 \u00b7 \ud83d\udccb 310 - 22% open \u00b7 \u23f1\ufe0f 01.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/inducer\/pyopencl\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyopencl) (\ud83d\udce5 24K \/ month \u00b7 \ud83d\udce6 190 \u00b7 \u23f1\ufe0f 19.04.2022):\n\t```\n\tpip install pyopencl\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyopencl) (\ud83d\udce5 590K \u00b7 \u23f1\ufe0f 19.04.2022):\n\t```\n\tconda install -c conda-forge pyopencl\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-learn-contrib\/hdbscan\">hdbscan<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 2.2K) - A high performance implementation of HDBSCAN clustering. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-learn-contrib\/hdbscan) (\ud83d\udc68\u200d\ud83d\udcbb 78 \u00b7 \ud83d\udd00 400 \u00b7 \ud83d\udce6 1.3K \u00b7 \ud83d\udccb 420 - 63% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-learn-contrib\/hdbscan\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/hdbscan) (\ud83d\udce5 380K \/ month \u00b7 \ud83d\udce6 150 \u00b7 \u23f1\ufe0f 08.02.2022):\n\t```\n\tpip install hdbscan\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/hdbscan) (\ud83d\udce5 1.1M \u00b7 \u23f1\ufe0f 11.02.2022):\n\t```\n\tconda install -c conda-forge hdbscan\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/PennyLaneAI\/pennylane\">PennyLane<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 1.2K) - PennyLane is a cross-platform Python library for differentiable.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/PennyLaneAI\/pennylane) (\ud83d\udc68\u200d\ud83d\udcbb 96 \u00b7 \ud83d\udd00 360 \u00b7 \ud83d\udce5 60 \u00b7 \ud83d\udccb 690 - 26% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/PennyLaneAI\/PennyLane\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pennylane) (\ud83d\udce5 14K \/ month \u00b7 \ud83d\udce6 32 \u00b7 \u23f1\ufe0f 25.04.2022):\n\t```\n\tpip install pennylane\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pennylane) (\ud83d\udce5 850 \u00b7 \u23f1\ufe0f 01.05.2022):\n\t```\n\tconda install -c conda-forge pennylane\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tensorly\/tensorly\">tensorly<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 1.2K) - TensorLy: Tensor Learning in Python. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tensorly\/tensorly) (\ud83d\udc68\u200d\ud83d\udcbb 52 \u00b7 \ud83d\udd00 230 \u00b7 \ud83d\udce6 250 \u00b7 \ud83d\udccb 190 - 25% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tensorly\/tensorly\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tensorly) (\ud83d\udce5 5.2K \/ month \u00b7 \ud83d\udce6 30 \u00b7 \u23f1\ufe0f 08.11.2021):\n\t```\n\tpip install tensorly\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/tensorly) (\ud83d\udce5 230K \u00b7 \u23f1\ufe0f 09.12.2021):\n\t```\n\tconda install -c conda-forge tensorly\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/wireservice\/agate\">agate<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 1.1K \u00b7 \ud83d\udca4) - A Python data analysis library that is optimized for humans instead of.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/wireservice\/agate) (\ud83d\udc68\u200d\ud83d\udcbb 49 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 890 \u00b7 \ud83d\udccb 680 - 7% open \u00b7 \u23f1\ufe0f 15.07.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/wireservice\/agate\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/agate) (\ud83d\udce5 1.9M \/ month \u00b7 \ud83d\udce6 130 \u00b7 \u23f1\ufe0f 15.07.2021):\n\t```\n\tpip install agate\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/agate) (\ud83d\udce5 81K \u00b7 \u23f1\ufe0f 16.07.2021):\n\t```\n\tconda install -c conda-forge agate\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/nicodv\/kmodes\">kmodes<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 1K) - Python implementations of the k-modes and k-prototypes clustering.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/nicodv\/kmodes) (\ud83d\udc68\u200d\ud83d\udcbb 21 \u00b7 \ud83d\udd00 370 \u00b7 \ud83d\udce6 1.2K \u00b7 \ud83d\udccb 140 - 10% open \u00b7 \u23f1\ufe0f 14.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/nicodv\/kmodes\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/kmodes) (\ud83d\udce5 340K \/ month \u00b7 \ud83d\udce6 26 \u00b7 \u23f1\ufe0f 14.04.2022):\n\t```\n\tpip install kmodes\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/kmodes) (\ud83d\udce5 8K \u00b7 \u23f1\ufe0f 15.04.2022):\n\t```\n\tconda install -c conda-forge kmodes\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pyjanitor-devs\/pyjanitor\">pyjanitor<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 910 \u00b7 \ud83d\udcc8) - Clean APIs for data cleaning. Python implementation of R package.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pyjanitor-devs\/pyjanitor) (\ud83d\udc68\u200d\ud83d\udcbb 100 \u00b7 \ud83d\udd00 150 \u00b7 \ud83d\udce6 170 \u00b7 \ud83d\udccb 470 - 22% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pyjanitor-devs\/pyjanitor\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyjanitor) (\ud83d\udce5 18K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install pyjanitor\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/pyjanitor) (\ud83d\udce5 120K \u00b7 \u23f1\ufe0f 22.11.2021):\n\t```\n\tconda install -c conda-forge pyjanitor\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/adapter-hub\/adapter-transformers\">adapter-transformers<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 790) - Huggingface Transformers + Adapters =. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code> <code>huggingface<\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/adapter-hub\/adapter-transformers) (\ud83d\udc68\u200d\ud83d\udcbb 1.2K \u00b7 \ud83d\udd00 130 \u00b7 \ud83d\udce6 73 \u00b7 \ud83d\udccb 170 - 26% open \u00b7 \u23f1\ufe0f 04.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/Adapter-Hub\/adapter-transformers\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/adapter-transformers) (\ud83d\udce5 54K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 23.03.2022):\n\t```\n\tpip install adapter-transformers\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/online-ml\/river\">River<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 3.4K) - Online machine learning in Python. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/online-ml\/river) (\ud83d\udc68\u200d\ud83d\udcbb 76 \u00b7 \ud83d\udd00 370 \u00b7 \ud83d\udce6 110 \u00b7 \ud83d\udccb 360 - 3% open \u00b7 \u23f1\ufe0f 28.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/online-ml\/river\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/river) (\ud83d\udce5 5.4K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 04.02.2022):\n\t```\n\tpip install river\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/river) (\ud83d\udce5 7.5K \u00b7 \u23f1\ufe0f 09.12.2021):\n\t```\n\tconda install -c conda-forge river\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/uber\/causalml\">causalml<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 3K) - Uplift modeling and causal inference with machine learning algorithms. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/uber\/causalml) (\ud83d\udc68\u200d\ud83d\udcbb 40 \u00b7 \ud83d\udd00 470 \u00b7 \ud83d\udce6 44 \u00b7 \ud83d\udccb 260 - 17% open \u00b7 \u23f1\ufe0f 02.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/uber\/causalml\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/causalml) (\ud83d\udce5 46K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 14.03.2022):\n\t```\n\tpip install causalml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/tableau\/TabPy\">TabPy<\/a><\/b> (\ud83e\udd4829 \u00b7  \u2b50 1.2K) - Execute Python code on the fly and display results in Tableau visualizations:. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/tableau\/TabPy) (\ud83d\udc68\u200d\ud83d\udcbb 46 \u00b7 \ud83d\udd00 460 \u00b7 \ud83d\udce6 86 \u00b7 \ud83d\udccb 290 - 3% open \u00b7 \u23f1\ufe0f 31.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/tableau\/TabPy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/tabpy) (\ud83d\udce5 19K \/ month \u00b7 \ud83d\udce6 2 \u00b7 \u23f1\ufe0f 20.01.2022):\n\t```\n\tpip install tabpy\n\t```\n- [Conda](https:\/\/anaconda.org\/anaconda\/tabpy-client) (\ud83d\udce5 2.7K \u00b7 \u23f1\ufe0f 02.05.2022):\n\t```\n\tconda install -c anaconda tabpy-client\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/trevorstephens\/gplearn\">gplearn<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 1.1K \u00b7 \ud83d\udcc8) - Genetic Programming in Python, with a scikit-learn inspired API. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/trevorstephens\/gplearn) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 200 \u00b7 \ud83d\udce6 240 \u00b7 \ud83d\udccb 190 - 18% open \u00b7 \u23f1\ufe0f 03.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/trevorstephens\/gplearn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/gplearn) (\ud83d\udce5 4.8K \/ month \u00b7 \ud83d\udce6 10 \u00b7 \u23f1\ufe0f 03.05.2022):\n\t```\n\tpip install gplearn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/gplearn) (\ud83d\udce5 2.2K \u00b7 \u23f1\ufe0f 04.05.2022):\n\t```\n\tconda install -c conda-forge gplearn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/google\/trax\">Trax<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 6.9K) - Trax Deep Learning with Clear Code and Speed. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/google\/trax) (\ud83d\udc68\u200d\ud83d\udcbb 77 \u00b7 \ud83d\udd00 700 \u00b7 \ud83d\udce6 52 \u00b7 \ud83d\udccb 210 - 41% open \u00b7 \u23f1\ufe0f 24.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/google\/trax\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/trax) (\ud83d\udce5 4.3K \/ month \u00b7 \u23f1\ufe0f 26.10.2021):\n\t```\n\tpip install trax\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/mars-project\/mars\">Mars<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 2.4K) - Mars is a tensor-based unified framework for large-scale data.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/mars-project\/mars) (\ud83d\udc68\u200d\ud83d\udcbb 44 \u00b7 \ud83d\udd00 300 \u00b7 \ud83d\udccb 1.1K - 16% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/mars-project\/mars\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pymars) (\ud83d\udce5 6.8K \/ month \u00b7 \ud83d\udce6 1 \u00b7 \u23f1\ufe0f 09.04.2022):\n\t```\n\tpip install pymars\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/SeldonIO\/alibi-detect\">alibi-detect<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 1.3K) - Algorithms for outlier, adversarial and drift detection. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/SeldonIO\/alibi-detect) (\ud83d\udc68\u200d\ud83d\udcbb 15 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 90 \u00b7 \ud83d\udccb 230 - 33% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/SeldonIO\/alibi-detect\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/alibi-detect) (\ud83d\udce5 25K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 01.04.2022):\n\t```\n\tpip install alibi-detect\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/sepandhaghighi\/pycm\">pycm<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 1.2K) - Multi-class confusion matrix library in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/sepandhaghighi\/pycm) (\ud83d\udc68\u200d\ud83d\udcbb 17 \u00b7 \ud83d\udd00 110 \u00b7 \ud83d\udce6 140 \u00b7 \ud83d\udccb 180 - 6% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/sepandhaghighi\/pycm\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pycm) (\ud83d\udce5 34K \/ month \u00b7 \ud83d\udce6 13 \u00b7 \u23f1\ufe0f 27.04.2022):\n\t```\n\tpip install pycm\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/scikit-learn-contrib\/metric-learn\">metric-learn<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 1.2K) - Metric learning algorithms in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/scikit-learn-contrib\/metric-learn) (\ud83d\udc68\u200d\ud83d\udcbb 22 \u00b7 \ud83d\udd00 220 \u00b7 \ud83d\udce6 200 \u00b7 \ud83d\udccb 170 - 30% open \u00b7 \u23f1\ufe0f 11.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/scikit-learn-contrib\/metric-learn\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/metric-learn) (\ud83d\udce5 15K \/ month \u00b7 \ud83d\udce6 11 \u00b7 \u23f1\ufe0f 02.07.2020):\n\t```\n\tpip install metric-learn\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/metric-learn) (\ud83d\udce5 5.6K \u00b7 \u23f1\ufe0f 02.07.2020):\n\t```\n\tconda install -c conda-forge metric-learn\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ContinualAI\/avalanche\">avalanche<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 890) - Avalanche: an End-to-End Library for Continual Learning based on PyTorch. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ContinualAI\/avalanche) (\ud83d\udc68\u200d\ud83d\udcbb 55 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 9 \u00b7 \ud83d\udccb 500 - 14% open \u00b7 \u23f1\ufe0f 05.05.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/ContinualAI\/avalanche\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/avalanche-lib) (\ud83d\udce5 530 \/ month \u00b7 \u23f1\ufe0f 16.12.2021):\n\t```\n\tpip install avalanche-lib\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/facebookresearch\/AugLy\">AugLy<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 4.4K) - A data augmentations library for audio, image, text, and video. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/facebookresearch\/AugLy) (\ud83d\udc68\u200d\ud83d\udcbb 20 \u00b7 \ud83d\udd00 240 \u00b7 \ud83d\udce6 31 \u00b7 \ud83d\udccb 65 - 20% open \u00b7 \u23f1\ufe0f 27.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/facebookresearch\/AugLy\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/augly) (\ud83d\udce5 1.8K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 28.03.2022):\n\t```\n\tpip install augly\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ljvmiranda921\/pyswarms\">PySwarms<\/a><\/b> (\ud83e\udd4925 \u00b7  \u2b50 920 \u00b7 \ud83d\udca4) - A research toolkit for particle swarm optimization in Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ljvmiranda921\/pyswarms) (\ud83d\udc68\u200d\ud83d\udcbb 43 \u00b7 \ud83d\udd00 290 \u00b7 \ud83d\udce6 170 \u00b7 \ud83d\udccb 210 - 7% open \u00b7 \u23f1\ufe0f 23.06.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/ljvmiranda921\/pyswarms\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pyswarms) (\ud83d\udce5 32K \/ month \u00b7 \ud83d\udce6 6 \u00b7 \u23f1\ufe0f 03.01.2021):\n\t```\n\tpip install pyswarms\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/astroML\/astroML\">AstroML<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 820) - Machine learning, statistics, and data mining for astronomy and.. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/astroML\/astroML) (\ud83d\udc68\u200d\ud83d\udcbb 30 \u00b7 \ud83d\udd00 280 \u00b7 \ud83d\udccb 150 - 41% open \u00b7 \u23f1\ufe0f 29.03.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/astroML\/astroML\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/astroML) (\ud83d\udce5 1.5K \/ month \u00b7 \ud83d\udce6 33 \u00b7 \u23f1\ufe0f 01.03.2022):\n\t```\n\tpip install astroML\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/astroml) (\ud83d\udce5 28K \u00b7 \u23f1\ufe0f 02.03.2022):\n\t```\n\tconda install -c conda-forge astroml\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/MaxHalford\/prince\">Prince<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 790) - Python factor analysis library (PCA, CA, MCA, MFA, FAMD). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/MaxHalford\/prince) (\ud83d\udc68\u200d\ud83d\udcbb 12 \u00b7 \ud83d\udd00 140 \u00b7 \ud83d\udce6 200 \u00b7 \ud83d\udccb 110 - 35% open \u00b7 \u23f1\ufe0f 28.12.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/MaxHalford\/prince\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/prince) (\ud83d\udce5 56K \/ month \u00b7 \ud83d\udce6 5 \u00b7 \u23f1\ufe0f 06.10.2020):\n\t```\n\tpip install prince\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/prince-factor-analysis) (\ud83d\udce5 9.6K \u00b7 \u23f1\ufe0f 30.04.2021):\n\t```\n\tconda install -c conda-forge prince-factor-analysis\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/minrk\/findspark\">findspark<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 440) - Find pyspark to make it importable. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1N\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/minrk\/findspark) (\ud83d\udc68\u200d\ud83d\udcbb 15 \u00b7 \ud83d\udd00 67 \u00b7 \ud83d\udce6 2.4K \u00b7 \ud83d\udccb 23 - 52% open \u00b7 \u23f1\ufe0f 11.02.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/minrk\/findspark\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/findspark) (\ud83d\udce5 2.2M \/ month \u00b7 \ud83d\udce6 140 \u00b7 \u23f1\ufe0f 11.02.2022):\n\t```\n\tpip install findspark\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/findspark) (\ud83d\udce5 640K \u00b7 \u23f1\ufe0f 11.02.2022):\n\t```\n\tconda install -c conda-forge findspark\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/solegalli\/feature_engine\">Feature Engine<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 860 \u00b7 \ud83d\udca4) - Feature engineering package with sklearn like functionality. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/solegalli\/feature_engine) (\ud83d\udc68\u200d\ud83d\udcbb 24 \u00b7 \ud83d\udd00 200 \u00b7 \u23f1\ufe0f 06.08.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/solegalli\/feature_engine\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/feature_engine) (\ud83d\udce5 70K \/ month \u00b7 \ud83d\udce6 66 \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tpip install feature_engine\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/feature_engine) (\ud83d\udce5 8.5K \u00b7 \u23f1\ufe0f 05.05.2022):\n\t```\n\tconda install -c conda-forge feature_engine\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/rasbt\/biopandas\">BioPandas<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 450) - Working with molecular structures in pandas DataFrames. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/rasbt\/biopandas) (\ud83d\udc68\u200d\ud83d\udcbb 10 \u00b7 \ud83d\udd00 96 \u00b7 \ud83d\udce6 98 \u00b7 \ud83d\udccb 42 - 40% open \u00b7 \u23f1\ufe0f 13.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/rasbt\/biopandas\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/biopandas) (\ud83d\udce5 4.7K \/ month \u00b7 \ud83d\udce6 14 \u00b7 \u23f1\ufe0f 07.04.2022):\n\t```\n\tpip install biopandas\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/biopandas) (\ud83d\udce5 100K \u00b7 \u23f1\ufe0f 31.08.2021):\n\t```\n\tconda install -c conda-forge biopandas\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/airbnb\/streamalert\">StreamAlert<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 2.7K) - StreamAlert is a serverless, realtime data analysis framework.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/airbnb\/streamalert) (\ud83d\udc68\u200d\ud83d\udcbb 33 \u00b7 \ud83d\udd00 320 \u00b7 \ud83d\udccb 340 - 24% open \u00b7 \u23f1\ufe0f 04.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/airbnb\/streamalert\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/ml-tooling\/opyrator\">opyrator<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 2.6K \u00b7 \ud83d\udca4) - Turns your machine learning code into microservices with web API,.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/ml-tooling\/opyrator) (\ud83d\udc68\u200d\ud83d\udcbb 4 \u00b7 \ud83d\udd00 120 \u00b7 \ud83d\udce6 32 \u00b7 \ud83d\udccb 26 - 7% open \u00b7 \u23f1\ufe0f 06.05.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/ml-tooling\/opyrator\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/opyrator) (\ud83d\udce5 840 \/ month \u00b7 \u23f1\ufe0f 04.05.2021):\n\t```\n\tpip install opyrator\n\t```\n- [Conda](https:\/\/anaconda.org\/conda-forge\/opyrator) (\ud83d\udce5 87 \u00b7 \u23f1\ufe0f 08.01.2022):\n\t```\n\tconda install -c conda-forge opyrator\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/eltonlaw\/impyute\">impyute<\/a><\/b> (\ud83e\udd4920 \u00b7  \u2b50 310) - Data imputations library to preprocess datasets with missing data. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/eltonlaw\/impyute) (\ud83d\udc68\u200d\ud83d\udcbb 11 \u00b7 \ud83d\udd00 44 \u00b7 \ud83d\udce6 130 \u00b7 \ud83d\udccb 64 - 42% open \u00b7 \u23f1\ufe0f 06.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/eltonlaw\/impyute\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/impyute) (\ud83d\udce5 5.3K \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 29.04.2019):\n\t```\n\tpip install impyute\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/yzhao062\/SUOD\">SUOD<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 320) - (MLSys 21) An Acceleration System for Large-scare Unsupervised Heterogeneous.. <code><a href=\"http:\/\/bit.ly\/3rqEWVr\">BSD-2<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/yzhao062\/SUOD) (\ud83d\udc68\u200d\ud83d\udcbb 1 \u00b7 \ud83d\udd00 41 \u00b7 \ud83d\udce6 420 \u00b7 \ud83d\udccb 9 - 66% open \u00b7 \u23f1\ufe0f 11.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/yzhao062\/SUOD\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/suod) (\ud83d\udce5 24K \/ month \u00b7 \u23f1\ufe0f 01.10.2021):\n\t```\n\tpip install suod\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/pykale\/pykale\">pykale<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 340) - Knowledge-Aware machine LEarning (KALE): accessible machine learning.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/pykale\/pykale) (\ud83d\udc68\u200d\ud83d\udcbb 16 \u00b7 \ud83d\udd00 44 \u00b7 \ud83d\udccb 97 - 14% open \u00b7 \u23f1\ufe0f 29.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/pykale\/pykale\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/pykale) (\ud83d\udce5 120 \/ month \u00b7 \u23f1\ufe0f 12.04.2022):\n\t```\n\tpip install pykale\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/jmschrei\/apricot\">apricot<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 420) - apricot implements submodular optimization for the purpose of selecting.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/jmschrei\/apricot) (\ud83d\udc68\u200d\ud83d\udcbb 4 \u00b7 \ud83d\udd00 40 \u00b7 \ud83d\udce5 10 \u00b7 \ud83d\udce6 25 \u00b7 \ud83d\udccb 24 - 25% open \u00b7 \u23f1\ufe0f 18.11.2021):\n\n\t```\n\tgit clone https:\/\/github.com\/jmschrei\/apricot\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/apricot-select) (\ud83d\udce5 430 \/ month \u00b7 \ud83d\udce6 3 \u00b7 \u23f1\ufe0f 28.09.2020):\n\t```\n\tpip install apricot-select\n\t```\n<\/details>\n<details><summary><b><a href=\"https:\/\/github.com\/SforAiDl\/KD_Lib\">KD-Lib<\/a><\/b> (\ud83e\udd4917 \u00b7  \u2b50 380) - A Pytorch Knowledge Distillation library for benchmarking and.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1Q\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code><\/summary>\n\n- [GitHub](https:\/\/github.com\/SforAiDl\/KD_Lib) (\ud83d\udc68\u200d\ud83d\udcbb 6 \u00b7 \ud83d\udd00 34 \u00b7 \ud83d\udccb 54 - 16% open \u00b7 \u23f1\ufe0f 10.04.2022):\n\n\t```\n\tgit clone https:\/\/github.com\/SforAiDl\/KD_Lib\n\t```\n- [PyPi](https:\/\/pypi.org\/project\/KD-Lib) (\ud83d\udce5 63 \/ month \u00b7 \u23f1\ufe0f 24.03.2022):\n\t```\n\tpip install KD-Lib\n\t```\n<\/details>\n<details><summary>Show 17 hidden projects...<\/summary>\n\n- <b><a href=\"https:\/\/github.com\/explosion\/cython-blis\">Cython BLIS<\/a><\/b> (\ud83e\udd4830 \u00b7  \u2b50 190) - Fast matrix-multiplication as a self-contained Python library no.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/deepmind\/pysc2\">pysc2<\/a><\/b> (\ud83e\udd4828 \u00b7  \u2b50 7.5K \u00b7 \ud83d\udc80) - StarCraft II Learning Environment. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/JustGlowing\/minisom\">minisom<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 1.1K) - MiniSom is a minimalistic implementation of the Self Organizing.. <code><a href=\"https:\/\/tldrlegal.com\/search?q=CC-BY-3.0\">\u2757\ufe0fCC-BY-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/annoviko\/pyclustering\">pyclustering<\/a><\/b> (\ud83e\udd4927 \u00b7  \u2b50 950 \u00b7 \ud83d\udc80) - pyclustring is a Python, C++ data mining library. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/cleanlab\/cleanlab\">cleanlab<\/a><\/b> (\ud83e\udd4926 \u00b7  \u2b50 3.3K) - The standard data-centric AI package for data quality and machine.. <code><a href=\"http:\/\/bit.ly\/3pwmjO5\">\u2757\ufe0fAGPL-3.0<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/modAL-python\/modAL\">modAL<\/a><\/b> (\ud83e\udd4924 \u00b7  \u2b50 1.7K \u00b7 \ud83d\udc80) - A modular active learning framework for Python. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/Project-MONAI\/MONAILabel\">MONAILabel<\/a><\/b> (\ud83e\udd4923 \u00b7  \u2b50 230) - MONAI Label is an intelligent open source image labeling and.. <code><a href=\"http:\/\/bit.ly\/3nYMfla\">Apache-2<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/vecxoz\/vecstack\">vecstack<\/a><\/b> (\ud83e\udd4922 \u00b7  \u2b50 660 \u00b7 \ud83d\udc80) - Python package for stacking (machine learning technique). <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/flennerhag\/mlens\">mlens<\/a><\/b> (\ud83e\udd4921 \u00b7  \u2b50 730 \u00b7 \ud83d\udc80) - ML-Ensemble high performance ensemble learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/EpistasisLab\/scikit-rebate\">scikit-rebate<\/a><\/b> (\ud83e\udd4919 \u00b7  \u2b50 360 \u00b7 \ud83d\udc80) - A scikit-learn-compatible Python implementation of.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/alegonz\/baikal\">baikal<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 590 \u00b7 \ud83d\udc80) - A graph-based functional API for building complex scikit-learn.. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/kLabUM\/rrcf\">rrcf<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 370 \u00b7 \ud83d\udc80) - Implementation of the Robust Random Cut Forest algorithm for anomaly.. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/pandas-ml\/pandas-ml\">pandas-ml<\/a><\/b> (\ud83e\udd4918 \u00b7  \u2b50 290 \u00b7 \ud83d\udc80) - pandas, scikit-learn, xgboost and seaborn integration. <code><a href=\"http:\/\/bit.ly\/3aKzpTv\">BSD-3<\/a><\/code> <code><img src=\"https:\/\/git.io\/JLy1F\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code> <code><img src=\"https:\/\/git.io\/JLy1S\" style=\"display:inline;\" width=\"13\" height=\"13\"><\/code>\n- <b><a href=\"https:\/\/github.com\/facebookresearch\/NeuralCompression\">NeuralCompression<\/a><\/b> (\ud83e\udd4915 \u00b7  \u2b50 250) - A collection of tools for neural compression enthusiasts. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/jrieke\/traingenerator\">traingenerator<\/a><\/b> (\ud83e\udd4913 \u00b7  \u2b50 1.2K \u00b7 \ud83d\udc80) - A web app to generate template code for machine learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/Palashio\/nylon\">nylon<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 77 \u00b7 \ud83d\udca4) - An intelligent, flexible grammar of machine learning. <code><a href=\"http:\/\/bit.ly\/34MBwT8\">MIT<\/a><\/code>\n- <b><a href=\"https:\/\/github.com\/dstackai\/dstack\">dstack<\/a><\/b> (\ud83e\udd4911 \u00b7  \u2b50 34 \u00b7 \ud83d\udc23) - dstack: the modern CI\/CD made for training models. <code><a href=\"http:\/\/bit.ly\/2M0xdwT\">\u2757\ufe0fGPL-3.0<\/a><\/code>\n<\/details>\n\n---\n\n## Related Resources\n\n- [**Papers With Code**](https:\/\/paperswithcode.com): Discover ML papers, code, and evaluation tables.\n- [**Sotabench**](https:\/\/sotabench.com): Discover & compare open-source ML models.\n- [**Google Dataset Search**](https:\/\/toolbox.google.com\/datasetsearch): Dataset search engine by Google.\n- [**Dataset List**](https:\/\/www.datasetlist.com\/): List of the biggest ML datasets from across the web.\n- [**Awesome Public Datasets**](https:\/\/github.com\/awesomedata\/awesome-public-datasets): A topic-centric list of open datasets.\n- [**Best-of lists**](https:\/\/best-of.org): Discover other best-of lists with awesome open-source projects on all kinds of topics.\n- [**best-of-python-dev**](https:\/\/github.com\/ml-tooling\/best-of-python-dev): A ranked list of awesome python developer tools and libraries.\n- [**best-of-web-python**](https:\/\/github.com\/ml-tooling\/best-of-web-python): A ranked list of awesome python libraries for web development.\n\n## Contribution\n\nContributions are encouraged and always welcome! If you like to add or update projects, choose one of the following ways:\n\n- Open an issue by selecting one of the provided categories from the [issue page](https:\/\/github.com\/ml-tooling\/best-of-ml-python\/issues\/new\/choose) and fill in the requested information.\n- Modify the [projects.yaml](https:\/\/github.com\/ml-tooling\/best-of-ml-python\/blob\/main\/projects.yaml) with your additions or changes, and submit a pull request. This can also be done directly via the [Github UI](https:\/\/github.com\/ml-tooling\/best-of-ml-python\/edit\/main\/projects.yaml).\n\nIf you like to contribute to or share suggestions regarding the project metadata collection or markdown generation, please refer to the [best-of-generator](https:\/\/github.com\/best-of-lists\/best-of-generator) repository. If you like to create your own best-of list, we recommend to follow [this guide](https:\/\/github.com\/best-of-lists\/best-of\/blob\/main\/create-best-of-list.md).\n\nFor more information on how to add or update projects, please read the [contribution guidelines](https:\/\/github.com\/ml-tooling\/best-of-ml-python\/blob\/main\/CONTRIBUTING.md). By participating in this project, you agree to abide by its [Code of Conduct](https:\/\/github.com\/ml-tooling\/best-of-ml-python\/blob\/main\/.github\/CODE_OF_CONDUCT.md).\n\n## License\n\n[![CC0](https:\/\/mirrors.creativecommons.org\/presskit\/buttons\/88x31\/svg\/by-sa.svg)](https:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/)\n","113":"# Machine learning algorithms\nA collection of minimal and clean implementations of machine learning algorithms.\n\n### Why?\nThis project is targeting people who want to learn internals of ml algorithms or implement them from scratch.  \nThe code is much easier to follow than the optimized libraries and easier to play with.  \nAll algorithms are implemented in Python, using numpy, scipy and autograd.  \n\n### Implemented:\n* [Deep learning (MLP, CNN, RNN, LSTM)](mla\/neuralnet)\n* [Linear regression, logistic regression](mla\/linear_models.py)\n* [Random Forests](mla\/ensemble\/random_forest.py)\n* [Support vector machine (SVM) with kernels (Linear, Poly, RBF)](mla\/svm)\n* [K-Means](mla\/kmeans.py)\n* [Gaussian Mixture Model](mla\/gaussian_mixture.py)\n* [K-nearest neighbors](mla\/knn.py)\n* [Naive bayes](mla\/naive_bayes.py)\n* [Principal component analysis (PCA)](mla\/pca.py)\n* [Factorization machines](mla\/fm.py)\n* [Restricted Boltzmann machine (RBM)](mla\/rbm.py)\n* [t-Distributed Stochastic Neighbor Embedding (t-SNE)](mla\/tsne.py)\n* [Gradient Boosting trees (also known as GBDT, GBRT, GBM, XGBoost)](mla\/ensemble\/gbm.py)\n* [Reinforcement learning (Deep Q learning)](mla\/rl)\n\n\n### Installation\n```sh\n        git clone https:\/\/github.com\/rushter\/MLAlgorithms\n        cd MLAlgorithms\n        pip install scipy numpy\n        python setup.py develop\n```\n### How to run examples without installation\n```sh\n        cd MLAlgorithms\n        python -m examples.linear_models\n```\n### How to run examples within Docker\n```sh\n        cd MLAlgorithms\n        docker build -t mlalgorithms .\n        docker run --rm -it mlalgorithms bash\n        python -m examples.linear_models\n```\n### Contributing\n\nYour contributions are always welcome!  \nFeel free to improve existing code, documentation or implement new algorithm.  \nPlease open an issue to propose your changes if they are big enough.  \n","114":"Master status: [![Master Build Status - Mac\/Linux](https:\/\/travis-ci.com\/EpistasisLab\/tpot.svg?branch=master)](https:\/\/travis-ci.com\/EpistasisLab\/tpot)\n[![Master Build Status - Windows](https:\/\/ci.appveyor.com\/api\/projects\/status\/b7bmpwpkjhifrm7v\/branch\/master?svg=true)](https:\/\/ci.appveyor.com\/project\/weixuanfu\/tpot?branch=master)\n[![Master Coverage Status](https:\/\/coveralls.io\/repos\/github\/EpistasisLab\/tpot\/badge.svg?branch=master)](https:\/\/coveralls.io\/github\/EpistasisLab\/tpot?branch=master)\n\nDevelopment status: [![Development Build Status - Mac\/Linux](https:\/\/travis-ci.com\/EpistasisLab\/tpot.svg?branch=development)](https:\/\/travis-ci.com\/EpistasisLab\/tpot\/branches)\n[![Development Build Status - Windows](https:\/\/ci.appveyor.com\/api\/projects\/status\/b7bmpwpkjhifrm7v\/branch\/development?svg=true)](https:\/\/ci.appveyor.com\/project\/weixuanfu\/tpot?branch=development)\n[![Development Coverage Status](https:\/\/coveralls.io\/repos\/github\/EpistasisLab\/tpot\/badge.svg?branch=development)](https:\/\/coveralls.io\/github\/EpistasisLab\/tpot?branch=development)\n\nPackage information: [![Python 3.7](https:\/\/img.shields.io\/badge\/python-3.7-blue.svg)](https:\/\/www.python.org\/downloads\/release\/python-370\/)\n[![License: LGPL v3](https:\/\/img.shields.io\/badge\/license-LGPL%20v3-blue.svg)](http:\/\/www.gnu.org\/licenses\/lgpl-3.0)\n[![PyPI version](https:\/\/badge.fury.io\/py\/TPOT.svg)](https:\/\/badge.fury.io\/py\/TPOT)\n\n<p align=\"center\">\n<img src=\"https:\/\/raw.githubusercontent.com\/EpistasisLab\/tpot\/master\/images\/tpot-logo.jpg\" width=300 \/>\n<\/p>\n\n**TPOT** stands for **T**ree-based **P**ipeline **O**ptimization **T**ool. Consider TPOT your **Data Science Assistant**. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\n\n![TPOT Demo](https:\/\/github.com\/EpistasisLab\/tpot\/blob\/master\/images\/tpot-demo.gif \"TPOT Demo\")\n\nTPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.\n\n![An example Machine Learning pipeline](https:\/\/github.com\/EpistasisLab\/tpot\/blob\/master\/images\/tpot-ml-pipeline.png \"An example Machine Learning pipeline\")\n\n<p align=\"center\"><strong>An example Machine Learning pipeline<\/strong><\/p>\n\nOnce TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.\n\n![An example TPOT pipeline](https:\/\/github.com\/EpistasisLab\/tpot\/blob\/master\/images\/tpot-pipeline-example.png \"An example TPOT pipeline\")\n\nTPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.\n\n**TPOT is still under active development** and we encourage you to check back on this repository regularly for updates.\n\nFor further information about TPOT, please see the [project documentation](http:\/\/epistasislab.github.io\/tpot\/).\n\n## License\n\nPlease see the [repository license](https:\/\/github.com\/EpistasisLab\/tpot\/blob\/master\/LICENSE) for the licensing and usage information for TPOT.\n\nGenerally, we have licensed TPOT to make it as widely usable as possible.\n\n## Installation\n\nWe maintain the [TPOT installation instructions](http:\/\/epistasislab.github.io\/tpot\/installing\/) in the documentation. TPOT requires a working installation of Python.\n\n## Usage\n\nTPOT can be used [on the command line](http:\/\/epistasislab.github.io\/tpot\/using\/#tpot-on-the-command-line) or [with Python code](http:\/\/epistasislab.github.io\/tpot\/using\/#tpot-with-code).\n\nClick on the corresponding links to find more information on TPOT usage in the documentation.\n\n## Examples\n\n### Classification\n\nBelow is a minimal working example with the optical recognition of handwritten digits dataset.\n\n```python\nfrom tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')\n```\n\nRunning this code should discover a pipeline that achieves about 98% testing accuracy, and the corresponding Python code should be exported to the `tpot_digits_pipeline.py` file and look similar to the following:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.builtins import StackingEstimator\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH\/TO\/DATA\/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: 0.9799428471757372\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    StackingEstimator(estimator=LogisticRegression(C=0.1, dual=False, penalty=\"l1\")),\n    RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.35000000000000003, min_samples_leaf=20, min_samples_split=19, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n```\n\n### Regression\n\nSimilarly, TPOT can optimize pipelines for regression problems. Below is a minimal working example with the practice Boston housing prices data set.\n\n```python\nfrom tpot import TPOTRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\nhousing = load_boston()\nX_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n```\n\nwhich should result in a pipeline that achieves about 12.77 mean squared error (MSE), and the Python code in `tpot_boston_pipeline.py` should look similar to:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH\/TO\/DATA\/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: -10.812040755234403\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    ExtraTreesRegressor(bootstrap=False, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n```\n\nCheck the documentation for [more examples and tutorials](http:\/\/epistasislab.github.io\/tpot\/examples\/).\n\n## Contributing to TPOT\n\nWe welcome you to [check the existing issues](https:\/\/github.com\/EpistasisLab\/tpot\/issues\/) for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please [file a new issue](https:\/\/github.com\/EpistasisLab\/tpot\/issues\/new) so we can discuss it.\n\nBefore submitting any contributions, please review our [contribution guidelines](http:\/\/epistasislab.github.io\/tpot\/contributing\/).\n\n## Having problems or have questions about TPOT?\n\nPlease [check the existing open and closed issues](https:\/\/github.com\/EpistasisLab\/tpot\/issues?utf8=%E2%9C%93&q=is%3Aissue) to see if your issue has already been attended to. If it hasn't, [file a new issue](https:\/\/github.com\/EpistasisLab\/tpot\/issues\/new) on this repository so we can review your issue.\n\n## Citing TPOT\n\nIf you use TPOT in a scientific publication, please consider citing at least one of the following papers:\n\nTrang T. Le, Weixuan Fu and Jason H. Moore (2020). [Scaling tree-based automated machine learning to biomedical big data with a feature set selector](https:\/\/academic.oup.com\/bioinformatics\/article\/36\/1\/250\/5511404). *Bioinformatics*.36(1): 250-256.\n\nBibTeX entry:\n\n```bibtex\n@article{le2020scaling,\n  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},\n  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},\n  journal={Bioinformatics},\n  volume={36},\n  number={1},\n  pages={250--256},\n  year={2020},\n  publisher={Oxford University Press}\n}\n```\n\n\nRandal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). [Automating biomedical data science through tree-based pipeline optimization](http:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-31204-0_9). *Applications of Evolutionary Computation*, pages 123-137.\n\nBibTeX entry:\n\n```bibtex\n@inbook{Olson2016EvoBio,\n    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},\n    editor={Squillero, Giovanni and Burelli, Paolo},\n    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},\n    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},\n    year={2016},\n    publisher={Springer International Publishing},\n    pages={123--137},\n    isbn={978-3-319-31204-0},\n    doi={10.1007\/978-3-319-31204-0_9},\n    url={http:\/\/dx.doi.org\/10.1007\/978-3-319-31204-0_9}\n}\n```\n\nRandal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). [Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science](http:\/\/dl.acm.org\/citation.cfm?id=2908918). *Proceedings of GECCO 2016*, pages 485-492.\n\nBibTeX entry:\n\n```bibtex\n@inproceedings{OlsonGECCO2016,\n    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},\n    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},\n    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n    series = {GECCO '16},\n    year = {2016},\n    isbn = {978-1-4503-4206-3},\n    location = {Denver, Colorado, USA},\n    pages = {485--492},\n    numpages = {8},\n    url = {http:\/\/doi.acm.org\/10.1145\/2908812.2908918},\n    doi = {10.1145\/2908812.2908918},\n    acmid = {2908918},\n    publisher = {ACM},\n    address = {New York, NY, USA},\n}\n```\n\nAlternatively, you can cite the repository directly with the following DOI:\n\n[![DOI](https:\/\/zenodo.org\/badge\/20747\/rhiever\/tpot.svg)](https:\/\/zenodo.org\/badge\/latestdoi\/20747\/rhiever\/tpot)\n\n## Support for TPOT\n\nTPOT was developed in the [Computational Genetics Lab](http:\/\/epistasis.org\/) at the [University of Pennsylvania](https:\/\/www.upenn.edu\/) with funding from the [NIH](http:\/\/www.nih.gov\/) under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.\n\nThe TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.\n","115":"![Ludwig logo](https:\/\/github.com\/ludwig-ai\/ludwig-docs\/raw\/master\/docs\/images\/ludwig_hero.png \"Ludwig logo\")\n\n<div align=\"center\">\n\n[![PyPI version](https:\/\/badge.fury.io\/py\/ludwig.svg)](https:\/\/badge.fury.io\/py\/ludwig)\n[![Build Status](https:\/\/github.com\/ludwig-ai\/ludwig\/actions\/workflows\/pytest.yml\/badge.svg)](https:\/\/github.com\/ludwig-ai\/ludwig\/actions\/workflows\/pytest.yml)\n[![CII Best Practices](https:\/\/bestpractices.coreinfrastructure.org\/projects\/4210\/badge)](https:\/\/bestpractices.coreinfrastructure.org\/projects\/4210)\n[![Slack](https:\/\/img.shields.io\/badge\/slack-chat-green.svg?logo=slack)](https:\/\/join.slack.com\/t\/ludwig-ai\/shared_invite\/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ)\n\n[![DockerHub](https:\/\/img.shields.io\/docker\/pulls\/ludwigai\/ludwig.svg)](https:\/\/hub.docker.com\/r\/ludwigai)\n[![Downloads](https:\/\/pepy.tech\/badge\/ludwig)](https:\/\/pepy.tech\/project\/ludwig)\n[![License](https:\/\/img.shields.io\/badge\/License-Apache%202.0-blue.svg)](https:\/\/github.com\/ludwig-ai\/ludwig\/blob\/master\/LICENSE)\n[![Twitter](https:\/\/img.shields.io\/twitter\/follow\/ludwig_ai.svg?style=social&logo=twitter)](https:\/\/twitter.com\/ludwig_ai)\n\n<\/div>\n\nTranslated in [\ud83c\uddf0\ud83c\uddf7Korean](README_KR.md)\n\n# What is Ludwig?\n\nLudwig is an open-source, [declarative machine learning framework](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/what_is_ludwig\/#why-declarative-machine-learning-systems)\nthat makes it easy to define deep learning pipelines  with a simple and flexible data-driven configuration system.\nLudwig is suitable for a wide variety of AI tasks, and is hosted by the [Linux Foundation AI & Data](https:\/\/lfaidata.foundation\/).\n\nLudwig allows users to define their deep learning pipeline by simply providing a configuration file, which lists the\ninputs and outputs, and their respective data types. Ludwig will then assemble and train a deep learning model and based\non the configuration file, determine how inputs and outputs are preprocessed, encoded, decoded and which metrics and\nloss criterion to use.\n\n![img](https:\/\/raw.githubusercontent.com\/ludwig-ai\/ludwig-docs\/master\/docs\/images\/ludwig_legos.gif)\n\nWriting a configuration file for Ludwig is easy. The configuration file flexibility allows for full control of every\naspect of the end-to-end pipeline. This includes exploring state-of-the-art model architectures, running a\nhyperparameter search, scaling up to larger than available memory datasets and multi-node clusters, and finally serving\nthe best model in production. All of this is achieved through simple configuration file changes.\n\nFinally, the use of abstract interfaces throughout the codebase makes it easy for users to extend Ludwig by adding new\nmodels, metrics, losses, preprocessing functions and register them to make them available immediately in the\nconfiguration system.\n\n# Main Features\n\n- [Data-Driven configuration system](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/how_ludwig_works)\n\n  A config YAML file that describes the schema of your data (input features, output features, and their types) is all\n  you need to start training deep learning models. Ludwig uses declared features to compose a deep learning model\n  accordingly.\n\n  ```yaml\n  input_features:\n    - name: data_column_1\n      type: number\n    - name: data_column_2\n      type: category\n    - name: data_column_3\n      type: text\n    - name: data_column_4\n      type: image\n    ...\n\n  output_features:\n    - name: data_column_5\n      type: number\n    - name: data_column_6\n      type: category\n    ...\n  ```\n\n- [Training, prediction, and evaluation from the command line](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/command_line_interface)\n\n  Simple commands can be used to train models and predict new data.\n\n  ```shell\n  ludwig train --config config.yaml --dataset data.csv\n  ludwig predict --model_path results\/experiment_run\/model --dataset test.csv\n  ludwig eval --model_path results\/experiment_run\/model --dataset test.csv\n  ```\n\n- [Programmatic API](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/api\/LudwigModel)\n\n  Ludwig also provides a simple programmatic API for all of the functionality described above and more.\n\n  ```python\n  from ludwig.api import LudwigModel\n\n  # train a model\n  config = {\n      \"input_features\": [...],\n      \"output_features\": [...],\n  }\n  model = LudwigModel(config)\n  data = pd.read_csv(\"data.csv\")\n  train_stats, _, model_dir = model.train(data)\n\n  # or load a model\n  model = LudwigModel.load(model_dir)\n\n  # obtain predictions\n  predictions = model.predict(data)\n  ```\n\n- [Distributed training](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/distributed_training)\n\n  Train models in a distributed setting using [Horovod](https:\/\/github.com\/horovod\/horovod), which allows training on a\n  single machine with multiple GPUs or multiple machines with multiple GPUs.\n\n- [Serving](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/serving)\n\n  Serve models using FastAPI.\n\n  ```shell\n  ludwig serve --model_path .\/results\/experiment_run\/model\n  curl http:\/\/0.0.0.0:8000\/predict -X POST -F \"movie_title=Friends With Money\" -F \"content_rating=R\" -F \"genres=Art House & International, Comedy, Drama\" -F \"runtime=88.0\" -F \"top_critic=TRUE\" -F \"review_content=The cast is terrific, the movie isn't.\"\n  ```\n\n- [Hyperparameter optimization](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/hyperopt)\n\n  Run hyperparameter optimization locally or using [Ray Tune](https:\/\/docs.ray.io\/en\/latest\/tune\/index.html).\n\n  ```shell\n  ludwig hyperopt --config config.yaml --dataset data.csv\n  ```\n\n- [AutoML](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/automl)\n\n  Ludwig AutoML takes a dataset, the target column, and a time budget, and returns a trained Ludwig model.\n\n- [Third-Party integrations](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/integrations)\n\n  Ludwig provides an extendable interface to integrate with third-party systems for tracking experiments. Third-party\n  integrations exist for Comet ML, Weights & Biases, WhyLabs and MLFlow.\n\n- [Extensibility](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/developer_guide)\n\n  Ludwig is built from the ground up with extensibility in mind. It is easy to add new data types by implementing clear,\n  well-documented abstract classes that define functions to preprocess, encode, and decode data.\n\n  Furthermore, new `torch nn.Module` models can be easily added by them to a registry. This encourages reuse and sharing\n  new models with the community. Refer to the [Developer Guide](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/developer_guide)\n  for further details.\n\n# Quick Start\n\nFor a full tutorial, check out the official [getting started guide](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/getting_started\/),\nor take a look at end-to-end [Examples](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples).\n\n## Step 1: Install\n\nInstall from PyPi. Be aware that Ludwig requires Python 3.7+.\n\n```shell\npip install ludwig\n```\n\n## Step 2: Define a configuration\n\nCreate a config that describes the schema of your data.\n\nAssume we have a text classification task, with data containing a sentence and class column like the following.\n\n|               sentence               |  class   |\n| :----------------------------------: | :------: |\n|  Former president Barack Obama ...   | politics |\n| Juventus hired Cristiano Ronaldo ... |  sport   |\n|  LeBron James joins the Lakers ...   |  sport   |\n|                 ...                  |   ...    |\n\nA configuration will look like this.\n\n```yaml\ninput_features:\n- name: sentence\n  type: text\n\noutput_features:\n- name: class\n  type: category\n```\n\nStarting from a simple config like the one above, any and all aspects of the model architecture, training loop,\nhyperparameter search, and backend infrastructure can be modified as additional fields in the declarative configuration\nto customize the pipeline to meet your requirements.\n\n```yaml\ninput_features:\n- name: sentence\n  type: text\n  encoder: transformer\n  layers: 6\n  embedding_size: 512\n\noutput_features:\n- name: class\n  type: category\n  loss: cross_entropy\n\ntrainer:\n  epochs: 50\n  batch_size: 64\n  optimizer:\n    type: adamw\n    beat1: 0.9\n  learning_rate: 0.001\n\nbackend:\n  type: ray\n  cache_format: parquet\n  processor:\n    type: dask\n  trainer:\n    use_gpu: true\n    num_workers: 4\n    resources_per_worker:\n      CPU: 4\n      GPU: 1\n\nhyperopt:\n  metric: f1\n  sampler: random\n  parameters:\n    title.num_layers:\n      lower: 1\n      upper: 5\n    trainer.learning_rate:\n      values: [0.01, 0.003, 0.001]\n```\n\nFor details on what can be configured, check out [Ludwig Configuration](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/configuration\/)\ndocs.\n\n## Step 3: Train a model\n\nSimple commands can be used to train models and predict new data.\n\n```shell\nludwig train --config config.yaml --dataset data.csv\n```\n\n## Step 4: Predict and evaluate\n\nThe training process will produce a model that can be used for evaluating on and obtaining predictions for new data.\n\n```shell\nludwig predict \u2013model path\/to\/trained\/model \u2013dataset heldout.csv\nludwig evaluate \u2013model path\/to\/trained\/model \u2013dataset heldout.csv\n```\n\n## Step 5: Visualize\n\nLudwig provides a suite of visualization tools allows you to analyze models' training and test performance and to\ncompare them.\n\n```shell\nludwig visualize --visualization compare_performance --test_statistics path\/to\/test_statistics_model_1.json path\/to\/test_statistics_model_2.json\n```\n\nFor the full set of visualization see the [Visualization Guide](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/visualizations).\n\n## Step 6: Happy modeling!\n\nTry applying Ludwig to your data. [Reach out](https:\/\/join.slack.com\/t\/ludwig-ai\/shared_invite\/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ)\nif you have any questions.\n\n# Advantages\n\nLudwig is a profound utility for research scientists, data scientists, and machine learning engineers.\n\n## Minimal machine learning boilerplate\n\nLudwig takes care of the engineering complexity of deep learning out of the box, enabling research scientists to focus on building models at the highest level of abstraction.\n\nData preprocessing, hyperparameter optimization, device management, and distributed training for newly registered `torch.nn.Module` models come completely free.\n\n## Easily build your benchmarks\n\nCreating a state-of-the-art baseline and comparing it with a new model is a simple config change.\n\n## Easily apply new architectures to multiple problems and datasets\n\nApply new models across the extensive set of tasks and datasets that Ludwig supports. Ludwig includes a [full benchmarking toolkit](https:\/\/arxiv.org\/abs\/2111.04260) accessible to any user, for running experiments with multiple models across multiple datasets with just a simple configuration.\n\n## Highly configurable data preprocessing, modeling, and metrics\n\nAny and all aspects of the model architecture, training loop, hyperparameter search, and backend infrastructure can be modified as additional fields in the declarative configuration to customize the pipeline to meet your requirements.\n\nFor details on what can be configured, check out [Ludwig Configuration](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/configuration\/) docs.\n\n## Multi-modal, multi-task learning out-of-the-box\n\nMix and match tabular data, text, images, and even audio into complex model configurations without writing code.\n\n## Rich model exporting and tracking\n\nAutomatically track all trials and metrics with tools like Tensorboard, Comet ML, Weights & Biases, and MLflow.\n\n## Automatically scale training to multi-GPU, multi-node clusters\n\nGo from training on your local machine to the cloud without code changes.\n\n## Low-code interface for state-of-the-art models, including pre-trained Huggingface Transformers\n\nLudwig also natively integrates with pre-trained models, such as the ones available in [Huggingface Transformers](https:\/\/huggingface.co\/docs\/transformers\/index). Users can choose from a vast collection of state-of-the-art pre-trained PyTorch models to use without needing to write any code at all. For example, training a BERT-based sentiment analysis model with Ludwig is as simple as:\n\n```shell\nludwig train --dataset sst5 -\u2013config_str \u201c{input_features: [{name: sentence, type: text, encoder: bert}], output_features: [{name: label, type: category}]}\u201d\n```\n\n## Low-code interface for AutoML\n\n[Ludwig AutoML](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/automl\/) allows users to obtain trained models by providing just a dataset, the target column, and a time budget.\n\n```python\nauto_train_results = ludwig.automl.auto_train(dataset=my_dataset_df, target=target_column_name, time_limit_s=7200)\n```\n\n## Easy productionisation\n\nLudwig makes it easy to serve deep learning models, including on GPUs. Launch a REST API for your trained Ludwig model.\n\n```shell\nludwig serve --model_path=\/path\/to\/model\n```\n\nLudwig supports exporting models to efficient Torschscript bundles.\n\n```shell\nludwig export_torchscript -\u2013model_path=\/path\/to\/model\n```\n\n# Tutorials\n\n- [Text Classification](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/text_classification)\n- [Tabular Data Classification](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/adult_census_income)\n- [Image Classification](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/mnist)\n- [Multimodal Classification](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/multimodal_classification)\n\n# Example Use Cases\n\n- [Named Entity Recognition Tagging](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/ner_tagging)\n- [Natural Language Understanding](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/nlu)\n- [Machine Translation](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/machine_translation)\n- [Chit-Chat Dialogue Modeling through seq2seq](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/seq2seq)\n- [Sentiment Analysis](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/sentiment_analysis)\n- [One-shot Learning with Siamese Networks](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/oneshot)\n- [Visual Question Answering](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/visual_qa)\n- [Spoken Digit Speech Recognition](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/speech_recognition)\n- [Speaker Verification](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/speaker_verification)\n- [Binary Classification (Titanic)](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/titanic)\n- [Timeseries forecasting](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/forecasting)\n- [Timeseries forecasting (Weather)](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/weather)\n- [Movie rating prediction](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/movie_ratings)\n- [Multi-label classification](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/multi_label)\n- [Multi-Task Learning](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/multi_task)\n- [Simple Regression: Fuel Efficiency Prediction](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/fuel_efficiency)\n- [Fraud Detection](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples\/fraud)\n\n# More Information\n\n[Full official documentation](https:\/\/ludwig-ai.github.io\/ludwig-docs\/).\n\nRead our publications on [Ludwig](https:\/\/arxiv.org\/pdf\/1909.07930.pdf), [declarative ML](https:\/\/arxiv.org\/pdf\/2107.08148.pdf), and [Ludwig\u2019s SoTA benchmarks](https:\/\/openreview.net\/pdf?id=hwjnu6qW7E4).\n\nLearn more about [how Ludwig works](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/user_guide\/how_ludwig_works\/), [how to get started](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/getting_started\/), and work through more [examples](https:\/\/ludwig-ai.github.io\/ludwig-docs\/latest\/examples).\n\nIf you are interested in contributing, have questions, comments, or thoughts to share, or if you just want to be in the\nknow, please consider [joining the Ludwig Slack](https:\/\/join.slack.com\/t\/ludwig-ai\/shared_invite\/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ) and follow us on [Twitter](https:\/\/twitter.com\/ludwig_ai)!\n\n# Getting Involved\n\n- [Slack](https:\/\/join.slack.com\/t\/ludwig-ai\/shared_invite\/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ)\n- [Twitter](https:\/\/twitter.com\/ludwig_ai)\n- [Medium](https:\/\/medium.com\/ludwig-ai)\n- [GitHub Issues](https:\/\/github.com\/ludwig-ai\/ludwig\/issues)\n","116":"Pattern\n=======\n\n[![Build Status](http:\/\/img.shields.io\/travis\/clips\/pattern\/master.svg?style=flat)](https:\/\/travis-ci.org\/clips\/pattern\/branches)\n[![Coverage](https:\/\/img.shields.io\/coveralls\/clips\/pattern\/master.svg?style=flat)](https:\/\/coveralls.io\/github\/clips\/pattern?branch=master)\n[![PyPi version](http:\/\/img.shields.io\/pypi\/v\/pattern.svg?style=flat)](https:\/\/pypi.python.org\/pypi\/pattern)\n[![License](https:\/\/img.shields.io\/badge\/License-BSD%203--Clause-green.svg?style=flat)](https:\/\/github.com\/clips\/pattern\/blob\/master\/LICENSE.txt)\n\nPattern is a web mining module for Python. It has tools for:\n\n * Data Mining: web services (Google, Twitter, Wikipedia), web crawler, HTML DOM parser\n * Natural Language Processing: part-of-speech taggers, n-gram search, sentiment analysis, WordNet\n * Machine Learning: vector space model, clustering, classification (KNN, SVM, Perceptron)\n * Network Analysis: graph centrality and visualization.\n\nIt is well documented, thoroughly tested with 350+ unit tests and comes bundled with 50+ examples. The source code is licensed under BSD.\n\n![Example workflow](https:\/\/raw.githubusercontent.com\/clips\/pattern\/master\/docs\/g\/pattern_schema.gif)\n\nExample\n-------\n\nThis example trains a classifier on adjectives mined from Twitter using Python 3. First, tweets that contain hashtag #win or #fail are collected. For example: *\"$20 tip off a sweet little old lady today #win\"*. The word part-of-speech tags are then parsed, keeping only adjectives. Each tweet is transformed to a vector, a dictionary of adjective \u2192 count items, labeled `WIN` or `FAIL`. The classifier uses the vectors to learn which other tweets look more like `WIN` or more like `FAIL`.\n\n```python\nfrom pattern.web import Twitter\nfrom pattern.en import tag\nfrom pattern.vector import KNN, count\n\ntwitter, knn = Twitter(), KNN()\n\nfor i in range(1, 3):\n    for tweet in twitter.search('#win OR #fail', start=i, count=100):\n        s = tweet.text.lower()\n        p = '#win' in s and 'WIN' or 'FAIL'\n        v = tag(s)\n        v = [word for word, pos in v if pos == 'JJ'] # JJ = adjective\n        v = count(v) # {'sweet': 1}\n        if v:\n            knn.train(v, type=p)\n\nprint(knn.classify('sweet potato burger'))\nprint(knn.classify('stupid autocorrect'))\n```\n\nInstallation\n------------\n\nPattern supports Python 2.7 and Python 3.6. To install Pattern so that it is available in all your scripts, unzip the download and from the command line do:\n```bash\ncd pattern-3.6\npython setup.py install\n```\n\nIf you have pip, you can automatically download and install from the [PyPI repository](https:\/\/pypi.python.org\/pypi\/pattern):\n```bash\npip install pattern\n```\n\nIf none of the above works, you can make Python aware of the module in three ways:\n- Put the pattern folder in the same folder as your script.\n- Put the pattern folder in the standard location for modules so it is available to all scripts:\n  * `c:\\python36\\Lib\\site-packages\\` (Windows),\n  * `\/Library\/Python\/3.6\/site-packages\/` (Mac OS X),\n  * `\/usr\/lib\/python3.6\/site-packages\/` (Unix).\n- Add the location of the module to `sys.path` in your script, before importing it:\n\n```python\nMODULE = '\/users\/tom\/desktop\/pattern'\nimport sys; if MODULE not in sys.path: sys.path.append(MODULE)\nfrom pattern.en import parsetree\n```\n\nDocumentation\n-------------\n\nFor documentation and examples see the [user documentation](https:\/\/github.com\/clips\/pattern\/wiki).\n\nVersion\n-------\n\n3.6\n\nLicense\n-------\n\n**BSD**, see `LICENSE.txt` for further details.\n\nReference\n---------\n\nDe Smedt, T., Daelemans, W. (2012). Pattern for Python. *Journal of Machine Learning Research, 13*, 2031\u20132035.\n\nContribute\n----------\n\nThe source code is hosted on GitHub and contributions or donations are welcomed.\n\nBundled dependencies\n--------------------\n\nPattern is bundled with the following data sets, algorithms and Python packages:\n\n- **Brill tagger**, Eric Brill\n- **Brill tagger for Dutch**, Jeroen Geertzen\n- **Brill tagger for German**, Gerold Schneider & Martin Volk\n- **Brill tagger for Spanish**, trained on Wikicorpus (Samuel Reese & Gemma Boleda et al.)\n- **Brill tagger for French**, trained on Lefff (Beno\u00eet Sagot & Lionel Cl\u00e9ment et al.)\n- **Brill tagger for Italian**, mined from Wiktionary\n- **English pluralization**, Damian Conway\n- **Spanish verb inflection**, Fred Jehle\n- **French verb inflection**, Bob Salita\n- **Graph JavaScript framework**, Aslak Hellesoy & Dave Hoover\n- **LIBSVM**, Chih-Chung Chang & Chih-Jen Lin\n- **LIBLINEAR**, Rong-En Fan et al.\n- **NetworkX centrality**, Aric Hagberg, Dan Schult & Pieter Swart\n- **spelling corrector**, Peter Norvig\n\nAcknowledgements\n----------------\n\n**Authors:**\n\n- Tom De Smedt (tom@organisms.be)\n- Walter Daelemans (walter.daelemans@ua.ac.be)\n\n**Contributors (chronological):**\n\n- Frederik De Bleser\n- Jason Wiener\n- Daniel Friesen\n- Jeroen Geertzen\n- Thomas Crombez\n- Ken Williams\n- Peteris Erins\n- Rajesh Nair\n- F. De Smedt\n- Radim \u0158eh\u016f\u0159ek\n- Tom Loredo\n- John DeBovis\n- Thomas Sileo\n- Gerold Schneider\n- Martin Volk\n- Samuel Joseph\n- Shubhanshu Mishra\n- Robert Elwell\n- Fred Jehle\n- Antoine Mazi\u00e8res + fabelier.org\n- R\u00e9mi de Zoeten + closealert.nl\n- Kenneth Koch\n- Jens Grivolla\n- Fabio Marfia\n- Steven Loria\n- Colin Molter + tevizz.com\n- Peter Bull\n- Maurizio Sambati\n- Dan Fu\n- Salvatore Di Dio\n- Vincent Van Asch\n- Frederik Elwert\n","117":"[![CircleCI](https:\/\/circleci.com\/gh\/gradio-app\/gradio.svg?style=svg)](https:\/\/circleci.com\/gh\/gradio-app\/gradio)  [![PyPI version](https:\/\/badge.fury.io\/py\/gradio.svg)](https:\/\/badge.fury.io\/py\/gradio)  [![codecov](https:\/\/codecov.io\/gh\/gradio-app\/gradio\/branch\/master\/graph\/badge.svg?token=NNVPX9KEGS)](https:\/\/codecov.io\/gh\/gradio-app\/gradio) [![PyPI - Downloads](https:\/\/img.shields.io\/pypi\/dm\/gradio)](https:\/\/pypi.org\/project\/gradio\/) [![Twitter Follow](https:\/\/img.shields.io\/twitter\/follow\/gradio.svg?style=social&label=Follow)](https:\/\/twitter.com\/gradio)\n\n#  Welcome to Gradio\n\nQuickly create beautiful user interfaces around your machine learning models. Gradio (pronounced GRAY-dee-oh) makes it easy for you to demo your model in your browser or let people \"try it out\" by dragging-and-dropping in their own images, pasting text, recording their own voice, etc. and seeing what the model outputs.  \n\n![Interface montage](website\/homepage\/src\/assets\/img\/montage.gif)\n\nGradio is useful for:\n\n* **Demoing** your machine learning models for clients \/ collaborators \/ users \/ students\n\n* **Deploying** your models quickly with automatic shareable links and getting feedback on model performance\n\n* **Debugging** your model interactively during development using built-in manipulation and interpretation tools\n\n**You can find an interactive version of the following Getting Started at [https:\/\/gradio.app\/getting_started](https:\/\/gradio.app\/getting_started).**\n\n\n## Getting Started\n\n**Prerequisite**: Python 3.7+ and that's it! \n\n### Quick Start\n\nTo get Gradio running with a simple \"Hello, World\" example, follow these three steps:\n\n<span>1.<\/span> Install Gradio from pip.\n\n```bash\npip install gradio\n```\n\n<span>2.<\/span> Run the code below as a Python script or in a Python notebook (or in a  [colab notebook](https:\/\/colab.research.google.com\/drive\/18ODkJvyxHutTN0P5APWyGFO_xwNcgHDZ?usp=sharing)).\n\n```python\nimport gradio as gr\n\n\ndef greet(name):\n    return \"Hello \" + name + \"!!\"\n\n\niface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\niface.launch()\n\n```\n\n<span>3.<\/span> The interface below will appear automatically within the Python notebook, or pop in a browser on  [http:\/\/localhost:7860](http:\/\/localhost:7860\/)  if running from a script.\n\n![hello_world interface](demo\/hello_world\/screenshot.gif)\n\n### Understanding the `Interface` class\n\nGradio can wrap almost any Python function with an easy-to-use user interface. In the example above, we saw a simple text-based function. But the function could be anything from image enhancer to a tax calculator to (most commonly) the prediction function of a pretrained machine learning model.\n\nThe core  `Interface`  class is initialized with three parameters:\n\n-   `fn`: the function to wrap\n-   `inputs`: the input component type(s), e.g. `\"image\"` or `\"audio\"` ([see docs for complete list](\/docs))\n-   `outputs`: the output component type(s) e.g. `\"image\"` or `\"label\"` ([see docs for complete list](\/docs))\n\nWith these three arguments, we can quickly create interfaces and  `launch()`  them. But what if you want to change how the UI components look or behave?\n\n### Customizable Components\n\nLet's say we want to customize the input text field - for example, we wanted it to be larger and have a text hint. If we use the actual input class for  `Textbox`  instead of using the string shortcut, we have access to much more customizability. To see a list of all the components we support and how you can customize them, check out the [Docs](https:\/\/gradio.app\/docs).\n\n```python\nimport gradio as gr\n\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\n\niface = gr.Interface(\n    fn=greet,\n    inputs=gr.inputs.Textbox(lines=2, placeholder=\"Name Here...\"),\n    outputs=\"text\",\n)\niface.launch()\n\n```\n![hello_world_2 interface](demo\/hello_world_2\/screenshot.gif)\n\n### Multiple Inputs and Outputs\n\nLet's say we had a much more complex function, with multiple inputs and outputs. In the example below, we have a function that takes a string, boolean, and number, and returns a string and number. Take a look how we pass a list of input and output components.\n\n```python\nimport gradio as gr\n\n\ndef greet(name, is_morning, temperature):\n    salutation = \"Good morning\" if is_morning else \"Good evening\"\n    greeting = \"%s %s. It is %s degrees today\" % (salutation, name, temperature)\n    celsius = (temperature - 32) * 5 \/ 9\n    return greeting, round(celsius, 2)\n\n\niface = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"checkbox\", gr.inputs.Slider(0, 100)],\n    outputs=[\"text\", \"number\"],\n)\niface.launch()\n\n```\n![hello_world_3 interface](demo\/hello_world_3\/screenshot.gif)\n\nWe simply wrap the components in a list. Each component in the `inputs` list corresponds to one of the parameters of the function, in order. Each component in the `outputs` list corresponds to one of the values returned by the function, again in order. \n\n### Working with Images\n\nLet's try an image-to-image function. When using the  `Image`  component, your function will receive a numpy array of your specified size, with the shape  `(width, height, 3)`, where the last dimension represents the RGB values. We'll return an image as well in the form of a numpy array.\n\n```python\nimport numpy as np\n\nimport gradio as gr\n\n\ndef sepia(input_img):\n    sepia_filter = np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    )\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img \/= sepia_img.max()\n    return sepia_img\n\n\niface = gr.Interface(sepia, gr.inputs.Image(shape=(200, 200)), \"image\")\n\niface.launch()\n\n```\n![sepia_filter interface](demo\/sepia_filter\/screenshot.gif)\n\nAdditionally, our  `Image`  input interface comes with an 'edit' button which opens tools for cropping, flipping, rotating, drawing over, and applying filters to images. We've found that manipulating images in this way will often reveal hidden flaws in a model.\n\nIn addition to images, Gradio supports other media input types, such as audio or video uploads, as well as many output components. Read about these in the [Docs](https:\/\/gradio.app\/docs).\n\n### Working with DataFrames and Graphs\n\nYou can use Gradio to support inputs and outputs from your typical data libraries, such as numpy arrays, pandas dataframes, and plotly graphs. Take a look at the demo below (ignore the complicated data manipulation in the function!)\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport gradio as gr\n\n\ndef sales_projections(employee_data):\n    sales_data = employee_data.iloc[:, 1:4].astype(\"int\").to_numpy()\n    regression_values = np.apply_along_axis(\n        lambda row: np.array(np.poly1d(np.polyfit([0, 1, 2], row, 2))), 0, sales_data\n    )\n    projected_months = np.repeat(\n        np.expand_dims(np.arange(3, 12), 0), len(sales_data), axis=0\n    )\n    projected_values = np.array(\n        [\n            month * month * regression[0] + month * regression[1] + regression[2]\n            for month, regression in zip(projected_months, regression_values)\n        ]\n    )\n    plt.plot(projected_values.T)\n    plt.legend(employee_data[\"Name\"])\n    return employee_data, plt.gcf(), regression_values\n\n\niface = gr.Interface(\n    sales_projections,\n    gr.inputs.Dataframe(\n        headers=[\"Name\", \"Jan Sales\", \"Feb Sales\", \"Mar Sales\"],\n        default=[[\"Jon\", 12, 14, 18], [\"Alice\", 14, 17, 2], [\"Sana\", 8, 9.5, 12]],\n    ),\n    [\"dataframe\", \"plot\", \"numpy\"],\n    description=\"Enter sales figures for employees to predict sales trajectory over year.\",\n)\niface.launch()\n\n```\n![sales_projections interface](demo\/sales_projections\/screenshot.gif)\n\n### Example Inputs\n\nYou can provide example data that a user can easily load into the model. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you provide a **nested list** to the  `examples=`  keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the  [Docs](https:\/\/gradio.app\/docs).\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 \/ num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.inputs.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    examples=[\n        [5, \"add\", 3],\n        [4, \"divide\", 2],\n        [-4, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"test calculator\",\n    description=\"heres a sample toy calculator. enjoy!\",\n    flagging_options=[\"this\", \"or\", \"that\"],\n)\n\niface.launch()\n\n```\n![calculator interface](demo\/calculator\/screenshot.gif)\n\nYou can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the `examples_per_page` argument of Interface) and you can use CTRL + arrow keys to navigate through the examples quickly.\n\n### Live Interfaces\n\nYou can make interfaces automatically refresh by setting `live=True` in the interface. Now the interface will recalculate as soon as the user input changes.\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 \/ num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.inputs.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    live=True,\n)\n\niface.launch()\n\n```\n![calculator_live interface](demo\/calculator_live\/screenshot.gif)\n\nNote there is no submit button, because the interface resubmits automatically on change.\n\n### Using State\n\nYour function may use data that persists beyond a single function call. If the data is something accessible to all function calls and all users, you can create a global variable outside the function call and access it inside the function. For example, you may load a large model outside the function and use it inside the function so that every function call does not need to reload the model.\n\nAnother type of data persistence Gradio supports is session **state**, where data persists across multiple submits within a page load. However, data is *not* shared between different users of your model. To store data in a session state, you need to do three things: (1) Pass in an extra parameter into your function, which represents the state of the interface. (2) At the end of the function, return the updated value of the state as an extra return value (3) Add the `'state'` input and `'state'` output components when creating your `Interface`. See the chatbot example below: \n\n```python\nimport random\n\nimport gradio as gr\n\n\ndef chat(message, history):\n    history = history or []\n    if message.startswith(\"How many\"):\n        response = random.randint(1, 10)\n    elif message.startswith(\"How\"):\n        response = random.choice([\"Great\", \"Good\", \"Okay\", \"Bad\"])\n    elif message.startswith(\"Where\"):\n        response = random.choice([\"Here\", \"There\", \"Somewhere\"])\n    else:\n        response = \"I don't know\"\n    history.append((message, response))\n    html = \"<div class='chatbot'>\"\n    for user_msg, resp_msg in history:\n        html += f\"<div class='user_msg'>{user_msg}<\/div>\"\n        html += f\"<div class='resp_msg'>{resp_msg}<\/div>\"\n    html += \"<\/div>\"\n    return html, history\n\n\niface = gr.Interface(\n    chat,\n    [\"text\", \"state\"],\n    [\"html\", \"state\"],\n    css=\"\"\"\n    .chatbox {display:flex;flex-direction:column}\n    .user_msg, .resp_msg {padding:4px;margin-bottom:4px;border-radius:4px;width:80%}\n    .user_msg {background-color:cornflowerblue;color:white;align-self:start}\n    .resp_msg {background-color:lightgray;align-self:self-end}\n\"\"\",\n    allow_screenshot=False,\n    allow_flagging=\"never\",\n)\niface.launch()\n\n```\n![chatbot interface](demo\/chatbot\/screenshot.gif)\n\nNotice how the state persists across submits within each page, but the state is not shared between the two pages. Some more points to note: you can pass in a default value to the state parameter, which is used as the initial value of the state. The state must be a something that can be serialized to a JSON format (e.g. a dictionary, a list, or a single value. Typically, objects will not work).  \n\n### Flagging\n\nUnderneath the output interfaces, there is a button marked \"Flag\". When a user testing your model sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for the interface creator to review. Within the directory provided by the  `flagging_dir=`  argument to the Interface constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.\n\nFor example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- calculator.py\n+-- flagged\/\n|   +-- logs.csv\n```\n\n*flagged\/logs.csv*\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\n\nWith the sepia interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- sepia.py\n+-- flagged\/\n|   +-- logs.csv\n|   +-- im\/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output\/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n*flagged\/logs.csv*\n```csv\nim,Output\nim\/0.png,Output\/0.png\nim\/1.png,Output\/1.png\n```\n\nYou can review these flagged inputs by manually exploring the flagging directory, or load them into the examples of the Gradio interface by pointing the  `examples=`  argument to the flagged directory. If you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.\n\n### Sharing Interfaces Publicly\n\nInterfaces can be easily shared publicly by setting `share=True` in the `launch()` method. Like this:\n\n```python\ngr.Interface(classify_image, \"image\", \"label\").launch(share=True)\n```\n\nThis generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on!), you don't have to worry about any packaging any dependencies. If you're working out of colab notebook, a share link is always automatically created. It usually looks something like this:  **XXXXX.gradio.app**. Although the link is served through a gradio link, we are only a proxy for your local server, and do not store any data sent through the interfaces.\n\nKeep in mind, however, that these links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. If you set `share=False` (the default), only a local link is created, which can be shared by  [port-forwarding](https:\/\/www.ssh.com\/ssh\/tunneling\/example)  with specific users. \n\nShare links expire after 72 hours. For permanent hosting, see Hosting Gradio Apps on Spaces below.\n\n![Sharing diagram](website\/homepage\/src\/assets\/img\/sharing.svg)\n\n### Hosting Gradio Apps on Spaces\n\nHuggingface provides the infrastructure to permanently host your Gradio model on the internet, for free! You can either drag and drop a folder containing your Gradio model and all related files, or you can point HF Spaces to your Git repository and HP Spaces will pull the Gradio interface from there. See [Huggingface Spaces](http:\/\/huggingface.co\/spaces\/) for more information. \n\n![Hosting Demo](website\/homepage\/src\/assets\/img\/hf_demo.gif)\n\n## Advanced Features\n<span id=\"advanced-features\"><\/span>\n\nHere, we go through several advanced functionalities that your Gradio demo can include without you needing to write much more code!\n\n### Authentication\n\nYou may wish to put an authentication page in front of your interface to limit who can open your interface. With the `auth=` keyword argument in the `launch()` method, you can pass a list of acceptable username\/password tuples; or, for more complex authentication handling, you can even pass a function that takes a username and password as arguments, and returns True to allow authentication, False otherwise. Here's an example that provides password-based authentication for a single user named \"admin\":\n\n```python\ngr.Interface(fn=classify_image, inputs=image, outputs=label).launch(auth=(\"admin\", \"pass1234\"))\n```\n\n### Interpreting your Predictions\n\nMost models are black boxes such that the internal logic of the function is hidden from the end user. To encourage transparency, we've made it very easy to add interpretation to your model by  simply setting the `interpretation` keyword in the `Interface` class to `default`. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation:\n\n```python\nimport requests\nimport tensorflow as tf\n\nimport gradio as gr\n\ninception_net = tf.keras.applications.MobileNetV2()  # load the model\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https:\/\/git.io\/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\nimage = gr.inputs.Image(shape=(224, 224))\nlabel = gr.outputs.Label(num_top_classes=3)\n\ngr.Interface(\n    fn=classify_image, inputs=image, outputs=label, interpretation=\"default\"\n).launch()\n\n```\n\n\nIn addition to `default`, Gradio also includes [Shapley-based interpretation](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html), which provides more accurate interpretations, albeit usually with a slower runtime. To use this, simply set the `interpretation` parameter to `\"shap\"` (note: also make sure the python package `shap` is installed). Optionally, you can modify the the `num_shap` parameter, which controls the tradeoff between accuracy and runtime (increasing this value generally increases accuracy). Here is an example:\n\n```python\ngr.Interface(fn=classify_image, inputs=image, outputs=label, interpretation=\"shap\", num_shap=5).launch()\n```\n\nThis will work for any function, even if internally, the model is a complex neural network or some other black box. If you use Gradio's `default` or `shap` interpretation, the output component must be a `Label`. All common input components are supported. Here is an example with text input.\n\n```python\nimport re\n\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count \/ total, \"female\": female_count \/ total}\n\n\niface = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.inputs.Textbox(default=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=\"default\",\n)\niface.launch()\n\n```\n\nSo what is happening under the hood? With these interpretation methods, Gradio runs the prediction multiple times with modified versions of the input. Based on the results, you'll see that the interface automatically highlights the parts of the text (or image, etc.) that contributed increased the likelihood of the class as red. The intensity of color corresponds to the importance of that part of the input. The parts that decrease the class confidence are highlighted blue.\n\nYou can also write your own interpretation function. The demo below adds custom interpretation to the previous demo. This function will take the same inputs as the main wrapped function. The output of this interpretation function will be used to highlight the input of each input interface - therefore the number of outputs here corresponds to the number of input interfaces. To see the format for interpretation for each input interface, check the Docs.\n\n```python\nimport re\n\nimport gradio as gr\n\nmale_words, female_words = [\"he\", \"his\", \"him\"], [\"she\", \"hers\", \"her\"]\n\n\ndef gender_of_sentence(sentence):\n    male_count = len([word for word in sentence.split() if word.lower() in male_words])\n    female_count = len(\n        [word for word in sentence.split() if word.lower() in female_words]\n    )\n    total = max(male_count + female_count, 1)\n    return {\"male\": male_count \/ total, \"female\": female_count \/ total}\n\n\ndef interpret_gender(sentence):\n    result = gender_of_sentence(sentence)\n    is_male = result[\"male\"] > result[\"female\"]\n    interpretation = []\n    for word in re.split(\"( )\", sentence):\n        score = 0\n        token = word.lower()\n        if (is_male and token in male_words) or (not is_male and token in female_words):\n            score = 1\n        elif (is_male and token in female_words) or (\n            not is_male and token in male_words\n        ):\n            score = -1\n        interpretation.append((word, score))\n    return interpretation\n\n\niface = gr.Interface(\n    fn=gender_of_sentence,\n    inputs=gr.inputs.Textbox(default=\"She went to his house to get her keys.\"),\n    outputs=\"label\",\n    interpretation=interpret_gender,\n    enable_queue=True,\n)\niface.launch()\n\n```\n\n### Themes and Custom Styling\n\nIf you'd like to change how your interface looks, you can select a different theme by simply passing in the `theme` parameter, like so:\n\n```python\ngr.Interface(fn=classify_image, inputs=image, outputs=label, theme=\"huggingface\").launch()\n```\n\nHere are the themes we currently support: `\"default\"`, `\"huggingface\"`, `\"grass\"`, `\"peach\"`, and the dark themes corresponding to each of these: `\"darkdefault\"`, `\"darkhuggingface\"`, `\"darkgrass\"`, `\"darkpeach\"`.\n\nIf you'd like to have more fine-grained control over any aspect of the app, you can also write your own css or pass in a css file, with the `css` parameter of the `Interface` class.\n\n### Custom Flagging Options\n\nIn some cases, you might like to provide your users or testers with *more* than just a binary option to flag a sample. You can provide `flagging_options` that they select from a dropdown each time they click the flag button. This lets them provide additional feedback every time they flag a sample.\n\nHere's an example:\n\n```python\ngr.Interface(fn=classify_image, inputs=image, outputs=label, flagging_options=[\"incorrect\", \"ambiguous\", \"offensive\", \"other\"]).launch()\n```\n\n### Loading Hugging Face Models and Spaces\n\nGradio integrates nicely with the Hugging Face Hub, allowing you to load models and Spaces with just one line of code. To use this, simply use the `load()` method in the `Interface` class. So:\n\n- To load any model from the Hugging Face Hub and create an interface around it, you pass `\"model\/\"` or `\"huggingface\/\"` followed by the model name, like these examples:\n\n```python\ngr.Interface.load(\"huggingface\/gpt2\").launch();\n```\n\n```python\ngr.Interface.load(\"huggingface\/EleutherAI\/gpt-j-6B\", \n    inputs=gr.inputs.Textbox(lines=5, label=\"Input Text\")  # customizes the input component\n).launch()\n```\n\n- To load any Space from the Hugging Face Hub and recreate it locally (so that you can customize the inputs and outputs for example), you pass `\"spaces\/\"` followed by the model name:\n\n```python\ngr.Interface.load(\"spaces\/eugenesiow\/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\").launch()\n```\n\nOne of the great things about loading Hugging Face models or spaces using Gradio is that you can then immediately use the resulting `Interface` object just like function in your Python code (this works for every type of model\/space: text, images, audio, video, and even multimodal models):\n\n```python\nio = gr.Interface.load(\"models\/EleutherAI\/gpt-neo-2.7B\")\nio(\"It was the best of times\")  # outputs model completion\n```\n\n### Putting Interfaces in Parallel and Series\n\nGradio also lets you mix interfaces very easily using the `gradio.Parallel` and `gradio.Series` classes. `Parallel` lets you put two similar models (if they have the same input type) in parallel to compare model predictions:\n\n```python\ngenerator1 = gr.Interface.load(\"huggingface\/gpt2\")\ngenerator2 = gr.Interface.load(\"huggingface\/EleutherAI\/gpt-neo-2.7B\")\ngenerator3 = gr.Interface.load(\"huggingface\/EleutherAI\/gpt-j-6B\")\n\ngr.Parallel(generator1, generator2, generator3).launch()\n```\n\n`Series` lets you put models and spaces in series, piping the output of one model into the input of the next model. \n\n```python\ngenerator = gr.Interface.load(\"huggingface\/gpt2\")\ntranslator = gr.Interface.load(\"huggingface\/t5-small\")\n\ngr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result.\n```\n\nAnd of course, you can also mix `Parallel` and `Series` together whenever that makes sense!\n\n### Queuing to Manage Long Inference Times\n\nIf many people are using your interface or if the inference time of your function is long (> 1min), simply set the `enable_queue` parameter in the `launch` method to `True` to prevent timeouts.\n\n```python\ngr.Interface(fn=classify_image, inputs=image, outputs=label).launch(enable_queue=True)\n```\n\nThis sets up a queue of workers to handle the predictions and return the response to the front end. This is strongly recommended if you are planning on uploading your demo to Hugging Face Spaces (as described above) so that you can manage a large number of users simultaneously using your demo.\n\n\n\n\n\n##  System Requirements:\n\nGradio requires Python `3.7+` and has been tested on the latest versions of Windows, MacOS, and various common Linux distributions (e.g. Ubuntu). For Python package requirements, please see the `setup.py` file.\n\n##  Contributing:\n\nIf you would like to contribute and your contribution is small, you can directly open a pull request (PR). If you would like to contribute a larger feature, we recommend first creating an issue with a proposed design for discussion. Please see our [contributing guidelines](https:\/\/github.com\/gradio-app\/gradio\/blob\/master\/CONTRIBUTING.md) for more info.\n\n##  License:\n\nGradio is licensed under the Apache License 2.0\n\n\n##  See more:\n\nYou can find many more examples as well as more info on usage on our website: www.gradio.app\n\nSee, also, the accompanying paper: [\"Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild\"](https:\/\/arxiv.org\/pdf\/1906.02569.pdf), *ICML HILL 2019*, and please use the citation below.\n\n```\n@article{abid2019gradio,\ntitle={Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},\nauthor={Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},\njournal={arXiv preprint arXiv:1906.02569},\nyear={2019}\n}\n```","118":"\n\n###################################################\nA Machine Learning Course with Python\n###################################################\n\n.. image:: https:\/\/img.shields.io\/badge\/contributions-welcome-brightgreen.svg?style=flat\n    :target: https:\/\/github.com\/pyairesearch\/machine-learning-for-everybody\/pulls\n.. image:: https:\/\/badges.frapsoft.com\/os\/v2\/open-source.png?v=103\n    :target: https:\/\/github.com\/ellerbrock\/open-source-badge\/\n.. image:: https:\/\/img.shields.io\/badge\/Made%20with-Python-1f425f.svg\n      :target: https:\/\/www.python.org\/\n.. image:: https:\/\/img.shields.io\/github\/contributors\/machinelearningmindset\/machine-learning-course.svg\n      :target: https:\/\/github.com\/machinelearningmindset\/machine-learning-course\/graphs\/contributors\n.. image:: https:\/\/img.shields.io\/badge\/book-pdf-blue.svg\n   :target: https:\/\/machinelearningmindset.com\/wp-content\/uploads\/2019\/06\/machine-learning-course.pdf\n.. image:: https:\/\/img.shields.io\/badge\/official-documentation-green.svg\n   :target: https:\/\/machine-learning-course.readthedocs.io\/en\/latest\/\n.. image:: https:\/\/img.shields.io\/twitter\/follow\/machinemindset.svg?label=Follow&style=social\n      :target: https:\/\/twitter.com\/machinemindset\n\n\n\n\n\n\n##################\nTable of Contents\n##################\n.. contents::\n  :local:\n  :depth: 4\n\n\n================================================\nDownload Free Deep Learning Resource Guide\n================================================\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. raw:: html\n\n  <a href=\"https:\/\/www.machinelearningmindset.com\/deep-learning-roadmap\/\" target=\"_blank\">\n    <img width=\"723\" height=\"400\" align=\"center\" src=\"_img\/deeplearningresource.png\"\/>\n  <\/a>\n\n.. raw:: html\n\n   <\/div>\n   \n\n================================================\nSlack Group\n================================================\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. raw:: html\n\n <a href=\"https:\/\/www.machinelearningmindset.com\/slack-group\/\" target=\"_blank\">\n  <img width=\"1033\" height=\"350\" align=\"center\" src=\"https:\/\/github.com\/machinelearningmindset\/TensorFlow-Course\/blob\/master\/_img\/0-welcome\/joinslack.png\"\/>\n <\/a>\n\n.. raw:: html\n\n   <\/div>\n\n========================\nIntroduction\n========================\n\nThe purpose of this project is to provide a comprehensive and yet simple course in Machine Learning using Python.\n\n.. You can access to the full documentation with the following links: |Book| |Documentation|\n\n.. .. |Book| image:: https:\/\/img.shields.io\/badge\/book-pdf-blue.svg\n   :target: https:\/\/machinelearningmindset.com\/wp-content\/uploads\/2019\/06\/machine-learning-course.pdf\n.. .. |Documentation| image:: https:\/\/img.shields.io\/badge\/official-documentation-green.svg\n   :target: https:\/\/machine-learning-course.readthedocs.io\/en\/latest\/\n\n============\nMotivation\n============\n\n``Machine Learning``, as a tool for ``Artificial Intelligence``, is one of the most widely adopted\nscientific fields. A considerable amount of literature has been published on Machine Learning.\nThe purpose of this project is to provide the most important aspects of ``Machine Learning`` by presenting a\nseries of simple and yet comprehensive tutorials using ``Python``. In this project, we built our\ntutorials using many different well-known Machine Learning frameworks such as ``Scikit-learn``. In this project you will learn:\n\n* What is the definition of Machine Learning?\n* When it started and what is the trending evolution?\n* What are the Machine Learning categories and subcategories?\n* What are the mostly used Machine Learning algorithms and how to implement them?\n\n\n\n=====================\nMachine Learning\n=====================\n\n+--------------------------------------------------------------------+-------------------------------+\n| Title                                                              |    Document                   |\n+====================================================================+===============================+\n| An Introduction to Machine Learning                                |   `Overview <Intro_>`_        |\n+--------------------------------------------------------------------+-------------------------------+\n\n.. _Intro: docs\/source\/intro\/intro.rst\n\n------------------------------------------------------------\nMachine Learning Basics\n------------------------------------------------------------\n\n.. figure:: _img\/intro.png\n.. _lrtutorial: docs\/source\/content\/overview\/linear-regression.rst\n.. _lrcode: https:\/\/github.com\/machinelearningmindset\/machine-learning-course\/blob\/master\/code\/overview\/linear_regression\/linearRegressionOneVariable.ipynb\n\n.. _overtutorial: docs\/source\/content\/overview\/overfitting.rst\n.. _overcode: code\/overview\/overfitting\n\n.. _regtutorial: docs\/source\/content\/overview\/regularization.rst\n.. _regcode: code\/overview\/regularization\n\n.. _crosstutorial: docs\/source\/content\/overview\/crossvalidation.rst\n.. _crosscode: code\/overview\/cross-validation\n\n\n\n\n+--------------------------------------------------------------------+-------------------------------+--------------------------------+\n| Title                                                              |    Code                       |    Document                    |\n+====================================================================+===============================+================================+\n| Linear Regression                                                  | `Python <lrcode_>`_           | `Tutorial <lrtutorial_>`_      |\n+--------------------------------------------------------------------+-------------------------------+--------------------------------+\n| Overfitting \/ Underfitting                                         | `Python <overcode_>`_         | `Tutorial <overtutorial_>`_    |\n+--------------------------------------------------------------------+-------------------------------+--------------------------------+\n| Regularization                                                     | `Python <regcode_>`_          | `Tutorial <regtutorial_>`_     |\n+--------------------------------------------------------------------+-------------------------------+--------------------------------+\n| Cross-Validation                                                   | `Python <crosscode_>`_        | `Tutorial <crosstutorial_>`_   |\n+--------------------------------------------------------------------+-------------------------------+--------------------------------+\n\n\n------------------------------------------------------------\nSupervised learning\n------------------------------------------------------------\n\n.. figure:: _img\/supervised.gif\n\n.. _dtdoc: docs\/source\/content\/supervised\/decisiontrees.rst\n.. _dtcode: code\/supervised\/DecisionTree\/decisiontrees.py\n\n.. _knndoc: docs\/source\/content\/supervised\/knn.rst\n.. _knncode: code\/supervised\/KNN\/knn.py\n\n.. _nbdoc: docs\/source\/content\/supervised\/bayes.rst\n.. _nbcode: code\/supervised\/Naive_Bayes\n\n.. _logisticrdoc: docs\/source\/content\/supervised\/logistic_regression.rst\n.. _logisticrcode: supervised\/Logistic_Regression\/logistic_ex1.py\n\n.. _linearsvmdoc: docs\/source\/content\/supervised\/linear_SVM.rst\n.. _linearsvmcode: code\/supervised\/Linear_SVM\/linear_svm.py\n\n\n\n+--------------------------------------------------------------------+-------------------------------+------------------------------+\n| Title                                                              |    Code                       |    Document                  |\n+====================================================================+===============================+==============================+\n| Decision Trees                                                     | `Python <dtcode_>`_           | `Tutorial <dtdoc_>`_         |\n+--------------------------------------------------------------------+-------------------------------+------------------------------+\n| K-Nearest Neighbors                                                | `Python <knncode_>`_          | `Tutorial <knndoc_>`_        |\n+--------------------------------------------------------------------+-------------------------------+------------------------------+\n| Naive Bayes                                                        | `Python <nbcode_>`_           |  `Tutorial <nbdoc_>`_        |\n+--------------------------------------------------------------------+-------------------------------+------------------------------+\n| Logistic Regression                                                | `Python <logisticrcode_>`_    |  `Tutorial <logisticrdoc_>`_ |\n+--------------------------------------------------------------------+-------------------------------+------------------------------+\n| Support Vector Machines                                            | `Python <linearsvmcode_>`_    | `Tutorial <linearsvmdoc_>`_  |\n+--------------------------------------------------------------------+-------------------------------+------------------------------+\n\n\n\n\n------------------------------------------------------------\nUnsupervised learning\n------------------------------------------------------------\n\n.. figure:: _img\/unsupervised.gif\n\n.. _clusteringdoc: docs\/source\/content\/unsupervised\/clustering.rst\n.. _clusteringcode: code\/unsupervised\/Clustering\n\n.. _pcadoc: docs\/source\/content\/unsupervised\/pca.rst\n.. _pcacode: code\/unsupervised\/PCA\n\n+--------------------------------------------------------------------+-------------------------------+--------------------------------+\n| Title                                                              |    Code                       |    Document                    |\n+====================================================================+===============================+================================+\n| Clustering                                                         | `Python <clusteringcode_>`_   | `Tutorial <clusteringdoc_>`_   |\n+--------------------------------------------------------------------+-------------------------------+--------------------------------+\n| Principal Components Analysis                                      | `Python <pcacode_>`_          | `Tutorial <pcadoc_>`_          |\n+--------------------------------------------------------------------+-------------------------------+--------------------------------+\n\n\n\n\n------------------------------------------------------------\nDeep Learning\n------------------------------------------------------------\n\n.. figure:: _img\/deeplearning.png\n\n.. _mlpdoc: docs\/source\/content\/deep_learning\/mlp.rst\n.. _mlpcode: code\/deep_learning\/mlp\n\n\n.. _cnndoc: docs\/source\/content\/deep_learning\/cnn.rst\n.. _cnncode: code\/deep_learning\/cnn\n\n.. _aedoc: docs\/source\/content\/deep_learning\/autoencoder.rst\n.. _aecode: code\/deep_learning\/autoencoder\n\n.. _rnndoc: code\/deep_learning\/rnn\/rnn.ipynb\n.. _rnncode: code\/deep_learning\/rnn\/rnn.py\n\n\n+--------------------------------------------------------------------+-------------------------------+---------------------------+\n| Title                                                              |    Code                       |    Document               |\n+====================================================================+===============================+===========================+\n| Neural Networks Overview                                           |    `Python <mlpcode_>`_       |  `Tutorial <mlpdoc_>`_    |\n+--------------------------------------------------------------------+-------------------------------+---------------------------+\n| Convolutional Neural Networks                                      |    `Python <cnncode_>`_       | `Tutorial <cnndoc_>`_     |\n+--------------------------------------------------------------------+-------------------------------+---------------------------+\n| Autoencoders                                                       |    `Python <aecode_>`_        | `Tutorial <aedoc_>`_      |\n+--------------------------------------------------------------------+-------------------------------+---------------------------+\n| Recurrent Neural Networks                                          |    `Python <rnncode_>`_       |  `IPython <rnndoc_>`_     |\n+--------------------------------------------------------------------+-------------------------------+---------------------------+\n\n\n\n========================\nPull Request Process\n========================\n\nPlease consider the following criterions in order to help us in a better way:\n\n1. The pull request is mainly expected to be a link suggestion.\n2. Please make sure your suggested resources are not obsolete or broken.\n3. Ensure any install or build dependencies are removed before the end of the layer when doing a\n   build and creating a pull request.\n4. Add comments with details of changes to the interface, this includes new environment\n   variables, exposed ports, useful file locations and container parameters.\n5. You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you\n   do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.\n\n========================\nFinal Note\n========================\n\nWe are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.\nFor contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate\nyour kind feedback and support.\n\n\n========================\nDevelopers\n========================\n\n**Creator**: Machine Learning Mindset [`Blog\n<https:\/\/machinelearningmindset.com\/blog\/>`_, `GitHub\n<https:\/\/github.com\/machinelearningmindset>`_, `Twitter\n<https:\/\/twitter.com\/machinemindset>`_]\n\n**Supervisor**: Amirsina Torfi [`GitHub\n<https:\/\/github.com\/astorfi>`_, `Personal Website\n<https:\/\/astorfi.github.io\/>`_, `Linkedin\n<https:\/\/www.linkedin.com\/in\/amirsinatorfi\/>`_ ]\n\n**Developers**: Brendan Sherman\\*, James E Hopkins\\* [`Linkedin <https:\/\/www.linkedin.com\/in\/jhopk>`_], Zac Smith [`Linkedin <https:\/\/www.linkedin.com\/in\/zac-smith-a7bb60185\/i>`_]\n\n**NOTE**: This project has been developed as a capstone project offered by [`CS 4624 Multimedia\/ Hypertext course at Virginia Tech <https:\/\/vtechworks.lib.vt.edu\/handle\/10919\/90655>`_] and\nSupervised and supported by [`Machine Learning Mindset <https:\/\/machinelearningmindset.com\/>`_].\n\n\\*: equally contributed\n\n======================\nCitation\n======================\n\nIf you found this course useful, please kindly consider citing it as below:\n\n.. code:: shell\n\n    @software{amirsina_torfi_2019_3585763,\n      author       = {Amirsina Torfi and\n                      Brendan Sherman and\n                      Jay Hopkins and\n                      Eric Wynn and\n                      hokie45 and\n                      Frederik De Bleser and\n                      \u674e\u660e\u5cb3 and\n                      Samuel Husso and\n                      Alain},\n      title        = {{machinelearningmindset\/machine-learning-course: \n                       Machine Learning with Python}},\n      month        = dec,\n      year         = 2019,\n      publisher    = {Zenodo},\n      version      = {1.0},\n      doi          = {10.5281\/zenodo.3585763},\n      url          = {https:\/\/doi.org\/10.5281\/zenodo.3585763}\n    }\n","119":"<h1 align=\"center\">\n\t<img width=\"300\" src=\"https:\/\/github.com\/mindsdb\/mindsdb_native\/blob\/stable\/assets\/MindsDBColorPurp@3x.png?raw=true\" alt=\"MindsDB\">\n\t<br>\n\n<\/h1>\n<div align=\"center\">\n\t<a href=\"https:\/\/github.com\/mindsdb\/mindsdb\/actions\"><img src=\"https:\/\/github.com\/mindsdb\/mindsdb\/workflows\/MindsDB%20workflow\/badge.svg\" alt=\"MindsDB workflow\"><\/a>\n\n  <a href=\"https:\/\/www.python.org\/downloads\/\" target=\"_blank\"><img src=\"https:\/\/img.shields.io\/badge\/python-3.6%20|%203.7|%203.8-brightgreen.svg\" alt=\"Python supported\"><\/a>\n   <a href=\"https:\/\/pypi.org\/project\/MindsDB\/\" target=\"_blank\"><img src=\"https:\/\/badge.fury.io\/py\/MindsDB.svg\" alt=\"PyPi Version\"><\/a>\n<img alt=\"PyPI - Downloads\" src=\"https:\/\/img.shields.io\/pypi\/dm\/Mindsdb\">  <a href=\"https:\/\/hub.docker.com\/u\/mindsdb\" target=\"_blank\"><img src=\"https:\/\/img.shields.io\/docker\/pulls\/mindsdb\/mindsdb\" alt=\"Docker pulls\"><\/a>\n  <a href=\"https:\/\/www.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/website?url=https%3A%2F%2Fwww.mindsdb.com%2F\" alt=\"MindsDB Website\"><\/a>\t\n    <a href=\"https:\/\/join.slack.com\/t\/mindsdbcommunity\/shared_invite\/zt-o8mrmx3l-5ai~5H66s6wlxFfBMVI6wQ\" target=\"_blank\"><img src=\"https:\/\/img.shields.io\/badge\/slack-@mindsdbcommunity-brightgreen.svg?logo=slack \" alt=\"MindsDB Community\"><\/a>\n\t<\/br>\n\t<a href=\"https:\/\/deepnote.com\/project\/Machine-Learning-With-SQL-8GDF7bc7SzKlhBLorqoIcw\/%2Fmindsdb_demo.ipynb\" target=\"_blank\"><img src=\"https:\/\/deepnote.com\/buttons\/launch-in-deepnote-white.svg\" alt=\"Launch in Deepnote\"><\/a>\n\t<\/br>\n\t\n  <h3 align=\"center\">\n    <a href=\"https:\/\/www.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Website<\/a>\n    <span> | <\/span>\n    <a href=\"https:\/\/docs.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Docs<\/a>\n    <span> | <\/span>\n    <a href=\"https:\/\/join.slack.com\/t\/mindsdbcommunity\/shared_invite\/zt-o8mrmx3l-5ai~5H66s6wlxFfBMVI6wQ\">Community Slack<\/a>\n    <span> | <\/span>\n    <a href=\"https:\/\/github.com\/mindsdb\/mindsdb\/projects\">Contribute<\/a>\n    <span> | <\/span>\n    <a href=\"https:\/\/cloud.mindsdb.com\">Demo<\/a>\n  <\/h3>\n  \n<\/div>\n\n----------------------------------------\n\n[MindsDB](https:\/\/mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo)  ML-SQL Server enables machine learning workflows for the most powerful databases and datawarehouses using SQL.  [![Tweet](https:\/\/img.shields.io\/twitter\/url\/http\/shields.io.svg?style=social)](https:\/\/twitter.com\/intent\/tweet?text=Machine%20Learning%20inside%20Databases%20&url=https:\/\/www.mindsdb.com&via=mindsdb&hashtags=ai,ml,machine_learning,neural_networks,databases,sql)\n* Developers can quickly add AI capabilities to your applications.\n* Data Scientists can streamline MLOps by deploying ML models as AI Tables.\n* Data Analysts can easily make forecasts on complex data (like multivariate time-series with high cardinality) and visualize them in BI tools like Tableau.\n\nIf you like our project then we would really appreciate **a Star \u2b50!**\n\nAlso, check-out the [rewards and community programs.](https:\/\/mindsdb.com\/community?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo)\n\n----------------------------------------\n\n[Installation](https:\/\/github.com\/mindsdb\/mindsdb#installation) - [Overview](https:\/\/github.com\/mindsdb\/mindsdb#overview) - [Features](https:\/\/github.com\/mindsdb\/mindsdb#features) - [Database Integrations](https:\/\/github.com\/mindsdb\/mindsdb#database-integrations) - [Quickstart](https:\/\/github.com\/mindsdb\/mindsdb#quickstart) - [Documentation](https:\/\/github.com\/mindsdb\/mindsdb#documentation) - [Support](https:\/\/github.com\/mindsdb\/mindsdb#support) - [Contributing](https:\/\/github.com\/mindsdb\/mindsdb#contribution) - [Mailing lists](https:\/\/github.com\/mindsdb\/mindsdb#mailing-lists) - [License](https:\/\/github.com\/mindsdb\/mindsdb#license)\n\n----------------------------\n\n<h2 align=\"center\">\n   Machine Learning using SQL\n   <br\/>\n   <br\/>\n  <img width=\"600\" src=\"https:\/\/docs.mindsdb.com\/assets\/mdb_image.png\" alt=\"MindsDB\">\t\n\n<\/h2>\n\n## Demo\n\nYou can try Mindsdb ML SQL server here [(demo)](https:\/\/cloud.mindsdb.com).\n\n## Installation\n\nTo install the latest version of MindsDB please pull the following Docker image:\n\n```\ndocker pull mindsdb\/mindsdb\n```\n\nOr, use PyPI:\n\n```\npip install mindsdb\n```\n\n## Overview\n\nMindsDB automates and abstracts machine learning models through virtual AI Tables:\n\nApart from abstracting ML models as AI Tables inside databases, MindsDB has a set of unique capabilities as:\n\n* Easily make predictions over very complex multivariate time-series data with high cardinality\n\n* An open JSON-AI syntax to tune ML models and optimize ML pipelines in a declarative way\n\n\n#### How it works:\n\n1. Let MindsDB connect to your database.\n\n2. Train a Predictor using a single SQL statement (make MindsDB learn from historical data automatically) or import your own ML model to a Predictor via JSON-AI . \n\n3. Make predictions with SQL statements (Predictor is exposed as virtual AI Tables). There\u2019s no need to deploy models since they are already part of the data layer.\n\nCheck our [docs](https:\/\/docs.mindsdb.com\/?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) and [blog](https:\/\/mindsdb.com\/blog\/?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) for tutorials and use case examples.\n\n\n## Features\n\n* Automatic data pre-processing, feature engineering and encoding\n* Classification, regression, time-series tasks\n* Bring models to production without \u201ctraditional deployment\u201d as AI Tables\n* Get mModels\u2019 accuracy scoring and confidence intervals for each prediction\n* Join ML models with existing data\n* Anomaly detection\n* Model explainability analysis \n* GPU support for models\u2019 training\n* Open JSON-AI syntax to build models and bring your own ML blocks in a declarative way \n* REST API available as well\n\n\n## Database Integrations\n\nMindsDB works with most of the SQL and NoSQL databases and data Streams for real-time ML.\n\n| Connect your Data |\n|-|\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/Apache Kafka-808080?style=for-the-badge&logo=apache-kafka&logoColor=white\" alt=\"Connect Apache Kafka\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/Amazon%20Redshift-0466C8?style=for-the-badge&logo=amazon-aws&logoColor=white\" alt=\"Connect Amazon Redshift\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/Cassandra-1287B1?style=for-the-badge&logo=apache%20cassandra&logoColor=white\" alt=\"Connect Cassandra\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/Clickhouse-e6e600?style=for-the-badge&logo=clickhouse&logoColor=white\" alt=\"Connect Clickhouse\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/CockroachDB-426EDF?style=for-the-badge&logo=cockroach-labs&logoColor=white\" alt=\"Connect CockroachDB\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/MariaDB-003545?style=for-the-badge&logo=mariadb&logoColor=white\" alt=\"Connect MariaDB\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/Microsoft%20SQL%20Sever-CC2927?style=for-the-badge&logo=microsoft%20sql%20server&logoColor=white\" alt=\"Connect SQL Server\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/MongoDB-4EA94B?style=for-the-badge&logo=mongodb&logoColor=white\" alt=\"Connect MongoDB\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/MySQL-00758F?style=for-the-badge&logo=mysql&logoColor=white\" alt=\"Connect MySQL\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white\" alt=\"Connect PostgreSQL\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/QuestDB-d14671?style=for-the-badge&logo=questdb&logoColor=white\" alt=\"Connect QuestDB\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/redis-%23DD0031.svg?&style=for-the-badge&logo=redis&logoColor=white\" alt=\"Connect Redis\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/ScyllaDB-53CADD?style=for-the-badge&logo=scylladbb&logoColor=white\" alt=\"Connect ScyllaDB\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/Singlestore-5f07b4?style=for-the-badge&logo=singlestore&logoColor=white\" alt=\"Connect Singlestore\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/Snowflake-35aedd?style=for-the-badge&logo=snowflake&logoColor=blue\" alt=\"Connect Snowflake\"><\/a> |\n| <a href=\"https:\/\/docs.mindsdb.com\/\"><img src=\"https:\/\/img.shields.io\/badge\/Trino-dd00a1?style=for-the-badge&logo=trino&logoColor=white\" alt=\"Connect Trino\"><\/a> |\n\n[:question: :wave: Missing integration?](https:\/\/github.com\/mindsdb\/mindsdb\/issues\/new?assignees=&labels=&template=feature-mindsdb-request.yaml)\n\n## Quickstart\n\nTo get your hands on MindsDB, we recommend using the [Docker image](https:\/\/docs.mindsdb.com\/deployment\/docker\/?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) or simply sign up for a [free cloud account](https:\/\/cloud.mindsdb.com\/signup?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\nFeel free to browse [documentation](https:\/\/docs.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) for other installation methods and tutorials.\n\n## Documentation\n\nYou can find the complete documentation of MindsDB at [docs.mindsdb.com](https:\/\/docs.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\nDocumentation for our HTTP API can be found at [apidocs.mindsdb.com](https:\/\/apidocs.mindsdb.com\/).\n\n## Support\n\nIf you found a bug, please submit an [issue on Github](https:\/\/github.com\/mindsdb\/mindsdb\/issues).\n\nTo get community support, you can:\n* Post at MindsDB [Slack community](https:\/\/join.slack.com\/t\/mindsdbcommunity\/shared_invite\/zt-o8mrmx3l-5ai~5H66s6wlxFfBMVI6wQ).\n* Ask for help at our [Github Discussions](https:\/\/github.com\/mindsdb\/mindsdb\/discussions).\n* Ask a question at [Stackoverflow](https:\/\/stackoverflow.com\/questions\/tagged\/mindsdb) with a MindsDB tag.\n\nIf you need commercial support, please [contact](https:\/\/mindsdb.com\/contact\/?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) the MindsDB team.\n\n## Contributing\n\nA great place to start contributing to MindsDB will be our GitHub projects for :checkered_flag:\n* Community writers [dashboard tasks](https:\/\/github.com\/mindsdb\/mindsdb\/projects\/7).\n* Community code contributors [dashboard tasks](https:\/\/github.com\/mindsdb\/mindsdb\/projects\/8).\n\nAlso, we are always open to suggestions so feel free to open new issues with your ideas and we can give you guidance!\n\nBeing part of the core team is accessible to anyone who is motivated and wants to be part of that journey!\nIf you'd like to contribute to the project, refer to the [contributing documentation](https:\/\/docs.mindsdb.com\/contribute\/?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\nPlease note that this project is released with a [Contributor Code of Conduct](https:\/\/github.com\/mindsdb\/mindsdb\/blob\/stable\/CODE_OF_CONDUCT.md). By participating in this project, you agree to abide by its terms.\n\n### Current contributors\n\n<a href=\"https:\/\/github.com\/mindsdb\/mindsdb\/graphs\/contributors\">\n  <img src=\"https:\/\/contributors-img.web.app\/image?repo=mindsdb\/mindsdb\" \/>\n<\/a>\n\nMade with [contributors-img](https:\/\/contributors-img.web.app).\n\n## Mailing lists\n\nSubscribe to MindsDB Monthly [Community Newsletter](https:\/\/mindsdb.com\/newsletter\/?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) to get general announcements, release notes, information about MindsDB events, and the latest blog posts.\nYou may also join our [beta-users](https:\/\/mindsdb.com\/beta-tester\/?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) group, and get access to new beta features.\n\n\n## License\n\nMindsDB is licensed under [GNU General Public License v3.0](https:\/\/github.com\/mindsdb\/mindsdb\/blob\/master\/LICENSE)\n","120":"machine_learning_examples\n=========================\n\nA collection of machine learning examples and tutorials.\n\nFind associated tutorials at https:\/\/lazyprogrammer.me\n\nFind associated courses at https:\/\/deeplearningcourses.com\n\nPlease note that not all code from all courses will be found in this repository. Some newer code examples (e.g. most of Tensorflow 2.0) were done in Google Colab. Therefore, you should check the instructions given in the lectures for the course you are taking.\n\n\nHow to I find the code for a particular course?\n===============================================\n\nThe code for each course is separated by folder. You can determine which folder corresponds with which course by watching the \"Where to get the code\" lecture inside the course (usually Lecture 2 or 3).\n\nRemember: one folder = one course.\n\n\nWhy you should not fork this repo\n=================================\n\nI've noticed that many people have out-of-date forks. Thus, I recommend not forking this repository if you take one of my courses. I am constantly updating my courses, and your fork will soon become out-of-date. You should clone the repository instead to make it easy to get updates (i.e. just \"git pull\" randomly and frequently).\n\n\nWhere is the code for your latest courses?\n==========================================\n\nBeginning with Tensorflow 2, I started to use Google Colab. For those courses, unless otherwise noted, the code will be on Google Colab. Links to the notebooks are provided in the course. See the lecture \"Where to get the code\" for further details.\n\n\nVIP Course Links\n===================\n\n*** Note: if any of these coupons becomes out of date, check my website (https:\/\/lazyprogrammer.me) for the latest version. I will probably just keep incrementing them numerically, e.g. FINANCEVIP2, FINANCEVIP3, etc..\n\n**Machine Learning: Natural Language Processing in Python (V2)** (VIP parts)\n\nhttps:\/\/deeplearningcourses.com\/c\/natural-language-processing-in-python\n\n\n**Time Series Analysis, Forecasting, and Machine Learning**\n\nhttps:\/\/www.udemy.com\/course\/time-series-analysis\/?couponCode=TIMEVIP4\n\n\n**Financial Engineering and Artificial Intelligence in Python**\n\nhttps:\/\/www.udemy.com\/course\/ai-finance\/?couponCode=FINANCEVIP13\n\n\n**PyTorch: Deep Learning and Artificial Intelligence**\n\nhttps:\/\/www.udemy.com\/course\/pytorch-deep-learning\/?couponCode=PYTORCHVIP18\n\n\n**Tensorflow 2.0: Deep Learning and Artificial Intelligence** (VIP Version)\nhttps:\/\/deeplearningcourses.com\/c\/deep-learning-tensorflow-2\n\n\n\nDeep Learning Courses Exclusives\n================================\n\nClassical Statistical Inference and A\/B Testing in Python\nhttps:\/\/deeplearningcourses.com\/c\/statistical-inference-in-python\n\nLinear Programming for Linear Regression in Python\nhttps:\/\/deeplearningcourses.com\/c\/linear-programming-python\n\nMATLAB for Students, Engineers, and Professionals in STEM\nhttps:\/\/deeplearningcourses.com\/c\/matlab\n\n\n\nOther Course Links\n==================\n\nMachine Learning: Natural Language Processing in Python (V2)\nhttps:\/\/bit.ly\/3idcaE5\n\nTensorflow 2.0: Deep Learning and Artificial Intelligence (non-VIP version)\nhttps:\/\/www.udemy.com\/course\/deep-learning-tensorflow-2\/?referralCode=E10B72D3848AB70FE1B8\n\nCutting-Edge AI: Deep Reinforcement Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/cutting-edge-artificial-intelligence\n\nRecommender Systems and Deep Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/recommender-systems\n\nMachine Learning and AI: Support Vector Machines in Python\nhttps:\/\/deeplearningcourses.com\/c\/support-vector-machines-in-python\n\nDeep Learning: Advanced Computer Vision\nhttps:\/\/deeplearningcourses.com\/c\/advanced-computer-vision\n\nDeep Learning: Advanced NLP and RNNs\nhttps:\/\/deeplearningcourses.com\/c\/deep-learning-advanced-nlp\n\nDeep Learning: GANs and Variational Autoencoders\nhttps:\/\/deeplearningcourses.com\/c\/deep-learning-gans-and-variational-autoencoders\n\nAdvanced AI: Deep Reinforcement Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/deep-reinforcement-learning-in-python\n\nArtificial Intelligence: Reinforcement Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/artificial-intelligence-reinforcement-learning-in-python\n\nNatural Language Processing with Deep Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/natural-language-processing-with-deep-learning-in-python\n\nDeep Learning: Recurrent Neural Networks in Python\nhttps:\/\/deeplearningcourses.com\/c\/deep-learning-recurrent-neural-networks-in-python\n\nUnsupervised Machine Learning: Hidden Markov Models in Python\nhttps:\/\/deeplearningcourses.com\/c\/unsupervised-machine-learning-hidden-markov-models-in-python\n\nDeep Learning Prerequisites: The Numpy Stack in Python\nhttps:\/\/deeplearningcourses.com\/c\/deep-learning-prerequisites-the-numpy-stack-in-python\n\nDeep Learning Prerequisites: Linear Regression in Python\nhttps:\/\/deeplearningcourses.com\/c\/data-science-linear-regression-in-python\n\nDeep Learning Prerequisites: Logistic Regression in Python\nhttps:\/\/deeplearningcourses.com\/c\/data-science-logistic-regression-in-python\n\nData Science: Deep Learning and Neural Networks in Python\nhttps:\/\/deeplearningcourses.com\/c\/data-science-deep-learning-in-python\n\nCluster Analysis and Unsupervised Machine Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/cluster-analysis-unsupervised-machine-learning-python\n\nData Science: Supervised Machine Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/data-science-supervised-machine-learning-in-python\n\nBayesian Machine Learning in Python: A\/B Testing\nhttps:\/\/deeplearningcourses.com\/c\/bayesian-machine-learning-in-python-ab-testing\n\nData Science: Natural Language Processing in Python\nhttps:\/\/deeplearningcourses.com\/c\/data-science-natural-language-processing-in-python\n\nModern Deep Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/data-science-deep-learning-in-theano-tensorflow\n\nEnsemble Machine Learning in Python: Random Forest and AdaBoost\nhttps:\/\/deeplearningcourses.com\/c\/machine-learning-in-python-random-forest-adaboost\n\nDeep Learning: Convolutional Neural Networks in Python\nhttps:\/\/deeplearningcourses.com\/c\/deep-learning-convolutional-neural-networks-theano-tensorflow\n\nUnsupervised Deep Learning in Python\nhttps:\/\/deeplearningcourses.com\/c\/unsupervised-deep-learning-in-python\n","121":".. image:: https:\/\/cdn.rawgit.com\/pymc-devs\/pymc\/main\/docs\/logos\/svg\/PyMC_banner.svg\n    :height: 100px\n    :alt: PyMC logo\n    :align: center\n\n|Build Status| |Coverage| |NumFOCUS_badge| |Binder| |Dockerhub| |DOIzenodo|\n\nPyMC (formerly PyMC3) is a Python package for Bayesian statistical modeling\nfocusing on advanced Markov chain Monte Carlo (MCMC) and variational inference (VI)\nalgorithms. Its flexibility and extensibility make it applicable to a\nlarge suite of problems.\n\nCheck out the `PyMC overview <https:\/\/docs.pymc.io\/en\/latest\/learn\/core_notebooks\/pymc_overview.html>`__,  or\none of `the many examples <https:\/\/docs.pymc.io\/projects\/examples\/en\/latest\/>`__!\nFor questions on PyMC, head on over to our `PyMC Discourse <https:\/\/discourse.pymc.io\/>`__ forum.\n\nFeatures\n========\n\n-  Intuitive model specification syntax, for example, ``x ~ N(0,1)``\n   translates to ``x = Normal('x',0,1)``\n-  **Powerful sampling algorithms**, such as the `No U-Turn\n   Sampler <http:\/\/www.jmlr.org\/papers\/v15\/hoffman14a.html>`__, allow complex models\n   with thousands of parameters with little specialized knowledge of\n   fitting algorithms.\n-  **Variational inference**: `ADVI <http:\/\/www.jmlr.org\/papers\/v18\/16-107.html>`__\n   for fast approximate posterior estimation as well as mini-batch ADVI\n   for large data sets.\n-  Relies on `Aesara <https:\/\/aesara.readthedocs.io\/en\/latest\/>`__ which provides:\n    *  Computation optimization and dynamic C or JAX compilation\n    *  NumPy broadcasting and advanced indexing\n    *  Linear algebra operators\n    *  Simple extensibility\n-  Transparent support for missing value imputation\n\nGetting started\n===============\n\nIf you already know about Bayesian statistics:\n----------------------------------------------\n\n-  `API quickstart guide <https:\/\/docs.pymc.io\/en\/stable\/pymc-examples\/examples\/pymc3_howto\/api_quickstart.html>`__\n-  The `PyMC tutorial <https:\/\/docs.pymc.io\/en\/latest\/learn\/core_notebooks\/pymc_overview.html>`__\n-  `PyMC examples <https:\/\/docs.pymc.io\/projects\/examples\/en\/latest\/>`__ and the `API reference <https:\/\/docs.pymc.io\/en\/stable\/api.html>`__\n\nLearn Bayesian statistics with a book together with PyMC\n--------------------------------------------------------\n\n-  `Probabilistic Programming and Bayesian Methods for Hackers <https:\/\/github.com\/CamDavidsonPilon\/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers>`__: Fantastic book with many applied code examples.\n-  `PyMC port of the book \"Doing Bayesian Data Analysis\" by John Kruschke <https:\/\/github.com\/aloctavodia\/Doing_bayesian_data_analysis>`__ as well as the `second edition <https:\/\/github.com\/JWarmenhoven\/DBDA-python>`__: Principled introduction to Bayesian data analysis.\n-  `PyMC port of the book \"Statistical Rethinking A Bayesian Course with Examples in R and Stan\" by Richard McElreath <https:\/\/github.com\/pymc-devs\/resources\/tree\/master\/Rethinking>`__\n-  `PyMC port of the book \"Bayesian Cognitive Modeling\" by Michael Lee and EJ Wagenmakers <https:\/\/github.com\/pymc-devs\/resources\/tree\/master\/BCM>`__: Focused on using Bayesian statistics in cognitive modeling.\n-  `Bayesian Analysis with Python  <https:\/\/www.packtpub.com\/big-data-and-business-intelligence\/bayesian-analysis-python-second-edition>`__ (second edition) by Osvaldo Martin: Great introductory book. (`code <https:\/\/github.com\/aloctavodia\/BAP>`__ and errata).\n\nAudio & Video\n-------------\n\n- Here is a `YouTube playlist <https:\/\/www.youtube.com\/playlist?list=PL1Ma_1DBbE82OVW8Fz_6Ts1oOeyOAiovy>`__ gathering several talks on PyMC.\n- You can also find all the talks given at **PyMCon 2020** `here <https:\/\/discourse.pymc.io\/c\/pymcon\/2020talks\/15>`__.\n- The `\"Learning Bayesian Statistics\" podcast <https:\/\/www.learnbayesstats.com\/>`__ helps you discover and stay up-to-date with the vast Bayesian community. Bonus: it's hosted by Alex Andorra, one of the PyMC core devs!\n\nInstallation\n============\n\nTo install PyMC on your system, follow the instructions on the appropriate installation guide:\n\n-  `Installing PyMC on MacOS <https:\/\/github.com\/pymc-devs\/pymc\/wiki\/Installation-Guide-(MacOS)>`__\n-  `Installing PyMC on Linux <https:\/\/github.com\/pymc-devs\/pymc\/wiki\/Installation-Guide-(Linux)>`__\n-  `Installing PyMC on Windows <https:\/\/github.com\/pymc-devs\/pymc\/wiki\/Installation-Guide-(Windows)>`__\n\n\nCiting PyMC\n===========\nPlease choose from the following:\n\n- |DOIpaper| *Probabilistic programming in Python using PyMC3*, Salvatier J., Wiecki T.V., Fonnesbeck C. (2016)\n- |DOIzenodo| A DOI for all versions.\n- DOIs for specific versions are shown on Zenodo and under `Releases <https:\/\/github.com\/pymc-devs\/pymc\/releases>`_\n\n.. |DOIpaper| image:: https:\/\/img.shields.io\/badge\/DOI-10.7717%2Fpeerj--cs.55-blue\n     :target: https:\/\/doi.org\/10.7717\/peerj-cs.55\n.. |DOIzenodo| image:: https:\/\/zenodo.org\/badge\/DOI\/10.5281\/zenodo.4603970.svg\n   :target: https:\/\/doi.org\/10.5281\/zenodo.4603970\n\nContact\n=======\n\nWe are using `discourse.pymc.io <https:\/\/discourse.pymc.io\/>`__ as our main communication channel. You can also follow us on `Twitter @pymc_devs <https:\/\/twitter.com\/pymc_devs>`__ for updates and other announcements.\n\nTo ask a question regarding modeling or usage of PyMC we encourage posting to our Discourse forum under the `\u201cQuestions\u201d Category <https:\/\/discourse.pymc.io\/c\/questions>`__. You can also suggest feature in the `\u201cDevelopment\u201d Category <https:\/\/discourse.pymc.io\/c\/development>`__.\n\nTo report an issue with PyMC please use the `issue tracker <https:\/\/github.com\/pymc-devs\/pymc\/issues>`__.\n\nFinally, if you need to get in touch for non-technical information about the project, `send us an e-mail <pymc.devs@gmail.com>`__.\n\nLicense\n=======\n\n`Apache License, Version\n2.0 <https:\/\/github.com\/pymc-devs\/pymc\/blob\/main\/LICENSE>`__\n\n\nSoftware using PyMC\n===================\n\nGeneral purpose\n---------------\n\n- `Bambi <https:\/\/github.com\/bambinos\/bambi>`__: BAyesian Model-Building Interface (BAMBI) in Python.\n- `SunODE <https:\/\/github.com\/aseyboldt\/sunode>`__: Fast ODE solver, much faster than the one that comes with PyMC.\n- `pymc-learn <https:\/\/github.com\/pymc-learn\/pymc-learn>`__: Custom PyMC models built on top of pymc3_models\/scikit-learn API\n- `fenics-pymc3 <https:\/\/github.com\/IvanYashchuk\/fenics-pymc3>`__: Differentiable interface to FEniCS, a library for solving partial differential equations.\n\nDomain specific\n---------------\n\n- `Exoplanet <https:\/\/github.com\/dfm\/exoplanet>`__: a toolkit for modeling of transit and\/or radial velocity observations of exoplanets and other astronomical time series.\n- `NiPyMC <https:\/\/github.com\/PsychoinformaticsLab\/nipymc>`__: Bayesian mixed-effects modeling of fMRI data in Python.\n- `beat <https:\/\/github.com\/hvasbath\/beat>`__: Bayesian Earthquake Analysis Tool.\n- `cell2location <https:\/\/github.com\/BayraktarLab\/cell2location>`__: Comprehensive mapping of tissue cell architecture via integrated single cell and spatial transcriptomics.\n\nPlease contact us if your software is not listed here.\n\nPapers citing PyMC\n==================\n\nSee `Google Scholar <https:\/\/scholar.google.de\/scholar?oi=bibs&hl=en&authuser=1&cites=6936955228135731011>`__ for a continuously updated list.\n\nContributors\n============\n\nSee the `GitHub contributor\npage <https:\/\/github.com\/pymc-devs\/pymc\/graphs\/contributors>`__. Also read our `Code of Conduct <https:\/\/github.com\/pymc-devs\/pymc\/blob\/main\/CODE_OF_CONDUCT.md>`__ guidelines for a better contributing experience.\n\nSupport\n=======\n\nPyMC is a non-profit project under NumFOCUS umbrella. If you want to support PyMC financially, you can donate `here <https:\/\/numfocus.salsalabs.org\/donate-to-pymc3\/index.html>`__.\n\nProfessional Consulting Support\n===============================\n\nYou can get professional consulting support from `PyMC Labs <https:\/\/www.pymc-labs.io>`__.\n\nSponsors\n========\n\n|NumFOCUS|\n\n|PyMCLabs|\n\n.. |Binder| image:: https:\/\/mybinder.org\/badge_logo.svg\n   :target: https:\/\/mybinder.org\/v2\/gh\/pymc-devs\/pymc\/main?filepath=%2Fdocs%2Fsource%2Fnotebooks\n.. |Build Status| image:: https:\/\/github.com\/pymc-devs\/pymc\/workflows\/pytest\/badge.svg\n   :target: https:\/\/github.com\/pymc-devs\/pymc\/actions\n.. |Coverage| image:: https:\/\/codecov.io\/gh\/pymc-devs\/pymc\/branch\/main\/graph\/badge.svg\n   :target: https:\/\/codecov.io\/gh\/pymc-devs\/pymc\n.. |Dockerhub| image:: https:\/\/img.shields.io\/docker\/automated\/pymc\/pymc.svg\n   :target: https:\/\/hub.docker.com\/r\/pymc\/pymc\n.. |NumFOCUS| image:: https:\/\/www.numfocus.org\/wp-content\/uploads\/2017\/03\/1457562110.png\n   :target: http:\/\/www.numfocus.org\/\n.. |NumFOCUS_badge| image:: https:\/\/img.shields.io\/badge\/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n   :target: http:\/\/www.numfocus.org\/\n.. |PyMCLabs| image:: https:\/\/raw.githubusercontent.com\/pymc-devs\/pymc\/main\/docs\/logos\/sponsors\/pymc-labs.png\n   :target: https:\/\/pymc-labs.io\n","122":"# auto-sklearn\n\n**auto-sklearn** is an automated machine learning toolkit and a drop-in replacement for a [scikit-learn](https:\/\/scikit-learn.org) estimator.\n\nFind the documentation **[here](https:\/\/automl.github.io\/auto-sklearn\/)**. Quick links:\n  * [Installation Guide](https:\/\/automl.github.io\/auto-sklearn\/master\/installation.html)\n  * [Releases](https:\/\/automl.github.io\/auto-sklearn\/master\/releases.html)\n  * [Manual](https:\/\/automl.github.io\/auto-sklearn\/master\/manual.html)\n  * [Examples](https:\/\/automl.github.io\/auto-sklearn\/master\/examples\/index.html)\n  * [API](https:\/\/automl.github.io\/auto-sklearn\/master\/api.html)\n\n## auto-sklearn in one image\n\n![image](doc\/images\/askl_pipeline.png)\n\n## auto-sklearn in four lines of code\n\n```python\nimport autosklearn.classification\ncls = autosklearn.classification.AutoSklearnClassifier()\ncls.fit(X_train, y_train)\npredictions = cls.predict(X_test)\n```\n\n## Relevant publications\n\nIf you use auto-sklearn in scientific publications, we would appreciate citations.\n\n**Efficient and Robust Automated Machine Learning**  \n*Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum and Frank Hutter*  \nAdvances in Neural Information Processing Systems 28 (2015)  \n\n[Link](https:\/\/papers.neurips.cc\/paper\/5872-efficient-and-robust-automated-machine-learning.pdf) to publication.\n```\n@inproceedings{feurer-neurips15a,\n    title     = {Efficient and Robust Automated Machine Learning},\n    author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina  Springenberg, Jost and Blum, Manuel and Hutter, Frank},\n    booktitle = {Advances in Neural Information Processing Systems 28 (2015)},\n    pages     = {2962--2970},\n    year      = {2015}\n}\n```\n\n----------------------------------------\n\n**Auto-Sklearn 2.0: The Next Generation**  \n*Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer and Frank Hutter**  \narXiv:2007.04074 [cs.LG], 2020\n\n[Link](https:\/\/arxiv.org\/abs\/2007.04074) to publication.\n```\n@article{feurer-arxiv20a,\n    title     = {Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning},\n    author    = {Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},\n    booktitle = {arXiv:2007.04074 [cs.LG]},\n    year      = {2020}\n}\n```\n\n----------------------------------------\n\nAlso, have a look at the blog on [automl.org](https:\/\/automl.org) where we regularly release blogposts.\n","123":"<div align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/doccano\/doccano\/master\/docs\/images\/logo\/doccano.png\">\n<\/div>\n\n# doccano\n\n[![Codacy Badge](https:\/\/app.codacy.com\/project\/badge\/Grade\/35ac8625a2bc4eddbff23dbc61bc6abb)](https:\/\/www.codacy.com\/gh\/doccano\/doccano\/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=doccano\/doccano&amp;utm_campaign=Badge_Grade)\n[![doccano CI](https:\/\/github.com\/doccano\/doccano\/actions\/workflows\/ci.yml\/badge.svg)](https:\/\/github.com\/doccano\/doccano\/actions\/workflows\/ci.yml)\n\ndoccano is an open source text annotation tool for humans. It provides annotation features for text classification, sequence labeling and sequence to sequence tasks. So, you can create labeled data for sentiment analysis, named entity recognition, text summarization and so on. Just create a project, upload data and start annotating. You can build a dataset in hours.\n\n## Demo\n\nYou can try the [annotation demo](http:\/\/doccano.herokuapp.com).\n\n![Demo image](https:\/\/raw.githubusercontent.com\/doccano\/doccano\/master\/docs\/images\/demo\/demo.gif)\n\n## Documentation\n\nRead the documentation at the <https:\/\/doccano.github.io\/doccano\/>.\n\n## Features\n\n- Collaborative annotation\n- Multi-language support\n- Mobile support\n- Emoji :smile: support\n- Dark theme\n- RESTful API\n\n## Usage\n\nThree options to run doccano:\n\n- pip (Python 3.8+)\n- Docker\n- Docker Compose\n\n### pip\n\nTo install doccano, simply run:\n\n```bash\npip install doccano\n```\n\nBy default, SQLite 3 is used for the default database. If you want to use PostgreSQL, install the additional dependencies:\n\n```bash\npip install 'doccano[postgresql]'\n```\nand set `DATABASE_URL` environment variable according to your PostgreSQL credentials:\n```bash\nDATABASE_URL=\"postgres:\/\/${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}\/${POSTGRES_DB}?sslmode=disable\"\n```\n\nAfter installation, run the following commands:\n\n```bash\n# Initialize database.\ndoccano init\n# Create a super user.\ndoccano createuser --username admin --password pass\n# Start a web server.\ndoccano webserver --port 8000\n```\n\nIn another terminal, run the following command:\n\n```bash\n# Start the task queue to handle file upload\/download.\ndoccano task\n```\n\nGo to <http:\/\/127.0.0.1:8000\/>.\n\n### Docker\n\nAs a one-time setup, create a Docker container as follows:\n\n```bash\ndocker pull doccano\/doccano\ndocker container create --name doccano \\\n  -e \"ADMIN_USERNAME=admin\" \\\n  -e \"ADMIN_EMAIL=admin@example.com\" \\\n  -e \"ADMIN_PASSWORD=password\" \\\n  -v doccano-db:\/data \\\n  -p 8000:8000 doccano\/doccano\n```\n\nNext, start doccano by running the container:\n\n```bash\ndocker container start doccano\n```\n\nGo to <http:\/\/127.0.0.1:8000\/>.\n\nTo stop the container, run `docker container stop doccano -t 5`.\nAll data created in the container will persist across restarts.\n\n### Docker Compose\n\nYou need to install Git and to clone the repository:\n\n```bash\ngit clone https:\/\/github.com\/doccano\/doccano.git\ncd doccano\n```\n\n_Note for Windows developers:_ Be sure to configure git to correctly handle line endings or you may encounter `status code 127` errors while running the services in future steps. Running with the git config options below will ensure your git directory correctly handles line endings.\n\n```bash\ngit clone https:\/\/github.com\/doccano\/doccano.git --config core.autocrlf=input\n```\n\nThen, create an `.env` file with variables in the following format (see [.\/docker\/.env.example](https:\/\/github.com\/doccano\/doccano\/blob\/master\/docker\/.env.example)):\n\n```plain\n# platform settings\nADMIN_USERNAME=admin\nADMIN_PASSWORD=password\nADMIN_EMAIL=admin@example.com\n\n# rabbit mq settings\nRABBITMQ_DEFAULT_USER=doccano\nRABBITMQ_DEFAULT_PASS=doccano\n\n# database settings\nPOSTGRES_USER=doccano\nPOSTGRES_PASSWORD=doccano\nPOSTGRES_DB=doccano\n```\n\nAfter running the following command, access <http:\/\/127.0.0.1\/>.\n\n```bash\ndocker-compose -f docker\/docker-compose.prod.yml --env-file .env up\n```\n\n### One-click Deployment\n\n| Service | Button |\n|---------|---|\n| AWS[^1]   | [![AWS CloudFormation Launch Stack SVG Button](https:\/\/cdn.rawgit.com\/buildkite\/cloudformation-launch-stack-button-svg\/master\/launch-stack.svg)](https:\/\/console.aws.amazon.com\/cloudformation\/home?#\/stacks\/new?stackName=doccano&templateURL=https:\/\/doccano.s3.amazonaws.com\/public\/cloudformation\/template.aws.yaml)  |\n| Heroku  | [![Deploy](https:\/\/www.herokucdn.com\/deploy\/button.svg)](https:\/\/dashboard.heroku.com\/new?template=https%3A%2F%2Fgithub.com%2Fdoccano%2Fdoccano)  |\n<!-- | GCP[^2] | [![GCP Cloud Run PNG Button](https:\/\/storage.googleapis.com\/gweb-cloudblog-publish\/images\/run_on_google_cloud.max-300x300.png)](https:\/\/console.cloud.google.com\/cloudshell\/editor?shellonly=true&cloudshell_image=gcr.io\/cloudrun\/button&cloudshell_git_repo=https:\/\/github.com\/doccano\/doccano.git&cloudshell_git_branch=CloudRunButton)  | -->\n\n> [^1]: (1) EC2 KeyPair cannot be created automatically, so make sure you have an existing EC2 KeyPair in one region. Or [create one yourself](https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ec2-key-pairs.html#having-ec2-create-your-key-pair). (2) If you want to access doccano via HTTPS in AWS, here is an [instruction](https:\/\/github.com\/doccano\/doccano\/wiki\/HTTPS-setting-for-doccano-in-AWS).\n<!-- > [^2]: Although this is a very cheap option, it is only suitable for very small teams (up to 80 concurrent requests). Read more on [Cloud Run docs](https:\/\/cloud.google.com\/run\/docs\/concepts). -->\n\n## FAQ\n\n- [How to create a user](https:\/\/doccano.github.io\/doccano\/faq\/#how-to-create-a-user)\n- [How to add a user to your project](https:\/\/doccano.github.io\/doccano\/faq\/#how-to-add-a-user-to-your-project)\n- [How to change the password](https:\/\/doccano.github.io\/doccano\/faq\/#how-to-change-the-password)\n\nSee the [documentation](https:\/\/doccano.github.io\/doccano\/) for details.\n\n## Contribution\n\nAs with any software, doccano is under continuous development. If you have requests for features, please file an issue describing your request. Also, if you want to see work towards a specific feature, feel free to contribute by working towards it. The standard procedure is to fork the repository, add a feature, fix a bug, then file a pull request that your changes are to be merged into the main repository and included in the next release.\n\nHere are some tips might be helpful. [How to Contribute to Doccano Project](https:\/\/github.com\/doccano\/doccano\/wiki\/How-to-Contribute-to-Doccano-Project)\n\n## Citation\n\n```tex\n@misc{doccano,\n  title={{doccano}: Text Annotation Tool for Human},\n  url={https:\/\/github.com\/doccano\/doccano},\n  note={Software available from https:\/\/github.com\/doccano\/doccano},\n  author={\n    Hiroki Nakayama and\n    Takahiro Kubo and\n    Junya Kamura and\n    Yasufumi Taniguchi and\n    Xu Liang},\n  year={2018},\n}\n```\n\n## Contact\n\nFor help and feedback, please feel free to contact [the author](https:\/\/github.com\/Hironsan).\n","124":"# Machine-Learning\n* [\u4e2d\u6587\uff08\u7b80\u4f53\uff09](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/README.md \"\u60ac\u505c\u663e\u793a\")<br>\n\n* Rome was not built in a day<br>\n\n* [Blog](http:\/\/blog.csdn.net\/c406495762 \"\u60ac\u505c\u663e\u793a\")<br>\n\n* Machine-Learning in Practice (Detailed Comments + Training Datasets), Keep updating!<br>\n\n* Welcome to my[[CSDN Column](http:\/\/blog.csdn.net\/column\/details\/16415.html \"\u60ac\u505c\u663e\u793a\")]<br>\n\n* Follow me on[[Zhihu Column](https:\/\/zhuanlan.zhihu.com\/ml-jack \"\u60ac\u505c\u663e\u793a\")]<br>\n\n* QQ Group for Communication[328127489]<a target=\"_blank\" href=\"\/\/shang.qq.com\/wpa\/qunwpa?idkey=e70f3fcff3761450fda9b43eadc1910dac308a962ef9e3e87941cd2c681c4bb4\"><img border=\"0\" src=\"https:\/\/github.com\/Jack-Cherish\/Pictures\/blob\/master\/qqgroup.png\" alt=\"Coder\" title=\"Coder\"><\/a><br>\n\n* My Website: http:\/\/cuijiahua.com\/\n\n### Article Debut\n\n*  Debut articles on my website and forward on orther platforms, click here to get the latest development: http:\/\/cuijiahua.com\/\n\n## Chapt. 2: K-Nearest Neighbors Algorithm\n\n|   Article   |  Personal Website  |    CSDN    |    Zhihu    |\n| :------  | :--------: | :--------: | :--------: |\n| Python3 <Machine-Learning in Practice> study notes (I): K-Nearest Neighbors Algorithm (Gorgeous and Splendid Tutorial) | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_1_knn.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/75172850 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/28656126 \"\u60ac\u505c\u663e\u793a\") |\n\n\n### Code\n\n* [1. Entry Level k-NN](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/tree\/master\/kNN\/1.%E7%AE%80%E5%8D%95k-NN \"\u60ac\u505c\u663e\u793a\")\n\n* [2. Miss Helen Dating History](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/tree\/master\/kNN\/2.%E6%B5%B7%E4%BC%A6%E7%BA%A6%E4%BC%9A \"\u60ac\u505c\u663e\u793a\")\n\n* [3. Digits Recognition](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/tree\/master\/kNN\/3.%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB \"\u60ac\u505c\u663e\u793a\")\n\n## Chapt. 3: Decision Tree\n\n|   Article   |  Personal Website  |    CSDN    |    Zhihu    |\n| :------  | :--------: | :--------: | :--------: |\n| Python3 <Machine-Learning in Practice> study notes (II): Decision Tree (Basic Concepts): Let's Start from Dating | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_2_decision_tree_1.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/75663451 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/28688281 \"\u60ac\u505c\u663e\u793a\") |\n| Python3 <Machine-Learning in Practice> study notes (III): Decision Tree (In Practice): I'm Looking for a Pair of Contact Lenses | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_3_decision_tree_2.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/76262487 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/28714382 \"\u60ac\u505c\u663e\u793a\") |\n\n### Code\n  \n* [1. Loan Prediction](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Decision%20Tree\/Decision%20Tree.py \"\u60ac\u505c\u663e\u793a\")\n\n* [2. Contact Lenses](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Decision%20Tree\/Sklearn-Decision%20Tree.py \"\u60ac\u505c\u663e\u793a\")\n\n##  Chapt. 4: Navie Bayes\n\n|   Article   |  Personal Website  |    CSDN    |    Zhihu    |\n| :------  | :--------: | :--------: | :--------: |\n| Python3 Python3 <Machine-Learning in Practice> study notes (IV): Navie Bayes (Basic Concepts): Comment Filter| [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_4_bayes_1.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/77341116 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/28719332 \"\u60ac\u505c\u663e\u793a\") |\n| Python3 Python3 <Machine-Learning in Practice> study notes (V): Navie Bayes (In Practice): Catalogues of Sina News | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_5_bayes_2.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/77500679 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/28720393 \"\u60ac\u505c\u663e\u793a\") |\n\n### Code\n  \n* [1. Comment Filter](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Naive%20Bayes\/bayes.py \"\u60ac\u505c\u663e\u793a\")\n\n* [2. Spam Filter](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Naive%20Bayes\/bayes-modify.py \"\u60ac\u505c\u663e\u793a\")\n\n* [3. News Catalogues](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Naive%20Bayes\/nbc.py \"\u60ac\u505c\u663e\u793a\")\n  \n## Chapt. 5: Logistic Regression\n\n|   Article   |  Personal Website  |    CSDN    |    Zhihu    |\n| :------  | :--------: | :--------: | :--------: |\n| Python3 <Machine-Learning in Practice> study notes (VI): Logistic Regression (Basic Concepts): Gradient Ascent Algorithm | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_6_logistic_1.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/77723333 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/28922957 \"\u60ac\u505c\u663e\u793a\") |\n| Python3 <Machine-Learning in Practice> study notes (VII): Logistic Regression (In Practice): Prediction of Horse Mortality | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_7_logistic_2.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/77851973 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/29073560 \"\u60ac\u505c\u663e\u793a\") |\n\n### Code\n\n* [1. Entry Level Exercise for Logistic Regression](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Logistic\/LogRegres.py \"\u60ac\u505c\u663e\u793a\")\n\n* [2. Improved Random Gradient Ascent Algorithm](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Logistic\/LogRegres-gj.py \"\u60ac\u505c\u663e\u793a\")\n\n* [3. Prediction of Horse Mortality](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Logistic\/colicLogRegres.py \"\u60ac\u505c\u663e\u793a\")\n\n## Chapt. 6: SVM (Support Vector Machine)\n\n|   Article   |  Personal Website  |    CSDN    |    Zhihu    |\n| :------  | :--------: | :--------: | :--------: |\n| Python3 <Machine-Learning in Practice> study notes (VIII): SVM (Basic Concepts): a Handcraft on Linear SVM | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_8_svm_1.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/78072313 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/29604517 \"\u60ac\u505c\u663e\u793a\") |\n| Python3 <Machine-Learning in Practice> study notes (IX): SVM (In Practice): Another Handcraft on Nonlinear SVM | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_9_svm_2.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/78158354 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/29872905 \"\u60ac\u505c\u663e\u793a\") |\n\n### Code\n\n* [1. Simplified SMO Alogrithm](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/SVM\/svm-simple.py \"\u60ac\u505c\u663e\u793a\")\n\n* [2. Complete SMO Alogrithm](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/SVM\/svm-smo.py \"\u60ac\u505c\u663e\u793a\")\n\n* [3. Nonlinear SVM Alogrithm](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/SVM\/svmMLiA.py \"\u60ac\u505c\u663e\u793a\")\n\n* [4. Sklearn SVC](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/SVM\/svm-svc.py \"\u60ac\u505c\u663e\u793a\")\n\n## Chapt. 7: AdaBoost\n\n|   Article   |  Personal Website  |    CSDN    |    Zhihu    |\n| :------  | :--------: | :--------: | :--------: |\n| Python3 <Machine-Learning in Practice> study notes (X): Classifier Sharpener -- AdaBoost | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_10_adaboost.html \"\u60ac\u505c\u663e\u793a\") | [CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/78212124 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/30035094 \"\u60ac\u505c\u663e\u793a\") |\n\n### Code\n\n* [1. Training Process of AdaBoost Based On Decision Stump](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/AdaBoost\/adaboost.py \"\u60ac\u505c\u663e\u793a\")\n\n* [2. AdaBoost on Hard Datasets](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/AdaBoost\/horse_adaboost.py \"\u60ac\u505c\u663e\u793a\")\n\n* [3. Implement AdaBoost by sklearn](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/AdaBoost\/sklearn_adaboost.py \"\u60ac\u505c\u663e\u793a\")\n\n* [4. ROC Curve Plot](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/AdaBoost\/ROC.py \"\u60ac\u505c\u663e\u793a\")\n\n## Chapt. 8: Linear Regression\n\n|   Article   |  Personal Website  |    CSDN    |    Zhihu    |\n| :------  | :--------: | :--------: | :--------: |\n| Python3 <Machine-Learning in Practice> study notes (XI):  | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/11\/ml_11_regression_1.html \"\u60ac\u505c\u663e\u793a\") |[CSDN](http:\/\/blog.csdn.net\/c406495762\/article\/details\/78760239 \"\u60ac\u505c\u663e\u793a\") | [Zhihu](https:\/\/zhuanlan.zhihu.com\/p\/31860100  \"\u60ac\u505c\u663e\u793a\")|\n| Python3 <Machine-Learning in Practice> study notes (XII): | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/12\/ml_12_regression_2.html \"\u60ac\u505c\u663e\u793a\") | no | no |\n\n### Code\n\n* [1. Linear Regression(Ordinary LR + Locally Weighted LR)](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Regression\/regression_old.py \"\u60ac\u505c\u663e\u793a\")\n\n* [2. Predicting the Age of Abalones(Ormers)](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Regression\/abalone.py \"\u60ac\u505c\u663e\u793a\")\n\n* [3. Stepwise Regression](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Regression\/regression.py \"\u60ac\u505c\u663e\u793a\")\n\n* [4. Predicting the Price of Second Hand Lego](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Regression\/lego.py \"\u60ac\u505c\u663e\u793a\")\n\n## Chapt. 9: Regression Tree\n\n|   Article   |  Personal Website  |    CSDN    |    Zhihu    |\n| :------  | :--------: | :--------: | :--------: |\n| Python3 <Machine-Learning in Practice> study notes (XIII): Regression Tree (Basic Concepts): CART Alogrithm and Pruning | [Personal Website](http:\/\/cuijiahua.com\/blog\/2017\/12\/ml_13_regtree_1.html \"\u60ac\u505c\u663e\u793a\") | no | no |\n\n\n###  Code\n\n* [1. Regression Tree](https:\/\/github.com\/Jack-Cherish\/Machine-Learning\/blob\/master\/Regression%20Trees\/regTrees.py \"\u60ac\u505c\u663e\u793a\")\n","125":".. -*- mode: rst -*-\n\n.. _scikit-learn: http:\/\/scikit-learn.org\/stable\/\n\n.. _scikit-learn-contrib: https:\/\/github.com\/scikit-learn-contrib\n\n|Azure|_ |Codecov|_ |CircleCI|_ |PythonVersion|_ |Pypi|_ |Gitter|_ |Black|_\n\n.. |Azure| image:: https:\/\/dev.azure.com\/imbalanced-learn\/imbalanced-learn\/_apis\/build\/status\/scikit-learn-contrib.imbalanced-learn?branchName=master\n.. _Azure: https:\/\/dev.azure.com\/imbalanced-learn\/imbalanced-learn\/_build\n\n.. |Codecov| image:: https:\/\/codecov.io\/gh\/scikit-learn-contrib\/imbalanced-learn\/branch\/master\/graph\/badge.svg\n.. _Codecov: https:\/\/codecov.io\/gh\/scikit-learn-contrib\/imbalanced-learn\n\n.. |CircleCI| image:: https:\/\/circleci.com\/gh\/scikit-learn-contrib\/imbalanced-learn.svg?style=shield&circle-token=:circle-token\n.. _CircleCI: https:\/\/circleci.com\/gh\/scikit-learn-contrib\/imbalanced-learn\/tree\/master\n\n.. |PythonVersion| image:: https:\/\/img.shields.io\/pypi\/pyversions\/imbalanced-learn.svg\n.. _PythonVersion: https:\/\/img.shields.io\/pypi\/pyversions\/imbalanced-learn.svg\n\n.. |Pypi| image:: https:\/\/badge.fury.io\/py\/imbalanced-learn.svg\n.. _Pypi: https:\/\/badge.fury.io\/py\/imbalanced-learn\n\n.. |Gitter| image:: https:\/\/badges.gitter.im\/scikit-learn-contrib\/imbalanced-learn.svg\n.. _Gitter: https:\/\/gitter.im\/scikit-learn-contrib\/imbalanced-learn?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n\n.. |Black| image:: https:\/\/img.shields.io\/badge\/code%20style-black-000000.svg\n.. _Black: :target: https:\/\/github.com\/psf\/black\n\n.. |PythonMinVersion| replace:: 3.7\n.. |NumPyMinVersion| replace:: 1.14.6\n.. |SciPyMinVersion| replace:: 1.1.0\n.. |ScikitLearnMinVersion| replace:: 1.0.1\n.. |MatplotlibMinVersion| replace:: 2.2.3\n.. |PandasMinVersion| replace:: 0.25.0\n.. |TensorflowMinVersion| replace:: 2.4.3\n.. |KerasMinVersion| replace:: 2.4.3\n.. |SeabornMinVersion| replace:: 0.9.0\n.. |PytestMinVersion| replace:: 5.0.1\n\nimbalanced-learn\n================\n\nimbalanced-learn is a python package offering a number of re-sampling techniques\ncommonly used in datasets showing strong between-class imbalance.\nIt is compatible with scikit-learn_ and is part of scikit-learn-contrib_\nprojects.\n\nDocumentation\n-------------\n\nInstallation documentation, API documentation, and examples can be found on the\ndocumentation_.\n\n.. _documentation: https:\/\/imbalanced-learn.org\/stable\/\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\n`imbalanced-learn` requires the following dependencies:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- Scikit-learn (>= |ScikitLearnMinVersion|)\n\nAdditionally, `imbalanced-learn` requires the following optional dependencies:\n\n- Pandas (>= |PandasMinVersion|) for dealing with dataframes\n- Tensorflow (>= |TensorflowMinVersion|) for dealing with TensorFlow models\n- Keras (>= |KerasMinVersion|) for dealing with Keras models\n\nThe examples will requires the following additional dependencies:\n\n- Matplotlib (>= |MatplotlibMinVersion|)\n- Seaborn (>= |SeabornMinVersion|)\n\nInstallation\n~~~~~~~~~~~~\n\nFrom PyPi or conda-forge repositories\n.....................................\n\nimbalanced-learn is currently available on the PyPi's repositories and you can\ninstall it via `pip`::\n\n  pip install -U imbalanced-learn\n\nThe package is release also in Anaconda Cloud platform::\n\n  conda install -c conda-forge imbalanced-learn\n\nFrom source available on GitHub\n...............................\n\nIf you prefer, you can clone it and run the setup.py file. Use the following\ncommands to get a copy from Github and install all dependencies::\n\n  git clone https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn.git\n  cd imbalanced-learn\n  pip install .\n\nBe aware that you can install in developer mode with::\n\n  pip install --no-build-isolation --editable .\n\nIf you wish to make pull-requests on GitHub, we advise you to install\npre-commit::\n\n  pip install pre-commit\n  pre-commit install\n\nTesting\n~~~~~~~\n\nAfter installation, you can use `pytest` to run the test suite::\n\n  make coverage\n\nDevelopment\n-----------\n\nThe development of this scikit-learn-contrib is in line with the one\nof the scikit-learn community. Therefore, you can refer to their\n`Development Guide\n<http:\/\/scikit-learn.org\/stable\/developers>`_.\n\nAbout\n-----\n\nIf you use imbalanced-learn in a scientific publication, we would appreciate\ncitations to the following paper::\n\n  @article{JMLR:v18:16-365,\n  author  = {Guillaume  Lema{{\\^i}}tre and Fernando Nogueira and Christos K. Aridas},\n  title   = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},\n  journal = {Journal of Machine Learning Research},\n  year    = {2017},\n  volume  = {18},\n  number  = {17},\n  pages   = {1-5},\n  url     = {http:\/\/jmlr.org\/papers\/v18\/16-365}\n  }\n\nMost classification algorithms will only perform optimally when the number of\nsamples of each class is roughly the same. Highly skewed datasets, where the\nminority is heavily outnumbered by one or more classes, have proven to be a\nchallenge while at the same time becoming more and more common.\n\nOne way of addressing this issue is by re-sampling the dataset as to offset this\nimbalance with the hope of arriving at a more robust and fair decision boundary\nthan you would otherwise.\n\nRe-sampling techniques are divided in two categories:\n    1. Under-sampling the majority class(es).\n    2. Over-sampling the minority class.\n    3. Combining over- and under-sampling.\n    4. Create ensemble balanced sets.\n\nBelow is a list of the methods currently implemented in this module.\n\n* Under-sampling\n    1. Random majority under-sampling with replacement\n    2. Extraction of majority-minority Tomek links [1]_\n    3. Under-sampling with Cluster Centroids\n    4. NearMiss-(1 & 2 & 3) [2]_\n    5. Condensed Nearest Neighbour [3]_\n    6. One-Sided Selection [4]_\n    7. Neighboorhood Cleaning Rule [5]_\n    8. Edited Nearest Neighbours [6]_\n    9. Instance Hardness Threshold [7]_\n    10. Repeated Edited Nearest Neighbours [14]_\n    11. AllKNN [14]_\n\n* Over-sampling\n    1. Random minority over-sampling with replacement\n    2. SMOTE - Synthetic Minority Over-sampling Technique [8]_\n    3. SMOTENC - SMOTE for Nominal and Continuous [8]_\n    4. SMOTEN - SMOTE for Nominal [8]_\n    5. bSMOTE(1 & 2) - Borderline SMOTE of types 1 and 2 [9]_\n    6. SVM SMOTE - Support Vectors SMOTE [10]_\n    7. ADASYN - Adaptive synthetic sampling approach for imbalanced learning [15]_\n    8. KMeans-SMOTE [17]_\n    9. ROSE - Random OverSampling Examples [19]_\n\n* Over-sampling followed by under-sampling\n    1. SMOTE + Tomek links [12]_\n    2. SMOTE + ENN [11]_\n\n* Ensemble classifier using samplers internally\n    1. Easy Ensemble classifier [13]_\n    2. Balanced Random Forest [16]_\n    3. Balanced Bagging\n    4. RUSBoost [18]_\n\n* Mini-batch resampling for Keras and Tensorflow\n\nThe different algorithms are presented in the sphinx-gallery_.\n\n.. _sphinx-gallery: https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/auto_examples\/index.html\n\n\nReferences:\n-----------\n\n.. [1] : I. Tomek, \u201cTwo modifications of CNN,\u201d IEEE Transactions on Systems, Man, and Cybernetics, vol. 6, pp. 769-772, 1976.\n\n.. [2] : I. Mani, J. Zhang. \u201ckNN approach to unbalanced data distributions: A case study involving information extraction,\u201d In Proceedings of the Workshop on Learning from Imbalanced Data Sets, pp. 1-7, 2003.\n\n.. [3] : P. E. Hart, \u201cThe condensed nearest neighbor rule,\u201d IEEE Transactions on Information Theory, vol. 14(3), pp. 515-516, 1968.\n\n.. [4] : M. Kubat, S. Matwin, \u201cAddressing the curse of imbalanced training sets: One-sided selection,\u201d In Proceedings of the 14th International Conference on Machine Learning, vol. 97, pp. 179-186, 1997.\n\n.. [5] : J. Laurikkala, \u201cImproving identification of difficult small classes by balancing class distribution,\u201d Proceedings of the 8th Conference on Artificial Intelligence in Medicine in Europe, pp. 63-66, 2001.\n\n.. [6] : D. Wilson, \u201cAsymptotic Properties of Nearest Neighbor Rules Using Edited Data,\u201d IEEE Transactions on Systems, Man, and Cybernetrics, vol. 2(3), pp. 408-421, 1972.\n\n.. [7] : M. R. Smith, T. Martinez, C. Giraud-Carrier, \u201cAn instance level analysis of data complexity,\u201d Machine learning, vol. 95(2), pp. 225-256, 2014.\n\n.. [8] : N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer, \u201cSMOTE: Synthetic minority over-sampling technique,\u201d Journal of Artificial Intelligence Research, vol. 16, pp. 321-357, 2002.\n\n.. [9] : H. Han, W.-Y. Wang, B.-H. Mao, \u201cBorderline-SMOTE: A new over-sampling method in imbalanced data sets learning,\u201d In Proceedings of the 1st International Conference on Intelligent Computing, pp. 878-887, 2005.\n\n.. [10] : H. M. Nguyen, E. W. Cooper, K. Kamei, \u201cBorderline over-sampling for imbalanced data classification,\u201d In Proceedings of the 5th International Workshop on computational Intelligence and Applications, pp. 24-29, 2009.\n\n.. [11] : G. E. A. P. A. Batista, R. C. Prati, M. C. Monard, \u201cA study of the behavior of several methods for balancing machine learning training data,\u201d ACM Sigkdd Explorations Newsletter, vol. 6(1), pp. 20-29, 2004.\n\n.. [12] : G. E. A. P. A. Batista, A. L. C. Bazzan, M. C. Monard, \u201cBalancing training data for automated annotation of keywords: A case study,\u201d In Proceedings of the 2nd Brazilian Workshop on Bioinformatics, pp. 10-18, 2003.\n\n.. [13] : X.-Y. Liu, J. Wu and Z.-H. Zhou, \u201cExploratory undersampling for class-imbalance learning,\u201d IEEE Transactions on Systems, Man, and Cybernetics, vol. 39(2), pp. 539-550, 2009.\n\n.. [14] : I. Tomek, \u201cAn experiment with the edited nearest-neighbor rule,\u201d IEEE Transactions on Systems, Man, and Cybernetics, vol. 6(6), pp. 448-452, 1976.\n\n.. [15] : H. He, Y. Bai, E. A. Garcia, S. Li, \u201cADASYN: Adaptive synthetic sampling approach for imbalanced learning,\u201d In Proceedings of the 5th IEEE International Joint Conference on Neural Networks, pp. 1322-1328, 2008.\n\n.. [16] : C. Chao, A. Liaw, and L. Breiman. \"Using random forest to learn imbalanced data.\" University of California, Berkeley 110 (2004): 1-12.\n\n.. [17] : Felix Last, Georgios Douzas, Fernando Bacao, \"Oversampling for Imbalanced Learning Based on K-Means and SMOTE\"\n\n.. [18] : Seiffert, C., Khoshgoftaar, T. M., Van Hulse, J., & Napolitano, A. \"RUSBoost: A hybrid approach to alleviating class imbalance.\" IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 40.1 (2010): 185-197.\n\n.. [19] : Menardi, G., Torelli, N.: \"Training and assessing classification rules with unbalanced data\", Data Mining and Knowledge Discovery,  28, (2014): 92\u2013122\n","126":"<a href=\"https:\/\/sktime.org\"><img src=\"https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/docs\/source\/images\/sktime-logo-no-text.jpg?raw=true)\" width=\"175\" align=\"right\" \/><\/a>\n\n# Welcome to sktime\n\n> A unified interface for machine learning with time series\n\n:rocket: **Version 0.11.3 out now!** [Check out the release notes here](https:\/\/www.sktime.org\/en\/latest\/changelog.html).\n\nsktime is a library for time series analysis in Python. It provides a unified interface for multiple time series learning tasks. Currently, this includes time series classification, regression, clustering, annotation and forecasting. It comes with [time series algorithms](https:\/\/www.sktime.org\/en\/stable\/estimator_overview.html) and [scikit-learn] compatible tools to build, tune and validate time series models.\n\n[scikit-learn]: https:\/\/scikit-learn.org\/stable\/\n\n| Overview | |\n|---|---|\n| **CI\/CD** | [![github-actions](https:\/\/img.shields.io\/github\/workflow\/status\/alan-turing-institute\/sktime\/build-and-test?logo=github)](https:\/\/github.com\/alan-turing-institute\/sktime\/actions?query=workflow%3Abuild-and-test) [![!codecov](https:\/\/img.shields.io\/codecov\/c\/github\/alan-turing-institute\/sktime?label=codecov&logo=codecov)](https:\/\/codecov.io\/gh\/alan-turing-institute\/sktime) [![readthedocs](https:\/\/img.shields.io\/readthedocs\/sktime?logo=readthedocs)](https:\/\/www.sktime.org\/en\/latest\/?badge=latest) [![platform](https:\/\/img.shields.io\/conda\/pn\/conda-forge\/sktime)](https:\/\/github.com\/alan-turing-institute\/sktime) |\n| **Code** |  [![!pypi](https:\/\/img.shields.io\/pypi\/v\/sktime?color=orange)](https:\/\/pypi.org\/project\/sktime\/) [![!conda](https:\/\/img.shields.io\/conda\/vn\/conda-forge\/sktime)](https:\/\/anaconda.org\/conda-forge\/sktime) [![!python-versions](https:\/\/img.shields.io\/pypi\/pyversions\/sktime)](https:\/\/www.python.org\/) [![!black](https:\/\/img.shields.io\/badge\/code%20style-black-000000.svg)](https:\/\/github.com\/psf\/black) [![Binder](https:\/\/mybinder.org\/badge_logo.svg)](https:\/\/mybinder.org\/v2\/gh\/alan-turing-institute\/sktime\/main?filepath=examples) |\n| **Downloads**| [![Downloads](https:\/\/static.pepy.tech\/personalized-badge\/sktime?period=week&units=international_system&left_color=grey&right_color=blue&left_text=weekly%20(pypi))](https:\/\/pepy.tech\/project\/sktime) [![Downloads](https:\/\/static.pepy.tech\/personalized-badge\/sktime?period=month&units=international_system&left_color=grey&right_color=blue&left_text=monthly%20(pypi))](https:\/\/pepy.tech\/project\/sktime) [![Downloads](https:\/\/static.pepy.tech\/personalized-badge\/sktime?period=total&units=international_system&left_color=grey&right_color=blue&left_text=cumulative%20(pypi))](https:\/\/pepy.tech\/project\/sktime) |\n| **Community** | [![!slack](https:\/\/img.shields.io\/static\/v1?logo=slack&label=slack&message=chat&color=lightgreen)](https:\/\/join.slack.com\/t\/sktime-group\/shared_invite\/zt-62i7aejn-vXc3nOWF26S_P3VXFPWisQ) [![!discord](https:\/\/img.shields.io\/static\/v1?logo=discord&label=discord&message=chat&color=lightgreen)](https:\/\/discord.com\/invite\/gqSab2K) [![!twitter](https:\/\/img.shields.io\/twitter\/follow\/sktime_toolbox?label=%20Twitter&style=social)](https:\/\/twitter.com\/sktime_toolbox) [![!youtube](https:\/\/img.shields.io\/youtube\/views\/ODspi8-uWgo?label=watch&style=social)](https:\/\/www.youtube.com\/watch?v=ODspi8-uWgo) |\n| **Citation** | [![!zenodo](https:\/\/zenodo.org\/badge\/DOI\/10.5281\/zenodo.3749000.svg)](https:\/\/doi.org\/10.5281\/zenodo.3749000) |\n\n## :books: Documentation\n\n| Documentation              |                                                                |\n| -------------------------- | -------------------------------------------------------------- |\n| :star: **[Tutorials]**        | New to sktime? Here's everything you need to know!              |\n| :clipboard: **[Binder Notebooks]** | Example notebooks to play with in your browser.              |\n| :woman_technologist: **[User Guides]**      | How to use sktime and its features.                             |\n| :scissors: **[Extension Templates]** | How to build your own estimator using sktime's API.            |\n| :control_knobs: **[API Reference]**      | The detailed reference for sktime's API.                        |\n| :tv: **[Video Tutorial]**            | Our video tutorial from the 2020 PyData Festival.      |\n| :hammer_and_wrench: **[Changelog]**          | Changes and version history.                                   |\n| :deciduous_tree: **[Roadmap]**          | sktime's software and community development plan.                                   |\n| :pencil: **[Related Software]**          | A list of related software. |\n\n[tutorials]: https:\/\/www.sktime.org\/en\/latest\/tutorials.html\n[binder notebooks]: https:\/\/mybinder.org\/v2\/gh\/alan-turing-institute\/sktime\/main?filepath=examples\n[user guides]: https:\/\/www.sktime.org\/en\/latest\/user_guide.html\n[video tutorial]: https:\/\/github.com\/sktime\/sktime-tutorial-pydata-amsterdam-2020\n[api reference]: https:\/\/www.sktime.org\/en\/latest\/api_reference.html\n[changelog]: https:\/\/www.sktime.org\/en\/latest\/changelog.html\n[roadmap]: https:\/\/www.sktime.org\/en\/latest\/roadmap.html\n[related software]: https:\/\/www.sktime.org\/en\/latest\/related_software.html\n\n## :speech_balloon: Where to ask questions\n\nQuestions and feedback are extremely welcome! Please understand that we won't be able to provide individual support via email. We also believe that help is much more valuable if it's shared publicly, so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| :bug: **Bug Reports**              | [GitHub Issue Tracker]                  |\n| :sparkles: **Feature Requests & Ideas** | [GitHub Issue Tracker]                       |\n| :woman_technologist: **Usage Questions**          | [GitHub Discussions] \u00b7 [Stack Overflow] |\n| :speech_balloon: **General Discussion**        | [GitHub Discussions] |\n| :factory: **Contribution & Development** | [Slack], contributors channel \u00b7 [Discord] |\n| :globe_with_meridians: **Community collaboration session** | [Discord] - Fridays 1pm UTC, dev\/meet-ups channel |\n\n[github issue tracker]: https:\/\/github.com\/alan-turing-institute\/sktime\/issues\n[github discussions]: https:\/\/github.com\/alan-turing-institute\/sktime\/discussions\n[stack overflow]: https:\/\/stackoverflow.com\/questions\/tagged\/sktime\n[discord]: https:\/\/discord.com\/invite\/gqSab2K\n[slack]: https:\/\/join.slack.com\/t\/sktime-group\/shared_invite\/zt-62i7aejn-vXc3nOWF26S_P3VXFPWisQ\n\n## :dizzy: Features\nOur aim is to make the time series analysis ecosystem more interoperable and usable as a whole. sktime provides a __unified interface for distinct but related time series learning tasks__. It features [__dedicated time series algorithms__](https:\/\/www.sktime.org\/en\/stable\/estimator_overview.html) and __tools for composite model building__ including pipelining, ensembling, tuning and reduction that enables users to apply an algorithm for one task to another.\n\nsktime also provides **interfaces to related libraries**, for example [scikit-learn], [statsmodels], [tsfresh], [PyOD] and [fbprophet], among others.\n\nFor **deep learning**, see our companion package: [sktime-dl](https:\/\/github.com\/sktime\/sktime-dl).\n\n[statsmodels]: https:\/\/www.statsmodels.org\/stable\/index.html\n[tsfresh]: https:\/\/tsfresh.readthedocs.io\/en\/latest\/\n[pyod]: https:\/\/pyod.readthedocs.io\/en\/latest\/\n[prophet]: https:\/\/facebook.github.io\/prophet\/\n\n| Module | Status | Links |\n|---|---|---|\n| **[Forecasting]** | stable | [Tutorial](https:\/\/www.sktime.org\/en\/latest\/examples\/01_forecasting.html) \u00b7 [API Reference](https:\/\/www.sktime.org\/en\/latest\/api_reference.html#sktime-forecasting-time-series-forecasting) \u00b7 [Extension Template](https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/extension_templates\/forecasting.py)  |\n| **[Time Series Classification]** | stable | [Tutorial](https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/examples\/02_classification.ipynb) \u00b7 [API Reference](https:\/\/www.sktime.org\/en\/latest\/api_reference.html#sktime-classification-time-series-classification) \u00b7 [Extension Template](https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/extension_templates\/classification.py) |\n| **[Time Series Regression]** | stable | [API Reference](https:\/\/www.sktime.org\/en\/latest\/api_reference.html#sktime-classification-time-series-regression) |\n| **[Transformations]** | maturing | [API Reference](https:\/\/www.sktime.org\/en\/latest\/api_reference.html#sktime-transformations-time-series-transformers) \u00b7 [Extension Template](https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/extension_templates\/transformer.py)  |\n| **[Time Series Clustering]** | maturing | [Extension Template](https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/extension_templates\/clustering.py) |\n| **[Time Series Distances\/Kernels]** | experimental | [Extension Template](https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/extension_templates\/dist_kern_panel.py) |\n| **[Annotation]** | experimental | [Extension Template](https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/extension_templates\/annotation.py) |\n\n[forecasting]: https:\/\/github.com\/alan-turing-institute\/sktime\/tree\/main\/sktime\/forecasting\n[time series classification]: https:\/\/github.com\/alan-turing-institute\/sktime\/tree\/main\/sktime\/classification\n[time series regression]: https:\/\/github.com\/alan-turing-institute\/sktime\/tree\/main\/sktime\/regression\n[time series clustering]: https:\/\/github.com\/alan-turing-institute\/sktime\/tree\/main\/sktime\/clustering\n[annotation]: https:\/\/github.com\/alan-turing-institute\/sktime\/tree\/main\/sktime\/annotation\n[time series distances\/kernels]: https:\/\/github.com\/alan-turing-institute\/sktime\/tree\/main\/sktime\/dists_kernels\n[transformations]: https:\/\/github.com\/alan-turing-institute\/sktime\/tree\/main\/sktime\/transformations\n\n\n## :hourglass_flowing_sand: Install sktime\nFor trouble shooting and detailed installation instructions, see the [documentation](https:\/\/www.sktime.org\/en\/latest\/installation.html).\n\n- **Operating system**: macOS X \u00b7 Linux \u00b7 Windows 8.1 or higher\n- **Python version**: Python 3.7, 3.8 and 3.9 (only 64 bit)\n- **Package managers**: [pip] \u00b7 [conda] (via `conda-forge`)\n\n[pip]: https:\/\/pip.pypa.io\/en\/stable\/\n[conda]: https:\/\/docs.conda.io\/en\/latest\/\n\n### pip\nUsing pip, sktime releases are available as source packages and binary wheels. You can see all available wheels [here](https:\/\/pypi.org\/simple\/sktime\/).\n\n```bash\npip install sktime\n```\n\nor, with maximum dependencies,\n\n```bash\npip install sktime[all_extras]\n```\n\n### conda\nYou can also install sktime from `conda` via the `conda-forge` channel. For the feedstock including the build recipe and configuration, check out [this repository](https:\/\/github.com\/conda-forge\/sktime-feedstock).\n\n```bash\nconda install -c conda-forge sktime\n```\n\nor, with maximum dependencies,\n\n```bash\nconda install -c conda-forge sktime-all-extras\n```\n\n## :zap: Quickstart\n\n### Forecasting\n\n```python\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.forecasting.theta import ThetaForecaster\nfrom sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y)\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nforecaster = ThetaForecaster(sp=12)  # monthly seasonal periodicity\nforecaster.fit(y_train)\ny_pred = forecaster.predict(fh)\nmean_absolute_percentage_error(y_test, y_pred)\n>>> 0.08661467738190656\n```\n\n### Time Series Classification\n\n```python\nfrom sktime.classification.interval_based import TimeSeriesForestClassifier\nfrom sktime.datasets import load_arrow_head\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX, y = load_arrow_head()\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nclassifier = TimeSeriesForestClassifier()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\naccuracy_score(y_test, y_pred)\n>>> 0.8679245283018868\n```\n\n## :wave: How to get involved\n\nThere are many ways to join the sktime community. We follow the [all-contributors](https:\/\/github.com\/all-contributors\/all-contributors) specification: all kinds of contributions are welcome - not just code.\n\n| Documentation              |                                                                |\n| -------------------------- | --------------------------------------------------------------        |\n| :gift_heart: **[Contribute]**        | How to contribute to sktime.          |\n| :school_satchel:  **[Mentoring]** | New to open source? Apply to our mentoring program! |\n| :date: **[Meetings]** | Join our discussions, tutorials, workshops and sprints! |\n| :woman_mechanic:  **[Developer Guides]**      | How to further develop sktime's code base.                             |\n| :construction: **[Enhancement Proposals]** | Design a new feature for sktime. |\n| :medal_sports: **[Contributors]** | A list of all contributors. |\n| :raising_hand: **[Roles]** | An overview of our core community roles. |\n| :money_with_wings: **[Donate]** | Fund sktime maintenance and development. |\n| :classical_building: **[Governance]** | How and by whom decisions are made in sktime's community.   |\n\n[contribute]: https:\/\/www.sktime.org\/en\/latest\/get_involved\/contributing.html\n[donate]: https:\/\/opencollective.com\/sktime\n[extension templates]: https:\/\/github.com\/alan-turing-institute\/sktime\/tree\/main\/extension_templates\n[developer guides]: https:\/\/www.sktime.org\/en\/latest\/developer_guide.html\n[contributors]: https:\/\/github.com\/alan-turing-institute\/sktime\/blob\/main\/CONTRIBUTORS.md\n[governance]: https:\/\/www.sktime.org\/en\/latest\/governance.html\n[mentoring]: https:\/\/github.com\/sktime\/mentoring\n[meetings]: https:\/\/calendar.google.com\/calendar\/u\/0\/embed?src=sktime.toolbox@gmail.com&ctz=UTC\n[enhancement proposals]: https:\/\/github.com\/sktime\/enhancement-proposals\n[roles]: https:\/\/www.sktime.org\/en\/latest\/about\/team.html\n\n## :bulb: Project vision\n\n* **by the community, for the community** -- developed by a friendly and collaborative community.\n* the **right tool for the right task** -- helping users to diagnose their learning problem and suitable scientific model types.\n* **embedded in state-of-art ecosystems** and **provider of interoperable interfaces** -- interoperable with [scikit-learn], [statsmodels], [tsfresh], and other community favourites.\n* **rich model composition and reduction functionality** -- build tuning and feature extraction pipelines, solve forecasting tasks with [scikit-learn] regressors.\n* **clean, descriptive specification syntax** -- based on modern object-oriented design principles for data science.\n* **fair model assessment and benchmarking** -- build your models, inspect your models, check your models, avoid pitfalls.\n* **easily extensible** -- easy extension templates to add your own algorithms compatible with sktime's API.\n","127":"# Have Fun with Machine Learning: A Guide for Beginners\nAlso available in [Chinese (Traditional)](README_zh-tw.md).  \nAlso available in [Korean](README_ko-KR.md).\n\n## Preface\n\nThis is a **hands-on guide** to machine learning for programmers with *no background* in\nAI. Using a neural network doesn\u2019t require a PhD, and you don\u2019t need to be the person who\nmakes the next breakthrough in AI in order to *use* what exists today.  What we have now\nis already breathtaking, and highly usable.  I believe that more of us need to play with\nthis stuff like we would any other open source technology, instead of treating it like a\nresearch topic.\n\nIn this guide our goal will be to write a program that uses machine learning to predict, with a\nhigh degree of certainty, whether the images in [data\/untrained-samples](data\/untrained-samples)\nare of **dolphins** or **seahorses** using only the images themselves, and without\nhaving seen them before.  Here are two example images we'll use:\n\n![A dolphin](data\/untrained-samples\/dolphin1.jpg?raw=true \"Dolphin\")\n![A seahorse](data\/untrained-samples\/seahorse1.jpg?raw=true \"Seahorse\")\n\nTo do that we\u2019re going to train and use a [Convolutional Neural Network (CNN)](https:\/\/en.wikipedia.org\/wiki\/Convolutional_neural_network).\nWe\u2019re going to approach this from the point of view of a practitioner vs.\nfrom first principles. There is so much excitement about AI right now,\nbut much of what\u2019s being written feels like being taught to do\ntricks on your bike by a physics professor at a chalkboard instead\nof your friends in the park.\n\nI\u2019ve decided to write this on Github vs. as a blog post\nbecause I\u2019m sure that some of what I\u2019ve written below is misleading, naive, or\njust plain wrong.  I\u2019m still learning myself, and I\u2019ve found the lack of solid\nbeginner documentation an obstacle.  If you see me making a mistake or missing\nimportant details, please send a pull request. \n\nWith all of that out the way, let me show you how to do some tricks on your bike!\n\n## Overview\n\nHere\u2019s what we\u2019re going to explore:\n\n* Setup and use existing, open source machine learning technologies, specifically [Caffe](http:\/\/caffe.berkeleyvision.org\/) and [DIGITS](https:\/\/developer.nvidia.com\/digits)\n* Create a dataset of images\n* Train a neural network from scratch\n* Test our neural network on images it has never seen before\n* Improve our neural network\u2019s accuracy by fine tuning existing neural networks (AlexNet and GoogLeNet)\n* Deploy and use our neural network\n\nThis guide won\u2019t teach you how neural networks are designed, cover much theory,\nor use a single mathematical expression.  I don\u2019t pretend to understand most of\nwhat I\u2019m going to show you.  Instead, we\u2019re going to use existing things in\ninteresting ways to solve a hard problem.\n\n> Q: \"I know you said we won\u2019t talk about the theory of neural networks, but I\u2019m\n> feeling like I\u2019d at least like an overview before we get going.  Where should I start?\"\n\nThere are literally hundreds of introductions to this, from short posts to full\nonline courses.  Depending on how you like to learn, here are three options\nfor a good starting point:\n\n* This fantastic [blog post](https:\/\/jalammar.github.io\/visual-interactive-guide-basics-neural-networks\/) by J Alammar,\nwhich introduces the concepts of neural networks using intuitive examples.\n* Similarly, [this video](https:\/\/www.youtube.com\/watch?v=FmpDIaiMIeA) introduction by [Brandon Rohrer](https:\/\/www.youtube.com\/channel\/UCsBKTrp45lTfHa_p49I2AEQ) is a really good intro to\nConvolutional Neural Networks like we'll be using\n* If you\u2019d rather have a bit more theory, I\u2019d recommend [this online book](http:\/\/neuralnetworksanddeeplearning.com\/chap1.html) by [Michael Nielsen](http:\/\/michaelnielsen.org\/).\n\n## Setup\n\nInstalling the software we'll use (Caffe and DIGITS) can be frustrating, depending on your platform\nand OS version.  By far the easiest way to do it is using Docker.  Below we examine how to do it with Docker,\nas well as how to do it natively.\n\n### Option 1a: Installing Caffe Natively\n\nFirst, we\u2019re going to be using the [Caffe deep learning framework](http:\/\/caffe.berkeleyvision.org\/)\nfrom the Berkely Vision and Learning Center (BSD licensed).\n\n> Q: \u201cWait a minute, why Caffe? Why not use something like TensorFlow,\n> which everyone is talking about these days\u2026\u201d  \n\nThere are a lot of great choices available, and you should look at all the\noptions.  [TensorFlow](https:\/\/www.tensorflow.org\/) is great, and you should\nplay with it.  However, I\u2019m using Caffe for a number of reasons:\n\n* It\u2019s tailormade for computer vision problems\n* It has support for C++, Python, (with [node.js support](https:\/\/github.com\/silklabs\/node-caffe) coming)\n* It\u2019s fast and stable\n\nBut the **number one reason** I\u2019m using Caffe is that you **don\u2019t need to write any code** to work\nwith it.  You can do everything declaratively (Caffe uses structured text files to define the\nnetwork architecture) and using command-line tools.  Also, you can use some nice front-ends for Caffe to make\ntraining and validating your network a lot easier.  We\u2019ll be using\n[nVidia\u2019s DIGITS](https:\/\/developer.nvidia.com\/digits) tool below for just this purpose.\n\nCaffe can be a bit of work to get installed.  There are [installation instructions](http:\/\/caffe.berkeleyvision.org\/installation.html)\nfor various platforms, including some prebuilt Docker or AWS configurations.  \n\n**NOTE:** when making my walkthrough, I used the following non-released version of Caffe from their Github repo:\nhttps:\/\/github.com\/BVLC\/caffe\/commit\/5a201dd960840c319cefd9fa9e2a40d2c76ddd73\n\nOn a Mac it can be frustrating to get working, with version issues halting\nyour progress at various steps in the build.  It took me a couple of days\nof trial and error.  There are a dozen guides I followed, each with slightly\ndifferent problems.  In the end I found [this one](https:\/\/gist.github.com\/doctorpangloss\/f8463bddce2a91b949639522ea1dcbe4) to be the closest.\nI\u2019d also recommend [this post](https:\/\/eddiesmo.wordpress.com\/2016\/12\/20\/how-to-set-up-caffe-environment-and-pycaffe-on-os-x-10-12-sierra\/),\nwhich is quite recent and links to many of the same discussions I saw. \n\nGetting Caffe installed is by far the hardest thing we'll do, which is pretty\nneat, since you\u2019d assume the AI aspects would be harder!  Don\u2019t give up if you have\nissues, it\u2019s worth the pain.  If I was doing this again, I\u2019d probably use an Ubuntu VM\ninstead of trying to do it on Mac directly.  There's also a [Caffe Users](https:\/\/groups.google.com\/forum\/#!forum\/caffe-users) group, if you need answers.\n\n> Q: \u201cDo I need powerful hardware to train a neural network? What if I don\u2019t have\n> access to fancy GPUs?\u201d\n\nIt\u2019s true, deep neural networks require a lot of computing power and energy to\ntrain...if you\u2019re training them from scratch and using massive datasets.\nWe aren\u2019t going to do that.  The secret is to use a pretrained network that someone\nelse has already invested hundreds of hours of compute time training, and then to fine\ntune it to your particular dataset.  We\u2019ll look at how to do this below, but suffice\nit to say that everything I\u2019m going to show you, I\u2019m doing on a year old MacBook\nPro without a fancy GPU.\n\nAs an aside, because I have an integrated Intel graphics card vs. an nVidia GPU,\nI decided to use the [OpenCL Caffe branch](https:\/\/github.com\/BVLC\/caffe\/tree\/opencl),\nand it\u2019s worked great on my laptop.\n\nWhen you\u2019re done installing Caffe, you should have, or be able to do all of the following:\n\n* A directory that contains your built caffe.  If you did this in the standard way,\nthere will be a `build\/` dir which contains everything you need to run caffe,\nthe Python bindings, etc.  The parent dir that contains `build\/` will be your\n`CAFFE_ROOT` (we\u2019ll need this later).\n* Running `make test && make runtest` should pass\n* After installing all the Python deps (doing `pip install -r requirements.txt` in `python\/`),\nrunning `make pycaffe && make pytest` should pass\n* You should also run `make distribute` in order to create a distributable version of caffe with all necessary headers, binaries, etc. in `distribute\/`.\n\nOn my machine, with Caffe fully built, I\u2019ve got the following basic layout in my CAFFE_ROOT dir:\n\n```\ncaffe\/\n    build\/\n        python\/\n        lib\/\n        tools\/\n            caffe \u2190 this is our main binary \n    distribute\/\n        python\/\n        lib\/\n        include\/\n        bin\/\n        proto\/\n```\n\nAt this point, we have everything we need to train, test, and program with neural\nnetworks.  In the next section we\u2019ll add a user-friendly, web-based front end to\nCaffe called DIGITS, which will make training and testing our networks much easier.\n\n### Option 1b: Installing DIGITS Natively\n\nnVidia\u2019s [Deep Learning GPU Training System, or DIGITS](https:\/\/github.com\/NVIDIA\/DIGITS),\nis BSD-licensed Python web app for training neural networks.  While it\u2019s\npossible to do everything DIGITS does in Caffe at the command-line, or with code,\nusing DIGITS makes it a lot easier to get started.  I also found it more fun, due\nto the great visualizations, real-time charts, and other graphical features.\nSince you\u2019re experimenting and trying to learn, I highly recommend beginning with DIGITS.\n\nThere are quite a few good docs at https:\/\/github.com\/NVIDIA\/DIGITS\/tree\/master\/docs,\nincluding a few [Installation](https:\/\/github.com\/NVIDIA\/DIGITS\/blob\/master\/docs\/BuildDigits.md),\n[Configuration](https:\/\/github.com\/NVIDIA\/DIGITS\/blob\/master\/docs\/Configuration.md),\nand [Getting Started](https:\/\/github.com\/NVIDIA\/DIGITS\/blob\/master\/docs\/GettingStarted.md)\npages.  I\u2019d recommend reading through everything there before you continue, as I\u2019m not\nan expert on everything you can do with DIGITS.  There's also a public [DIGITS User Group](https:\/\/groups.google.com\/forum\/#!forum\/digits-users) if you have questions you need to ask.\n\nThere are various ways to install and run DIGITS, from Docker to pre-baked packages\non Linux, or you can build it from source. I\u2019m on a Mac, so I built it from source.\n\n**NOTE:** In my walkthrough I've used the following non-released version of DIGITS\nfrom their Github repo: https:\/\/github.com\/NVIDIA\/DIGITS\/commit\/81be5131821ade454eb47352477015d7c09753d9\n\nBecause it\u2019s just a bunch of Python scripts, it was fairly painless to get working.\nThe one thing you need to do is tell DIGITS where your `CAFFE_ROOT` is by setting\nan environment variable before starting the server:\n\n```bash\nexport CAFFE_ROOT=\/path\/to\/caffe\n.\/digits-devserver\n```\n\nNOTE: on Mac I had issues with the server scripts assuming my Python binary was\ncalled `python2`, where I only have `python2.7`.  You can symlink it in `\/usr\/bin`\nor modify the DIGITS startup script(s) to use the proper binary on your system.\n\nOnce the server is started, you can do everything else via your web browser at http:\/\/localhost:5000, which is what I'll do below.\n\n### Option 2: Caffe and DIGITS using Docker\n\nInstall [Docker](https:\/\/www.docker.com\/), if not already installed, then run the following command\nin order to pull and run a full Caffe + Digits container.  A few things to note:\n* make sure port 8080 isn't allocated by another program. If so, change it to any other port you want.\n* change `\/path\/to\/this\/repository` to the location of this cloned repo, and `\/data\/repo` within the container\nwill be bound to this directory.  This is useful for accessing the images discussed below.\n\n```bash\ndocker run --name digits -d -p 8080:5000 -v \/path\/to\/this\/repository:\/data\/repo kaixhin\/digits\n```\n\nNow that we have our container running you can open up your web browser and open\u00a0`http:\/\/localhost:8080`. Everything in the repository is now in the container directory `\/data\/repo`.  That's it. You've now got Caffe and DIGITS working.\n\nIf you need shell access, use the following command:\n\n```bash\ndocker exec -it digits \/bin\/bash\n```\n\n## Training a Neural Network\n\nTraining a neural network involves a few steps:\n\n1. Assemble and prepare a dataset of categorized images\n2. Define the network\u2019s architecture\n3. Train and Validate this network using the prepared dataset\n\nWe\u2019re going to do this 3 different ways, in order to show the difference\nbetween starting from scratch and using a pretrained network, and also to show\nhow to work with two popular pretrained networks (AlexNet, GoogLeNet) that are\ncommonly used with Caffe and DIGITs.\n\nFor our training attempts, we\u2019ll use a small dataset of Dolphins and Seahorses.\nI\u2019ve put the images I used in [data\/dolphins-and-seahorses](data\/dolphins-and-seahorses).\nYou need at least 2 categories, but could have many more (some of the networks\nwe\u2019ll use were trained on 1000+ image categories).  Our goal is to be able to\ngive an image to our network and have it tell us whether it\u2019s a Dolphin or a Seahorse.\n\n### Prepare the Dataset\n\nThe easiest way to begin is to divide your images into a categorized directory layout:\n\n```\ndolphins-and-seahorses\/\n    dolphin\/\n        image_0001.jpg\n        image_0002.jpg\n        image_0003.jpg\n        ...\n    seahorse\/\n        image_0001.jpg\n        image_0002.jpg\n        image_0003.jpg\n        ...\n```\n\nHere each directory is a category we want to classify, and each image within\nthat category dir an example we\u2019ll use for training and validation. \n\n> Q: \u201cDo my images have to be the same size?  What about the filenames, do they matter?\u201d\n\nNo to both. The images sizes will be normalized before we feed them into\nthe network.  We\u2019ll eventually want colour images of 256 x 256 pixels, but\nDIGITS will crop or squash (we'll squash) our images automatically in a moment.\nThe filenames are irrelevant--it\u2019s only important which category they are contained\nwithin.\n\n> Q: \u201cCan I do more complex segmentation of my categories?\u201d\n\nYes. See https:\/\/github.com\/NVIDIA\/DIGITS\/blob\/digits-4.0\/docs\/ImageFolderFormat.md.\n\nWe want to use these images on disk to create a **New Dataset**, and specifically,\na **Classification Dataset**.\n\n![Create New Dataset](images\/create-new-dataset.png?raw=true \"Create New Dataset\")\n\nWe\u2019ll use the defaults DIGITS gives us, and point **Training Images** at the path\nto our [data\/dolphins-and-seahorses](data\/dolphins-and-seahorses) folder.\nDIGITS will use the categories (`dolphin` and `seahorse`) to create a database\nof squashed, 256 x 256 Training (75%) and Testing (25%) images.\n\nGive your Dataset a name,`dolphins-and-seahorses`, and click **Create**.\n\n![New Image Classification Dataset](images\/new-image-classification-dataset.png?raw=true \"New Image Classification Dataset\")\n\nThis will create our dataset, which took only 4s on my laptop.  In the end I\nhave 92 Training images (49 dolphin, 43 seahorse) in 2 categories, with 30\nValidation images (16 dolphin, 14 seahorse).  It\u2019s a really small dataset, but perfect\nfor our experimentation and learning purposes, because it won\u2019t take forever to train\nand validate a network that uses it. \n\nYou can **Explore the db** if you want to see the images after they have been squashed. \n\n![Explore the db](images\/explore-dataset.png?raw=true \"Explore the db\")\n\n### Training: Attempt 1, from Scratch\n\nBack in the DIGITS Home screen, we need to create a new **Classification Model**:\n\n![Create Classification Model](images\/create-classification-model.png?raw=true \"Create Classification Model\")\n\nWe\u2019ll start by training a model that uses our `dolphins-and-seahorses` dataset,\nand the default settings DIGITS provides.  For our first network, we\u2019ll choose to\nuse one of the standard network architectures, [AlexNet (pdf)](http:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). [AlexNet\u2019s design](http:\/\/vision.stanford.edu\/teaching\/cs231b_spring1415\/slides\/alexnet_tugce_kyunghee.pdf)\nwon a major computer vision competition called ImageNet in 2012.  The competition\nrequired categorizing 1000+ image categories across 1.2 million images.\n \n![New Classification Model 1](images\/new-image-classification-model-attempt1.png?raw=true \"Model 1\")\n\nCaffe uses structured text files to define network architectures.  These text files\nare based on [Google\u2019s Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers\/).\nYou can read the [full schema](https:\/\/github.com\/BVLC\/caffe\/blob\/master\/src\/caffe\/proto\/caffe.proto) Caffe uses.\nFor the most part we\u2019re not going to work with these, but it\u2019s good to be aware of their\nexistence, since we\u2019ll have to modify them in later steps.  The AlexNet prototxt file\nlooks like this, for example: https:\/\/github.com\/BVLC\/caffe\/blob\/master\/models\/bvlc_alexnet\/train_val.prototxt. \n\nWe\u2019ll train our network for **30 epochs**, which means that it will learn (with our\ntraining images) then test itself (using our validation images), and adjust the\nnetwork\u2019s weights depending on how well it\u2019s doing, and repeat this process 30 times.\nEach time it completes a cycle we\u2019ll get info about **Accuracy** (0% to 100%,\nwhere higher is better) and what our **Loss** is (the sum of all the mistakes that were\nmade, where lower is better).  Ideally we want a network that is able to predict with\nhigh accuracy, and with few errors (small loss).\n\n**NOTE:** some people have [reported hitting errors in DIGITS](https:\/\/github.com\/humphd\/have-fun-with-machine-learning\/issues\/17)\ndoing this training run. For many, the problem related to available memory (the process\nneeds a lot of memory to work).  If you're using Docker, you might want to try\nincreasing the amount of memory available to DIGITS (in Docker, preferences -> advanced -> memory).\n\nInitially, our network\u2019s accuracy is a bit below 50%.  This makes sense, because at first it\u2019s\njust \u201cguessing\u201d between two categories using randomly assigned weights.  Over time\nit\u2019s able to achieve 87.5% accuracy, with a loss of 0.37.  The entire 30 epoch run\ntook me just under 6 minutes.\n\n![Model Attempt 1](images\/model-attempt1.png?raw=true \"Model Attempt 1\")\n\nWe can test our model using an image we upload or a URL to an image on the web.\nLet\u2019s test it on a few examples that weren\u2019t in our training\/validation dataset:\n\n![Model 1 Classify 1](images\/model-attempt1-classify1.png?raw=true \"Model 1 Classify 1\")\n\n![Model 1 Classify 2](images\/model-attempt1-classify2.png?raw=true \"Model 1 Classify 2\")\n\nIt almost seems perfect, until we try another:\n\n![Model 1 Classify 3](images\/model-attempt1-classify3.png?raw=true \"Model 1 Classify 3\")\n\nHere it falls down completely, and confuses a seahorse for a dolphin, and worse,\ndoes so with a high degree of confidence.\n\nThe reality is that our dataset is too small to be useful for training a really good\nneural network.  We really need 10s or 100s of thousands of images, and with that, a\nlot of computing power to process everything.\n\n### Training: Attempt 2, Fine Tuning AlexNet\n\n#### How Fine Tuning works\n\nDesigning a neural network from scratch, collecting data sufficient to train\nit (e.g., millions of images), and accessing GPUs for weeks to complete the\ntraining is beyond the reach of most of us.  To make it practical for smaller amounts\nof data to be used, we employ a technique called **Transfer Learning**, or **Fine Tuning**.\nFine tuning takes advantage of the layout of deep neural networks, and uses\npretrained networks to do the hard work of initial object detection.\n\nImagine using a neural network to be like looking at something far away with a \npair of binoculars.  You first put the binoculars to your eyes, and everything is\nblurry.  As you adjust the focus, you start to see colours, lines, shapes, and eventually\nyou are able to pick out the shape of a bird, then with some more adjustment you can\nidentify the species of bird.\n\nIn a multi-layered network, the initial layers extract features (e.g., edges), with\nlater layers using these features to detect shapes (e.g., a wheel, an eye), which are\nthen feed into final classification layers that detect items based on accumulated \ncharacteristics from previous layers (e.g., a cat vs. a dog).  A network has to be \nable to go from pixels to circles to eyes to two eyes placed in a particular orientation, \nand so on up to being able to finally conclude that an image depicts a cat.\n\nWhat we\u2019d like to do is to specialize an existing, pretrained network for classifying \na new set of image classes instead of the ones on which it was initially trained. Because\nthe network already knows how to \u201csee\u201d features in images, we\u2019d like to retrain \nit to \u201csee\u201d our particular image types.  We don\u2019t need to start from scratch with the \nmajority of the layers--we want to transfer the learning already done in these layers \nto our new classification task.  Unlike our previous attempt, which used random weights, \nwe\u2019ll use the existing weights of the final network in our training.  However, we\u2019ll \nthrow away the final classification layer(s) and retrain the network with *our* image \ndataset, fine tuning it to our image classes.\n\nFor this to work, we need a pretrained network that is similar enough to our own data\nthat the learned weights will be useful.  Luckily, the networks we\u2019ll use below were \ntrained on millions of natural images from [ImageNet](http:\/\/image-net.org\/), which \nis useful across a broad range of classification tasks.\n\nThis technique has been used to do interesting things like screening for eye diseases \nfrom medical imagery, identifying plankton species from microscopic images collected at \nsea, to categorizing the artistic style of Flickr images.\n\nDoing this perfectly, like all of machine learning, requires you to understand the\ndata and network architecture--you have to be careful with overfitting of the data, \nmight need to fix some of the layers, might need to insert new layers, etc. However,\nmy experience is that it \u201cJust Works\u201d much of the time, and it\u2019s worth you simply doing\nan experiment to see what you can achieve using our naive approach.\n\n#### Uploading Pretrained Networks\n\nIn our first attempt, we used AlexNet\u2019s architecture, but started with random\nweights in the network\u2019s layers.  What we\u2019d like to do is download and use a\nversion of AlexNet that has already been trained on a massive dataset.\n\nThankfully we can do exactly this.  A snapshot of AlexNet is available for download: https:\/\/github.com\/BVLC\/caffe\/tree\/master\/models\/bvlc_alexnet.\nWe need the binary `.caffemodel` file, which is what contains the trained weights, and it\u2019s\navailable for download at http:\/\/dl.caffe.berkeleyvision.org\/bvlc_alexnet.caffemodel.\n\nWhile you\u2019re downloading pretrained models, let\u2019s get one more at the same time.\nIn 2014, Google won the same ImageNet competition with [GoogLeNet](https:\/\/research.google.com\/pubs\/pub43022.html) (codenamed Inception):\na 22-layer neural network. A snapshot of GoogLeNet is available for download\nas well, see https:\/\/github.com\/BVLC\/caffe\/tree\/master\/models\/bvlc_googlenet.\nAgain, we\u2019ll need the `.caffemodel` file with all the pretrained weights,\nwhich is available for download at http:\/\/dl.caffe.berkeleyvision.org\/bvlc_googlenet.caffemodel. \n\nWith these `.caffemodel` files in hand, we can upload them into DIGITs.  Go to\nthe **Pretrained Models** tab in DIGITs home page and choose **Upload Pretrained Model**:\n\n![Load Pretrained Model](images\/load-pretrained-model.png?raw=true \"Load Pretrained Model\")\n\nFor both of these pretrained models, we can use the defaults DIGITs provides\n(i.e., colour, squashed images of 256 x 256).  We just need to provide the \n`Weights (**.caffemodel)` and `Model Definition (original.prototxt)`.\nClick each of those buttons to select a file.\n\nFor the model definitions we can use https:\/\/github.com\/BVLC\/caffe\/blob\/master\/models\/bvlc_googlenet\/train_val.prototxt\nfor GoogLeNet and https:\/\/github.com\/BVLC\/caffe\/blob\/master\/models\/bvlc_alexnet\/train_val.prototxt\nfor AlexNet.  We aren\u2019t going to use the classification labels of these networks,\nso we\u2019ll skip adding a `labels.txt` file:\n \n![Upload Pretrained Model](images\/upload-pretrained-model.png?raw=true \"Upload Pretrained Model\")\n\nRepeat this process for both AlexNet and GoogLeNet, as we\u2019ll use them both in the coming steps.\n\n> Q: \"Are there other networks that would be good as a basis for fine tuning?\"\n\nThe [Caffe Model Zoo](http:\/\/caffe.berkeleyvision.org\/model_zoo.html) has quite a few other\npretrained networks that could be used, see https:\/\/github.com\/BVLC\/caffe\/wiki\/Model-Zoo.\n\n#### Fine Tuning AlexNet for Dolphins and Seahorses\n\nTraining a network using a pretrained Caffe Model is similar to starting from scratch,\nthough we have to make a few adjustments.  First, we\u2019ll adjust the **Base Learning Rate**\nto 0.001 from 0.01, since we don\u2019t need to make such large jumps (i.e., we\u2019re fine tuning).\nWe\u2019ll also use a **Pretrained Network**, and **Customize** it.\n\n![New Image Classification](images\/new-image-classification-model-attempt2.png?raw=true \"New Image Classification\")\n\nIn the pretrained model\u2019s definition (i.e., prototext), we need to rename all\nreferences to the final **Fully Connected Layer** (where the end result classifications\nhappen).  We do this because we want the model to re-learn new categories from\nour dataset vs. its original training data (i.e., we want to throw away the current\nfinal layer).  We have to rename the last fully connected layer from \u201cfc8\u201d to\nsomething else, \u201cfc9\u201d for example.  Finally, we also need to adjust the number\nof categories from `1000` to `2`, by changing `num_output` to `2`.\n\nHere are the changes we need to make:\n\n```diff\n@@ -332,8 +332,8 @@\n }\n layer {\n-  name: \"fc8\"\n+  name: \"fc9\"\n   type: \"InnerProduct\"\n   bottom: \"fc7\"\n-  top: \"fc8\"\n+  top: \"fc9\"\n   param {\n     lr_mult: 1\n@@ -345,5 +345,5 @@\n   }\n   inner_product_param {\n-    num_output: 1000\n+    num_output: 2\n     weight_filler {\n       type: \"gaussian\"\n@@ -359,5 +359,5 @@\n   name: \"accuracy\"\n   type: \"Accuracy\"\n-  bottom: \"fc8\"\n+  bottom: \"fc9\"\n   bottom: \"label\"\n   top: \"accuracy\"\n@@ -367,5 +367,5 @@\n   name: \"loss\"\n   type: \"SoftmaxWithLoss\"\n-  bottom: \"fc8\"\n+  bottom: \"fc9\"\n   bottom: \"label\"\n   top: \"loss\"\n@@ -375,5 +375,5 @@\n   name: \"softmax\"\n   type: \"Softmax\"\n-  bottom: \"fc8\"\n+  bottom: \"fc9\"\n   top: \"softmax\"\n   include { stage: \"deploy\" }\n```\n\nI\u2019ve included the fully modified file I\u2019m using in [src\/alexnet-customized.prototxt](src\/alexnet-customized.prototxt).\n\nThis time our accuracy starts at ~60% and climbs right away to 87.5%, then to 96%\nand all the way up to 100%, with the Loss steadily decreasing. After 5 minutes we\nend up with an accuracy of 100% and a loss of 0.0009.\n\n![Model Attempt 2](images\/model-attempt2.png?raw=true \"Model Attempt 2\")\n\nTesting the same seahorse image our previous network got wrong, we see a complete\nreversal: 100% seahorse.\n\n![Model 2 Classify 1](images\/model-attempt2-classify1.png?raw=true \"Model 2 Classify 1\")\n\nEven a children\u2019s drawing of a seahorse works:\n\n![Model 2 Classify 2](images\/model-attempt2-classify2.png?raw=true \"Model 2 Classify 2\")\n\nThe same goes for a dolphin:\n\n![Model 2 Classify 3](images\/model-attempt2-classify3.png?raw=true \"Model 2 Classify 3\")\n\nEven with images that you think might be hard, like this one that has multiple dolphins\nclose together, and with their bodies mostly underwater, it does the right thing:\n\n![Model 2 Classify 4](images\/model-attempt2-classify4.png?raw=true \"Model 2 Classify 4\")\n\n### Training: Attempt 3, Fine Tuning GoogLeNet\n\nLike the previous AlexNet model we used for fine tuning, we can use GoogLeNet as well.\nModifying the network is a bit trickier, since you have to redefine three fully\nconnected layers instead of just one.\n\nTo fine tune GoogLeNet for our use case, we need to once again create a\nnew **Classification Model**:\n\n![New Classification Model](images\/new-image-classification-model-attempt3.png?raw=true \"New Classification Model\")\n\nWe rename all references to the three fully connected classification layers,\n`loss1\/classifier`, `loss2\/classifier`, and `loss3\/classifier`, and redefine\nthe number of categories (`num_output: 2`).  Here are the changes we need to make\nin order to rename the 3 classifier layers, as well as to change from 1000 to 2 categories:\n\n```diff\n@@ -917,10 +917,10 @@\n   exclude { stage: \"deploy\" }\n }\n layer {\n-  name: \"loss1\/classifier\"\n+  name: \"loss1a\/classifier\"\n   type: \"InnerProduct\"\n   bottom: \"loss1\/fc\"\n-  top: \"loss1\/classifier\"\n+  top: \"loss1a\/classifier\"\n   param {\n     lr_mult: 1\n     decay_mult: 1\n@@ -930,7 +930,7 @@\n     decay_mult: 0\n   }\n   inner_product_param {\n-    num_output: 1000\n+    num_output: 2\n     weight_filler {\n       type: \"xavier\"\n       std: 0.0009765625\n@@ -945,7 +945,7 @@\n layer {\n   name: \"loss1\/loss\"\n   type: \"SoftmaxWithLoss\"\n-  bottom: \"loss1\/classifier\"\n+  bottom: \"loss1a\/classifier\"\n   bottom: \"label\"\n   top: \"loss1\/loss\"\n   loss_weight: 0.3\n@@ -954,7 +954,7 @@\n layer {\n   name: \"loss1\/top-1\"\n   type: \"Accuracy\"\n-  bottom: \"loss1\/classifier\"\n+  bottom: \"loss1a\/classifier\"\n   bottom: \"label\"\n   top: \"loss1\/accuracy\"\n   include { stage: \"val\" }\n@@ -962,7 +962,7 @@\n layer {\n   name: \"loss1\/top-5\"\n   type: \"Accuracy\"\n-  bottom: \"loss1\/classifier\"\n+  bottom: \"loss1a\/classifier\"\n   bottom: \"label\"\n   top: \"loss1\/accuracy-top5\"\n   include { stage: \"val\" }\n@@ -1705,10 +1705,10 @@\n   exclude { stage: \"deploy\" }\n }\n layer {\n-  name: \"loss2\/classifier\"\n+  name: \"loss2a\/classifier\"\n   type: \"InnerProduct\"\n   bottom: \"loss2\/fc\"\n-  top: \"loss2\/classifier\"\n+  top: \"loss2a\/classifier\"\n   param {\n     lr_mult: 1\n     decay_mult: 1\n@@ -1718,7 +1718,7 @@\n     decay_mult: 0\n   }\n   inner_product_param {\n-    num_output: 1000\n+    num_output: 2\n     weight_filler {\n       type: \"xavier\"\n       std: 0.0009765625\n@@ -1733,7 +1733,7 @@\n layer {\n   name: \"loss2\/loss\"\n   type: \"SoftmaxWithLoss\"\n-  bottom: \"loss2\/classifier\"\n+  bottom: \"loss2a\/classifier\"\n   bottom: \"label\"\n   top: \"loss2\/loss\"\n   loss_weight: 0.3\n@@ -1742,7 +1742,7 @@\n layer {\n   name: \"loss2\/top-1\"\n   type: \"Accuracy\"\n-  bottom: \"loss2\/classifier\"\n+  bottom: \"loss2a\/classifier\"\n   bottom: \"label\"\n   top: \"loss2\/accuracy\"\n   include { stage: \"val\" }\n@@ -1750,7 +1750,7 @@\n layer {\n   name: \"loss2\/top-5\"\n   type: \"Accuracy\"\n-  bottom: \"loss2\/classifier\"\n+  bottom: \"loss2a\/classifier\"\n   bottom: \"label\"\n   top: \"loss2\/accuracy-top5\"\n   include { stage: \"val\" }\n@@ -2435,10 +2435,10 @@\n   }\n }\n layer {\n-  name: \"loss3\/classifier\"\n+  name: \"loss3a\/classifier\"\n   type: \"InnerProduct\"\n   bottom: \"pool5\/7x7_s1\"\n-  top: \"loss3\/classifier\"\n+  top: \"loss3a\/classifier\"\n   param {\n     lr_mult: 1\n     decay_mult: 1\n@@ -2448,7 +2448,7 @@\n     decay_mult: 0\n   }\n   inner_product_param {\n-    num_output: 1000\n+    num_output: 2\n     weight_filler {\n       type: \"xavier\"\n     }\n@@ -2461,7 +2461,7 @@\n layer {\n   name: \"loss3\/loss\"\n   type: \"SoftmaxWithLoss\"\n-  bottom: \"loss3\/classifier\"\n+  bottom: \"loss3a\/classifier\"\n   bottom: \"label\"\n   top: \"loss\"\n   loss_weight: 1\n@@ -2470,7 +2470,7 @@\n layer {\n   name: \"loss3\/top-1\"\n   type: \"Accuracy\"\n-  bottom: \"loss3\/classifier\"\n+  bottom: \"loss3a\/classifier\"\n   bottom: \"label\"\n   top: \"accuracy\"\n   include { stage: \"val\" }\n@@ -2478,7 +2478,7 @@\n layer {\n   name: \"loss3\/top-5\"\n   type: \"Accuracy\"\n-  bottom: \"loss3\/classifier\"\n+  bottom: \"loss3a\/classifier\"\n   bottom: \"label\"\n   top: \"accuracy-top5\"\n   include { stage: \"val\" }\n@@ -2489,7 +2489,7 @@\n layer {\n   name: \"softmax\"\n   type: \"Softmax\"\n-  bottom: \"loss3\/classifier\"\n+  bottom: \"loss3a\/classifier\"\n   top: \"softmax\"\n   include { stage: \"deploy\" }\n }\n```\n\nI\u2019ve put the complete file in [src\/googlenet-customized.prototxt](src\/googlenet-customized.prototxt).\n\n> Q: \"What about changes to the prototext definitions of these networks?\n> We changed the fully connected layer name(s), and the number of categories.\n> What else could, or should be changed, and in what circumstances?\"\n\nGreat question, and it's something I'm wondering, too.  For example, I know that we can\n[\"fix\" certain layers](https:\/\/github.com\/BVLC\/caffe\/wiki\/Fine-Tuning-or-Training-Certain-Layers-Exclusively)\nso the weights don't change.  Doing other things involves understanding how the layers work,\nwhich is beyond this guide, and also beyond its author at present!\n\nLike we did with fine tuning AlexNet, we also reduce the learning rate by\n10% from `0.01` to `0.001`.\n\n> Q: \"What other changes would make sense when fine tuning these networks?\n> What about different numbers of epochs, batch sizes, solver types (Adam, AdaDelta, AdaGrad, etc),\n> learning rates, policies (Exponential Decay, Inverse Decay, Sigmoid Decay, etc),\n> step sizes, and gamma values?\"\n\nGreat question, and one that I wonder about as well.  I only have a vague understanding of these\nand it\u2019s likely that there are improvements we can make if you know how to alter these\nvalues when training.  This is something that needs better documentation.\n\nBecause GoogLeNet has a more complicated architecture than AlexNet, fine tuning it requires\nmore time.  On my laptop, it takes 10 minutes to retrain GoogLeNet with our dataset,\nachieving 100% accuracy and a loss of 0.0070:\n\n![Model Attempt 3](images\/model-attempt3.png?raw=true \"Model Attempt 3\")\n\nJust as we saw with the fine tuned version of AlexNet, our modified GoogLeNet\nperforms amazing well--the best so far:\n\n![Model Attempt 3 Classify 1](images\/model-attempt3-classify1.png?raw=true \"Model Attempt 3 Classify 1\")\n\n![Model Attempt 3 Classify 2](images\/model-attempt3-classify2.png?raw=true \"Model Attempt 3 Classify 2\")\n\n![Model Attempt 3 Classify 3](images\/model-attempt3-classify3.png?raw=true \"Model Attempt 3 Classify 3\")\n\n## Using our Model\n\nWith our network trained and tested, it\u2019s time to download and use it.  Each of the models\nwe trained in DIGITS has a **Download Model** button, as well as a way to select different\nsnapshots within our training run (e.g., `Epoch #30`):\n\n![Trained Models](images\/trained-models.png?raw=true \"Trained Models\")\n\nClicking **Download Model** downloads a `tar.gz` archive containing the following files:\n\n```\ndeploy.prototxt\nmean.binaryproto\nsolver.prototxt\ninfo.json\noriginal.prototxt\nlabels.txt\nsnapshot_iter_90.caffemodel\ntrain_val.prototxt\n```\n\nThere\u2019s a [nice description](https:\/\/github.com\/BVLC\/caffe\/wiki\/Using-a-Trained-Network:-Deploy) in\nthe Caffe documentation about how to use the model we just built.  It says:\n\n> A network is defined by its design (.prototxt), and its weights (.caffemodel). As a network is\n> being trained, the current state of that network's weights are stored in a .caffemodel. With both\n> of these we can move from the train\/test phase into the production phase.\n>\n> In its current state, the design of the network is not designed for deployment. Before we can\n> release our network as a product, we often need to alter it in a few ways:\n>\n> 1. Remove the data layer that was used for training, as for in the case of classification we are no longer providing labels for our data.\n> 2. Remove any layer that is dependent upon data labels.\n> 3. Set the network up to accept data.\n> 4. Have the network output the result.\n\nDIGITS has already done the work for us, separating out the different versions of our `prototxt` files.\nThe files we\u2019ll care about when using this network are:\n\n* `deploy.prototxt` - the definition of our network, ready for accepting image input data\n* `mean.binaryproto` - our model will need us to subtract the image mean from each image that it processes, and this is the mean image.\n* `labels.txt` - a list of our labels (`dolphin`, `seahorse`) in case we want to print them vs. just the category number\n* `snapshot_iter_90.caffemodel` - these are the trained weights for our network\n\nWe can use these files in a number of ways to classify new images.  For example, in our\n`CAFFE_ROOT` we can use `build\/examples\/cpp_classification\/classification.bin` to classify one image:\n\n```bash\n$ cd $CAFFE_ROOT\/build\/examples\/cpp_classification\n$ .\/classification.bin deploy.prototxt snapshot_iter_90.caffemodel mean.binaryproto labels.txt dolphin1.jpg\n```\n\nThis will spit out a bunch of debug text, followed by the predictions for each of our two categories:\n\n```\n0.9997 - \u201cdolphin\u201d\n0.0003 - \u201cseahorse\u201d\n```\n\nYou can read the [complete C++ source](https:\/\/github.com\/BVLC\/caffe\/tree\/master\/examples\/cpp_classification)\nfor this in the [Caffe examples](https:\/\/github.com\/BVLC\/caffe\/tree\/master\/examples).\n\nFor a classification version that uses the Python interface, DIGITS includes a [nice example](https:\/\/github.com\/NVIDIA\/DIGITS\/tree\/master\/examples\/classification).  There's also a fairly\n[well documented Python walkthrough](https:\/\/github.com\/BVLC\/caffe\/blob\/master\/examples\/00-classification.ipynb) in the Caffe examples.\n\n### Python example\n\nLet's write a program that uses our fine-tuned GoogLeNet model to classify the untrained images\nwe have in [data\/untrained-samples](data\/untrained-samples).  I've cobbled this together based on\nthe examples above, as well as the `caffe` [Python module's source](https:\/\/github.com\/BVLC\/caffe\/tree\/master\/python),\nwhich you should prefer to anything I'm about to say.\n\nA full version of what I'm going to discuss is available in [src\/classify-samples.py](src\/classify-samples.py).\nLet's begin!\n\nFirst, we'll need the [NumPy](http:\/\/www.numpy.org\/) module.  In a moment we'll be using [NumPy](http:\/\/www.numpy.org\/)\nto work with [`ndarray`s](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.ndarray.html), which Caffe uses a lot.\nIf you haven't used them before, as I had not, you'd do well to begin by reading this\n[Quickstart tutorial](https:\/\/docs.scipy.org\/doc\/numpy-dev\/user\/quickstart.html).\n\nSecond, we'll need to load the `caffe` module from our `CAFFE_ROOT` dir.  If it's not already included\nin your Python environment, you can force it to load by adding it manually. Along with it we'll\nalso import caffe's protobuf module:\n\n```python\nimport numpy as np\n\ncaffe_root = '\/path\/to\/your\/caffe_root'\nsys.path.insert(0, os.path.join(caffe_root, 'python'))\nimport caffe\nfrom caffe.proto import caffe_pb2\n```\n\nNext we need to tell Caffe whether to [use the CPU or GPU](https:\/\/github.com\/BVLC\/caffe\/blob\/61944afd4e948a4e2b4ef553919a886a8a8b8246\/python\/caffe\/_caffe.cpp#L50-L52).\nFor our experiments, the CPU is fine:\n\n```python\ncaffe.set_mode_cpu()\n```\n\nNow we can use `caffe` to load our trained network.  To do so, we'll need some of the files we downloaded\nfrom DIGITS, namely:\n\n* `deploy.prototxt` - our \"network file\", the description of the network.\n* `snapshot_iter_90.caffemodel` - our trained \"weights\"\n\nWe obviously need to provide the full path, and I'll assume that my files are in a dir called `model\/`:\n\n```python\nmodel_dir = 'model'\ndeploy_file = os.path.join(model_dir, 'deploy.prototxt')\nweights_file = os.path.join(model_dir, 'snapshot_iter_90.caffemodel')\nnet = caffe.Net(deploy_file, caffe.TEST, weights=weights_file)\n```\n\nThe `caffe.Net()` [constructor](https:\/\/github.com\/BVLC\/caffe\/blob\/61944afd4e948a4e2b4ef553919a886a8a8b8246\/python\/caffe\/_caffe.cpp#L91-L117)\ntakes a network file, a phase (`caffe.TEST` or `caffe.TRAIN`), as well as an optional weights filename.  When\nwe provide a weights file, the `Net` will automatically load them for us. The `Net` has a number of\n[methods and attributes](https:\/\/github.com\/BVLC\/caffe\/blob\/master\/python\/caffe\/pycaffe.py) you can use.\n\n**Note:** There is also a [deprecated version of this constructor](https:\/\/github.com\/BVLC\/caffe\/blob\/61944afd4e948a4e2b4ef553919a886a8a8b8246\/python\/caffe\/_caffe.cpp#L119-L134),\nwhich seems to get used often in sample code on the web. It looks like this, in case you encounter it:\n\n```python\nnet = caffe.Net(str(deploy_file), str(model_file), caffe.TEST)\n```\n\nWe're interested in loading images of various sizes into our network for testing. As a result,\nwe'll need to *transform* them into a shape that our network can use (i.e., colour, 256x256).\nCaffe provides the [`Transformer` class](https:\/\/github.com\/BVLC\/caffe\/blob\/61944afd4e948a4e2b4ef553919a886a8a8b8246\/python\/caffe\/io.py#L98)\nfor this purpose.  We'll use it to create a transformation appropriate for our images\/network:\n\n```python\ntransformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n# set_transpose: https:\/\/github.com\/BVLC\/caffe\/blob\/61944afd4e948a4e2b4ef553919a886a8a8b8246\/python\/caffe\/io.py#L187\ntransformer.set_transpose('data', (2, 0, 1))\n# set_raw_scale: https:\/\/github.com\/BVLC\/caffe\/blob\/61944afd4e948a4e2b4ef553919a886a8a8b8246\/python\/caffe\/io.py#L221\ntransformer.set_raw_scale('data', 255)\n# set_channel_swap: https:\/\/github.com\/BVLC\/caffe\/blob\/61944afd4e948a4e2b4ef553919a886a8a8b8246\/python\/caffe\/io.py#L203\ntransformer.set_channel_swap('data', (2, 1, 0))\n```\n\nWe can also use the `mean.binaryproto` file DIGITS gave us to set our transformer's mean:\n\n```python\n# This code for setting the mean from https:\/\/github.com\/NVIDIA\/DIGITS\/tree\/master\/examples\/classification\nmean_file = os.path.join(model_dir, 'mean.binaryproto')\nwith open(mean_file, 'rb') as infile:\n    blob = caffe_pb2.BlobProto()\n    blob.MergeFromString(infile.read())\n    if blob.HasField('shape'):\n        blob_dims = blob.shape\n        assert len(blob_dims) == 4, 'Shape should have 4 dimensions - shape is %s' % blob.shape\n    elif blob.HasField('num') and blob.HasField('channels') and \\\n            blob.HasField('height') and blob.HasField('width'):\n        blob_dims = (blob.num, blob.channels, blob.height, blob.width)\n    else:\n        raise ValueError('blob does not provide shape or 4d dimensions')\n    pixel = np.reshape(blob.data, blob_dims[1:]).mean(1).mean(1)\n    transformer.set_mean('data', pixel)\n```\n\nIf we had a lot of labels, we might also choose to read in our labels file, which we can use\nlater by looking up the label for a probability using its position (e.g., 0=dolphin, 1=seahorse):\n\n```python\nlabels_file = os.path.join(model_dir, 'labels.txt')\nlabels = np.loadtxt(labels_file, str, delimiter='\\n')\n``` \n\nNow we're ready to classify an image.  We'll use [`caffe.io.load_image()`](https:\/\/github.com\/BVLC\/caffe\/blob\/61944afd4e948a4e2b4ef553919a886a8a8b8246\/python\/caffe\/io.py#L279)\nto read our image file, then use our transformer to reshape it and set it as our network's data layer:\n\n```python\n# Load the image from disk using caffe's built-in I\/O module\nimage = caffe.io.load_image(fullpath)\n# Preprocess the image into the proper format for feeding into the model\nnet.blobs['data'].data[...] = transformer.preprocess('data', image)\n```\n\n> Q: \"How could I use images (i.e., frames) from a camera or video stream instead of files?\"\n\nGreat question, here's a skeleton to get you started:\n\n```python\nimport cv2\n...\n# Get the shape of our input data layer, so we can resize the image\ninput_shape = net.blobs['data'].data.shape\n...\nwebCamCap = cv2.VideoCapture(0) # could also be a URL, filename\nif webCamCap.isOpened():\n    rval, frame = webCamCap.read()\nelse:\n    rval = False\n\nwhile rval:\n    rval, frame = webCamCap.read()\n    net.blobs['data'].data[...] = transformer.preprocess('data', frame)\n    ...\n\nwebCamCap.release()\n```\n\nBack to our problem, we next need to run the image data through our network and read out\nthe probabilities from our network's final `'softmax'` layer, which will be in order by label category:\n\n```python\n# Run the image's pixel data through the network\nout = net.forward()\n# Extract the probabilities of our two categories from the final layer\nsoftmax_layer = out['softmax']\n# Here we're converting to Python types from ndarray floats\ndolphin_prob = softmax_layer.item(0)\nseahorse_prob = softmax_layer.item(1)\n\n# Print the results. I'm using labels just to show how it's done\nlabel = labels[0] if dolphin_prob > seahorse_prob else labels[1]\nfilename = os.path.basename(fullpath)\nprint '%s is a %s dolphin=%.3f%% seahorse=%.3f%%' % (filename, label, dolphin_prob*100, seahorse_prob*100)\n```\n\nRunning the full version of this (see [src\/classify-samples.py](src\/classify-samples.py)) using our\nfine-tuned GoogLeNet network on our [data\/untrained-samples](data\/untrained-samples) images gives\nme the following output:\n\n```\n[...truncated caffe network output...]\ndolphin1.jpg is a dolphin dolphin=99.968% seahorse=0.032%\ndolphin2.jpg is a dolphin dolphin=99.997% seahorse=0.003%\ndolphin3.jpg is a dolphin dolphin=99.943% seahorse=0.057%\nseahorse1.jpg is a seahorse dolphin=0.365% seahorse=99.635%\nseahorse2.jpg is a seahorse dolphin=0.000% seahorse=100.000%\nseahorse3.jpg is a seahorse dolphin=0.014% seahorse=99.986%\n```\n\nI'm still trying to learn all the best practices for working with models in code. I wish I had more\nand better documented code examples, APIs, premade modules, etc to show you here. To be honest,\nmost of the code examples I\u2019ve found are terse, and poorly documented--Caffe\u2019s\ndocumentation is spotty, and assumes a lot.\n\nIt seems to me like there\u2019s an opportunity for someone to build higher-level tools on top of the\nCaffe interfaces for beginners and basic workflows like we've done here.  It would be great if\nthere were more simple modules in high-level languages that I could point you at that \u201cdid the\nright thing\u201d with our model; someone could\/should take this on, and make *using* Caffe\nmodels as easy as DIGITS makes *training* them.  I\u2019d love to have something I could use in node.js,\nfor example.  Ideally one shouldn\u2019t be required to know so much about the internals of the model or Caffe.\nI haven\u2019t used it yet, but [DeepDetect](https:\/\/deepdetect.com\/) looks interesting on this front,\nand there are likely many other tools I don\u2019t know about.\n\n## Results\n\nAt the beginning we said that our goal was to write a program that used a neural network to\ncorrectly classify all of the images in [data\/untrained-samples](data\/untrained-samples).\nThese are images of dolphins and seahorses that were never used in the training or validation\ndata:\n\n### Untrained Dolphin Images\n\n![Dolphin 1](data\/untrained-samples\/dolphin1.jpg?raw=true \"Dolphin 1\")\n![Dolphin 2](data\/untrained-samples\/dolphin2.jpg?raw=true \"Dolphin 2\")\n![Dolphin 3](data\/untrained-samples\/dolphin3.jpg?raw=true \"Dolphin 3\")\n\n### Untrained Seahorse Images\n\n![Seahorse 1](data\/untrained-samples\/seahorse1.jpg?raw=true \"Seahorse 1\")\n![Seahorse 2](data\/untrained-samples\/seahorse2.jpg?raw=true \"Seahorse 2\")\n![Seahorse 3](data\/untrained-samples\/seahorse3.jpg?raw=true \"Seahorse 3\")\n\nLet's look at how each of our three attempts did with this challenge:\n\n### Model Attempt 1: AlexNet from Scratch (3rd Place)\n\n| Image | Dolphin | Seahorse | Result | \n|-------|---------|----------|--------|\n|[dolphin1.jpg](data\/untrained-samples\/dolphin1.jpg)| 71.11% | 28.89% | :expressionless: |\n|[dolphin2.jpg](data\/untrained-samples\/dolphin2.jpg)| 99.2% | 0.8% | :sunglasses: |\n|[dolphin3.jpg](data\/untrained-samples\/dolphin3.jpg)| 63.3% | 36.7% | :confused: |\n|[seahorse1.jpg](data\/untrained-samples\/seahorse1.jpg)| 95.04% | 4.96% | :disappointed: |\n|[seahorse2.jpg](data\/untrained-samples\/seahorse2.jpg)| 56.64% | 43.36 |  :confused: |\n|[seahorse3.jpg](data\/untrained-samples\/seahorse3.jpg)| 7.06% | 92.94% |  :grin: |\n\n### Model Attempt 2: Fine Tuned AlexNet (2nd Place)\n\n| Image | Dolphin | Seahorse | Result | \n|-------|---------|----------|--------|\n|[dolphin1.jpg](data\/untrained-samples\/dolphin1.jpg)| 99.1% | 0.09% |  :sunglasses: |\n|[dolphin2.jpg](data\/untrained-samples\/dolphin2.jpg)| 99.5% | 0.05% |  :sunglasses: |\n|[dolphin3.jpg](data\/untrained-samples\/dolphin3.jpg)| 91.48% | 8.52% |  :grin: |\n|[seahorse1.jpg](data\/untrained-samples\/seahorse1.jpg)| 0% | 100% |  :sunglasses: |\n|[seahorse2.jpg](data\/untrained-samples\/seahorse2.jpg)| 0% | 100% |  :sunglasses: |\n|[seahorse3.jpg](data\/untrained-samples\/seahorse3.jpg)| 0% | 100% |  :sunglasses: |\n\n### Model Attempt 3: Fine Tuned GoogLeNet (1st Place)\n\n| Image | Dolphin | Seahorse | Result | \n|-------|---------|----------|--------|\n|[dolphin1.jpg](data\/untrained-samples\/dolphin1.jpg)| 99.86% | 0.14% |  :sunglasses: |\n|[dolphin2.jpg](data\/untrained-samples\/dolphin2.jpg)| 100% | 0% |  :sunglasses: |\n|[dolphin3.jpg](data\/untrained-samples\/dolphin3.jpg)| 100% | 0% |  :sunglasses: |\n|[seahorse1.jpg](data\/untrained-samples\/seahorse1.jpg)| 0.5% | 99.5% |  :sunglasses: |\n|[seahorse2.jpg](data\/untrained-samples\/seahorse2.jpg)| 0% | 100% |  :sunglasses: |\n|[seahorse3.jpg](data\/untrained-samples\/seahorse3.jpg)| 0.02% | 99.98% |  :sunglasses: |\n\n## Conclusion\n\nIt\u2019s amazing how well our model works, and what\u2019s possible by fine tuning a pretrained network.\nObviously our dolphin vs. seahorse example is contrived, and the dataset overly limited--we really\ndo want more and better data if we want our network to be robust.  But since our goal was to examine\nthe tools and workflows of neural networks, it\u2019s turned out to be an ideal case, especially since it\ndidn\u2019t require expensive equipment or massive amounts of time.\n\nAbove all I hope that this experience helps to remove the overwhelming fear of getting started.\nDeciding whether or not it\u2019s worth investing time in learning the theories of machine learning and\nneural networks is easier when you\u2019ve been able to see it work in a small way.  Now that you\u2019ve got\na setup and a working approach, you can try doing other sorts of classifications.  You might also look\nat the other types of things you can do with Caffe and DIGITS, for example, finding objects within an\nimage, or doing segmentation.\n\nHave fun with machine learning!\n","128":"## whereami\n\n[![Build Status](https:\/\/travis-ci.org\/kootenpv\/whereami.svg?branch=master)](https:\/\/travis-ci.org\/kootenpv\/whereami)\n[![Coverage Status](https:\/\/coveralls.io\/repos\/github\/kootenpv\/whereami\/badge.svg?branch=master)](https:\/\/coveralls.io\/github\/kootenpv\/whereami?branch=master)\n[![PyPI](https:\/\/img.shields.io\/pypi\/v\/whereami.svg?style=flat-square)](https:\/\/pypi.python.org\/pypi\/whereami\/)\n[![PyPI](https:\/\/img.shields.io\/pypi\/pyversions\/whereami.svg?style=flat-square)](https:\/\/pypi.python.org\/pypi\/whereami\/)\n\nUses WiFi signals and machine learning (sklearn's RandomForest) to predict where you are. Even works for small distances like 2-10 meters.\n\nYour computer will known whether you are on Couch #1 or Couch #2.\n\n## Cross-platform\n\nWorks on OSX, Windows, Linux (tested on Ubuntu\/Arch Linux).\n\nThe package [access_points](https:\/\/github.com\/kootenpv\/access_points) was created in the process to allow scanning wifi in a cross platform manner. Using `access_points` at command-line will allow you to scan wifi yourself and get JSON output.\n`whereami` builds on top of it.\n\n### Installation\n\n    pip install whereami\n\n### Usage\n\n```bash\n# in your bedroom, takes a sample\nwhereami learn -l bedroom\n\n# in your kitchen, takes a sample\nwhereami learn -l kitchen\n\n# get a list of already learned locations\nwhereami locations\n\n# cross-validated accuracy on historic data\nwhereami crossval\n# 0.99319\n\n# use in other applications, e.g. by piping the most likely answer:\nwhereami predict | say\n# Computer Voice says: \"bedroom\"\n\n# probabilities per class\nwhereami predict_proba\n# {\"bedroom\": 0.99, \"kitchen\": 0.01}\n```\n\nIf you want to delete some of the last lines, or the data in general, visit your `$USER\/.whereami` folder.\n\n### Python\n\nAny of the functionality is available in python as well. Generally speaking, commands can be imported:\n\n    from whereami import learn\n    from whereami import get_pipeline\n    from whereami import predict, predict_proba, crossval, locations\n\n### Accuracy\nk\nGenerally it should work really well. I've been able to learn using only 7 access points at home (test using `access_points -n`). At organizations you might see 70+.\n\nDistance: anything around ~10 meters or more should get >99% accuracy.\n\nIf you're adventurous and you want to learn to distinguish between couch #1 and couch #2 (i.e. 2 meters apart), it is the most robust when you switch locations and train in turn. E.g. first in Spot A, then in Spot B then start again with A.\nDoing this in spot A, then spot B and then immediately using \"predict\" will yield spot B as an answer usually. No worries, the effect of this temporal overfitting disappears over time. And, in fact, this is only a real concern for the very short distances. Just take a sample after some time in both locations and it should become very robust.\n\nHeight: Surprisingly, vertical difference in location is typically even more distinct than horizontal differences.\n\n### Related Projects\n- The [wherearehue](https:\/\/github.com\/DeastinY\/wherearehue) project can be used to toggle Hue light bulbs based on the learned locations.\n\n###  Almost entirely \"copied\" from:\n\nhttps:\/\/github.com\/schollz\/find\n\nThat project used to be in Python, but is now written in Go. `whereami` is in Python with lessons learned implemented.\n\n### Tests\n\nIt's possible to locally run tests for python 2.7, 3.4 and 3.5 using tox.\n\n    git clone https:\/\/github.com\/kootenpv\/whereami\n    cd whereami\n    python setup.py install\n    tox\n","129":"![AugmentorLogo](https:\/\/github.com\/mdbloice\/AugmentorFiles\/blob\/master\/Misc\/AugmentorLogo.png)\n\nAugmentor is an image augmentation library in Python for machine learning. It aims to be a standalone library that is platform and framework independent, which is more convenient, allows for finer grained control over augmentation, and implements the most real-world relevant augmentation techniques. It employs a stochastic approach using building blocks that allow for operations to be pieced together in a pipeline.\n\n[![PyPI](https:\/\/img.shields.io\/badge\/Augmentor-v0.2.10-blue.svg?maxAge=2592000)](https:\/\/pypi.python.org\/pypi\/Augmentor)\n[![Supported Python Versions](https:\/\/img.shields.io\/badge\/python-2.7%20%7C%203.5%20%7C%203.6%20%7C%203.7%20%7C%203.8%20%7C%203.9-blue.svg)](https:\/\/pypi.python.org\/pypi\/Augmentor)\n[![PyPI Install](https:\/\/github.com\/mdbloice\/Augmentor\/actions\/workflows\/PyPI.yml\/badge.svg)](https:\/\/github.com\/mdbloice\/Augmentor\/actions\/workflows\/PyPI.yml)\n[![Pytest](https:\/\/github.com\/mdbloice\/Augmentor\/actions\/workflows\/package-tests.yml\/badge.svg)](https:\/\/github.com\/mdbloice\/Augmentor\/actions\/workflows\/package-tests.yml)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/augmentor\/badge\/?version=master)](https:\/\/augmentor.readthedocs.io\/en\/master\/?badge=master)\n[![License](http:\/\/img.shields.io\/badge\/license-MIT-brightgreen.svg?style=flat)](LICENSE.md)\n[![Project Status: Active \u2013 The project has reached a stable, usable state and is being actively developed.](http:\/\/www.repostatus.org\/badges\/latest\/active.svg)](http:\/\/www.repostatus.org\/#active)\n[![Binder](https:\/\/mybinder.org\/badge.svg)](https:\/\/mybinder.org\/v2\/gh\/4QuantOSS\/Augmentor\/master)\n\n## Installation\n\nAugmentor is written in Python. A Julia version of the package is also being developed as a sister project and is available [here](https:\/\/github.com\/Evizero\/Augmentor.jl).\n\nInstall using `pip` from the command line:\n\n```python\npip install Augmentor\n```\n\nSee the documentation for building from source. To upgrade from a previous version, use `pip install Augmentor --upgrade`.\n\n## Documentation\n\nComplete documentation can be found on Read the Docs: <http:\/\/augmentor.readthedocs.io\/>\n\n## Quick Start Guide and Usage\nThe purpose of _Augmentor_ is to automate image augmentation (artificial data generation) in order to expand datasets as input for machine learning algorithms, especially neural networks and deep learning.\n\nThe package works by building an augmentation **pipeline** where you define a series of operations to perform on a set of images. Operations, such as rotations or transforms, are added one by one to create an augmentation pipeline: when complete, the pipeline can be executed and an augmented dataset is created.\n\nTo begin, instantiate a `Pipeline` object that points to a directory on your file system:\n\n```python\nimport Augmentor\np = Augmentor.Pipeline(\"\/path\/to\/images\")\n```\n\nYou can then add operations to the Pipeline object `p` as follows:\n\n```python\np.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\np.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n```\n\nEvery function requires you to specify a probability, which is used to decide if an operation is applied to an image as it is passed through the augmentation pipeline.\n\nOnce you have created a pipeline, you can sample from it like so:\n\n```python\np.sample(10000)\n```\n\nwhich will generate 10,000 augmented images based on your specifications. By default these will be written to the disk in a directory named `output` relative to the path specified when initialising the `p` pipeline object above.\n\nIf you wish to process each image in the pipeline exactly once, use `process()`:\n\n```python\np.process()\n```\n\nThis function might be useful for resizing a dataset for example. It would make sense to create a pipeline where all of its operations have their probability set to `1` when using the `process()` method.\n\n### Multi-threading\n\nAugmentor (version >=0.2.1) now uses multi-threading to increase the speed of generating images.\n\nThis *may* slow down some pipelines if the original images are very small. Set `multi_threaded` to ``False`` if slowdown is experienced:\n\n```python\np.sample(100, multi_threaded=False)\n```\n\nHowever, by default the `sample()` function uses multi-threading. This is currently only implemented when saving to disk. Generators will use multi-threading in the next version update.\n\n\n### Ground Truth Data\n\nImages can be passed through the pipeline in groups of two or more so that ground truth data can be identically augmented.\n\n| Original image and mask<sup>[3]<\/sup>                                                                               | Augmented original and mask images                                                                               |\n|---------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|\n| ![OriginalMask](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/original-with-mask.png) | ![AugmentedMask](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/ground-truth.gif)   |\n\nTo augment ground truth data in parallel to any original data, add a ground truth directory to a pipeline using the [ground_truth()](https:\/\/augmentor.readthedocs.io\/en\/master\/code.html#Augmentor.Pipeline.Pipeline.ground_truth) function:\n\n```python\np = Augmentor.Pipeline(\"\/path\/to\/images\")\n# Point to a directory containing ground truth data.\n# Images with the same file names will be added as ground truth data\n# and augmented in parallel to the original data.\np.ground_truth(\"\/path\/to\/ground_truth_images\")\n# Add operations to the pipeline as normal:\np.rotate(probability=1, max_left_rotation=5, max_right_rotation=5)\np.flip_left_right(probability=0.5)\np.zoom_random(probability=0.5, percentage_area=0.8)\np.flip_top_bottom(probability=0.5)\np.sample(50)\n```\n\n### Multiple Mask\/Image Augmentation\n\nUsing the `DataPipeline` class (Augmentor version >= 0.2.3), images that have multiple associated masks can be augmented:\n\n| Multiple Mask Augmentation                                                                               |\n|----------------------------------------------------------------------------------------------------------|\n| ![MultipleMask](https:\/\/github.com\/mdbloice\/AugmentorFiles\/blob\/master\/UsageGuide\/merged-multi-mask.gif) |\n\nArbitrarily long lists of images can be passed through the pipeline in groups and augmented identically using the `DataPipeline` class. This is useful for ground truth images that have several masks, for example.\n\nIn the example below, the images and their masks are contained in the `images` data structure (as lists of lists), while their labels are contained in `y`:\n\n```python\np = Augmentor.DataPipeline(images, y)\np.rotate(1, max_left_rotation=5, max_right_rotation=5)\np.flip_top_bottom(0.5)\np.zoom_random(1, percentage_area=0.5)\n\naugmented_images, labels = p.sample(100)\n```\n\nThe `DataPipeline` returns images directly (`augmented_images` above), and does not save them to disk, nor does it read data from the disk. Images are passed directly to `DataPipeline` during initialisation.\n\nFor details of the `images` data structure and how to create it, see the [`Multiple-Mask-Augmentation.ipynb`](https:\/\/github.com\/mdbloice\/Augmentor\/blob\/master\/notebooks\/Multiple-Mask-Augmentation.ipynb) Jupyter notebook.\n\n### Generators for Keras and PyTorch\n\nIf you do not wish to save to disk, you can use a generator (in this case with Keras):\n\n```python\ng = p.keras_generator(batch_size=128)\nimages, labels = next(g)\n```\n\nwhich returns a batch of images of size 128 and their corresponding labels. Generators return data indefinitely, and can be used to train neural networks with augmented data on the fly.\n\nAlternatively, you can integrate it with PyTorch:\n\n```python\nimport torchvision\ntransforms = torchvision.transforms.Compose([\n    p.torch_transform(),\n    torchvision.transforms.ToTensor(),\n])\n```\n\n## Main Features\n\n### Elastic Distortions\n\nUsing elastic distortions, one image can be used to generate many images that are real-world feasible and label preserving:\n\n| Input Image                                                                                                                       |   | Augmented Images                                                                                                        |\n|-----------------------------------------------------------------------------------------------------------------------------------|---|-------------------------------------------------------------------------------------------------------------------------|\n| ![eight_hand_drawn_border](https:\/\/cloud.githubusercontent.com\/assets\/16042756\/23697279\/79850d52-03e7-11e7-9445-475316b702a3.png) | \u2192 | ![eights_border](https:\/\/cloud.githubusercontent.com\/assets\/16042756\/23697283\/802698a6-03e7-11e7-94b7-f0b61977ef33.gif) |\n\nThe input image has a 1 pixel black border to emphasise that you are getting distortions without changing the size or aspect ratio of the original image, and without any black\/transparent padding around the newly generated images.\n\nThe functionality can be more clearly seen here:\n\n| Original Image<sup>[1]<\/sup>                                                                      | Random distortions applied                                                                            |\n|---------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/orig.png) | ![Distorted](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/distort.gif) |\n\n### Perspective Transforms\n\nThere are a total of 12 different types of perspective transform available. Four of the most common are shown below.\n\n| Tilt Left                                                                                               | Tilt Right                                                                                               | Tilt Forward                                                                                               | Tilt Backward                                                                                               |\n|---------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n| ![TiltLeft](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/TiltLeft_s.png) | ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/TiltRight_s.png) | ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/TiltForward_s.png) | ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/TiltBackward_s.png) |\n\nThe remaining eight types of transform are as follows:\n\n| Skew Type 0                                                                                         | Skew Type 1                                                                                         | Skew Type 2                                                                                         | Skew Type 3                                                                                         |\n|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| ![Skew0](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/Corner0_s.png) | ![Skew1](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/Corner1_s.png) | ![Skew2](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/Corner2_s.png) | ![Skew3](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/Corner3_s.png) |\n\n| Skew Type 4                                                                                         | Skew Type 5                                                                                         | Skew Type 6                                                                                         | Skew Type 7                                                                                         |\n|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| ![Skew4](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/Corner4_s.png) | ![Skew5](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/Corner5_s.png) | ![Skew6](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/Corner6_s.png) | ![Skew7](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/Corner7_s.png) |\n\n### Size Preserving Rotations\n\nRotations by default preserve the file size of the original images:\n\n| Original Image                                                                                    | Rotated 10 degrees, automatically cropped                                                               |\n|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/orig.png) | ![Rotate](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/rotate_aug_b.png) |\n\nCompared to rotations by other software:\n\n| Original Image                                                                                    | Rotated 10 degrees                                                                                |\n|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/orig.png) | ![Rotate](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/rotate.png) |\n\n### Size Preserving Shearing\n\nShearing will also automatically crop the correct area from the sheared image, so that you have an image with no black space or padding.\n\n| Original image                                                                                    | Shear (x-axis) 20 degrees                                                                              | Shear (y-axis) 20 degrees                                                                              |\n|---------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/orig.png) | ![ShearX](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/shear_x_aug.png) | ![ShearY](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/shear_y_aug.png) |\n\nCompare this to how this is normally done:\n\n| Original image                                                                                    | Shear (x-axis) 20 degrees                                                                          | Shear (y-axis) 20 degrees                                                                          |\n|---------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/orig.png) | ![ShearX](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/shear_x.png) | ![ShearY](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/shear_y.png) |\n\n### Cropping\n\nCropping can also be handled in a manner more suitable for machine learning image augmentation:\n\n| Original image                                                                                    | Random crops + resize operation                                                                          |\n|---------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/orig.png) | ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/crop_resize.gif) |\n\n### Random Erasing\n\nRandom Erasing is a technique used to make models robust to occlusion. This may be useful for training neural networks used in object detection in navigation scenarios, for example.\n\n| Original image<sup>[2]<\/sup>                                                                                               | Random Erasing                                                                                                                        |\n|----------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/city-road-street-italy-scaled.jpg) | ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/city-road-street-italy-animation.gif) |\n\nSee the [Pipeline.random_erasing()](http:\/\/augmentor.readthedocs.io\/en\/master\/code.html#Augmentor.Pipeline.Pipeline.random_erasing) documentation for usage.\n\n### Chaining Operations in a Pipeline\n\nWith only a few operations, a single image can be augmented to produce large numbers of new, label-preserving samples:\n\n| Original image                                                                                           | Distortions + mirroring                                                                                          |\n|----------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/eight_200px.png) | ![DistortFlipFlop](https:\/\/raw.githubusercontent.com\/mdbloice\/AugmentorFiles\/master\/UsageGuide\/flip_distort.gif) |\n\nIn the example above, we have applied three operations: first we randomly distort the image, then we flip it horizontally with a probability of 0.5 and then vertically with a probability of 0.5. We then sample from this pipeline 100 times to create 100 new data.\n\n```python\np.random_distortion(probability=1, grid_width=4, grid_height=4, magnitude=8)\np.flip_left_right(probability=0.5)\np.flip_top_bottom(probability=0.5)\np.sample(100)\n```\n\n## Tutorial Notebooks\n\n### Integration with Keras using Generators\nAugmentor can be used as a replacement for Keras' augmentation functionality. Augmentor can create a generator which produces augmented data indefinitely, according to the pipeline you have defined. See the following notebooks for details:\n\n- Reading images from a local directory, augmenting them at run-time, and using a generator to pass the augmented stream of images to a Keras convolutional neural network, see [`Augmentor_Keras.ipynb`](https:\/\/github.com\/mdbloice\/Augmentor\/blob\/master\/notebooks\/Augmentor_Keras.ipynb)\n- Augmenting data in-memory (in array format) and using a generator to pass these new images to the Keras neural network, see [`Augmentor_Keras_Array_Data.ipynb`](https:\/\/github.com\/mdbloice\/Augmentor\/blob\/master\/notebooks\/Augmentor_Keras_Array_Data.ipynb)\n\n### Per-Class Augmentation Strategies\nAugmentor allows for pipelines to be defined per class. That is, you can define different augmentation strategies on a class-by-class basis for a given classification problem.\n\nSee an example of this in the following Jupyter notebook: [`Per_Class_Augmentation_Strategy.ipynb`](https:\/\/github.com\/mdbloice\/Augmentor\/blob\/master\/notebooks\/Per_Class_Augmentation_Strategy.ipynb)\n\n## Complete Example\n\nLet's perform an augmentation task on a single image, demonstrating the pipeline and several features of Augmentor.\n\nFirst import the package and initialise a Pipeline object by pointing it to a directory containing your images:\n\n```python\nimport Augmentor\n\np = Augmentor.Pipeline(\"\/home\/user\/augmentor_data_tests\")\n```\n\nNow you can begin adding operations to the pipeline object:\n\n```python\np.rotate90(probability=0.5)\np.rotate270(probability=0.5)\np.flip_left_right(probability=0.8)\np.flip_top_bottom(probability=0.3)\np.crop_random(probability=1, percentage_area=0.5)\np.resize(probability=1.0, width=120, height=120)\n```\n\nOnce you have added the operations you require, you can sample images from this pipeline:\n\n```python\np.sample(100)\n```\n\nSome sample output:\n\n| Input Image<sup>[3]<\/sup>                                                                                          |   | Augmented Images                                                                                                    |\n|--------------------------------------------------------------------------------------------------------------------|---|---------------------------------------------------------------------------------------------------------------------|\n| ![Original](https:\/\/cloud.githubusercontent.com\/assets\/16042756\/23019262\/b696e3a6-f441-11e6-958d-17f18f2cd35e.jpg) | \u2192 | ![Augmented](https:\/\/cloud.githubusercontent.com\/assets\/16042756\/23018832\/cda6967e-f43f-11e6-9082-765c291f1fd6.gif) |\n\nThe augmented images may be useful for a boundary detection task, for example.\n\n## Licence and Acknowledgements\n\nAugmentor is made available under the terms of the MIT Licence. See [`Licence.md`](https:\/\/github.com\/mdbloice\/Augmentor\/blob\/master\/LICENSE.md).\n\n[1] Checkerboard image obtained from Wikimedia Commons and is in the public domain: <https:\/\/commons.wikimedia.org\/wiki\/File:Checkerboard_pattern.svg>\n\n[2] Street view image is in the public domain: <http:\/\/stokpic.com\/project\/italian-city-street-with-shoppers\/>\n\n[3] Skin lesion image obtained from the ISIC Archive:\n\n- Image id = 5436e3abbae478396759f0cf\n- Download: <https:\/\/isic-archive.com:443\/api\/v1\/image\/5436e3abbae478396759f0cf\/download>\n\nYou can use `urllib` to obtain the skin lesion image in order to reproduce the augmented images above:\n\n```python\n>>> from urllib import urlretrieve\n>>> im_url = \"https:\/\/isic-archive.com:443\/api\/v1\/image\/5436e3abbae478396759f0cf\/download\"\n>>> urlretrieve(im_url, \"ISIC_0000000.jpg\")\n('ISIC_0000000.jpg', <httplib.HTTPMessage instance at 0x7f7bd949a950>)\n```\n\nNote: For Python 3, use `from urllib.request import urlretrieve`.\n\nLogo created at [LogoMakr.com](https:\/\/logomakr.com)\n\n## Tests\nTo run the automated tests, clone the repository and run:\n\n```bash\n$ py.test -v\n```\n\nfrom the command line. To view the CI tests that are run after each commit, see <https:\/\/travis-ci.org\/mdbloice\/Augmentor>.\n\n## Citing Augmentor\nIf you find this package useful and wish to cite it, you can use\n\nMarcus D Bloice, Peter M Roth, Andreas Holzinger, Biomedical image augmentation using Augmentor, *Bioinformatics*, <https:\/\/doi.org\/10.1093\/bioinformatics\/btz259>\n\n## Asciicast\n\nClick the preview below to view a video demonstration of Augmentor in use:\n\n[![asciicast](https:\/\/asciinema.org\/a\/105368.png)](https:\/\/asciinema.org\/a\/105368?autoplay=1&speed=3)\n","130":"\u673a\u5668\u5b66\u4e60\u7b97\u6cd5Python\u5b9e\u73b0\n=========\n\n[![MIT license](https:\/\/img.shields.io\/dub\/l\/vibe-d.svg)](https:\/\/github.com\/lawlite19\/MachineLearning_Python\/blob\/master\/LICENSE)\n\n## \u76ee\u5f55\n* [\u673a\u5668\u5b66\u4e60\u7b97\u6cd5Python\u5b9e\u73b0](#\u673a\u5668\u5b66\u4e60\u7b97\u6cd5python\u5b9e\u73b0)\n\t* [\u4e00\u3001\u7ebf\u6027\u56de\u5f52](#\u4e00\u7ebf\u6027\u56de\u5f52)\n\t\t* [1\u3001\u4ee3\u4ef7\u51fd\u6570](#1\u4ee3\u4ef7\u51fd\u6570)\n\t\t* [2\u3001\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5](#2\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5)\n\t\t* [3\u3001\u5747\u503c\u5f52\u4e00\u5316](#3\u5747\u503c\u5f52\u4e00\u5316)\n\t\t* [4\u3001\u6700\u7ec8\u8fd0\u884c\u7ed3\u679c](#4\u6700\u7ec8\u8fd0\u884c\u7ed3\u679c)\n\t\t* [5\u3001\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u7ebf\u6027\u6a21\u578b\u5b9e\u73b0](#5\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u7ebf\u6027\u6a21\u578b\u5b9e\u73b0)\n\t* [\u4e8c\u3001\u903b\u8f91\u56de\u5f52](#\u4e8c\u903b\u8f91\u56de\u5f52)\n\t\t* [1\u3001\u4ee3\u4ef7\u51fd\u6570](#1\u4ee3\u4ef7\u51fd\u6570)\n\t\t* [2\u3001\u68af\u5ea6](#2\u68af\u5ea6)\n\t\t* [3\u3001\u6b63\u5219\u5316](#3\u6b63\u5219\u5316)\n\t\t* [4\u3001S\u578b\u51fd\u6570\uff08\u5373\uff09](#4s\u578b\u51fd\u6570\u5373)\n\t\t* [5\u3001\u6620\u5c04\u4e3a\u591a\u9879\u5f0f](#5\u6620\u5c04\u4e3a\u591a\u9879\u5f0f)\n\t\t* [6\u3001\u4f7f\u7528\u7684\u4f18\u5316\u65b9\u6cd5](#6\u4f7f\u7528scipy\u7684\u4f18\u5316\u65b9\u6cd5)\n\t\t* [7\u3001\u8fd0\u884c\u7ed3\u679c](#7\u8fd0\u884c\u7ed3\u679c)\n\t\t* [8\u3001\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5b9e\u73b0](#8\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5b9e\u73b0)\n\t* [\u903b\u8f91\u56de\u5f52_\u624b\u5199\u6570\u5b57\u8bc6\u522b_OneVsAll](#\u903b\u8f91\u56de\u5f52_\u624b\u5199\u6570\u5b57\u8bc6\u522b_onevsall)\n\t\t* [1\u3001\u968f\u673a\u663e\u793a100\u4e2a\u6570\u5b57](#1\u968f\u673a\u663e\u793a100\u4e2a\u6570\u5b57)\n\t\t* [2\u3001OneVsAll](#2onevsall)\n\t\t* [3\u3001\u624b\u5199\u6570\u5b57\u8bc6\u522b](#3\u624b\u5199\u6570\u5b57\u8bc6\u522b)\n\t\t* [4\u3001\u9884\u6d4b](#4\u9884\u6d4b)\n\t\t* [5\u3001\u8fd0\u884c\u7ed3\u679c](#5\u8fd0\u884c\u7ed3\u679c)\n\t\t* [6\u3001\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5b9e\u73b0](#6\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5b9e\u73b0)\n\t* [\u4e09\u3001BP\u795e\u7ecf\u7f51\u7edc](#\u4e09bp\u795e\u7ecf\u7f51\u7edc)\n\t\t* [1\u3001\u795e\u7ecf\u7f51\u7edcmodel](#1\u795e\u7ecf\u7f51\u7edcmodel)\n\t\t* [2\u3001\u4ee3\u4ef7\u51fd\u6570](#2\u4ee3\u4ef7\u51fd\u6570)\n\t\t* [3\u3001\u6b63\u5219\u5316](#3\u6b63\u5219\u5316)\n\t\t* [4\u3001\u53cd\u5411\u4f20\u64adBP](#4\u53cd\u5411\u4f20\u64adbp)\n\t\t* [5\u3001BP\u53ef\u4ee5\u6c42\u68af\u5ea6\u7684\u539f\u56e0](#5bp\u53ef\u4ee5\u6c42\u68af\u5ea6\u7684\u539f\u56e0)\n\t\t* [6\u3001\u68af\u5ea6\u68c0\u67e5](#6\u68af\u5ea6\u68c0\u67e5)\n\t\t* [7\u3001\u6743\u91cd\u7684\u968f\u673a\u521d\u59cb\u5316](#7\u6743\u91cd\u7684\u968f\u673a\u521d\u59cb\u5316)\n\t\t* [8\u3001\u9884\u6d4b](#8\u9884\u6d4b)\n\t\t* [9\u3001\u8f93\u51fa\u7ed3\u679c](#9\u8f93\u51fa\u7ed3\u679c)\n\t* [\u56db\u3001SVM\u652f\u6301\u5411\u91cf\u673a](#\u56dbsvm\u652f\u6301\u5411\u91cf\u673a)\n\t\t* [1\u3001\u4ee3\u4ef7\u51fd\u6570](#1\u4ee3\u4ef7\u51fd\u6570)\n\t\t* [2\u3001Large Margin](#2large-margin)\n\t\t* [3\u3001SVM Kernel\uff08\u6838\u51fd\u6570\uff09](#3svm-kernel\u6838\u51fd\u6570)\n\t\t* [4\u3001\u4f7f\u7528\u4e2d\u7684\u6a21\u578b\u4ee3\u7801](#4\u4f7f\u7528scikit-learn\u4e2d\u7684svm\u6a21\u578b\u4ee3\u7801)\n\t\t* [5\u3001\u8fd0\u884c\u7ed3\u679c](#5\u8fd0\u884c\u7ed3\u679c)\n\t* [\u4e94\u3001K-Means\u805a\u7c7b\u7b97\u6cd5](#\u4e94k-means\u805a\u7c7b\u7b97\u6cd5)\n\t\t* [1\u3001\u805a\u7c7b\u8fc7\u7a0b](#1\u805a\u7c7b\u8fc7\u7a0b)\n\t\t* [2\u3001\u76ee\u6807\u51fd\u6570](#2\u76ee\u6807\u51fd\u6570)\n\t\t* [3\u3001\u805a\u7c7b\u4e2d\u5fc3\u7684\u9009\u62e9](#3\u805a\u7c7b\u4e2d\u5fc3\u7684\u9009\u62e9)\n\t\t* [4\u3001\u805a\u7c7b\u4e2a\u6570K\u7684\u9009\u62e9](#4\u805a\u7c7b\u4e2a\u6570k\u7684\u9009\u62e9)\n\t\t* [5\u3001\u5e94\u7528\u2014\u2014\u56fe\u7247\u538b\u7f29](#5\u5e94\u7528\u56fe\u7247\u538b\u7f29)\n\t\t* [6\u3001\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u7ebf\u6027\u6a21\u578b\u5b9e\u73b0\u805a\u7c7b](#6\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u7ebf\u6027\u6a21\u578b\u5b9e\u73b0\u805a\u7c7b)\n\t\t* [7\u3001\u8fd0\u884c\u7ed3\u679c](#7\u8fd0\u884c\u7ed3\u679c)\n\t* [\u516d\u3001PCA\u4e3b\u6210\u5206\u5206\u6790\uff08\u964d\u7ef4\uff09](#\u516dpca\u4e3b\u6210\u5206\u5206\u6790\u964d\u7ef4)\n\t\t* [1\u3001\u7528\u5904](#1\u7528\u5904)\n\t\t* [2\u30012D-->1D\uff0cnD-->kD](#22d--1dnd--kd)\n\t\t* [3\u3001\u4e3b\u6210\u5206\u5206\u6790PCA\u4e0e\u7ebf\u6027\u56de\u5f52\u7684\u533a\u522b](#3\u4e3b\u6210\u5206\u5206\u6790pca\u4e0e\u7ebf\u6027\u56de\u5f52\u7684\u533a\u522b)\n\t\t* [4\u3001PCA\u964d\u7ef4\u8fc7\u7a0b](#4pca\u964d\u7ef4\u8fc7\u7a0b)\n\t\t* [5\u3001\u6570\u636e\u6062\u590d](#5\u6570\u636e\u6062\u590d)\n\t\t* [6\u3001\u4e3b\u6210\u5206\u4e2a\u6570\u7684\u9009\u62e9\uff08\u5373\u8981\u964d\u7684\u7ef4\u5ea6\uff09](#6\u4e3b\u6210\u5206\u4e2a\u6570\u7684\u9009\u62e9\u5373\u8981\u964d\u7684\u7ef4\u5ea6)\n\t\t* [7\u3001\u4f7f\u7528\u5efa\u8bae](#7\u4f7f\u7528\u5efa\u8bae)\n\t\t* [8\u3001\u8fd0\u884c\u7ed3\u679c](#8\u8fd0\u884c\u7ed3\u679c)\n\t\t* [9\u3001\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684PCA\u5b9e\u73b0\u964d\u7ef4](#9\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684pca\u5b9e\u73b0\u964d\u7ef4)\n\t* [\u4e03\u3001\u5f02\u5e38\u68c0\u6d4b Anomaly Detection](#\u4e03\u5f02\u5e38\u68c0\u6d4b-anomaly-detection)\n\t\t* [1\u3001\u9ad8\u65af\u5206\u5e03\uff08\u6b63\u6001\u5206\u5e03\uff09](#1\u9ad8\u65af\u5206\u5e03\u6b63\u6001\u5206\u5e03gaussian-distribution)\n\t\t* [2\u3001\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5](#2\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5)\n\t\t* [3\u3001\u8bc4\u4ef7\u7684\u597d\u574f\uff0c\u4ee5\u53ca\u7684\u9009\u53d6](#3\u8bc4\u4ef7px\u7684\u597d\u574f\u4ee5\u53ca\u03b5\u7684\u9009\u53d6)\n\t\t* [4\u3001\u9009\u62e9\u4f7f\u7528\u4ec0\u4e48\u6837\u7684feature\uff08\u5355\u5143\u9ad8\u65af\u5206\u5e03\uff09](#4\u9009\u62e9\u4f7f\u7528\u4ec0\u4e48\u6837\u7684feature\u5355\u5143\u9ad8\u65af\u5206\u5e03)\n\t\t* [5\u3001\u591a\u5143\u9ad8\u65af\u5206\u5e03](#5\u591a\u5143\u9ad8\u65af\u5206\u5e03)\n\t\t* [6\u3001\u5355\u5143\u548c\u591a\u5143\u9ad8\u65af\u5206\u5e03\u7279\u70b9](#6\u5355\u5143\u548c\u591a\u5143\u9ad8\u65af\u5206\u5e03\u7279\u70b9)\n\t\t* [7\u3001\u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c](#7\u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c)\n\n## \u4e00\u3001[\u7ebf\u6027\u56de\u5f52](\/LinearRegression)\n- [\u5168\u90e8\u4ee3\u7801](\/LinearRegression\/LinearRegression.py)\n\n### 1\u3001\u4ee3\u4ef7\u51fd\u6570\n- ![J(\\theta ) = \\frac{1}{{2{\\text{m}}}}\\sum\\limits_{i = 1}^m {{{({h_\\theta }({x^{(i)}}) - {y^{(i)}})}^2}} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%5Cfrac%7B1%7D%7B%7B2%7B%5Ctext%7Bm%7D%7D%7D%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7B%7B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29%7D%5E2%7D%7D%20)\n- \u5176\u4e2d\uff1a\n![{h_\\theta }(x) = {\\theta _0} + {\\theta _1}{x_1} + {\\theta _2}{x_2} + ...](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20%7B%5Ctheta%20_0%7D%20%2B%20%7B%5Ctheta%20_1%7D%7Bx_1%7D%20%2B%20%7B%5Ctheta%20_2%7D%7Bx_2%7D%20%2B%20...)\n\n- \u4e0b\u9762\u5c31\u662f\u8981\u6c42\u51fatheta\uff0c\u4f7f\u4ee3\u4ef7\u6700\u5c0f\uff0c\u5373\u4ee3\u8868\u6211\u4eec\u62df\u5408\u51fa\u6765\u7684\u65b9\u7a0b\u8ddd\u79bb\u771f\u5b9e\u503c\u6700\u8fd1\n- \u5171\u6709m\u6761\u6570\u636e\uff0c\u5176\u4e2d![{{{({h_\\theta }({x^{(i)}}) - {y^{(i)}})}^2}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%7B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29%7D%5E2%7D%7D)\u4ee3\u8868\u6211\u4eec\u8981\u62df\u5408\u51fa\u6765\u7684\u65b9\u7a0b\u5230\u771f\u5b9e\u503c\u8ddd\u79bb\u7684\u5e73\u65b9\uff0c\u5e73\u65b9\u7684\u539f\u56e0\u662f\u56e0\u4e3a\u53ef\u80fd\u6709\u8d1f\u503c\uff0c\u6b63\u8d1f\u53ef\u80fd\u4f1a\u62b5\u6d88\n- \u524d\u9762\u6709\u7cfb\u6570`2`\u7684\u539f\u56e0\u662f\u4e0b\u9762\u6c42\u68af\u5ea6\u662f\u5bf9\u6bcf\u4e2a\u53d8\u91cf\u6c42\u504f\u5bfc\uff0c`2`\u53ef\u4ee5\u6d88\u53bb\n\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570\ndef computerCost(X,y,theta):\n    m = len(y)\n    J = 0\n    \n    J = (np.transpose(X*theta-y))*(X*theta-y)\/(2*m) #\u8ba1\u7b97\u4ee3\u4ef7J\n    return J\n```\n - \u6ce8\u610f\u8fd9\u91cc\u7684X\u662f\u771f\u5b9e\u6570\u636e\u524d\u52a0\u4e86\u4e00\u52171\uff0c\u56e0\u4e3a\u6709theta(0)\n\n### 2\u3001\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\n- \u4ee3\u4ef7\u51fd\u6570\u5bf9![{{\\theta _j}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Ctheta%20_j%7D%7D)\u6c42\u504f\u5bfc\u5f97\u5230\uff1a   \n![\\frac{{\\partial J(\\theta )}}{{\\partial {\\theta _j}}} = \\frac{1}{m}\\sum\\limits_{i = 1}^m {[({h_\\theta }({x^{(i)}}) - {y^{(i)}})x_j^{(i)}]} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5Ctheta%20%29%7D%7D%7B%7B%5Cpartial%20%7B%5Ctheta%20_j%7D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20)\n- \u6240\u4ee5\u5bf9theta\u7684\u66f4\u65b0\u53ef\u4ee5\u5199\u4e3a\uff1a   \n![{\\theta _j} = {\\theta _j} - \\alpha \\frac{1}{m}\\sum\\limits_{i = 1}^m {[({h_\\theta }({x^{(i)}}) - {y^{(i)}})x_j^{(i)}]} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ctheta%20_j%7D%20%3D%20%7B%5Ctheta%20_j%7D%20-%20%5Calpha%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20)\n- \u5176\u4e2d![\\alpha ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Calpha%20)\u4e3a\u5b66\u4e60\u901f\u7387\uff0c\u63a7\u5236\u68af\u5ea6\u4e0b\u964d\u7684\u901f\u5ea6\uff0c\u4e00\u822c\u53d6**0.01,0.03,0.1,0.3.....**\n- \u4e3a\u4ec0\u4e48\u68af\u5ea6\u4e0b\u964d\u53ef\u4ee5\u9010\u6b65\u51cf\u5c0f\u4ee3\u4ef7\u51fd\u6570\n - \u5047\u8bbe\u51fd\u6570`f(x)`\n - \u6cf0\u52d2\u5c55\u5f00\uff1a`f(x+\u25b3x)=f(x)+f'(x)*\u25b3x+o(\u25b3x)`\n - \u4ee4\uff1a`\u25b3x=-\u03b1*f'(x)`   ,\u5373\u8d1f\u68af\u5ea6\u65b9\u5411\u4e58\u4ee5\u4e00\u4e2a\u5f88\u5c0f\u7684\u6b65\u957f`\u03b1`\n - \u5c06`\u25b3x`\u4ee3\u5165\u6cf0\u52d2\u5c55\u5f00\u5f0f\u4e2d\uff1a`f(x+\u25b3x)=f(x)-\u03b1*[f'(x)]\u00b2+o(\u25b3x)`\n - \u53ef\u4ee5\u770b\u51fa\uff0c`\u03b1`\u662f\u53d6\u5f97\u5f88\u5c0f\u7684\u6b63\u6570\uff0c`[f'(x)]\u00b2`\u4e5f\u662f\u6b63\u6570\uff0c\u6240\u4ee5\u53ef\u4ee5\u5f97\u51fa\uff1a`f(x+\u25b3x)<=f(x)`\n - \u6240\u4ee5\u6cbf\u7740**\u8d1f\u68af\u5ea6**\u653e\u4e0b\uff0c\u51fd\u6570\u5728\u51cf\u5c0f\uff0c\u591a\u7ef4\u60c5\u51b5\u4e00\u6837\u3002\n- \u5b9e\u73b0\u4ee3\u7801\n```\n# \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\ndef gradientDescent(X,y,theta,alpha,num_iters):\n    m = len(y)      \n    n = len(theta)\n    \n    temp = np.matrix(np.zeros((n,num_iters)))   # \u6682\u5b58\u6bcf\u6b21\u8fed\u4ee3\u8ba1\u7b97\u7684theta\uff0c\u8f6c\u5316\u4e3a\u77e9\u9635\u5f62\u5f0f\n    \n    \n    J_history = np.zeros((num_iters,1)) #\u8bb0\u5f55\u6bcf\u6b21\u8fed\u4ee3\u8ba1\u7b97\u7684\u4ee3\u4ef7\u503c\n    \n    for i in range(num_iters):  # \u904d\u5386\u8fed\u4ee3\u6b21\u6570    \n        h = np.dot(X,theta)     # \u8ba1\u7b97\u5185\u79ef\uff0cmatrix\u53ef\u4ee5\u76f4\u63a5\u4e58\n        temp[:,i] = theta - ((alpha\/m)*(np.dot(np.transpose(X),h-y)))   #\u68af\u5ea6\u7684\u8ba1\u7b97\n        theta = temp[:,i]\n        J_history[i] = computerCost(X,y,theta)      #\u8c03\u7528\u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570\n        print '.',      \n    return theta,J_history  \n```\n\n### 3\u3001\u5747\u503c\u5f52\u4e00\u5316\n- \u76ee\u7684\u662f\u4f7f\u6570\u636e\u90fd\u7f29\u653e\u5230\u4e00\u4e2a\u8303\u56f4\u5185\uff0c\u4fbf\u4e8e\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\n- ![{x_i} = \\frac{{{x_i} - {\\mu _i}}}{{{s_i}}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_i%7D%20%3D%20%5Cfrac%7B%7B%7Bx_i%7D%20-%20%7B%5Cmu%20_i%7D%7D%7D%7B%7B%7Bs_i%7D%7D%7D)\n- \u5176\u4e2d ![{{\\mu _i}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Cmu%20_i%7D%7D) \u4e3a\u6240\u6709\u6b64feture\u6570\u636e\u7684\u5e73\u5747\u503c\n- ![{{s_i}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7Bs_i%7D%7D)\u53ef\u4ee5\u662f**\u6700\u5927\u503c-\u6700\u5c0f\u503c**\uff0c\u4e5f\u53ef\u4ee5\u662f\u8fd9\u4e2afeature\u5bf9\u5e94\u7684\u6570\u636e\u7684**\u6807\u51c6\u5dee**\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u5f52\u4e00\u5316feature\ndef featureNormaliza(X):\n    X_norm = np.array(X)            #\u5c06X\u8f6c\u5316\u4e3anumpy\u6570\u7ec4\u5bf9\u8c61\uff0c\u624d\u53ef\u4ee5\u8fdb\u884c\u77e9\u9635\u7684\u8fd0\u7b97\n    #\u5b9a\u4e49\u6240\u9700\u53d8\u91cf\n    mu = np.zeros((1,X.shape[1]))   \n    sigma = np.zeros((1,X.shape[1]))\n    \n    mu = np.mean(X_norm,0)          # \u6c42\u6bcf\u4e00\u5217\u7684\u5e73\u5747\u503c\uff080\u6307\u5b9a\u4e3a\u5217\uff0c1\u4ee3\u8868\u884c\uff09\n    sigma = np.std(X_norm,0)        # \u6c42\u6bcf\u4e00\u5217\u7684\u6807\u51c6\u5dee\n    for i in range(X.shape[1]):     # \u904d\u5386\u5217\n        X_norm[:,i] = (X_norm[:,i]-mu[i])\/sigma[i]  # \u5f52\u4e00\u5316\n    \n    return X_norm,mu,sigma\n```\n- \u6ce8\u610f\u9884\u6d4b\u7684\u65f6\u5019\u4e5f\u9700\u8981\u5747\u503c\u5f52\u4e00\u5316\u6570\u636e\n\n### 4\u3001\u6700\u7ec8\u8fd0\u884c\u7ed3\u679c\n- \u4ee3\u4ef7\u968f\u8fed\u4ee3\u6b21\u6570\u7684\u53d8\u5316   \n![enter description here][1]\n\n\n### 5\u3001[\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u7ebf\u6027\u6a21\u578b\u5b9e\u73b0](\/LinearRegression\/LinearRegression_scikit-learn.py)\n- \u5bfc\u5165\u5305\n```\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler    #\u5f15\u5165\u7f29\u653e\u7684\u5305\n```\n- \u5f52\u4e00\u5316\n```\n    # \u5f52\u4e00\u5316\u64cd\u4f5c\n    scaler = StandardScaler()   \n    scaler.fit(X)\n    x_train = scaler.transform(X)\n    x_test = scaler.transform(np.array([1650,3]))\n```\n- \u7ebf\u6027\u6a21\u578b\u62df\u5408\n```\n    # \u7ebf\u6027\u6a21\u578b\u62df\u5408\n    model = linear_model.LinearRegression()\n    model.fit(x_train, y)\n``` \n- \u9884\u6d4b\n```\n    #\u9884\u6d4b\u7ed3\u679c\n    result = model.predict(x_test)\n```\n\n-------------------\n\n  \n## \u4e8c\u3001[\u903b\u8f91\u56de\u5f52](\/LogisticRegression)\n- [\u5168\u90e8\u4ee3\u7801](\/LogisticRegression\/LogisticRegression.py)\n\n### 1\u3001\u4ee3\u4ef7\u51fd\u6570\n- ![\\left\\{ \\begin{gathered}\n  J(\\theta ) = \\frac{1}{m}\\sum\\limits_{i = 1}^m {\\cos t({h_\\theta }({x^{(i)}}),{y^{(i)}})}  \\hfill \\\\\n  \\cos t({h_\\theta }(x),y) = \\left\\{ {\\begin{array}{c}    { - \\log ({h_\\theta }(x))} \\\\    { - \\log (1 - {h_\\theta }(x))}  \\end{array} \\begin{array}{c}    {y = 1} \\\\    {y = 0}  \\end{array} } \\right. \\hfill \\\\ \n\\end{gathered}  \\right.](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%5Cbegin%7Bgathered%7D%20J%28%5Ctheta%20%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%2C%7By%5E%7B%28i%29%7D%7D%29%7D%20%5Chfill%20%5C%5C%20%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28x%29%2Cy%29%20%3D%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5C%5C%20%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5Cend%7Barray%7D%20%5Cbegin%7Barray%7D%7Bc%7D%20%7By%20%3D%201%7D%20%5C%5C%20%7By%20%3D%200%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%20%5Chfill%20%5C%5C%20%5Cend%7Bgathered%7D%20%5Cright.)\n- \u53ef\u4ee5\u7efc\u5408\u8d77\u6765\u4e3a\uff1a\n![J(\\theta ) =  - \\frac{1}{m}\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\log ({h_\\theta }({x^{(i)}}) + (1 - } {y^{(i)}})\\log (1 - {h_\\theta }({x^{(i)}})]](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D)\n\u5176\u4e2d\uff1a\n![{h_\\theta }(x) = \\frac{1}{{1 + {e^{ - x}}}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20x%7D%7D%7D%7D)\n- \u4e3a\u4ec0\u4e48\u4e0d\u7528\u7ebf\u6027\u56de\u5f52\u7684\u4ee3\u4ef7\u51fd\u6570\u8868\u793a\uff0c\u56e0\u4e3a\u7ebf\u6027\u56de\u5f52\u7684\u4ee3\u4ef7\u51fd\u6570\u53ef\u80fd\u662f\u975e\u51f8\u7684\uff0c\u5bf9\u4e8e\u5206\u7c7b\u95ee\u9898\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u5f88\u96be\u5f97\u5230\u6700\u5c0f\u503c\uff0c\u4e0a\u9762\u7684\u4ee3\u4ef7\u51fd\u6570\u662f\u51f8\u51fd\u6570\n- ![{ - \\log ({h_\\theta }(x))}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D)\u7684\u56fe\u50cf\u5982\u4e0b\uff0c\u5373`y=1`\u65f6\uff1a\n![enter description here][2]\n\n\u53ef\u4ee5\u770b\u51fa\uff0c\u5f53![{{h_\\theta }(x)}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)\u8d8b\u4e8e`1`\uff0c`y=1`,\u4e0e\u9884\u6d4b\u503c\u4e00\u81f4\uff0c\u6b64\u65f6\u4ed8\u51fa\u7684\u4ee3\u4ef7`cost`\u8d8b\u4e8e`0`\uff0c\u82e5![{{h_\\theta }(x)}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)\u8d8b\u4e8e`0`\uff0c`y=1`,\u6b64\u65f6\u7684\u4ee3\u4ef7`cost`\u503c\u975e\u5e38\u5927\uff0c\u6211\u4eec\u6700\u7ec8\u7684\u76ee\u7684\u662f\u6700\u5c0f\u5316\u4ee3\u4ef7\u503c\n- \u540c\u7406![{ - \\log (1 - {h_\\theta }(x))}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D)\u7684\u56fe\u50cf\u5982\u4e0b\uff08`y=0`\uff09\uff1a   \n![enter description here][3]\n\n### 2\u3001\u68af\u5ea6\n- \u540c\u6837\u5bf9\u4ee3\u4ef7\u51fd\u6570\u6c42\u504f\u5bfc\uff1a\n![\\frac{{\\partial J(\\theta )}}{{\\partial {\\theta _j}}} = \\frac{1}{m}\\sum\\limits_{i = 1}^m {[({h_\\theta }({x^{(i)}}) - {y^{(i)}})x_j^{(i)}]} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5Ctheta%20%29%7D%7D%7B%7B%5Cpartial%20%7B%5Ctheta%20_j%7D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20)   \n\u53ef\u4ee5\u770b\u51fa\u4e0e\u7ebf\u6027\u56de\u5f52\u7684\u504f\u5bfc\u6570\u4e00\u81f4\n- \u63a8\u5230\u8fc7\u7a0b\n![enter description here][4]\n\n### 3\u3001\u6b63\u5219\u5316\n- \u76ee\u7684\u662f\u4e3a\u4e86\u9632\u6b62\u8fc7\u62df\u5408\n- \u5728\u4ee3\u4ef7\u51fd\u6570\u4e2d\u52a0\u4e0a\u4e00\u9879![J(\\theta ) =  - \\frac{1}{m}\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\log ({h_\\theta }({x^{(i)}}) + (1 - } {y^{(i)}})\\log (1 - {h_\\theta }({x^{(i)}})] + \\frac{\\lambda }{{2m}}\\sum\\limits_{j = 1}^n {\\theta _j^2} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D%20%2B%20%5Cfrac%7B%5Clambda%20%7D%7B%7B2m%7D%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5En%20%7B%5Ctheta%20_j%5E2%7D%20)\n- \u6ce8\u610fj\u662f\u91cd1\u5f00\u59cb\u7684\uff0c\u56e0\u4e3atheta(0)\u4e3a\u4e00\u4e2a\u5e38\u6570\u9879\uff0cX\u4e2d\u6700\u524d\u9762\u4e00\u5217\u4f1a\u52a0\u4e0a1\u52171\uff0c\u6240\u4ee5\u4e58\u79ef\u8fd8\u662ftheta(0),feature\u6ca1\u6709\u5173\u7cfb\uff0c\u6ca1\u6709\u5fc5\u8981\u6b63\u5219\u5316\n- \u6b63\u5219\u5316\u540e\u7684\u4ee3\u4ef7\uff1a\n```\n# \u4ee3\u4ef7\u51fd\u6570\ndef costFunction(initial_theta,X,y,inital_lambda):\n    m = len(y)\n    J = 0\n    \n    h = sigmoid(np.dot(X,initial_theta))    # \u8ba1\u7b97h(z)\n    theta1 = initial_theta.copy()           # \u56e0\u4e3a\u6b63\u5219\u5316j=1\u4ece1\u5f00\u59cb\uff0c\u4e0d\u5305\u542b0\uff0c\u6240\u4ee5\u590d\u5236\u4e00\u4efd\uff0c\u524dtheta(0)\u503c\u4e3a0 \n    theta1[0] = 0   \n    \n    temp = np.dot(np.transpose(theta1),theta1)\n    J = (-np.dot(np.transpose(y),np.log(h))-np.dot(np.transpose(1-y),np.log(1-h))+temp*inital_lambda\/2)\/m   # \u6b63\u5219\u5316\u7684\u4ee3\u4ef7\u65b9\u7a0b\n    return J\n```\n- \u6b63\u5219\u5316\u540e\u7684\u4ee3\u4ef7\u7684\u68af\u5ea6\n```\n# \u8ba1\u7b97\u68af\u5ea6\ndef gradient(initial_theta,X,y,inital_lambda):\n    m = len(y)\n    grad = np.zeros((initial_theta.shape[0]))\n    \n    h = sigmoid(np.dot(X,initial_theta))# \u8ba1\u7b97h(z)\n    theta1 = initial_theta.copy()\n    theta1[0] = 0\n\n    grad = np.dot(np.transpose(X),h-y)\/m+inital_lambda\/m*theta1 #\u6b63\u5219\u5316\u7684\u68af\u5ea6\n    return grad  \n```\n\n### 4\u3001S\u578b\u51fd\u6570\uff08\u5373![{{h_\\theta }(x)}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)\uff09\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# S\u578b\u51fd\u6570    \ndef sigmoid(z):\n    h = np.zeros((len(z),1))    # \u521d\u59cb\u5316\uff0c\u4e0ez\u7684\u957f\u5ea6\u4e00\u7f6e\n    \n    h = 1.0\/(1.0+np.exp(-z))\n    return h\n```\n\n### 5\u3001\u6620\u5c04\u4e3a\u591a\u9879\u5f0f\n- \u56e0\u4e3a\u6570\u636e\u7684feture\u53ef\u80fd\u5f88\u5c11\uff0c\u5bfc\u81f4\u504f\u5dee\u5927\uff0c\u6240\u4ee5\u521b\u9020\u51fa\u4e00\u4e9bfeture\u7ed3\u5408\n- eg:\u6620\u5c04\u4e3a2\u6b21\u65b9\u7684\u5f62\u5f0f:![1 + {x_1} + {x_2} + x_1^2 + {x_1}{x_2} + x_2^2](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=1%20%2B%20%7Bx_1%7D%20%2B%20%7Bx_2%7D%20%2B%20x_1%5E2%20%2B%20%7Bx_1%7D%7Bx_2%7D%20%2B%20x_2%5E2)\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u6620\u5c04\u4e3a\u591a\u9879\u5f0f \ndef mapFeature(X1,X2):\n    degree = 3;                     # \u6620\u5c04\u7684\u6700\u9ad8\u6b21\u65b9\n    out = np.ones((X1.shape[0],1))  # \u6620\u5c04\u540e\u7684\u7ed3\u679c\u6570\u7ec4\uff08\u53d6\u4ee3X\uff09\n    '''\n    \u8fd9\u91cc\u4ee5degree=2\u4e3a\u4f8b\uff0c\u6620\u5c04\u4e3a1,x1,x2,x1^2,x1,x2,x2^2\n    '''\n    for i in np.arange(1,degree+1): \n        for j in range(i+1):\n            temp = X1**(i-j)*(X2**j)    #\u77e9\u9635\u76f4\u63a5\u4e58\u76f8\u5f53\u4e8ematlab\u4e2d\u7684\u70b9\u4e58.*\n            out = np.hstack((out, temp.reshape(-1,1)))\n    return out\n```\n\n### 6\u3001\u4f7f\u7528`scipy`\u7684\u4f18\u5316\u65b9\u6cd5\n- \u68af\u5ea6\u4e0b\u964d\u4f7f\u7528`scipy`\u4e2d`optimize`\u4e2d\u7684`fmin_bfgs`\u51fd\u6570\n- \u8c03\u7528scipy\u4e2d\u7684\u4f18\u5316\u7b97\u6cd5fmin_bfgs\uff08\u62df\u725b\u987f\u6cd5Broyden-Fletcher-Goldfarb-Shanno\n - costFunction\u662f\u81ea\u5df1\u5b9e\u73b0\u7684\u4e00\u4e2a\u6c42\u4ee3\u4ef7\u7684\u51fd\u6570\uff0c\n - initial_theta\u8868\u793a\u521d\u59cb\u5316\u7684\u503c,\n - fprime\u6307\u5b9acostFunction\u7684\u68af\u5ea6\n - args\u662f\u5176\u4f59\u6d4b\u53c2\u6570\uff0c\u4ee5\u5143\u7ec4\u7684\u5f62\u5f0f\u4f20\u5165\uff0c\u6700\u540e\u4f1a\u5c06\u6700\u5c0f\u5316costFunction\u7684theta\u8fd4\u56de \n```\n    result = optimize.fmin_bfgs(costFunction, initial_theta, fprime=gradient, args=(X,y,initial_lambda))    \n```   \n\n### 7\u3001\u8fd0\u884c\u7ed3\u679c\n- data1\u51b3\u7b56\u8fb9\u754c\u548c\u51c6\u786e\u5ea6  \n![enter description here][5]\n![enter description here][6]\n- data2\u51b3\u7b56\u8fb9\u754c\u548c\u51c6\u786e\u5ea6  \n![enter description here][7]\n![enter description here][8]\n\n### 8\u3001[\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5b9e\u73b0](\/LogisticRegression\/LogisticRegression_scikit-learn.py)\n- \u5bfc\u5165\u5305\n```\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nimport numpy as np\n```\n- \u5212\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n```\n    # \u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n```\n- \u5f52\u4e00\u5316\n```\n    # \u5f52\u4e00\u5316\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_test = scaler.fit_transform(x_test)\n```\n- \u903b\u8f91\u56de\u5f52\n```\n    #\u903b\u8f91\u56de\u5f52\n    model = LogisticRegression()\n    model.fit(x_train,y_train)\n``` \n- \u9884\u6d4b\n```\n    # \u9884\u6d4b\n    predict = model.predict(x_test)\n    right = sum(predict == y_test)\n    \n    predict = np.hstack((predict.reshape(-1,1),y_test.reshape(-1,1)))   # \u5c06\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u653e\u5728\u4e00\u5757\uff0c\u597d\u89c2\u5bdf\n    print predict\n    print ('\u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\uff1a%f%%'%(right*100.0\/predict.shape[0]))          #\u8ba1\u7b97\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u5ea6\n```\n\n\n-------------\n\n## [\u903b\u8f91\u56de\u5f52_\u624b\u5199\u6570\u5b57\u8bc6\u522b_OneVsAll](\/LogisticRegression)\n- [\u5168\u90e8\u4ee3\u7801](\/LogisticRegression\/LogisticRegression_OneVsAll.py)\n\n### 1\u3001\u968f\u673a\u663e\u793a100\u4e2a\u6570\u5b57\n- \u6211\u6ca1\u6709\u4f7f\u7528scikit-learn\u4e2d\u7684\u6570\u636e\u96c6\uff0c\u50cf\u7d20\u662f20*20px\uff0c\u5f69\u8272\u56fe\u5982\u4e0b\n![enter description here][9]\n\u7070\u5ea6\u56fe\uff1a\n![enter description here][10]\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u663e\u793a100\u4e2a\u6570\u5b57\ndef display_data(imgData):\n    sum = 0\n    '''\n    \u663e\u793a100\u4e2a\u6570\uff08\u82e5\u662f\u4e00\u4e2a\u4e00\u4e2a\u7ed8\u5236\u5c06\u4f1a\u975e\u5e38\u6162\uff0c\u53ef\u4ee5\u5c06\u8981\u753b\u7684\u6570\u5b57\u6574\u7406\u597d\uff0c\u653e\u5230\u4e00\u4e2a\u77e9\u9635\u4e2d\uff0c\u663e\u793a\u8fd9\u4e2a\u77e9\u9635\u5373\u53ef\uff09\n    - \u521d\u59cb\u5316\u4e00\u4e2a\u4e8c\u7ef4\u6570\u7ec4\n    - \u5c06\u6bcf\u884c\u7684\u6570\u636e\u8c03\u6574\u6210\u56fe\u50cf\u7684\u77e9\u9635\uff0c\u653e\u8fdb\u4e8c\u7ef4\u6570\u7ec4\n    - \u663e\u793a\u5373\u53ef\n    '''\n    pad = 1\n    display_array = -np.ones((pad+10*(20+pad),pad+10*(20+pad)))\n    for i in range(10):\n        for j in range(10):\n            display_array[pad+i*(20+pad):pad+i*(20+pad)+20,pad+j*(20+pad):pad+j*(20+pad)+20] = (imgData[sum,:].reshape(20,20,order=\"F\"))    # order=F\u6307\u5b9a\u4ee5\u5217\u4f18\u5148\uff0c\u5728matlab\u4e2d\u662f\u8fd9\u6837\u7684\uff0cpython\u4e2d\u9700\u8981\u6307\u5b9a\uff0c\u9ed8\u8ba4\u4ee5\u884c\n            sum += 1\n            \n    plt.imshow(display_array,cmap='gray')   #\u663e\u793a\u7070\u5ea6\u56fe\u50cf\n    plt.axis('off')\n    plt.show()\n```\n\n### 2\u3001OneVsAll\n- \u5982\u4f55\u5229\u7528\u903b\u8f91\u56de\u5f52\u89e3\u51b3\u591a\u5206\u7c7b\u7684\u95ee\u9898\uff0cOneVsAll\u5c31\u662f\u628a\u5f53\u524d\u67d0\u4e00\u7c7b\u770b\u6210\u4e00\u7c7b\uff0c\u5176\u4ed6\u6240\u6709\u7c7b\u522b\u770b\u4f5c\u4e00\u7c7b\uff0c\u8fd9\u6837\u6709\u6210\u4e86\u4e8c\u5206\u7c7b\u7684\u95ee\u9898\u4e86\n- \u5982\u4e0b\u56fe\uff0c\u628a\u9014\u4e2d\u7684\u6570\u636e\u5206\u6210\u4e09\u7c7b\uff0c\u5148\u628a\u7ea2\u8272\u7684\u770b\u6210\u4e00\u7c7b\uff0c\u628a\u5176\u4ed6\u7684\u770b\u4f5c\u53e6\u5916\u4e00\u7c7b\uff0c\u8fdb\u884c\u903b\u8f91\u56de\u5f52\uff0c\u7136\u540e\u628a\u84dd\u8272\u7684\u770b\u6210\u4e00\u7c7b\uff0c\u5176\u4ed6\u7684\u518d\u770b\u6210\u4e00\u7c7b\uff0c\u4ee5\u6b64\u7c7b\u63a8...\n![enter description here][11]\n- \u53ef\u4ee5\u770b\u51fa\u5927\u4e8e2\u7c7b\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u591a\u5c11\u7c7b\u5c31\u8981\u8fdb\u884c\u591a\u5c11\u6b21\u7684\u903b\u8f91\u56de\u5f52\u5206\u7c7b\n\n### 3\u3001\u624b\u5199\u6570\u5b57\u8bc6\u522b\n- \u5171\u67090-9\uff0c10\u4e2a\u6570\u5b57\uff0c\u9700\u898110\u6b21\u5206\u7c7b\n- \u7531\u4e8e**\u6570\u636e\u96c6y**\u7ed9\u51fa\u7684\u662f`0,1,2...9`\u7684\u6570\u5b57\uff0c\u800c\u8fdb\u884c\u903b\u8f91\u56de\u5f52\u9700\u8981`0\/1`\u7684label\u6807\u8bb0\uff0c\u6240\u4ee5\u9700\u8981\u5bf9y\u5904\u7406\n- \u8bf4\u4e00\u4e0b\u6570\u636e\u96c6\uff0c\u524d`500`\u4e2a\u662f`0`,`500-1000`\u662f`1`,`...`,\u6240\u4ee5\u5982\u4e0b\u56fe\uff0c\u5904\u7406\u540e\u7684`y`\uff0c**\u524d500\u884c\u7684\u7b2c\u4e00\u5217\u662f1\uff0c\u5176\u4f59\u90fd\u662f0,500-1000\u884c\u7b2c\u4e8c\u5217\u662f1\uff0c\u5176\u4f59\u90fd\u662f0....**\n![enter description here][12]\n- \u7136\u540e\u8c03\u7528**\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5**\u6c42\u89e3`theta`\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u6c42\u6bcf\u4e2a\u5206\u7c7b\u7684theta\uff0c\u6700\u540e\u8fd4\u56de\u6240\u6709\u7684all_theta    \ndef oneVsAll(X,y,num_labels,Lambda):\n    # \u521d\u59cb\u5316\u53d8\u91cf\n    m,n = X.shape\n    all_theta = np.zeros((n+1,num_labels))  # \u6bcf\u4e00\u5217\u5bf9\u5e94\u76f8\u5e94\u5206\u7c7b\u7684theta,\u517110\u5217\n    X = np.hstack((np.ones((m,1)),X))       # X\u524d\u8865\u4e0a\u4e00\u52171\u7684\u504f\u7f6ebias\n    class_y = np.zeros((m,num_labels))      # \u6570\u636e\u7684y\u5bf9\u5e940-9\uff0c\u9700\u8981\u6620\u5c04\u4e3a0\/1\u7684\u5173\u7cfb\n    initial_theta = np.zeros((n+1,1))       # \u521d\u59cb\u5316\u4e00\u4e2a\u5206\u7c7b\u7684theta\n    \n    # \u6620\u5c04y\n    for i in range(num_labels):\n        class_y[:,i] = np.int32(y==i).reshape(1,-1) # \u6ce8\u610freshape(1,-1)\u624d\u53ef\u4ee5\u8d4b\u503c\n    \n    #np.savetxt(\"class_y.csv\", class_y[0:600,:], delimiter=',')    \n    \n    '''\u904d\u5386\u6bcf\u4e2a\u5206\u7c7b\uff0c\u8ba1\u7b97\u5bf9\u5e94\u7684theta\u503c'''\n    for i in range(num_labels):\n        result = optimize.fmin_bfgs(costFunction, initial_theta, fprime=gradient, args=(X,class_y[:,i],Lambda)) # \u8c03\u7528\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u5316\u65b9\u6cd5\n        all_theta[:,i] = result.reshape(1,-1)   # \u653e\u5165all_theta\u4e2d\n        \n    all_theta = np.transpose(all_theta) \n    return all_theta\n```\n\n### 4\u3001\u9884\u6d4b\n- \u4e4b\u524d\u8bf4\u8fc7\uff0c\u9884\u6d4b\u7684\u7ed3\u679c\u662f\u4e00\u4e2a**\u6982\u7387\u503c**\uff0c\u5229\u7528\u5b66\u4e60\u51fa\u6765\u7684`theta`\u4ee3\u5165\u9884\u6d4b\u7684**S\u578b\u51fd\u6570**\u4e2d\uff0c\u6bcf\u884c\u7684\u6700\u5927\u503c\u5c31\u662f\u662f\u67d0\u4e2a\u6570\u5b57\u7684\u6700\u5927\u6982\u7387\uff0c\u6240\u5728\u7684**\u5217\u53f7**\u5c31\u662f\u9884\u6d4b\u7684\u6570\u5b57\u7684\u771f\u5b9e\u503c,\u56e0\u4e3a\u5728\u5206\u7c7b\u65f6\uff0c\u6240\u6709\u4e3a`0`\u7684\u5c06`y`\u6620\u5c04\u5728\u7b2c\u4e00\u5217\uff0c\u4e3a1\u7684\u6620\u5c04\u5728\u7b2c\u4e8c\u5217\uff0c\u4f9d\u6b21\u7c7b\u63a8\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u9884\u6d4b\ndef predict_oneVsAll(all_theta,X):\n    m = X.shape[0]\n    num_labels = all_theta.shape[0]\n    p = np.zeros((m,1))\n    X = np.hstack((np.ones((m,1)),X))   #\u5728X\u6700\u524d\u9762\u52a0\u4e00\u52171\n    \n    h = sigmoid(np.dot(X,np.transpose(all_theta)))  #\u9884\u6d4b\n\n    '''\n    \u8fd4\u56deh\u4e2d\u6bcf\u4e00\u884c\u6700\u5927\u503c\u6240\u5728\u7684\u5217\u53f7\n    - np.max(h, axis=1)\u8fd4\u56deh\u4e2d\u6bcf\u4e00\u884c\u7684\u6700\u5927\u503c\uff08\u662f\u67d0\u4e2a\u6570\u5b57\u7684\u6700\u5927\u6982\u7387\uff09\n    - \u6700\u540ewhere\u627e\u5230\u7684\u6700\u5927\u6982\u7387\u6240\u5728\u7684\u5217\u53f7\uff08\u5217\u53f7\u5373\u662f\u5bf9\u5e94\u7684\u6570\u5b57\uff09\n    '''\n    p = np.array(np.where(h[0,:] == np.max(h, axis=1)[0]))  \n    for i in np.arange(1, m):\n        t = np.array(np.where(h[i,:] == np.max(h, axis=1)[i]))\n        p = np.vstack((p,t))\n    return p\n```\n\n### 5\u3001\u8fd0\u884c\u7ed3\u679c\n- 10\u6b21\u5206\u7c7b\uff0c\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684\u51c6\u786e\u5ea6\uff1a   \n![enter description here][13]\n\n### 6\u3001[\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5b9e\u73b0](\/LogisticRegression\/LogisticRegression_OneVsAll_scikit-learn.py)\n- 1\u3001\u5bfc\u5165\u5305\n```\nfrom scipy import io as spio\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\n```\n- 2\u3001\u52a0\u8f7d\u6570\u636e\n```\n    data = loadmat_data(\"data_digits.mat\") \n    X = data['X']   # \u83b7\u53d6X\u6570\u636e\uff0c\u6bcf\u4e00\u884c\u5bf9\u5e94\u4e00\u4e2a\u6570\u5b5720x20px\n    y = data['y']   # \u8fd9\u91cc\u8bfb\u53d6mat\u6587\u4ef6y\u7684shape=(5000, 1)\n    y = np.ravel(y) # \u8c03\u7528sklearn\u9700\u8981\u8f6c\u5316\u6210\u4e00\u7ef4\u7684(5000,)\n```\n- 3\u3001\u62df\u5408\u6a21\u578b\n```\n    model = LogisticRegression()\n    model.fit(X, y) # \u62df\u5408\n```\n- 4\u3001\u9884\u6d4b\n```\n    predict = model.predict(X) #\u9884\u6d4b\n    \n    print u\"\u9884\u6d4b\u51c6\u786e\u5ea6\u4e3a\uff1a%f%%\"%np.mean(np.float64(predict == y)*100)\n```\n- 5\u3001\u8f93\u51fa\u7ed3\u679c\uff08\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684\u51c6\u786e\u5ea6\uff09\n![enter description here][14]\n\n\n----------\n\n## \u4e09\u3001BP\u795e\u7ecf\u7f51\u7edc\n- [\u5168\u90e8\u4ee3\u7801](\/NeuralNetwok\/NeuralNetwork.py)\n\n### 1\u3001\u795e\u7ecf\u7f51\u7edcmodel\n- \u5148\u4ecb\u7ecd\u4e2a\u4e09\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5982\u4e0b\u56fe\u6240\u793a\n - \u8f93\u5165\u5c42\uff08input layer\uff09\u6709\u4e09\u4e2aunits\uff08![{x_0}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_0%7D)\u4e3a\u8865\u4e0a\u7684bias\uff0c\u901a\u5e38\u8bbe\u4e3a`1`\uff09\n - ![a_i^{(j)}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=a_i%5E%7B%28j%29%7D)\u8868\u793a\u7b2c`j`\u5c42\u7684\u7b2c`i`\u4e2a\u6fc0\u52b1\uff0c\u4e5f\u79f0\u4e3a\u4e3a\u5355\u5143unit\n - ![{\\theta ^{(j)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ctheta%20%5E%7B%28j%29%7D%7D)\u4e3a\u7b2c`j`\u5c42\u5230\u7b2c`j+1`\u5c42\u6620\u5c04\u7684\u6743\u91cd\u77e9\u9635\uff0c\u5c31\u662f\u6bcf\u6761\u8fb9\u7684\u6743\u91cd\n![enter description here][15]\n\n- \u6240\u4ee5\u53ef\u4ee5\u5f97\u5230\uff1a\n - \u9690\u542b\u5c42\uff1a  \n![a_1^{(2)} = g(\\theta _{10}^{(1)}{x_0} + \\theta _{11}^{(1)}{x_1} + \\theta _{12}^{(1)}{x_2} + \\theta _{13}^{(1)}{x_3})](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=a_1%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B10%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B11%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B12%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B13%7D%5E%7B%281%29%7D%7Bx_3%7D%29)   \n![a_2^{(2)} = g(\\theta _{20}^{(1)}{x_0} + \\theta _{21}^{(1)}{x_1} + \\theta _{22}^{(1)}{x_2} + \\theta _{23}^{(1)}{x_3})](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=a_2%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B20%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B21%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B22%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B23%7D%5E%7B%281%29%7D%7Bx_3%7D%29)   \n![a_3^{(2)} = g(\\theta _{30}^{(1)}{x_0} + \\theta _{31}^{(1)}{x_1} + \\theta _{32}^{(1)}{x_2} + \\theta _{33}^{(1)}{x_3})](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=a_3%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B30%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B31%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B32%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B33%7D%5E%7B%281%29%7D%7Bx_3%7D%29)\n - \u8f93\u51fa\u5c42   \n![{h_\\theta }(x) = a_1^{(3)} = g(\\theta _{10}^{(2)}a_0^{(2)} + \\theta _{11}^{(2)}a_1^{(2)} + \\theta _{12}^{(2)}a_2^{(2)} + \\theta _{13}^{(2)}a_3^{(2)})](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20a_1%5E%7B%283%29%7D%20%3D%20g%28%5Ctheta%20_%7B10%7D%5E%7B%282%29%7Da_0%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B11%7D%5E%7B%282%29%7Da_1%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B12%7D%5E%7B%282%29%7Da_2%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B13%7D%5E%7B%282%29%7Da_3%5E%7B%282%29%7D%29) \u5176\u4e2d\uff0c**S\u578b\u51fd\u6570**![g(z) = \\frac{1}{{1 + {e^{ - z}}}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20z%7D%7D%7D%7D)\uff0c\u4e5f\u6210\u4e3a**\u6fc0\u52b1\u51fd\u6570**\n- \u53ef\u4ee5\u770b\u51fa![{\\theta ^{(1)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ctheta%20%5E%7B%281%29%7D%7D) \u4e3a3x4\u7684\u77e9\u9635\uff0c![{\\theta ^{(2)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ctheta%20%5E%7B%282%29%7D%7D)\u4e3a1x4\u7684\u77e9\u9635\n - ![{\\theta ^{(j)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ctheta%20%5E%7B%28j%29%7D%7D) ==\u300b`j+1`\u7684\u5355\u5143\u6570x\uff08`j`\u5c42\u7684\u5355\u5143\u6570+1\uff09\n\n### 2\u3001\u4ee3\u4ef7\u51fd\u6570\n- \u5047\u8bbe\u6700\u540e\u8f93\u51fa\u7684![{h_\\Theta }(x) \\in {R^K}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bh_%5CTheta%20%7D%28x%29%20%5Cin%20%7BR%5EK%7D)\uff0c\u5373\u4ee3\u8868\u8f93\u51fa\u5c42\u6709K\u4e2a\u5355\u5143\n- ![J(\\Theta ) =  - \\frac{1}{m}\\sum\\limits_{i = 1}^m {\\sum\\limits_{k = 1}^K {[y_k^{(i)}\\log {{({h_\\Theta }({x^{(i)}}))}_k}} }  + (1 - y_k^{(i)})\\log {(1 - {h_\\Theta }({x^{(i)}}))_k}]](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5CTheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5Csum%5Climits_%7Bk%20%3D%201%7D%5EK%20%7B%5By_k%5E%7B%28i%29%7D%5Clog%20%7B%7B%28%7Bh_%5CTheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%29%7D_k%7D%7D%20%7D%20%20%2B%20%281%20-%20y_k%5E%7B%28i%29%7D%29%5Clog%20%7B%281%20-%20%7Bh_%5CTheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%29_k%7D%5D) \u5176\u4e2d\uff0c![{({h_\\Theta }(x))_i}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%28%7Bh_%5CTheta%20%7D%28x%29%29_i%7D)\u4ee3\u8868\u7b2c`i`\u4e2a\u5355\u5143\u8f93\u51fa\n- \u4e0e\u903b\u8f91\u56de\u5f52\u7684\u4ee3\u4ef7\u51fd\u6570![J(\\theta ) =  - \\frac{1}{m}\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\log ({h_\\theta }({x^{(i)}}) + (1 - } {y^{(i)}})\\log (1 - {h_\\theta }({x^{(i)}})]](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D)\u5dee\u4e0d\u591a\uff0c\u5c31\u662f\u7d2f\u52a0\u4e0a\u6bcf\u4e2a\u8f93\u51fa\uff08\u5171\u6709K\u4e2a\u8f93\u51fa\uff09\n\n\n\n### 3\u3001\u6b63\u5219\u5316\n- `L`-->\u6240\u6709\u5c42\u7684\u4e2a\u6570\n- ![{S_l}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BS_l%7D)-->\u7b2c`l`\u5c42unit\u7684\u4e2a\u6570\n- \u6b63\u5219\u5316\u540e\u7684**\u4ee3\u4ef7\u51fd\u6570**\u4e3a  \n![enter description here][16]\n - ![\\theta ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Ctheta%20)\u5171\u6709`L-1`\u5c42\uff0c\n - \u7136\u540e\u662f\u7d2f\u52a0\u5bf9\u5e94\u6bcf\u4e00\u5c42\u7684theta\u77e9\u9635\uff0c\u6ce8\u610f\u4e0d\u5305\u542b\u52a0\u4e0a\u504f\u7f6e\u9879\u5bf9\u5e94\u7684theta(0)\n- \u6b63\u5219\u5316\u540e\u7684\u4ee3\u4ef7\u51fd\u6570\u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u4ee3\u4ef7\u51fd\u6570\ndef nnCostFunction(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda):\n    length = nn_params.shape[0] # theta\u7684\u4e2d\u957f\u5ea6\n    # \u8fd8\u539ftheta1\u548ctheta2\n    Theta1 = nn_params[0:hidden_layer_size*(input_layer_size+1)].reshape(hidden_layer_size,input_layer_size+1)\n    Theta2 = nn_params[hidden_layer_size*(input_layer_size+1):length].reshape(num_labels,hidden_layer_size+1)\n    \n    # np.savetxt(\"Theta1.csv\",Theta1,delimiter=',')\n    \n    m = X.shape[0]\n    class_y = np.zeros((m,num_labels))      # \u6570\u636e\u7684y\u5bf9\u5e940-9\uff0c\u9700\u8981\u6620\u5c04\u4e3a0\/1\u7684\u5173\u7cfb\n    # \u6620\u5c04y\n    for i in range(num_labels):\n        class_y[:,i] = np.int32(y==i).reshape(1,-1) # \u6ce8\u610freshape(1,-1)\u624d\u53ef\u4ee5\u8d4b\u503c\n     \n    '''\u53bb\u6389theta1\u548ctheta2\u7684\u7b2c\u4e00\u5217\uff0c\u56e0\u4e3a\u6b63\u5219\u5316\u65f6\u4ece1\u5f00\u59cb'''    \n    Theta1_colCount = Theta1.shape[1]    \n    Theta1_x = Theta1[:,1:Theta1_colCount]\n    Theta2_colCount = Theta2.shape[1]    \n    Theta2_x = Theta2[:,1:Theta2_colCount]\n    # \u6b63\u5219\u5316\u5411theta^2\n    term = np.dot(np.transpose(np.vstack((Theta1_x.reshape(-1,1),Theta2_x.reshape(-1,1)))),np.vstack((Theta1_x.reshape(-1,1),Theta2_x.reshape(-1,1))))\n    \n    '''\u6b63\u5411\u4f20\u64ad,\u6bcf\u6b21\u9700\u8981\u8865\u4e0a\u4e00\u52171\u7684\u504f\u7f6ebias'''\n    a1 = np.hstack((np.ones((m,1)),X))      \n    z2 = np.dot(a1,np.transpose(Theta1))    \n    a2 = sigmoid(z2)\n    a2 = np.hstack((np.ones((m,1)),a2))\n    z3 = np.dot(a2,np.transpose(Theta2))\n    h  = sigmoid(z3)    \n    '''\u4ee3\u4ef7'''    \n    J = -(np.dot(np.transpose(class_y.reshape(-1,1)),np.log(h.reshape(-1,1)))+np.dot(np.transpose(1-class_y.reshape(-1,1)),np.log(1-h.reshape(-1,1)))-Lambda*term\/2)\/m   \n    \n    return np.ravel(J)\n```\n\n### 4\u3001\u53cd\u5411\u4f20\u64adBP\n- \u4e0a\u9762\u6b63\u5411\u4f20\u64ad\u53ef\u4ee5\u8ba1\u7b97\u5f97\u5230`J(\u03b8)`,\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u8fd8\u9700\u8981\u6c42\u5b83\u7684\u68af\u5ea6\n- BP\u53cd\u5411\u4f20\u64ad\u7684\u76ee\u7684\u5c31\u662f\u6c42\u4ee3\u4ef7\u51fd\u6570\u7684\u68af\u5ea6\n- \u5047\u8bbe4\u5c42\u7684\u795e\u7ecf\u7f51\u7edc,![\\delta _{\\text{j}}^{(l)}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cdelta%20_%7B%5Ctext%7Bj%7D%7D%5E%7B%28l%29%7D)\u8bb0\u4e3a-->`l`\u5c42\u7b2c`j`\u4e2a\u5355\u5143\u7684\u8bef\u5dee\n - ![\\delta _{\\text{j}}^{(4)} = a_j^{(4)} - {y_i}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cdelta%20_%7B%5Ctext%7Bj%7D%7D%5E%7B%284%29%7D%20%3D%20a_j%5E%7B%284%29%7D%20-%20%7By_i%7D)\u300a===\u300b![{\\delta ^{(4)}} = {a^{(4)}} - y](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cdelta%20%5E%7B%284%29%7D%7D%20%3D%20%7Ba%5E%7B%284%29%7D%7D%20-%20y)\uff08\u5411\u91cf\u5316\uff09\n - ![{\\delta ^{(3)}} = {({\\theta ^{(3)}})^T}{\\delta ^{(4)}}.*{g^}({a^{(3)}})](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cdelta%20%5E%7B%283%29%7D%7D%20%3D%20%7B%28%7B%5Ctheta%20%5E%7B%283%29%7D%7D%29%5ET%7D%7B%5Cdelta%20%5E%7B%284%29%7D%7D.%2A%7Bg%5E%7D%28%7Ba%5E%7B%283%29%7D%7D%29)\n - ![{\\delta ^{(2)}} = {({\\theta ^{(2)}})^T}{\\delta ^{(3)}}.*{g^}({a^{(2)}})](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cdelta%20%5E%7B%282%29%7D%7D%20%3D%20%7B%28%7B%5Ctheta%20%5E%7B%282%29%7D%7D%29%5ET%7D%7B%5Cdelta%20%5E%7B%283%29%7D%7D.%2A%7Bg%5E%7D%28%7Ba%5E%7B%282%29%7D%7D%29)\n - \u6ca1\u6709![{\\delta ^{(1)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cdelta%20%5E%7B%281%29%7D%7D)\uff0c\u56e0\u4e3a\u5bf9\u4e8e\u8f93\u5165\u6ca1\u6709\u8bef\u5dee\n- \u56e0\u4e3aS\u578b\u51fd\u6570![{\\text{g(z)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ctext%7Bg%28z%29%7D%7D)\u7684\u5bfc\u6570\u4e3a\uff1a![{g^}(z){\\text{ = g(z)(1 - g(z))}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg%5E%7D%28z%29%7B%5Ctext%7B%20%3D%20g%28z%29%281%20-%20g%28z%29%29%7D%7D)\uff0c\u6240\u4ee5\u4e0a\u9762\u7684![{g^}({a^{(3)}})](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg%5E%7D%28%7Ba%5E%7B%283%29%7D%7D%29)\u548c![{g^}({a^{(2)}})](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg%5E%7D%28%7Ba%5E%7B%282%29%7D%7D%29)\u53ef\u4ee5\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u8ba1\u7b97\u51fa\u6765\n\n- \u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\u7684\u8fc7\u7a0b\u4e3a\uff1a\n - ![\\Delta _{ij}^{(l)} = 0](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%3D%200)\uff08![\\Delta ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5CDelta%20)\u662f\u5927\u5199\u7684![\\delta ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cdelta%20)\uff09\n - for i=1-m:     \n -![{a^{(1)}} = {x^{(i)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Ba%5E%7B%281%29%7D%7D%20%3D%20%7Bx%5E%7B%28i%29%7D%7D)       \n-\u6b63\u5411\u4f20\u64ad\u8ba1\u7b97![{a^{(l)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Ba%5E%7B%28l%29%7D%7D)\uff08l=2,3,4...L\uff09      \n-\u53cd\u5411\u8ba1\u7b97![{\\delta ^{(L)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cdelta%20%5E%7B%28L%29%7D%7D)\u3001![{\\delta ^{(L - 1)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cdelta%20%5E%7B%28L%20-%201%29%7D%7D)...![{\\delta ^{(2)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cdelta%20%5E%7B%282%29%7D%7D)\uff1b       \n-![\\Delta _{ij}^{(l)} = \\Delta _{ij}^{(l)} + a_j^{(l)}{\\delta ^{(l + 1)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20a_j%5E%7B%28l%29%7D%7B%5Cdelta%20%5E%7B%28l%20%2B%201%29%7D%7D)          \n-![D_{ij}^{(l)} = \\frac{1}{m}\\Delta _{ij}^{(l)} + \\lambda \\theta _{ij}^l\\begin{array}{c}    {}&amp; {(j \\ne 0)}  \\end{array} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=D_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20%5Clambda%20%5Ctheta%20_%7Bij%7D%5El%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7D%26%20%7B%28j%20%5Cne%200%29%7D%20%20%5Cend%7Barray%7D%20)      \n![D_{ij}^{(l)} = \\frac{1}{m}\\Delta _{ij}^{(l)} + \\lambda \\theta _{ij}^lj = 0\\begin{array}{c}    {}&amp; {j = 0}  \\end{array} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=D_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20%5Clambda%20%5Ctheta%20_%7Bij%7D%5Elj%20%3D%200%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7D%26%20%7Bj%20%3D%200%7D%20%20%5Cend%7Barray%7D%20)     \n\n- \u6700\u540e![\\frac{{\\partial J(\\Theta )}}{{\\partial \\Theta _{ij}^{(l)}}} = D_{ij}^{(l)}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5CTheta%20%29%7D%7D%7B%7B%5Cpartial%20%5CTheta%20_%7Bij%7D%5E%7B%28l%29%7D%7D%7D%20%3D%20D_%7Bij%7D%5E%7B%28l%29%7D)\uff0c\u5373\u5f97\u5230\u4ee3\u4ef7\u51fd\u6570\u7684\u68af\u5ea6\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u68af\u5ea6\ndef nnGradient(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda):\n    length = nn_params.shape[0]\n    Theta1 = nn_params[0:hidden_layer_size*(input_layer_size+1)].reshape(hidden_layer_size,input_layer_size+1).copy()   # \u8fd9\u91cc\u4f7f\u7528copy\u51fd\u6570\uff0c\u5426\u5219\u4e0b\u9762\u4fee\u6539Theta\u7684\u503c\uff0cnn_params\u4e5f\u4f1a\u4e00\u8d77\u4fee\u6539\n    Theta2 = nn_params[hidden_layer_size*(input_layer_size+1):length].reshape(num_labels,hidden_layer_size+1).copy()\n    m = X.shape[0]\n    class_y = np.zeros((m,num_labels))      # \u6570\u636e\u7684y\u5bf9\u5e940-9\uff0c\u9700\u8981\u6620\u5c04\u4e3a0\/1\u7684\u5173\u7cfb    \n    # \u6620\u5c04y\n    for i in range(num_labels):\n        class_y[:,i] = np.int32(y==i).reshape(1,-1) # \u6ce8\u610freshape(1,-1)\u624d\u53ef\u4ee5\u8d4b\u503c\n     \n    '''\u53bb\u6389theta1\u548ctheta2\u7684\u7b2c\u4e00\u5217\uff0c\u56e0\u4e3a\u6b63\u5219\u5316\u65f6\u4ece1\u5f00\u59cb'''\n    Theta1_colCount = Theta1.shape[1]    \n    Theta1_x = Theta1[:,1:Theta1_colCount]\n    Theta2_colCount = Theta2.shape[1]    \n    Theta2_x = Theta2[:,1:Theta2_colCount]\n    \n    Theta1_grad = np.zeros((Theta1.shape))  #\u7b2c\u4e00\u5c42\u5230\u7b2c\u4e8c\u5c42\u7684\u6743\u91cd\n    Theta2_grad = np.zeros((Theta2.shape))  #\u7b2c\u4e8c\u5c42\u5230\u7b2c\u4e09\u5c42\u7684\u6743\u91cd\n      \n   \n    '''\u6b63\u5411\u4f20\u64ad\uff0c\u6bcf\u6b21\u9700\u8981\u8865\u4e0a\u4e00\u52171\u7684\u504f\u7f6ebias'''\n    a1 = np.hstack((np.ones((m,1)),X))\n    z2 = np.dot(a1,np.transpose(Theta1))\n    a2 = sigmoid(z2)\n    a2 = np.hstack((np.ones((m,1)),a2))\n    z3 = np.dot(a2,np.transpose(Theta2))\n    h  = sigmoid(z3)\n    \n    \n    '''\u53cd\u5411\u4f20\u64ad\uff0cdelta\u4e3a\u8bef\u5dee\uff0c'''\n    delta3 = np.zeros((m,num_labels))\n    delta2 = np.zeros((m,hidden_layer_size))\n    for i in range(m):\n        #delta3[i,:] = (h[i,:]-class_y[i,:])*sigmoidGradient(z3[i,:])  # \u5747\u65b9\u8bef\u5dee\u7684\u8bef\u5dee\u7387\n        delta3[i,:] = h[i,:]-class_y[i,:]                              # \u4ea4\u53c9\u71b5\u8bef\u5dee\u7387\n        Theta2_grad = Theta2_grad+np.dot(np.transpose(delta3[i,:].reshape(1,-1)),a2[i,:].reshape(1,-1))\n        delta2[i,:] = np.dot(delta3[i,:].reshape(1,-1),Theta2_x)*sigmoidGradient(z2[i,:])\n        Theta1_grad = Theta1_grad+np.dot(np.transpose(delta2[i,:].reshape(1,-1)),a1[i,:].reshape(1,-1))\n    \n    Theta1[:,0] = 0\n    Theta2[:,0] = 0          \n    '''\u68af\u5ea6'''\n    grad = (np.vstack((Theta1_grad.reshape(-1,1),Theta2_grad.reshape(-1,1)))+Lambda*np.vstack((Theta1.reshape(-1,1),Theta2.reshape(-1,1))))\/m\n    return np.ravel(grad)\n```\n\n### 5\u3001BP\u53ef\u4ee5\u6c42\u68af\u5ea6\u7684\u539f\u56e0\n- \u5b9e\u9645\u662f\u5229\u7528\u4e86`\u94fe\u5f0f\u6c42\u5bfc`\u6cd5\u5219\n- \u56e0\u4e3a\u4e0b\u4e00\u5c42\u7684\u5355\u5143\u5229\u7528\u4e0a\u4e00\u5c42\u7684\u5355\u5143\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u8ba1\u7b97\n- \u5927\u4f53\u7684\u63a8\u5bfc\u8fc7\u7a0b\u5982\u4e0b\uff0c\u6700\u7ec8\u6211\u4eec\u662f\u60f3\u9884\u6d4b\u51fd\u6570\u4e0e\u5df2\u77e5\u7684`y`\u975e\u5e38\u63a5\u8fd1\uff0c\u6c42\u5747\u65b9\u5dee\u7684\u68af\u5ea6\u6cbf\u7740\u6b64\u68af\u5ea6\u65b9\u5411\u53ef\u4f7f\u4ee3\u4ef7\u51fd\u6570\u6700\u5c0f\u5316\u3002\u53ef\u5bf9\u7167\u4e0a\u9762\u6c42\u68af\u5ea6\u7684\u8fc7\u7a0b\u3002\n![enter description here][17]\n- \u6c42\u8bef\u5dee\u66f4\u8be6\u7ec6\u7684\u63a8\u5bfc\u8fc7\u7a0b\uff1a\n![enter description here][18]\n\n### 6\u3001\u68af\u5ea6\u68c0\u67e5\n- \u68c0\u67e5\u5229\u7528`BP`\u6c42\u7684\u68af\u5ea6\u662f\u5426\u6b63\u786e\n- \u5229\u7528\u5bfc\u6570\u7684\u5b9a\u4e49\u9a8c\u8bc1\uff1a\n![\\frac{{dJ(\\theta )}}{{d\\theta }} \\approx \\frac{{J(\\theta  + \\varepsilon ) - J(\\theta  - \\varepsilon )}}{{2\\varepsilon }}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cfrac%7B%7BdJ%28%5Ctheta%20%29%7D%7D%7B%7Bd%5Ctheta%20%7D%7D%20%5Capprox%20%5Cfrac%7B%7BJ%28%5Ctheta%20%20%2B%20%5Cvarepsilon%20%29%20-%20J%28%5Ctheta%20%20-%20%5Cvarepsilon%20%29%7D%7D%7B%7B2%5Cvarepsilon%20%7D%7D)\n- \u6c42\u51fa\u6765\u7684\u6570\u503c\u68af\u5ea6\u5e94\u8be5\u4e0eBP\u6c42\u51fa\u7684\u68af\u5ea6\u975e\u5e38\u63a5\u8fd1\n- \u9a8c\u8bc1BP\u6b63\u786e\u540e\u5c31\u4e0d\u9700\u8981\u518d\u6267\u884c\u9a8c\u8bc1\u68af\u5ea6\u7684\u7b97\u6cd5\u4e86\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u68c0\u9a8c\u68af\u5ea6\u662f\u5426\u8ba1\u7b97\u6b63\u786e\n# \u68c0\u9a8c\u68af\u5ea6\u662f\u5426\u8ba1\u7b97\u6b63\u786e\ndef checkGradient(Lambda = 0):\n    '''\u6784\u9020\u4e00\u4e2a\u5c0f\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\uff0c\u56e0\u4e3a\u6570\u503c\u6cd5\u8ba1\u7b97\u68af\u5ea6\u5f88\u6d6a\u8d39\u65f6\u95f4\uff0c\u800c\u4e14\u9a8c\u8bc1\u6b63\u786e\u540e\u4e4b\u540e\u5c31\u4e0d\u518d\u9700\u8981\u9a8c\u8bc1\u4e86'''\n    input_layer_size = 3\n    hidden_layer_size = 5\n    num_labels = 3\n    m = 5\n    initial_Theta1 = debugInitializeWeights(input_layer_size,hidden_layer_size); \n    initial_Theta2 = debugInitializeWeights(hidden_layer_size,num_labels)\n    X = debugInitializeWeights(input_layer_size-1,m)\n    y = 1+np.transpose(np.mod(np.arange(1,m+1), num_labels))# \u521d\u59cb\u5316y\n    \n    y = y.reshape(-1,1)\n    nn_params = np.vstack((initial_Theta1.reshape(-1,1),initial_Theta2.reshape(-1,1)))  #\u5c55\u5f00theta \n    '''BP\u6c42\u51fa\u68af\u5ea6'''\n    grad = nnGradient(nn_params, input_layer_size, hidden_layer_size, \n                     num_labels, X, y, Lambda)  \n    '''\u4f7f\u7528\u6570\u503c\u6cd5\u8ba1\u7b97\u68af\u5ea6'''\n    num_grad = np.zeros((nn_params.shape[0]))\n    step = np.zeros((nn_params.shape[0]))\n    e = 1e-4\n    for i in range(nn_params.shape[0]):\n        step[i] = e\n        loss1 = nnCostFunction(nn_params-step.reshape(-1,1), input_layer_size, hidden_layer_size, \n                              num_labels, X, y, \n                              Lambda)\n        loss2 = nnCostFunction(nn_params+step.reshape(-1,1), input_layer_size, hidden_layer_size, \n                              num_labels, X, y, \n                              Lambda)\n        num_grad[i] = (loss2-loss1)\/(2*e)\n        step[i]=0\n    # \u663e\u793a\u4e24\u5217\u6bd4\u8f83\n    res = np.hstack((num_grad.reshape(-1,1),grad.reshape(-1,1)))\n    print res\n```\n\n### 7\u3001\u6743\u91cd\u7684\u968f\u673a\u521d\u59cb\u5316\n- \u795e\u7ecf\u7f51\u7edc\u4e0d\u80fd\u50cf\u903b\u8f91\u56de\u5f52\u90a3\u6837\u521d\u59cb\u5316`theta`\u4e3a`0`,\u56e0\u4e3a\u82e5\u662f\u6bcf\u6761\u8fb9\u7684\u6743\u91cd\u90fd\u4e3a0\uff0c\u6bcf\u4e2a\u795e\u7ecf\u5143\u90fd\u662f\u76f8\u540c\u7684\u8f93\u51fa\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u4e5f\u4f1a\u5f97\u5230\u540c\u6837\u7684\u68af\u5ea6\uff0c\u6700\u7ec8\u53ea\u4f1a\u9884\u6d4b\u4e00\u79cd\u7ed3\u679c\u3002\n- \u6240\u4ee5\u5e94\u8be5\u521d\u59cb\u5316\u4e3a\u63a5\u8fd10\u7684\u6570\n- \u5b9e\u73b0\u4ee3\u7801\n```\n# \u968f\u673a\u521d\u59cb\u5316\u6743\u91cdtheta\ndef randInitializeWeights(L_in,L_out):\n    W = np.zeros((L_out,1+L_in))    # \u5bf9\u5e94theta\u7684\u6743\u91cd\n    epsilon_init = (6.0\/(L_out+L_in))**0.5\n    W = np.random.rand(L_out,1+L_in)*2*epsilon_init-epsilon_init # np.random.rand(L_out,1+L_in)\u4ea7\u751fL_out*(1+L_in)\u5927\u5c0f\u7684\u968f\u673a\u77e9\u9635\n    return W\n```\n\n### 8\u3001\u9884\u6d4b\n- \u6b63\u5411\u4f20\u64ad\u9884\u6d4b\u7ed3\u679c\n- \u5b9e\u73b0\u4ee3\u7801\n```\n# \u9884\u6d4b\ndef predict(Theta1,Theta2,X):\n    m = X.shape[0]\n    num_labels = Theta2.shape[0]\n    #p = np.zeros((m,1))\n    '''\u6b63\u5411\u4f20\u64ad\uff0c\u9884\u6d4b\u7ed3\u679c'''\n    X = np.hstack((np.ones((m,1)),X))\n    h1 = sigmoid(np.dot(X,np.transpose(Theta1)))\n    h1 = np.hstack((np.ones((m,1)),h1))\n    h2 = sigmoid(np.dot(h1,np.transpose(Theta2)))\n    \n    '''\n    \u8fd4\u56deh\u4e2d\u6bcf\u4e00\u884c\u6700\u5927\u503c\u6240\u5728\u7684\u5217\u53f7\n    - np.max(h, axis=1)\u8fd4\u56deh\u4e2d\u6bcf\u4e00\u884c\u7684\u6700\u5927\u503c\uff08\u662f\u67d0\u4e2a\u6570\u5b57\u7684\u6700\u5927\u6982\u7387\uff09\n    - \u6700\u540ewhere\u627e\u5230\u7684\u6700\u5927\u6982\u7387\u6240\u5728\u7684\u5217\u53f7\uff08\u5217\u53f7\u5373\u662f\u5bf9\u5e94\u7684\u6570\u5b57\uff09\n    '''\n    #np.savetxt(\"h2.csv\",h2,delimiter=',')\n    p = np.array(np.where(h2[0,:] == np.max(h2, axis=1)[0]))  \n    for i in np.arange(1, m):\n        t = np.array(np.where(h2[i,:] == np.max(h2, axis=1)[i]))\n        p = np.vstack((p,t))\n    return p \n```\n\n### 9\u3001\u8f93\u51fa\u7ed3\u679c\n- \u68af\u5ea6\u68c0\u67e5\uff1a     \n![enter description here][19]\n- \u968f\u673a\u663e\u793a100\u4e2a\u624b\u5199\u6570\u5b57     \n![enter description here][20]\n- \u663e\u793atheta1\u6743\u91cd     \n![enter description here][21]\n- \u8bad\u7ec3\u96c6\u9884\u6d4b\u51c6\u786e\u5ea6     \n![enter description here][22]\n- \u5f52\u4e00\u5316\u540e\u8bad\u7ec3\u96c6\u9884\u6d4b\u51c6\u786e\u5ea6     \n![enter description here][23]\n\n--------------------\n\n## \u56db\u3001SVM\u652f\u6301\u5411\u91cf\u673a\n\n### 1\u3001\u4ee3\u4ef7\u51fd\u6570\n- \u5728\u903b\u8f91\u56de\u5f52\u4e2d\uff0c\u6211\u4eec\u7684\u4ee3\u4ef7\u4e3a\uff1a   \n![\\cos t({h_\\theta }(x),y) = \\left\\{ {\\begin{array}{c}    { - \\log ({h_\\theta }(x))} \\\\    { - \\log (1 - {h_\\theta }(x))}  \\end{array} \\begin{array}{c}    {y = 1} \\\\    {y = 0}  \\end{array} } \\right.](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28x%29%2Cy%29%20%3D%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5C%5C%20%20%20%20%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%20%5Cend%7Barray%7D%20%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7By%20%3D%201%7D%20%5C%5C%20%20%20%20%7By%20%3D%200%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright.)\uff0c    \n\u5176\u4e2d\uff1a![{h_\\theta }({\\text{z}}) = \\frac{1}{{1 + {e^{ - z}}}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bh_%5Ctheta%20%7D%28%7B%5Ctext%7Bz%7D%7D%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20z%7D%7D%7D%7D)\uff0c![z = {\\theta ^T}x](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=z%20%3D%20%7B%5Ctheta%20%5ET%7Dx)\n- \u5982\u56fe\u6240\u793a\uff0c\u5982\u679c`y=1`\uff0c`cost`\u4ee3\u4ef7\u51fd\u6570\u5982\u56fe\u6240\u793a    \n![enter description here][24]    \n\u6211\u4eec\u60f3\u8ba9![{\\theta ^T}x &gt;  &gt; 0](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ctheta%20%5ET%7Dx%20%3E%20%20%3E%200)\uff0c\u5373`z>>0`\uff0c\u8fd9\u6837\u7684\u8bdd`cost`\u4ee3\u4ef7\u51fd\u6570\u624d\u4f1a\u8d8b\u4e8e\u6700\u5c0f\uff08\u8fd9\u662f\u6211\u4eec\u60f3\u8981\u7684\uff09\uff0c\u6240\u4ee5\u7528\u9014\u4e2d**\u7ea2\u8272**\u7684\u51fd\u6570![\\cos {t_1}(z)](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Ccos%20%7Bt_1%7D%28z%29)\u4ee3\u66ff\u903b\u8f91\u56de\u5f52\u4e2d\u7684cost\n- \u5f53`y=0`\u65f6\u540c\u6837\uff0c\u7528![\\cos {t_0}(z)](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Ccos%20%7Bt_0%7D%28z%29)\u4ee3\u66ff\n![enter description here][25]\n- \u6700\u7ec8\u5f97\u5230\u7684\u4ee3\u4ef7\u51fd\u6570\u4e3a\uff1a    \n![J(\\theta ) = C\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\cos {t_1}({\\theta ^T}{x^{(i)}}) + (1 - {y^{(i)}})\\cos {t_0}({\\theta ^T}{x^{(i)}})} ] + \\frac{1}{2}\\sum\\limits_{j = 1}^{\\text{n}} {\\theta _j^2} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20C%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%7D%20%5D%20%2B%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20)   \n\u6700\u540e\u6211\u4eec\u60f3\u8981![\\mathop {\\min }\\limits_\\theta  J(\\theta )](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20%5Cmathop%20%7B%5Cmin%20%7D%5Climits_%5Ctheta%20J%28%5Ctheta%20%29)\n- \u4e4b\u524d\u6211\u4eec\u903b\u8f91\u56de\u5f52\u4e2d\u7684\u4ee3\u4ef7\u51fd\u6570\u4e3a\uff1a   \n![J(\\theta ) =  - \\frac{1}{m}\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\log ({h_\\theta }({x^{(i)}}) + (1 - } {y^{(i)}})\\log (1 - {h_\\theta }({x^{(i)}})] + \\frac{\\lambda }{{2m}}\\sum\\limits_{j = 1}^n {\\theta _j^2} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D%20%2B%20%5Cfrac%7B%5Clambda%20%7D%7B%7B2m%7D%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5En%20%7B%5Ctheta%20_j%5E2%7D%20)   \n\u53ef\u4ee5\u8ba4\u4e3a\u8fd9\u91cc\u7684![C = \\frac{m}{\\lambda }](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=C%20%3D%20%5Cfrac%7Bm%7D%7B%5Clambda%20%7D)\uff0c\u53ea\u662f\u8868\u8fbe\u5f62\u5f0f\u95ee\u9898\uff0c\u8fd9\u91cc`C`\u7684\u503c\u8d8a\u5927\uff0cSVM\u7684\u51b3\u7b56\u8fb9\u754c\u7684`margin`\u4e5f\u8d8a\u5927\uff0c\u4e0b\u9762\u4f1a\u8bf4\u660e\n\n### 2\u3001Large Margin\n- \u5982\u4e0b\u56fe\u6240\u793a,SVM\u5206\u7c7b\u4f1a\u4f7f\u7528\u6700\u5927\u7684`margin`\u5c06\u5176\u5206\u5f00    \n![enter description here][26]\n- \u5148\u8bf4\u4e00\u4e0b\u5411\u91cf\u5185\u79ef\n - ![u = \\left[ {\\begin{array}{c}    {{u_1}} \\\\    {{u_2}}  \\end{array} } \\right]](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=u%20%3D%20%5Cleft%5B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7Bu_1%7D%7D%20%5C%5C%20%20%20%20%7B%7Bu_2%7D%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright%5D)\uff0c![v = \\left[ {\\begin{array}{c}    {{v_1}} \\\\    {{v_2}}  \\end{array} } \\right]](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=v%20%3D%20%5Cleft%5B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7Bv_1%7D%7D%20%5C%5C%20%20%20%20%7B%7Bv_2%7D%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright%5D)    \n - ![||u||](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7C%7Cu%7C%7C)\u8868\u793a`u`\u7684**\u6b27\u51e0\u91cc\u5f97\u8303\u6570**\uff08\u6b27\u5f0f\u8303\u6570\uff09\uff0c![||u||{\\text{ = }}\\sqrt {{\\text{u}}_1^2 + u_2^2} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7C%7Cu%7C%7C%7B%5Ctext%7B%20%3D%20%7D%7D%5Csqrt%20%7B%7B%5Ctext%7Bu%7D%7D_1%5E2%20%2B%20u_2%5E2%7D%20)\n - `\u5411\u91cfV`\u5728`\u5411\u91cfu`\u4e0a\u7684\u6295\u5f71\u7684\u957f\u5ea6\u8bb0\u4e3a`p`\uff0c\u5219\uff1a\u5411\u91cf\u5185\u79ef\uff1a    \n ![{{\\text{u}}^T}v = p||u|| = {u_1}{v_1} + {u_2}{v_2}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Ctext%7Bu%7D%7D%5ET%7Dv%20%3D%20p%7C%7Cu%7C%7C%20%3D%20%7Bu_1%7D%7Bv_1%7D%20%2B%20%7Bu_2%7D%7Bv_2%7D)      \n ![enter description here][27]  \n\u6839\u636e\u5411\u91cf\u5939\u89d2\u516c\u5f0f\u63a8\u5bfc\u4e00\u4e0b\u5373\u53ef\uff0c![\\cos \\theta  = \\frac{{\\overrightarrow {\\text{u}} \\overrightarrow v }}{{|\\overrightarrow {\\text{u}} ||\\overrightarrow v |}}](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20%5Ccos%20%5Ctheta%20%3D%20%5Cfrac%7B%7B%5Coverrightarrow%20%7B%5Ctext%7Bu%7D%7D%20%5Coverrightarrow%20v%20%7D%7D%7B%7B%7C%5Coverrightarrow%20%7B%5Ctext%7Bu%7D%7D%20%7C%7C%5Coverrightarrow%20v%20%7C%7D%7D)\n\n- \u524d\u9762\u8bf4\u8fc7\uff0c\u5f53`C`\u8d8a\u5927\u65f6\uff0c`margin`\u4e5f\u5c31\u8d8a\u5927\uff0c\u6211\u4eec\u7684\u76ee\u7684\u662f\u6700\u5c0f\u5316\u4ee3\u4ef7\u51fd\u6570`J(\u03b8)`,\u5f53`margin`\u6700\u5927\u65f6\uff0c`C`\u7684\u4e58\u79ef\u9879![\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\cos {t_1}({\\theta ^T}{x^{(i)}}) + (1 - {y^{(i)}})\\cos {t_0}({\\theta ^T}{x^{(i)}})} ]](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%7D%20%5D)\u8981\u5f88\u5c0f\uff0c\u6240\u4ee5\u8fd1\u4f3c\u4e3a\uff1a   \n![J(\\theta ) = C0 + \\frac{1}{2}\\sum\\limits_{j = 1}^{\\text{n}} {\\theta _j^2}  = \\frac{1}{2}\\sum\\limits_{j = 1}^{\\text{n}} {\\theta _j^2}  = \\frac{1}{2}(\\theta _1^2 + \\theta _2^2) = \\frac{1}{2}{\\sqrt {\\theta _1^2 + \\theta _2^2} ^2}](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20J%28%5Ctheta%20%29%20%3D%20C0%20&plus;%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20%3D%20%5Cfrac%7B1%7D%7B2%7D%28%5Ctheta%20_1%5E2%20&plus;%20%5Ctheta%20_2%5E2%29%20%3D%20%5Cfrac%7B1%7D%7B2%7D%7B%5Csqrt%20%7B%5Ctheta%20_1%5E2%20&plus;%20%5Ctheta%20_2%5E2%7D%20%5E2%7D)\uff0c      \n\u6211\u4eec\u6700\u540e\u7684\u76ee\u7684\u5c31\u662f\u6c42\u4f7f\u4ee3\u4ef7\u6700\u5c0f\u7684`\u03b8`\n- \u7531   \n![\\left\\{ {\\begin{array}{c}    {{\\theta ^T}{x^{(i)}} \\geqslant 1} \\\\    {{\\theta ^T}{x^{(i)}} \\leqslant  - 1}  \\end{array} } \\right.\\begin{array}{c}    {({y^{(i)}} = 1)} \\\\    {({y^{(i)}} = 0)}  \\end{array} ](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%20%5Cgeqslant%201%7D%20%5C%5C%20%7B%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%20%5Cleqslant%20-%201%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%5Cbegin%7Barray%7D%7Bc%7D%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%201%29%7D%20%5C%5C%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%200%29%7D%20%5Cend%7Barray%7D)\u53ef\u4ee5\u5f97\u5230\uff1a    \n![\\left\\{ {\\begin{array}{c}    {{p^{(i)}}||\\theta || \\geqslant 1} \\\\    {{p^{(i)}}||\\theta || \\leqslant  - 1}  \\end{array} } \\right.\\begin{array}{c}    {({y^{(i)}} = 1)} \\\\    {({y^{(i)}} = 0)}  \\end{array} ](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%7Bp%5E%7B%28i%29%7D%7D%7C%7C%5Ctheta%20%7C%7C%20%5Cgeqslant%201%7D%20%5C%5C%20%7B%7Bp%5E%7B%28i%29%7D%7D%7C%7C%5Ctheta%20%7C%7C%20%5Cleqslant%20-%201%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%5Cbegin%7Barray%7D%7Bc%7D%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%201%29%7D%20%5C%5C%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%200%29%7D%20%5Cend%7Barray%7D)\uff0c`p`\u5373\u4e3a`x`\u5728`\u03b8`\u4e0a\u7684\u6295\u5f71\n- \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5047\u8bbe\u51b3\u7b56\u8fb9\u754c\u5982\u56fe\uff0c\u627e\u5176\u4e2d\u7684\u4e00\u4e2a\u70b9\uff0c\u5230`\u03b8`\u4e0a\u7684\u6295\u5f71\u4e3a`p`,\u5219![p||\\theta || \\geqslant 1](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20p%7C%7C%5Ctheta%20%7C%7C%20%5Cgeqslant%201)\u6216\u8005![p||\\theta || \\leqslant  - 1](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20p%7C%7C%5Ctheta%20%7C%7C%20%5Cleqslant%20-%201)\uff0c\u82e5\u662f`p`\u5f88\u5c0f\uff0c\u5219\u9700\u8981![||\\theta ||](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7C%7C%5Ctheta%20%7C%7C)\u5f88\u5927\uff0c\u8fd9\u4e0e\u6211\u4eec\u8981\u6c42\u7684`\u03b8`\u4f7f![||\\theta || = \\frac{1}{2}\\sqrt {\\theta _1^2 + \\theta _2^2} ](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20%7C%7C%5Ctheta%20%7C%7C%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csqrt%20%7B%5Ctheta%20_1%5E2%20&plus;%20%5Ctheta%20_2%5E2%7D)\u6700\u5c0f\u76f8\u8fdd\u80cc\uff0c**\u6240\u4ee5**\u6700\u540e\u6c42\u7684\u662f`large margin`   \n![enter description here][28]\n\n### 3\u3001SVM Kernel\uff08\u6838\u51fd\u6570\uff09\n- \u5bf9\u4e8e\u7ebf\u6027\u53ef\u5206\u7684\u95ee\u9898\uff0c\u4f7f\u7528**\u7ebf\u6027\u6838\u51fd\u6570**\u5373\u53ef\n- \u5bf9\u4e8e\u7ebf\u6027\u4e0d\u53ef\u5206\u7684\u95ee\u9898\uff0c\u5728\u903b\u8f91\u56de\u5f52\u4e2d\uff0c\u6211\u4eec\u662f\u5c06`feature`\u6620\u5c04\u4e3a\u4f7f\u7528\u591a\u9879\u5f0f\u7684\u5f62\u5f0f![1 + {x_1} + {x_2} + x_1^2 + {x_1}{x_2} + x_2^2](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=1%20%2B%20%7Bx_1%7D%20%2B%20%7Bx_2%7D%20%2B%20x_1%5E2%20%2B%20%7Bx_1%7D%7Bx_2%7D%20%2B%20x_2%5E2)\uff0c`SVM`\u4e2d\u4e5f\u6709**\u591a\u9879\u5f0f\u6838\u51fd\u6570**\uff0c\u4f46\u662f\u66f4\u5e38\u7528\u7684\u662f**\u9ad8\u65af\u6838\u51fd\u6570**\uff0c\u4e5f\u79f0\u4e3a**RBF\u6838**\n- \u9ad8\u65af\u6838\u51fd\u6570\u4e3a\uff1a![f(x) = {e^{ - \\frac{{||x - u|{|^2}}}{{2{\\sigma ^2}}}}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=f%28x%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20u%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D)     \n\u5047\u8bbe\u5982\u56fe\u51e0\u4e2a\u70b9\uff0c\n![enter description here][29]\n\u4ee4\uff1a   \n![{f_1} = similarity(x,{l^{(1)}}) = {e^{ - \\frac{{||x - {l^{(1)}}|{|^2}}}{{2{\\sigma ^2}}}}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bf_1%7D%20%3D%20similarity%28x%2C%7Bl%5E%7B%281%29%7D%7D%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20%7Bl%5E%7B%281%29%7D%7D%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D)   \n![{f_2} = similarity(x,{l^{(2)}}) = {e^{ - \\frac{{||x - {l^{(2)}}|{|^2}}}{{2{\\sigma ^2}}}}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bf_2%7D%20%3D%20similarity%28x%2C%7Bl%5E%7B%282%29%7D%7D%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20%7Bl%5E%7B%282%29%7D%7D%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D)\n.\n.\n.\n- \u53ef\u4ee5\u770b\u51fa\uff0c\u82e5\u662f`x`\u4e0e![{l^{(1)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bl%5E%7B%281%29%7D%7D)\u8ddd\u79bb\u8f83\u8fd1\uff0c==\u300b![{f_1} \\approx {e^0} = 1](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bf_1%7D%20%5Capprox%20%7Be%5E0%7D%20%3D%201)\uff0c\uff08\u5373\u76f8\u4f3c\u5ea6\u8f83\u5927\uff09   \n\u82e5\u662f`x`\u4e0e![{l^{(1)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bl%5E%7B%281%29%7D%7D)\u8ddd\u79bb\u8f83\u8fdc\uff0c==\u300b![{f_2} \\approx {e^{ - \\infty }} = 0](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bf_2%7D%20%5Capprox%20%7Be%5E%7B%20-%20%5Cinfty%20%7D%7D%20%3D%200)\uff0c\uff08\u5373\u76f8\u4f3c\u5ea6\u8f83\u4f4e\uff09\n- \u9ad8\u65af\u6838\u51fd\u6570\u7684`\u03c3`\u8d8a\u5c0f\uff0c`f`\u4e0b\u964d\u7684\u8d8a\u5feb      \n![enter description here][30]\n![enter description here][31]\n\n- \u5982\u4f55\u9009\u62e9\u521d\u59cb\u7684![{l^{(1)}}{l^{(2)}}{l^{(3)}}...](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bl%5E%7B%281%29%7D%7D%7Bl%5E%7B%282%29%7D%7D%7Bl%5E%7B%283%29%7D%7D...)\n - \u8bad\u7ec3\u96c6\uff1a![(({x^{(1)}},{y^{(1)}}),({x^{(2)}},{y^{(2)}}),...({x^{(m)}},{y^{(m)}}))](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%28%28%7Bx%5E%7B%281%29%7D%7D%2C%7By%5E%7B%281%29%7D%7D%29%2C%28%7Bx%5E%7B%282%29%7D%7D%2C%7By%5E%7B%282%29%7D%7D%29%2C...%28%7Bx%5E%7B%28m%29%7D%7D%2C%7By%5E%7B%28m%29%7D%7D%29%29)\n - \u9009\u62e9\uff1a![{l^{(1)}} = {x^{(1)}},{l^{(2)}} = {x^{(2)}}...{l^{(m)}} = {x^{(m)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bl%5E%7B%281%29%7D%7D%20%3D%20%7Bx%5E%7B%281%29%7D%7D%2C%7Bl%5E%7B%282%29%7D%7D%20%3D%20%7Bx%5E%7B%282%29%7D%7D...%7Bl%5E%7B%28m%29%7D%7D%20%3D%20%7Bx%5E%7B%28m%29%7D%7D)\n - \u5bf9\u4e8e\u7ed9\u51fa\u7684`x`\uff0c\u8ba1\u7b97`f`,\u4ee4\uff1a![f_0^{(i)} = 1](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=f_0%5E%7B%28i%29%7D%20%3D%201)\u6240\u4ee5\uff1a![{f^{(i)}} \\in {R^{m + 1}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bf%5E%7B%28i%29%7D%7D%20%5Cin%20%7BR%5E%7Bm%20%2B%201%7D%7D)\n - \u6700\u5c0f\u5316`J`\u6c42\u51fa`\u03b8`\uff0c          \n ![J(\\theta ) = C\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\cos {t_1}({\\theta ^T}{f^{(i)}}) + (1 - {y^{(i)}})\\cos {t_0}({\\theta ^T}{f^{(i)}})} ] + \\frac{1}{2}\\sum\\limits_{j = 1}^{\\text{n}} {\\theta _j^2} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20C%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bf%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bf%5E%7B%28i%29%7D%7D%29%7D%20%5D%20%2B%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20)\n - \u5982\u679c![{\\theta ^T}f \\geqslant 0](http:\/\/latex.codecogs.com\/gif.latex?%5Clarge%20%7B%5Ctheta%20%5ET%7Df%20%5Cgeqslant%200)\uff0c==\u300b\u9884\u6d4b`y=1`\n\n### 4\u3001\u4f7f\u7528`scikit-learn`\u4e2d\u7684`SVM`\u6a21\u578b\u4ee3\u7801\n- [\u5168\u90e8\u4ee3\u7801](\/SVM\/SVM_scikit-learn.py)\n- \u7ebf\u6027\u53ef\u5206\u7684,\u6307\u5b9a\u6838\u51fd\u6570\u4e3a`linear`\uff1a\n```\n    '''data1\u2014\u2014\u7ebf\u6027\u5206\u7c7b'''\n    data1 = spio.loadmat('data1.mat')\n    X = data1['X']\n    y = data1['y']\n    y = np.ravel(y)\n    plot_data(X,y)\n    \n    model = svm.SVC(C=1.0,kernel='linear').fit(X,y) # \u6307\u5b9a\u6838\u51fd\u6570\u4e3a\u7ebf\u6027\u6838\u51fd\u6570\n```\n- \u975e\u7ebf\u6027\u53ef\u5206\u7684\uff0c\u9ed8\u8ba4\u6838\u51fd\u6570\u4e3a`rbf`\n```\n    '''data2\u2014\u2014\u975e\u7ebf\u6027\u5206\u7c7b'''\n    data2 = spio.loadmat('data2.mat')\n    X = data2['X']\n    y = data2['y']\n    y = np.ravel(y)\n    plt = plot_data(X,y)\n    plt.show()\n    \n    model = svm.SVC(gamma=100).fit(X,y)     # gamma\u4e3a\u6838\u51fd\u6570\u7684\u7cfb\u6570\uff0c\u503c\u8d8a\u5927\u62df\u5408\u7684\u8d8a\u597d\n```\n### 5\u3001\u8fd0\u884c\u7ed3\u679c\n- \u7ebf\u6027\u53ef\u5206\u7684\u51b3\u7b56\u8fb9\u754c\uff1a    \n![enter description here][32]\n- \u7ebf\u6027\u4e0d\u53ef\u5206\u7684\u51b3\u7b56\u8fb9\u754c\uff1a   \n![enter description here][33]\n\n--------------------------\n\n## \u4e94\u3001K-Means\u805a\u7c7b\u7b97\u6cd5\n- [\u5168\u90e8\u4ee3\u7801](\/K-Means\/K-Menas.py)\n\n### 1\u3001\u805a\u7c7b\u8fc7\u7a0b\n- \u805a\u7c7b\u5c5e\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u4e0d\u77e5\u9053y\u7684\u6807\u8bb0\u5206\u4e3aK\u7c7b\n- K-Means\u7b97\u6cd5\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\n - \u7b2c\u4e00\u6b65\uff1a\u7c07\u5206\u914d\uff0c\u968f\u673a\u9009`K`\u4e2a\u70b9\u4f5c\u4e3a\u4e2d\u5fc3\uff0c\u8ba1\u7b97\u5230\u8fd9`K`\u4e2a\u70b9\u7684\u8ddd\u79bb\uff0c\u5206\u4e3a`K`\u4e2a\u7c07\n - \u7b2c\u4e8c\u6b65\uff1a\u79fb\u52a8\u805a\u7c7b\u4e2d\u5fc3\uff1a\u91cd\u65b0\u8ba1\u7b97\u6bcf\u4e2a**\u7c07**\u7684\u4e2d\u5fc3\uff0c\u79fb\u52a8\u4e2d\u5fc3\uff0c\u91cd\u590d\u4ee5\u4e0a\u6b65\u9aa4\u3002\n- \u5982\u4e0b\u56fe\u6240\u793a\uff1a\n - \u968f\u673a\u5206\u914d\u7684\u805a\u7c7b\u4e2d\u5fc3  \n ![enter description here][34]\n - \u91cd\u65b0\u8ba1\u7b97\u805a\u7c7b\u4e2d\u5fc3\uff0c\u79fb\u52a8\u4e00\u6b21  \n ![enter description here][35]\n - \u6700\u540e`10`\u6b65\u4e4b\u540e\u7684\u805a\u7c7b\u4e2d\u5fc3  \n ![enter description here][36]\n\n- \u8ba1\u7b97\u6bcf\u6761\u6570\u636e\u5230\u54ea\u4e2a\u4e2d\u5fc3\u6700\u8fd1\u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u627e\u5230\u6bcf\u6761\u6570\u636e\u8ddd\u79bb\u54ea\u4e2a\u7c7b\u4e2d\u5fc3\u6700\u8fd1    \ndef findClosestCentroids(X,initial_centroids):\n    m = X.shape[0]                  # \u6570\u636e\u6761\u6570\n    K = initial_centroids.shape[0]  # \u7c7b\u7684\u603b\u6570\n    dis = np.zeros((m,K))           # \u5b58\u50a8\u8ba1\u7b97\u6bcf\u4e2a\u70b9\u5206\u522b\u5230K\u4e2a\u7c7b\u7684\u8ddd\u79bb\n    idx = np.zeros((m,1))           # \u8981\u8fd4\u56de\u7684\u6bcf\u6761\u6570\u636e\u5c5e\u4e8e\u54ea\u4e2a\u7c7b\n    \n    '''\u8ba1\u7b97\u6bcf\u4e2a\u70b9\u5230\u6bcf\u4e2a\u7c7b\u4e2d\u5fc3\u7684\u8ddd\u79bb'''\n    for i in range(m):\n        for j in range(K):\n            dis[i,j] = np.dot((X[i,:]-initial_centroids[j,:]).reshape(1,-1),(X[i,:]-initial_centroids[j,:]).reshape(-1,1))\n    \n    '''\u8fd4\u56dedis\u6bcf\u4e00\u884c\u7684\u6700\u5c0f\u503c\u5bf9\u5e94\u7684\u5217\u53f7\uff0c\u5373\u4e3a\u5bf9\u5e94\u7684\u7c7b\u522b\n    - np.min(dis, axis=1)\u8fd4\u56de\u6bcf\u4e00\u884c\u7684\u6700\u5c0f\u503c\n    - np.where(dis == np.min(dis, axis=1).reshape(-1,1)) \u8fd4\u56de\u5bf9\u5e94\u6700\u5c0f\u503c\u7684\u5750\u6807\n     - \u6ce8\u610f\uff1a\u53ef\u80fd\u6700\u5c0f\u503c\u5bf9\u5e94\u7684\u5750\u6807\u6709\u591a\u4e2a\uff0cwhere\u90fd\u4f1a\u627e\u51fa\u6765\uff0c\u6240\u4ee5\u8fd4\u56de\u65f6\u8fd4\u56de\u524dm\u4e2a\u9700\u8981\u7684\u5373\u53ef\uff08\u56e0\u4e3a\u5bf9\u4e8e\u591a\u4e2a\u6700\u5c0f\u503c\uff0c\u5c5e\u4e8e\u54ea\u4e2a\u7c7b\u522b\u90fd\u53ef\u4ee5\uff09\n    '''  \n    dummy,idx = np.where(dis == np.min(dis, axis=1).reshape(-1,1))\n    return idx[0:dis.shape[0]]  # \u6ce8\u610f\u622a\u53d6\u4e00\u4e0b\n```\n- \u8ba1\u7b97\u7c7b\u4e2d\u5fc3\u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u8ba1\u7b97\u7c7b\u4e2d\u5fc3\ndef computerCentroids(X,idx,K):\n    n = X.shape[1]\n    centroids = np.zeros((K,n))\n    for i in range(K):\n        centroids[i,:] = np.mean(X[np.ravel(idx==i),:], axis=0).reshape(1,-1)   # \u7d22\u5f15\u8981\u662f\u4e00\u7ef4\u7684,axis=0\u4e3a\u6bcf\u4e00\u5217\uff0cidx==i\u4e00\u6b21\u627e\u51fa\u5c5e\u4e8e\u54ea\u4e00\u7c7b\u7684\uff0c\u7136\u540e\u8ba1\u7b97\u5747\u503c\n    return centroids\n```\n\n### 2\u3001\u76ee\u6807\u51fd\u6570\n- \u4e5f\u53eb\u505a**\u5931\u771f\u4ee3\u4ef7\u51fd\u6570**\n- ![J({c^{(1)}}, \\cdots ,{c^{(m)}},{u_1}, \\cdots ,{u_k}) = \\frac{1}{m}\\sum\\limits_{i = 1}^m {||{x^{(i)}} - {u_{{c^{(i)}}}}|{|^2}} ](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%7Bc%5E%7B%281%29%7D%7D%2C%20%5Ccdots%20%2C%7Bc%5E%7B%28m%29%7D%7D%2C%7Bu_1%7D%2C%20%5Ccdots%20%2C%7Bu_k%7D%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20%7Bu_%7B%7Bc%5E%7B%28i%29%7D%7D%7D%7D%7C%7B%7C%5E2%7D%7D%20)\n- \u6700\u540e\u6211\u4eec\u60f3\u5f97\u5230\uff1a  \n![enter description here][37]\n- \u5176\u4e2d![{c^{(i)}}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bc%5E%7B%28i%29%7D%7D)\u8868\u793a\u7b2c`i`\u6761\u6570\u636e\u8ddd\u79bb\u54ea\u4e2a\u7c7b\u4e2d\u5fc3\u6700\u8fd1\uff0c\n- \u5176\u4e2d![{u_i}](http:\/\/chart.apis.google.com\/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bu_i%7D)\u5373\u4e3a\u805a\u7c7b\u7684\u4e2d\u5fc3\n\n### 3\u3001\u805a\u7c7b\u4e2d\u5fc3\u7684\u9009\u62e9\n- \u968f\u673a\u521d\u59cb\u5316\uff0c\u4ece\u7ed9\u5b9a\u7684\u6570\u636e\u4e2d\u968f\u673a\u62bd\u53d6K\u4e2a\u4f5c\u4e3a\u805a\u7c7b\u4e2d\u5fc3\n- \u968f\u673a\u4e00\u6b21\u7684\u7ed3\u679c\u53ef\u80fd\u4e0d\u597d\uff0c\u53ef\u4ee5\u968f\u673a\u591a\u6b21\uff0c\u6700\u540e\u53d6\u4f7f\u4ee3\u4ef7\u51fd\u6570\u6700\u5c0f\u7684\u4f5c\u4e3a\u4e2d\u5fc3\n- \u5b9e\u73b0\u4ee3\u7801\uff1a(\u8fd9\u91cc\u968f\u673a\u4e00\u6b21)\n```\n# \u521d\u59cb\u5316\u7c7b\u4e2d\u5fc3--\u968f\u673a\u53d6K\u4e2a\u70b9\u4f5c\u4e3a\u805a\u7c7b\u4e2d\u5fc3\ndef kMeansInitCentroids(X,K):\n    m = X.shape[0]\n    m_arr = np.arange(0,m)      # \u751f\u62100-m-1\n    centroids = np.zeros((K,X.shape[1]))\n    np.random.shuffle(m_arr)    # \u6253\u4e71m_arr\u987a\u5e8f    \n    rand_indices = m_arr[:K]    # \u53d6\u524dK\u4e2a\n    centroids = X[rand_indices,:]\n    return centroids\n```\n\n### 4\u3001\u805a\u7c7b\u4e2a\u6570K\u7684\u9009\u62e9\n- \u805a\u7c7b\u662f\u4e0d\u77e5\u9053y\u7684label\u7684\uff0c\u6240\u4ee5\u4e0d\u77e5\u9053\u771f\u6b63\u7684\u805a\u7c7b\u4e2a\u6570\n- \u8098\u90e8\u6cd5\u5219\uff08Elbow method\uff09\n - \u4f5c\u4ee3\u4ef7\u51fd\u6570`J`\u548c`K`\u7684\u56fe\uff0c\u82e5\u662f\u51fa\u73b0\u4e00\u4e2a\u62d0\u70b9\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c`K`\u5c31\u53d6\u62d0\u70b9\u5904\u7684\u503c\uff0c\u4e0b\u56fe\u6b64\u65f6`K=3`\n ![enter description here][38]\n - \u82e5\u662f\u5f88\u5e73\u6ed1\u5c31\u4e0d\u660e\u786e\uff0c\u4eba\u4e3a\u9009\u62e9\u3002\n- \u7b2c\u4e8c\u79cd\u5c31\u662f\u4eba\u4e3a\u89c2\u5bdf\u9009\u62e9\n\n### 5\u3001\u5e94\u7528\u2014\u2014\u56fe\u7247\u538b\u7f29\n- \u5c06\u56fe\u7247\u7684\u50cf\u7d20\u5206\u4e3a\u82e5\u5e72\u7c7b\uff0c\u7136\u540e\u7528\u8fd9\u4e2a\u7c7b\u4ee3\u66ff\u539f\u6765\u7684\u50cf\u7d20\u503c\n- \u6267\u884c\u805a\u7c7b\u7684\u7b97\u6cd5\u4ee3\u7801\uff1a\n```\n# \u805a\u7c7b\u7b97\u6cd5\ndef runKMeans(X,initial_centroids,max_iters,plot_process):\n    m,n = X.shape                   # \u6570\u636e\u6761\u6570\u548c\u7ef4\u5ea6\n    K = initial_centroids.shape[0]  # \u7c7b\u6570\n    centroids = initial_centroids   # \u8bb0\u5f55\u5f53\u524d\u7c7b\u4e2d\u5fc3\n    previous_centroids = centroids  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u7c7b\u4e2d\u5fc3\n    idx = np.zeros((m,1))           # \u6bcf\u6761\u6570\u636e\u5c5e\u4e8e\u54ea\u4e2a\u7c7b\n    \n    for i in range(max_iters):      # \u8fed\u4ee3\u6b21\u6570\n        print u'\u8fed\u4ee3\u8ba1\u7b97\u6b21\u6570\uff1a%d'%(i+1)\n        idx = findClosestCentroids(X, centroids)\n        if plot_process:    # \u5982\u679c\u7ed8\u5236\u56fe\u50cf\n            plt = plotProcessKMeans(X,centroids,previous_centroids) # \u753b\u805a\u7c7b\u4e2d\u5fc3\u7684\u79fb\u52a8\u8fc7\u7a0b\n            previous_centroids = centroids  # \u91cd\u7f6e\n        centroids = computerCentroids(X, idx, K)    # \u91cd\u65b0\u8ba1\u7b97\u7c7b\u4e2d\u5fc3\n    if plot_process:    # \u663e\u793a\u6700\u7ec8\u7684\u7ed8\u5236\u7ed3\u679c\n        plt.show()\n    return centroids,idx    # \u8fd4\u56de\u805a\u7c7b\u4e2d\u5fc3\u548c\u6570\u636e\u5c5e\u4e8e\u54ea\u4e2a\u7c7b\n```\n\n### 6\u3001[\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684\u7ebf\u6027\u6a21\u578b\u5b9e\u73b0\u805a\u7c7b](\/K-Means\/K-Means_scikit-learn.py)\n\n- \u5bfc\u5165\u5305\n```\n    from sklearn.cluster import KMeans\n```\n- \u4f7f\u7528\u6a21\u578b\u62df\u5408\u6570\u636e\n```\n    model = KMeans(n_clusters=3).fit(X) # n_clusters\u6307\u5b9a3\u7c7b\uff0c\u62df\u5408\u6570\u636e\n```\n- \u805a\u7c7b\u4e2d\u5fc3\n```\n    centroids = model.cluster_centers_  # \u805a\u7c7b\u4e2d\u5fc3\n```\n\n### 7\u3001\u8fd0\u884c\u7ed3\u679c\n- \u4e8c\u7ef4\u6570\u636e\u7c7b\u4e2d\u5fc3\u7684\u79fb\u52a8  \n![enter description here][39]\n- \u56fe\u7247\u538b\u7f29  \n![enter description here][40]\n\n\n----------------------\n\n## \u516d\u3001PCA\u4e3b\u6210\u5206\u5206\u6790\uff08\u964d\u7ef4\uff09\n- [\u5168\u90e8\u4ee3\u7801](\/PCA\/PCA.py)\n\n### 1\u3001\u7528\u5904\n- \u6570\u636e\u538b\u7f29\uff08Data Compression\uff09,\u4f7f\u7a0b\u5e8f\u8fd0\u884c\u66f4\u5feb\n- \u53ef\u89c6\u5316\u6570\u636e\uff0c\u4f8b\u5982`3D-->2D`\u7b49\n- ......\n\n### 2\u30012D-->1D\uff0cnD-->kD\n- \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6240\u6709\u6570\u636e\u70b9\u53ef\u4ee5\u6295\u5f71\u5230\u4e00\u6761\u76f4\u7ebf\uff0c\u662f**\u6295\u5f71\u8ddd\u79bb\u7684\u5e73\u65b9\u548c**\uff08\u6295\u5f71\u8bef\u5dee\uff09\u6700\u5c0f\n![enter description here][41]\n- \u6ce8\u610f\u6570\u636e\u9700\u8981`\u5f52\u4e00\u5316`\u5904\u7406\n- \u601d\u8def\u662f\u627e`1`\u4e2a`\u5411\u91cfu`,\u6240\u6709\u6570\u636e\u6295\u5f71\u5230\u4e0a\u9762\u4f7f\u6295\u5f71\u8ddd\u79bb\u6700\u5c0f\n- \u90a3\u4e48`nD-->kD`\u5c31\u662f\u627e`k`\u4e2a\u5411\u91cf![$${u^{(1)}},{u^{(2)}} \\ldots {u^{(k)}}$$](http:\/\/latex.codecogs.com\/gif.latex?%24%24%7Bu%5E%7B%281%29%7D%7D%2C%7Bu%5E%7B%282%29%7D%7D%20%5Cldots%20%7Bu%5E%7B%28k%29%7D%7D%24%24)\uff0c\u6240\u6709\u6570\u636e\u6295\u5f71\u5230\u4e0a\u9762\u4f7f\u6295\u5f71\u8bef\u5dee\u6700\u5c0f\n - eg:3D-->2D,2\u4e2a\u5411\u91cf![$${u^{(1)}},{u^{(2)}}$$](http:\/\/latex.codecogs.com\/gif.latex?%24%24%7Bu%5E%7B%281%29%7D%7D%2C%7Bu%5E%7B%282%29%7D%7D%24%24)\u5c31\u4ee3\u8868\u4e00\u4e2a\u5e73\u9762\u4e86\uff0c\u6240\u6709\u70b9\u6295\u5f71\u5230\u8fd9\u4e2a\u5e73\u9762\u7684\u6295\u5f71\u8bef\u5dee\u6700\u5c0f\u5373\u53ef\n\n### 3\u3001\u4e3b\u6210\u5206\u5206\u6790PCA\u4e0e\u7ebf\u6027\u56de\u5f52\u7684\u533a\u522b\n- \u7ebf\u6027\u56de\u5f52\u662f\u627e`x`\u4e0e`y`\u7684\u5173\u7cfb\uff0c\u7136\u540e\u7528\u4e8e\u9884\u6d4b`y`\n- `PCA`\u662f\u627e\u4e00\u4e2a\u6295\u5f71\u9762\uff0c\u6700\u5c0f\u5316data\u5230\u8fd9\u4e2a\u6295\u5f71\u9762\u7684\u6295\u5f71\u8bef\u5dee\n\n### 4\u3001PCA\u964d\u7ef4\u8fc7\u7a0b\n- \u6570\u636e\u9884\u5904\u7406\uff08\u5747\u503c\u5f52\u4e00\u5316\uff09\n - \u516c\u5f0f\uff1a![$${\\rm{x}}_j^{(i)} = {{{\\rm{x}}_j^{(i)} - {u_j}} \\over {{s_j}}}$$](http:\/\/latex.codecogs.com\/gif.latex?%24%24%7B%5Crm%7Bx%7D%7D_j%5E%7B%28i%29%7D%20%3D%20%7B%7B%7B%5Crm%7Bx%7D%7D_j%5E%7B%28i%29%7D%20-%20%7Bu_j%7D%7D%20%5Cover%20%7B%7Bs_j%7D%7D%7D%24%24)\n - \u5c31\u662f\u51cf\u53bb\u5bf9\u5e94feature\u7684\u5747\u503c\uff0c\u7136\u540e\u9664\u4ee5\u5bf9\u5e94\u7279\u5f81\u7684\u6807\u51c6\u5dee\uff08\u4e5f\u53ef\u4ee5\u662f\u6700\u5927\u503c-\u6700\u5c0f\u503c\uff09\n - \u5b9e\u73b0\u4ee3\u7801\uff1a\n ```\n     # \u5f52\u4e00\u5316\u6570\u636e\n    def featureNormalize(X):\n        '''\uff08\u6bcf\u4e00\u4e2a\u6570\u636e-\u5f53\u524d\u5217\u7684\u5747\u503c\uff09\/\u5f53\u524d\u5217\u7684\u6807\u51c6\u5dee'''\n        n = X.shape[1]\n        mu = np.zeros((1,n));\n        sigma = np.zeros((1,n))\n        \n        mu = np.mean(X,axis=0)\n        sigma = np.std(X,axis=0)\n        for i in range(n):\n            X[:,i] = (X[:,i]-mu[i])\/sigma[i]\n        return X,mu,sigma\n ```\n- \u8ba1\u7b97`\u534f\u65b9\u5dee\u77e9\u9635\u03a3`\uff08Covariance Matrix\uff09\uff1a![$$\\Sigma  = {1 \\over m}\\sum\\limits_{i = 1}^n {{x^{(i)}}{{({x^{(i)}})}^T}} $$](http:\/\/latex.codecogs.com\/gif.latex?%24%24%5CSigma%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5En%20%7B%7Bx%5E%7B%28i%29%7D%7D%7B%7B%28%7Bx%5E%7B%28i%29%7D%7D%29%7D%5ET%7D%7D%20%24%24)\n - \u6ce8\u610f\u8fd9\u91cc\u7684`\u03a3`\u548c\u6c42\u548c\u7b26\u53f7\u4e0d\u540c\n - \u534f\u65b9\u5dee\u77e9\u9635`\u5bf9\u79f0\u6b63\u5b9a`\uff08\u4e0d\u7406\u89e3\u6b63\u5b9a\u7684\u770b\u770b\u7ebf\u4ee3\uff09\n - \u5927\u5c0f\u4e3a`nxn`,`n`\u4e3a`feature`\u7684\u7ef4\u5ea6\n - \u5b9e\u73b0\u4ee3\u7801\uff1a\n ```\n Sigma = np.dot(np.transpose(X_norm),X_norm)\/m  # \u6c42Sigma\n ```\n- \u8ba1\u7b97`\u03a3`\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\n - \u53ef\u4ee5\u662f\u7528`svd`\u5947\u5f02\u503c\u5206\u89e3\u51fd\u6570\uff1a`U,S,V = svd(\u03a3)`\n - \u8fd4\u56de\u7684\u662f\u4e0e`\u03a3`\u540c\u6837\u5927\u5c0f\u7684\u5bf9\u89d2\u9635`S`\uff08\u7531`\u03a3`\u7684\u7279\u5f81\u503c\u7ec4\u6210\uff09[**\u6ce8\u610f**\uff1a`matlab`\u4e2d\u51fd\u6570\u8fd4\u56de\u7684\u662f\u5bf9\u89d2\u9635\uff0c\u5728`python`\u4e2d\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u8282\u7701\u7a7a\u95f4]\n - \u8fd8\u6709\u4e24\u4e2a**\u9149\u77e9\u9635**U\u548cV\uff0c\u4e14![$$\\Sigma  = US{V^T}$$](http:\/\/latex.codecogs.com\/gif.latex?%24%24%5CSigma%20%3D%20US%7BV%5ET%7D%24%24)\n - ![enter description here][42]\n - **\u6ce8\u610f**\uff1a`svd`\u51fd\u6570\u6c42\u51fa\u7684`S`\u662f\u6309\u7279\u5f81\u503c\u964d\u5e8f\u6392\u5217\u7684\uff0c\u82e5\u4e0d\u662f\u4f7f\u7528`svd`,\u9700\u8981\u6309**\u7279\u5f81\u503c**\u5927\u5c0f\u91cd\u65b0\u6392\u5217`U`\n- \u964d\u7ef4\n - \u9009\u53d6`U`\u4e2d\u7684\u524d`K`\u5217\uff08\u5047\u8bbe\u8981\u964d\u4e3a`K`\u7ef4\uff09\n - ![enter description here][43]\n - `Z`\u5c31\u662f\u5bf9\u5e94\u964d\u7ef4\u4e4b\u540e\u7684\u6570\u636e\n - \u5b9e\u73b0\u4ee3\u7801\uff1a\n ```\n     # \u6620\u5c04\u6570\u636e\n    def projectData(X_norm,U,K):\n        Z = np.zeros((X_norm.shape[0],K))\n        \n        U_reduce = U[:,0:K]          # \u53d6\u524dK\u4e2a\n        Z = np.dot(X_norm,U_reduce) \n        return Z\n ```\n- \u8fc7\u7a0b\u603b\u7ed3\uff1a\n - `Sigma = X'*X\/m`\n - `U,S,V = svd(Sigma)`\n - `Ureduce = U[:,0:k]`\n - `Z = Ureduce'*x`\n\n### 5\u3001\u6570\u636e\u6062\u590d\n - \u56e0\u4e3a\uff1a![$${Z^{(i)}} = U_{reduce}^T*{X^{(i)}}$$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24%7BZ%5E%7B%28i%29%7D%7D%20%3D%20U_%7Breduce%7D%5ET*%7BX%5E%7B%28i%29%7D%7D%24%24)\n - \u6240\u4ee5\uff1a![$${X_{approx}} = {(U_{reduce}^T)^{ - 1}}Z$$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24%7BX_%7Bapprox%7D%7D%20%3D%20%7B%28U_%7Breduce%7D%5ET%29%5E%7B%20-%201%7D%7DZ%24%24)     \uff08\u6ce8\u610f\u8fd9\u91cc\u662fX\u7684\u8fd1\u4f3c\u503c\uff09\n - \u53c8\u56e0\u4e3a`Ureduce`\u4e3a\u6b63\u5b9a\u77e9\u9635\uff0c\u3010\u6b63\u5b9a\u77e9\u9635\u6ee1\u8db3\uff1a![$$A{A^T} = {A^T}A = E$$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24A%7BA%5ET%7D%20%3D%20%7BA%5ET%7DA%20%3D%20E%24%24)\uff0c\u6240\u4ee5\uff1a![$${A^{ - 1}} = {A^T}$$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24%7BA%5E%7B%20-%201%7D%7D%20%3D%20%7BA%5ET%7D%24%24)\u3011\uff0c\u6240\u4ee5\u8fd9\u91cc\uff1a\n - ![$${X_{approx}} = {(U_{reduce}^{ - 1})^{ - 1}}Z = {U_{reduce}}Z$$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24%7BX_%7Bapprox%7D%7D%20%3D%20%7B%28U_%7Breduce%7D%5E%7B%20-%201%7D%29%5E%7B%20-%201%7D%7DZ%20%3D%20%7BU_%7Breduce%7D%7DZ%24%24)\n - \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n    # \u6062\u590d\u6570\u636e \n    def recoverData(Z,U,K):\n        X_rec = np.zeros((Z.shape[0],U.shape[0]))\n        U_recude = U[:,0:K]\n        X_rec = np.dot(Z,np.transpose(U_recude))  # \u8fd8\u539f\u6570\u636e\uff08\u8fd1\u4f3c\uff09\n        return X_rec\n```\n\n### 6\u3001\u4e3b\u6210\u5206\u4e2a\u6570\u7684\u9009\u62e9\uff08\u5373\u8981\u964d\u7684\u7ef4\u5ea6\uff09\n- \u5982\u4f55\u9009\u62e9\n - **\u6295\u5f71\u8bef\u5dee**\uff08project error\uff09\uff1a![$${1 \\over m}\\sum\\limits_{i = 1}^m {||{x^{(i)}} - x_{approx}^{(i)}|{|^2}} $$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20x_%7Bapprox%7D%5E%7B%28i%29%7D%7C%7B%7C%5E2%7D%7D%20%24%24)\n - **\u603b\u53d8\u5dee**\uff08total variation\uff09:![$${1 \\over m}\\sum\\limits_{i = 1}^m {||{x^{(i)}}|{|^2}} $$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%7C%7B%7C%5E2%7D%7D%20%24%24)\n - \u82e5**\u8bef\u5dee\u7387**\uff08error ratio\uff09\uff1a![$${{{1 \\over m}\\sum\\limits_{i = 1}^m {||{x^{(i)}} - x_{approx}^{(i)}|{|^2}} } \\over {{1 \\over m}\\sum\\limits_{i = 1}^m {||{x^{(i)}}|{|^2}} }} \\le 0.01$$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24%7B%7B%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20x_%7Bapprox%7D%5E%7B%28i%29%7D%7C%7B%7C%5E2%7D%7D%20%7D%20%5Cover%20%7B%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%7C%7B%7C%5E2%7D%7D%20%7D%7D%20%5Cle%200.01%24%24)\uff0c\u5219\u79f0`99%`\u4fdd\u7559\u5dee\u5f02\u6027\n - \u8bef\u5dee\u7387\u4e00\u822c\u53d6`1%\uff0c5%\uff0c10%`\u7b49\n- \u5982\u4f55\u5b9e\u73b0\n - \u82e5\u662f\u4e00\u4e2a\u4e2a\u8bd5\u7684\u8bdd\u4ee3\u4ef7\u592a\u5927\n - \u4e4b\u524d`U,S,V = svd(Sigma)`,\u6211\u4eec\u5f97\u5230\u4e86`S`\uff0c\u8fd9\u91cc\u8bef\u5dee\u7387error ratio:    \n ![$$error{\\kern 1pt} \\;ratio = 1 - {{\\sum\\limits_{i = 1}^k {{S_{ii}}} } \\over {\\sum\\limits_{i = 1}^n {{S_{ii}}} }} \\le threshold$$](http:\/\/latex.codecogs.com\/gif.latex?%5Cfn_cm%20%24%24error%7B%5Ckern%201pt%7D%20%5C%3Bratio%20%3D%201%20-%20%7B%7B%5Csum%5Climits_%7Bi%20%3D%201%7D%5Ek%20%7B%7BS_%7Bii%7D%7D%7D%20%7D%20%5Cover%20%7B%5Csum%5Climits_%7Bi%20%3D%201%7D%5En%20%7B%7BS_%7Bii%7D%7D%7D%20%7D%7D%20%5Cle%20threshold%24%24)\n - \u53ef\u4ee5\u4e00\u70b9\u70b9\u589e\u52a0`K`\u5c1d\u8bd5\u3002\n\n### 7\u3001\u4f7f\u7528\u5efa\u8bae\n- \u4e0d\u8981\u4f7f\u7528PCA\u53bb\u89e3\u51b3\u8fc7\u62df\u5408\u95ee\u9898`Overfitting`\uff0c\u8fd8\u662f\u4f7f\u7528\u6b63\u5219\u5316\u7684\u65b9\u6cd5\uff08\u5982\u679c\u4fdd\u7559\u4e86\u5f88\u9ad8\u7684\u5dee\u5f02\u6027\u8fd8\u662f\u53ef\u4ee5\u7684\uff09\n- \u53ea\u6709\u5728\u539f\u6570\u636e\u4e0a\u6709\u597d\u7684\u7ed3\u679c\uff0c\u4f46\u662f\u8fd0\u884c\u5f88\u6162\uff0c\u624d\u8003\u8651\u4f7f\u7528PCA\n\n### 8\u3001\u8fd0\u884c\u7ed3\u679c\n- 2\u7ef4\u6570\u636e\u964d\u4e3a1\u7ef4\n - \u8981\u6295\u5f71\u7684\u65b9\u5411     \n![enter description here][44]\n - 2D\u964d\u4e3a1D\u53ca\u5bf9\u5e94\u5173\u7cfb        \n![enter description here][45]\n- \u4eba\u8138\u6570\u636e\u964d\u7ef4\n - \u539f\u59cb\u6570\u636e         \n ![enter description here][46]\n - \u53ef\u89c6\u5316\u90e8\u5206`U`\u77e9\u9635\u4fe1\u606f    \n ![enter description here][47]\n - \u6062\u590d\u6570\u636e    \n ![enter description here][48]\n\n### 9\u3001[\u4f7f\u7528scikit-learn\u5e93\u4e2d\u7684PCA\u5b9e\u73b0\u964d\u7ef4](\/PCA\/PCA.py_scikit-learn.py)\n- \u5bfc\u5165\u9700\u8981\u7684\u5305\uff1a\n```\n#-*- coding: utf-8 -*-\n# Author:bob\n# Date:2016.12.22\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy import io as spio\nfrom sklearn.decomposition import pca\nfrom sklearn.preprocessing import StandardScaler\n```\n- \u5f52\u4e00\u5316\u6570\u636e\n```\n    '''\u5f52\u4e00\u5316\u6570\u636e\u5e76\u4f5c\u56fe'''\n    scaler = StandardScaler()\n    scaler.fit(X)\n    x_train = scaler.transform(X)\n```\n- \u4f7f\u7528PCA\u6a21\u578b\u62df\u5408\u6570\u636e\uff0c\u5e76\u964d\u7ef4\n - `n_components`\u5bf9\u5e94\u8981\u5c06\u7684\u7ef4\u5ea6\n```\n    '''\u62df\u5408\u6570\u636e'''\n    K=1 # \u8981\u964d\u7684\u7ef4\u5ea6\n    model = pca.PCA(n_components=K).fit(x_train)   # \u62df\u5408\u6570\u636e\uff0cn_components\u5b9a\u4e49\u8981\u964d\u7684\u7ef4\u5ea6\n    Z = model.transform(x_train)    # transform\u5c31\u4f1a\u6267\u884c\u964d\u7ef4\u64cd\u4f5c\n```\n\n- \u6570\u636e\u6062\u590d\n - `model.components_`\u4f1a\u5f97\u5230\u964d\u7ef4\u4f7f\u7528\u7684`U`\u77e9\u9635 \n```\n    '''\u6570\u636e\u6062\u590d\u5e76\u4f5c\u56fe'''\n    Ureduce = model.components_     # \u5f97\u5230\u964d\u7ef4\u7528\u7684Ureduce\n    x_rec = np.dot(Z,Ureduce)       # \u6570\u636e\u6062\u590d\n```\n\n\n\n---------------------------------------------------------------\n\n\n## \u4e03\u3001\u5f02\u5e38\u68c0\u6d4b Anomaly Detection\n- [\u5168\u90e8\u4ee3\u7801](\/AnomalyDetection\/AnomalyDetection.py)\n\n### 1\u3001\u9ad8\u65af\u5206\u5e03\uff08\u6b63\u6001\u5206\u5e03\uff09`Gaussian distribution` \n- \u5206\u5e03\u51fd\u6570\uff1a![$$p(x) = {1 \\over {\\sqrt {2\\pi } \\sigma }}{e^{ - {{{{(x - u)}^2}} \\over {2{\\sigma ^2}}}}}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20%7B1%20%5Cover%20%7B%5Csqrt%20%7B2%5Cpi%20%7D%20%5Csigma%20%7D%7D%7Be%5E%7B%20-%20%7B%7B%7B%7B%28x%20-%20u%29%7D%5E2%7D%7D%20%5Cover%20%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D%24%24)\n - \u5176\u4e2d\uff0c`u`\u4e3a\u6570\u636e\u7684**\u5747\u503c**\uff0c`\u03c3`\u4e3a\u6570\u636e\u7684**\u6807\u51c6\u5dee**\n - `\u03c3`\u8d8a**\u5c0f**\uff0c\u5bf9\u5e94\u7684\u56fe\u50cf\u8d8a**\u5c16**\n- \u53c2\u6570\u4f30\u8ba1\uff08`parameter estimation`\uff09\n - ![$$u = {1 \\over m}\\sum\\limits_{i = 1}^m {{x^{(i)}}} $$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24u%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7Bx%5E%7B%28i%29%7D%7D%7D%20%24%24)\n - ![$${\\sigma ^2} = {1 \\over m}\\sum\\limits_{i = 1}^m {{{({x^{(i)}} - u)}^2}} $$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%7B%5Csigma%20%5E2%7D%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7B%7B%28%7Bx%5E%7B%28i%29%7D%7D%20-%20u%29%7D%5E2%7D%7D%20%24%24)\n\n### 2\u3001\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\n- \u4f8b\u5b50\n - \u8bad\u7ec3\u96c6\uff1a![$$\\{ {x^{(1)}},{x^{(2)}}, \\cdots {x^{(m)}}\\} $$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%5C%7B%20%7Bx%5E%7B%281%29%7D%7D%2C%7Bx%5E%7B%282%29%7D%7D%2C%20%5Ccdots%20%7Bx%5E%7B%28m%29%7D%7D%5C%7D%20%24%24),\u5176\u4e2d![$$x \\in {R^n}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24x%20%5Cin%20%7BR%5En%7D%24%24)\n - \u5047\u8bbe![$${x_1},{x_2} \\cdots {x_n}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%7Bx_1%7D%2C%7Bx_2%7D%20%5Ccdots%20%7Bx_n%7D%24%24)\u76f8\u4e92\u72ec\u7acb\uff0c\u5efa\u7acbmodel\u6a21\u578b\uff1a![$$p(x) = p({x_1};{u_1},\\sigma _1^2)p({x_2};{u_2},\\sigma _2^2) \\cdots p({x_n};{u_n},\\sigma _n^2) = \\prod\\limits_{j = 1}^n {p({x_j};{u_j},\\sigma _j^2)} $$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20p%28%7Bx_1%7D%3B%7Bu_1%7D%2C%5Csigma%20_1%5E2%29p%28%7Bx_2%7D%3B%7Bu_2%7D%2C%5Csigma%20_2%5E2%29%20%5Ccdots%20p%28%7Bx_n%7D%3B%7Bu_n%7D%2C%5Csigma%20_n%5E2%29%20%3D%20%5Cprod%5Climits_%7Bj%20%3D%201%7D%5En%20%7Bp%28%7Bx_j%7D%3B%7Bu_j%7D%2C%5Csigma%20_j%5E2%29%7D%20%24%24)\n- \u8fc7\u7a0b\n - \u9009\u62e9\u5177\u6709\u4ee3\u8868\u5f02\u5e38\u7684`feature`:xi\n - \u53c2\u6570\u4f30\u8ba1\uff1a![$${u_1},{u_2}, \\cdots ,{u_n};\\sigma _1^2,\\sigma _2^2 \\cdots ,\\sigma _n^2$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%7Bu_1%7D%2C%7Bu_2%7D%2C%20%5Ccdots%20%2C%7Bu_n%7D%3B%5Csigma%20_1%5E2%2C%5Csigma%20_2%5E2%20%5Ccdots%20%2C%5Csigma%20_n%5E2%24%24)\n - \u8ba1\u7b97`p(x)`,\u82e5\u662f`P(x)<\u03b5`\u5219\u8ba4\u4e3a\u5f02\u5e38\uff0c\u5176\u4e2d`\u03b5`\u4e3a\u6211\u4eec\u8981\u6c42\u7684\u6982\u7387\u7684\u4e34\u754c\u503c`threshold`\n- \u8fd9\u91cc\u53ea\u662f**\u5355\u5143\u9ad8\u65af\u5206\u5e03**\uff0c\u5047\u8bbe\u4e86`feature`\u4e4b\u95f4\u662f\u72ec\u7acb\u7684\uff0c\u4e0b\u9762\u4f1a\u8bb2\u5230**\u591a\u5143\u9ad8\u65af\u5206\u5e03**\uff0c\u4f1a\u81ea\u52a8\u6355\u6349\u5230`feature`\u4e4b\u95f4\u7684\u5173\u7cfb\n- **\u53c2\u6570\u4f30\u8ba1**\u5b9e\u73b0\u4ee3\u7801\n```\n# \u53c2\u6570\u4f30\u8ba1\u51fd\u6570\uff08\u5c31\u662f\u6c42\u5747\u503c\u548c\u65b9\u5dee\uff09\ndef estimateGaussian(X):\n    m,n = X.shape\n    mu = np.zeros((n,1))\n    sigma2 = np.zeros((n,1))\n    \n    mu = np.mean(X, axis=0) # axis=0\u8868\u793a\u5217\uff0c\u6bcf\u5217\u7684\u5747\u503c\n    sigma2 = np.var(X,axis=0) # \u6c42\u6bcf\u5217\u7684\u65b9\u5dee\n    return mu,sigma2\n```\n\n### 3\u3001\u8bc4\u4ef7`p(x)`\u7684\u597d\u574f\uff0c\u4ee5\u53ca`\u03b5`\u7684\u9009\u53d6\n- \u5bf9**\u504f\u659c\u6570\u636e**\u7684\u9519\u8bef\u5ea6\u91cf\n - \u56e0\u4e3a\u6570\u636e\u53ef\u80fd\u662f\u975e\u5e38**\u504f\u659c**\u7684\uff08\u5c31\u662f`y=1`\u7684\u4e2a\u6570\u975e\u5e38\u5c11\uff0c(`y=1`\u8868\u793a\u5f02\u5e38)\uff09\uff0c\u6240\u4ee5\u53ef\u4ee5\u4f7f\u7528`Precision\/Recall`\uff0c\u8ba1\u7b97`F1Score`(\u5728**CV\u4ea4\u53c9\u9a8c\u8bc1\u96c6**\u4e0a)\n - \u4f8b\u5982\uff1a\u9884\u6d4b\u764c\u75c7\uff0c\u5047\u8bbe\u6a21\u578b\u53ef\u4ee5\u5f97\u5230`99%`\u80fd\u591f\u9884\u6d4b\u6b63\u786e\uff0c`1%`\u7684\u9519\u8bef\u7387\uff0c\u4f46\u662f\u5b9e\u9645\u764c\u75c7\u7684\u6982\u7387\u5f88\u5c0f\uff0c\u53ea\u6709`0.5%`\uff0c\u90a3\u4e48\u6211\u4eec\u59cb\u7ec8\u9884\u6d4b\u6ca1\u6709\u764c\u75c7y=0\u53cd\u800c\u53ef\u4ee5\u5f97\u5230\u66f4\u5c0f\u7684\u9519\u8bef\u7387\u3002\u4f7f\u7528`error rate`\u6765\u8bc4\u4f30\u5c31\u4e0d\u79d1\u5b66\u4e86\u3002\n - \u5982\u4e0b\u56fe\u8bb0\u5f55\uff1a    \n ![enter description here][49]\n - ![$$\\Pr ecision = {{TP} \\over {TP + FP}}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%5CPr%20ecision%20%3D%20%7B%7BTP%7D%20%5Cover%20%7BTP%20&plus;%20FP%7D%7D%24%24) \uff0c\u5373\uff1a**\u6b63\u786e\u9884\u6d4b\u6b63\u6837\u672c\/\u6240\u6709\u9884\u6d4b\u6b63\u6837\u672c**\n - ![$${\\mathop{\\rm Re}\\nolimits} {\\rm{call}} = {{TP} \\over {TP + FN}}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%7B%5Cmathop%7B%5Crm%20Re%7D%5Cnolimits%7D%20%7B%5Crm%7Bcall%7D%7D%20%3D%20%7B%7BTP%7D%20%5Cover%20%7BTP%20&plus;%20FN%7D%7D%24%24) \uff0c\u5373\uff1a**\u6b63\u786e\u9884\u6d4b\u6b63\u6837\u672c\/\u771f\u5b9e\u503c\u4e3a\u6b63\u6837\u672c**\n - \u603b\u662f\u8ba9`y=1`(\u8f83\u5c11\u7684\u7c7b)\uff0c\u8ba1\u7b97`Precision`\u548c`Recall`\n - ![$${F_1}Score = 2{{PR} \\over {P + R}}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%7BF_1%7DScore%20%3D%202%7B%7BPR%7D%20%5Cover%20%7BP%20&plus;%20R%7D%7D%24%24)\n - \u8fd8\u662f\u4ee5\u764c\u75c7\u9884\u6d4b\u4e3a\u4f8b\uff0c\u5047\u8bbe\u9884\u6d4b\u90fd\u662fno-cancer\uff0cTN=199\uff0cFN=1\uff0cTP=0\uff0cFP=0\uff0c\u6240\u4ee5\uff1aPrecision=0\/0\uff0cRecall=0\/1=0\uff0c\u5c3d\u7ba1accuracy=199\/200=99.5%\uff0c\u4f46\u662f\u4e0d\u53ef\u4fe1\u3002\n\n- `\u03b5`\u7684\u9009\u53d6\n - \u5c1d\u8bd5\u591a\u4e2a`\u03b5`\u503c\uff0c\u4f7f`F1Score`\u7684\u503c\u9ad8\n- \u5b9e\u73b0\u4ee3\u7801\n```\n# \u9009\u62e9\u6700\u4f18\u7684epsilon\uff0c\u5373\uff1a\u4f7fF1Score\u6700\u5927    \ndef selectThreshold(yval,pval):\n    '''\u521d\u59cb\u5316\u6240\u9700\u53d8\u91cf'''\n    bestEpsilon = 0.\n    bestF1 = 0.\n    F1 = 0.\n    step = (np.max(pval)-np.min(pval))\/1000\n    '''\u8ba1\u7b97'''\n    for epsilon in np.arange(np.min(pval),np.max(pval),step):\n        cvPrecision = pval<epsilon\n        tp = np.sum((cvPrecision == 1) & (yval == 1).ravel()).astype(float)  # sum\u6c42\u548c\u662fint\u578b\u7684\uff0c\u9700\u8981\u8f6c\u4e3afloat\n        fp = np.sum((cvPrecision == 1) & (yval == 0).ravel()).astype(float)\n        fn = np.sum((cvPrecision == 0) & (yval == 1).ravel()).astype(float)\n        precision = tp\/(tp+fp)  # \u7cbe\u51c6\u5ea6\n        recision = tp\/(tp+fn)   # \u53ec\u56de\u7387\n        F1 = (2*precision*recision)\/(precision+recision)  # F1Score\u8ba1\u7b97\u516c\u5f0f\n        if F1 > bestF1:  # \u4fee\u6539\u6700\u4f18\u7684F1 Score\n            bestF1 = F1\n            bestEpsilon = epsilon\n    return bestEpsilon,bestF1\n```\n\n### 4\u3001\u9009\u62e9\u4f7f\u7528\u4ec0\u4e48\u6837\u7684feature\uff08\u5355\u5143\u9ad8\u65af\u5206\u5e03\uff09\n- \u5982\u679c\u4e00\u4e9b\u6570\u636e\u4e0d\u662f\u6ee1\u8db3\u9ad8\u65af\u5206\u5e03\u7684\uff0c\u53ef\u4ee5\u53d8\u5316\u4e00\u4e0b\u6570\u636e\uff0c\u4f8b\u5982`log(x+C),x^(1\/2)`\u7b49\n- \u5982\u679c`p(x)`\u7684\u503c\u65e0\u8bba\u5f02\u5e38\u4e0e\u5426\u90fd\u5f88\u5927\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u7ec4\u5408\u591a\u4e2a`feature`,(\u56e0\u4e3afeature\u4e4b\u95f4\u53ef\u80fd\u662f\u6709\u5173\u7cfb\u7684)\n\n### 5\u3001\u591a\u5143\u9ad8\u65af\u5206\u5e03\n- \u5355\u5143\u9ad8\u65af\u5206\u5e03\u5b58\u5728\u7684\u95ee\u9898\n - \u5982\u4e0b\u56fe\uff0c\u7ea2\u8272\u7684\u70b9\u4e3a\u5f02\u5e38\u70b9\uff0c\u5176\u4ed6\u7684\u90fd\u662f\u6b63\u5e38\u70b9\uff08\u6bd4\u5982CPU\u548cmemory\u7684\u53d8\u5316\uff09   \n ![enter description here][50]\n - x1\u5bf9\u5e94\u7684\u9ad8\u65af\u5206\u5e03\u5982\u4e0b\uff1a   \n ![enter description here][51]\n - x2\u5bf9\u5e94\u7684\u9ad8\u65af\u5206\u5e03\u5982\u4e0b\uff1a   \n ![enter description here][52]\n - \u53ef\u4ee5\u770b\u51fa\u5bf9\u5e94\u7684p(x1)\u548cp(x2)\u7684\u503c\u53d8\u5316\u5e76\u4e0d\u5927\uff0c\u5c31\u4e0d\u4f1a\u8ba4\u4e3a\u5f02\u5e38\n - \u56e0\u4e3a\u6211\u4eec\u8ba4\u4e3afeature\u4e4b\u95f4\u662f\u76f8\u4e92\u72ec\u7acb\u7684\uff0c\u6240\u4ee5\u5982\u4e0a\u56fe\u662f\u4ee5**\u6b63\u5706**\u7684\u65b9\u5f0f\u6269\u5c55\n- \u591a\u5143\u9ad8\u65af\u5206\u5e03\n - ![$$x \\in {R^n}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24x%20%5Cin%20%7BR%5En%7D%24%24)\uff0c\u5e76\u4e0d\u662f\u5efa\u7acb`p(x1),p(x2)...p(xn)`\uff0c\u800c\u662f\u7edf\u4e00\u5efa\u7acb`p(x)`\n - \u5176\u4e2d\u53c2\u6570\uff1a![$$\\mu  \\in {R^n},\\Sigma  \\in {R^{n \\times {\\rm{n}}}}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%5Cmu%20%5Cin%20%7BR%5En%7D%2C%5CSigma%20%5Cin%20%7BR%5E%7Bn%20%5Ctimes%20%7B%5Crm%7Bn%7D%7D%7D%7D%24%24),`\u03a3`\u4e3a**\u534f\u65b9\u5dee\u77e9\u9635**\n - ![$$p(x) = {1 \\over {{{(2\\pi )}^{{n \\over 2}}}|\\Sigma {|^{{1 \\over 2}}}}}{e^{ - {1 \\over 2}{{(x - u)}^T}{\\Sigma ^{ - 1}}(x - u)}}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20%7B1%20%5Cover%20%7B%7B%7B%282%5Cpi%20%29%7D%5E%7B%7Bn%20%5Cover%202%7D%7D%7D%7C%5CSigma%20%7B%7C%5E%7B%7B1%20%5Cover%202%7D%7D%7D%7D%7D%7Be%5E%7B%20-%20%7B1%20%5Cover%202%7D%7B%7B%28x%20-%20u%29%7D%5ET%7D%7B%5CSigma%20%5E%7B%20-%201%7D%7D%28x%20-%20u%29%7D%7D%24%24)\n - \u540c\u6837\uff0c`|\u03a3|`\u8d8a\u5c0f\uff0c`p(x)`\u8d8a\u5c16\n - \u4f8b\u5982\uff1a    \n ![enter description here][53]\uff0c  \n \u8868\u793ax1,x2**\u6b63\u76f8\u5173**\uff0c\u5373x1\u8d8a\u5927\uff0cx2\u4e5f\u5c31\u8d8a\u5927\uff0c\u5982\u4e0b\u56fe\uff0c\u4e5f\u5c31\u53ef\u4ee5\u5c06\u7ea2\u8272\u7684\u5f02\u5e38\u70b9\u68c0\u67e5\u51fa\u4e86\n ![enter description here][54]      \n \u82e5\uff1a   \n  ![enter description here][55]\uff0c   \n \u8868\u793ax1,x2**\u8d1f\u76f8\u5173**\n- \u5b9e\u73b0\u4ee3\u7801\uff1a\n```\n# \u591a\u5143\u9ad8\u65af\u5206\u5e03\u51fd\u6570    \ndef multivariateGaussian(X,mu,Sigma2):\n    k = len(mu)\n    if (Sigma2.shape[0]>1):\n        Sigma2 = np.diag(Sigma2)\n    '''\u591a\u5143\u9ad8\u65af\u5206\u5e03\u51fd\u6570'''    \n    X = X-mu\n    argu = (2*np.pi)**(-k\/2)*np.linalg.det(Sigma2)**(-0.5)\n    p = argu*np.exp(-0.5*np.sum(np.dot(X,np.linalg.inv(Sigma2))*X,axis=1))  # axis\u8868\u793a\u6bcf\u884c\n    return p\n```\n### 6\u3001\u5355\u5143\u548c\u591a\u5143\u9ad8\u65af\u5206\u5e03\u7279\u70b9\n- \u5355\u5143\u9ad8\u65af\u5206\u5e03\n - \u4eba\u4e3a\u53ef\u4ee5\u6355\u6349\u5230`feature`\u4e4b\u95f4\u7684\u5173\u7cfb\u65f6\u53ef\u4ee5\u4f7f\u7528\n - \u8ba1\u7b97\u91cf\u5c0f\n- \u591a\u5143\u9ad8\u65af\u5206\u5e03\n - \u81ea\u52a8\u6355\u6349\u5230\u76f8\u5173\u7684feature\n - \u8ba1\u7b97\u91cf\u5927\uff0c\u56e0\u4e3a\uff1a![$$\\Sigma  \\in {R^{n \\times {\\rm{n}}}}$$](http:\/\/latex.codecogs.com\/png.latex?%5Cfn_cm%20%24%24%5CSigma%20%5Cin%20%7BR%5E%7Bn%20%5Ctimes%20%7B%5Crm%7Bn%7D%7D%7D%7D%24%24)\n - `m>n`\u6216`\u03a3`\u53ef\u9006\u65f6\u53ef\u4ee5\u4f7f\u7528\u3002\uff08\u82e5\u4e0d\u53ef\u9006\uff0c\u53ef\u80fd\u6709\u5197\u4f59\u7684x\uff0c\u56e0\u4e3a\u7ebf\u6027\u76f8\u5173\uff0c\u4e0d\u53ef\u9006\uff0c\u6216\u8005\u5c31\u662fm<n\uff09\n\n### 7\u3001\u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c\n- \u663e\u793a\u6570\u636e     \n![enter description here][56]\n- \u7b49\u9ad8\u7ebf      \n![enter description here][57]\n- \u5f02\u5e38\u70b9\u6807\u6ce8   \n![enter description here][58]\n\n\n\n----------------------------------\n\n\n  [1]: .\/images\/LinearRegression_01.png \"LinearRegression_01.png\"\n  [2]: .\/images\/LogisticRegression_01.png \"LogisticRegression_01.png\"\n  [3]: .\/images\/LogisticRegression_02.png \"LogisticRegression_02.png\"\n  [4]: .\/images\/LogisticRegression_03.jpg \"LogisticRegression_03.jpg\"\n  [5]: .\/images\/LogisticRegression_04.png \"LogisticRegression_04.png\"\n  [6]: .\/images\/LogisticRegression_05.png \"LogisticRegression_05.png\"\n  [7]: .\/images\/LogisticRegression_06.png \"LogisticRegression_06.png\"\n  [8]: .\/images\/LogisticRegression_07.png \"LogisticRegression_07.png\"\n  [9]: .\/images\/LogisticRegression_08.png \"LogisticRegression_08.png\"\n  [10]: .\/images\/LogisticRegression_09.png \"LogisticRegression_09.png\"\n  [11]: .\/images\/LogisticRegression_11.png \"LogisticRegression_11.png\"\n  [12]: .\/images\/LogisticRegression_10.png \"LogisticRegression_10.png\"\n  [13]: .\/images\/LogisticRegression_12.png \"LogisticRegression_12.png\"\n  [14]: .\/images\/LogisticRegression_13.png \"LogisticRegression_13.png\"\n  [15]: .\/images\/NeuralNetwork_01.png \"NeuralNetwork_01.png\"\n  [16]: .\/images\/NeuralNetwork_02.png \"NeuralNetwork_02.png\"\n  [17]: .\/images\/NeuralNetwork_03.jpg \"NeuralNetwork_03.jpg\"\n  [18]: .\/images\/NeuralNetwork_04.png \"NeuralNetwork_04.png\"\n  [19]: .\/images\/NeuralNetwork_05.png \"NeuralNetwork_05.png\"\n  [20]: .\/images\/NeuralNetwork_06.png \"NeuralNetwork_06.png\"\n  [21]: .\/images\/NeuralNetwork_07.png \"NeuralNetwork_07.png\"\n  [22]: .\/images\/NeuralNetwork_08.png \"NeuralNetwork_08.png\"\n  [23]: .\/images\/NeuralNetwork_09.png \"NeuralNetwork_09.png\"\n  [24]: .\/images\/SVM_01.png \"SVM_01.png\"\n  [25]: .\/images\/SVM_02.png \"SVM_02.png\"\n  [26]: .\/images\/SVM_03.png \"SVM_03.png\"\n  [27]: .\/images\/SVM_04.png \"SVM_04.png\"\n  [28]: .\/images\/SVM_05.png \"SVM_05.png\"\n  [29]: .\/images\/SVM_06.png \"SVM_06.png\"\n  [30]: .\/images\/SVM_07.png \"SVM_07.png\"\n  [31]: .\/images\/SVM_08.png \"SVM_08.png\"\n  [32]: .\/images\/SVM_09.png \"SVM_09.png\"\n  [33]: .\/images\/SVM_10.png \"SVM_10.png\"\n  [34]: .\/images\/K-Means_01.png \"K-Means_01.png\"\n  [35]: .\/images\/K-Means_02.png \"K-Means_02.png\"\n  [36]: .\/images\/K-Means_03.png \"K-Means_03.png\"\n  [37]: .\/images\/K-Means_07.png \"K-Means_07.png\"\n  [38]: .\/images\/K-Means_04.png \"K-Means_04.png\"\n  [39]: .\/images\/K-Means_05.png \"K-Means_05.png\"\n  [40]: .\/images\/K-Means_06.png \"K-Means_06.png\"\n  [41]: .\/images\/PCA_01.png \"PCA_01.png\"\n  [42]: .\/images\/PCA_02.png \"PCA_02.png\"\n  [43]: .\/images\/PCA_03.png \"PCA_03.png\"\n  [44]: .\/images\/PCA_04.png \"PCA_04.png\"\n  [45]: .\/images\/PCA_05.png \"PCA_05.png\"\n  [46]: .\/images\/PCA_06.png \"PCA_06.png\"\n  [47]: .\/images\/PCA_07.png \"PCA_07.png\"\n  [48]: .\/images\/PCA_08.png \"PCA_08.png\"\n  [49]: .\/images\/AnomalyDetection_01.png \"AnomalyDetection_01.png\"\n  [50]: .\/images\/AnomalyDetection_04.png \"AnomalyDetection_04.png\"\n  [51]: .\/images\/AnomalyDetection_02.png \"AnomalyDetection_02.png\"\n  [52]: .\/images\/AnomalyDetection_03.png \"AnomalyDetection_03.png\"\n  [53]: .\/images\/AnomalyDetection_05.png \"AnomalyDetection_05.png\"\n  [54]: .\/images\/AnomalyDetection_07.png \"AnomalyDetection_07.png\"\n  [55]: .\/images\/AnomalyDetection_06.png \"AnomalyDetection_06.png\"\n  [56]: .\/images\/AnomalyDetection_08.png \"AnomalyDetection_08.png\"\n  [57]: .\/images\/AnomalyDetection_09.png \"AnomalyDetection_09.png\"\n  [58]: .\/images\/AnomalyDetection_10.png \"AnomalyDetection_10.png\"\n","131":"# Python Data Science Tutorials \n- This repo contains a curated list of Python tutorials for Data Science, NLP and Machine Learning.\n\n- [**Curated list of R tutorials for Data Science, NLP and Machine Learning**](https:\/\/github.com\/ujjwalkarn\/DataScienceR).\n\n- [Comprehensive topic-wise list of Machine Learning and Deep Learning tutorials, codes, articles and other resources](https:\/\/github.com\/ujjwalkarn\/Machine-Learning-Tutorials\/blob\/master\/README.md).\n\n## The Python Language\n- [Python 3 in one picture](https:\/\/fossbytes.com\/wp-content\/uploads\/2015\/09\/python-3-in-one-pic.png)\n- [**Awesome Python**](https:\/\/github.com\/vinta\/awesome-python)\n- [**Jargon from the functional programming world in simple terms!**](https:\/\/github.com\/hemanth\/functional-programming-jargon)\n- [**Dive Into Python**](http:\/\/www.diveintopython.net\/index.html)\n- [Learn Python Wiki on Reddit](https:\/\/www.reddit.com\/r\/learnpython\/wiki\/index)\n- [Learn 90% of Python in 90 Minutes](https:\/\/www.slideshare.net\/MattHarrison4\/learn-90)\n- [Highest Voted Python Questions](http:\/\/stackoverflow.com\/questions\/tagged\/python?sort=votes&pageSize=50)\n- [Python Basic Concepts](https:\/\/github.com\/gumption\/Python_for_Data_Science\/blob\/master\/3_Python_Basic_Concepts.ipynb)\n- [Quick Reference to Python](http:\/\/www.dataschool.io\/python-quick-reference\/)\n- [The Elements of Python Style](https:\/\/github.com\/amontalenti\/elements-of-python-style)\n- [**What does the yield keyword do in Python?**](http:\/\/stackoverflow.com\/questions\/231767\/what-does-the-yield-keyword-do-in-python)\n- [Parsing values from a JSON file in Python](http:\/\/stackoverflow.com\/questions\/2835559\/parsing-values-from-a-json-file-in-python)\n- [**Python Quora FAQs**](https:\/\/www.quora.com\/topic\/Python-programming-language-1)\n- [time-complexity of various operations - list\/dict - in current CPython](https:\/\/wiki.python.org\/moin\/TimeComplexity)\n- Scripting in Python\n    - [Python Scripting Tutorial](http:\/\/www.dreamsyssoft.com\/python-scripting-tutorial\/intro-tutorial.php)\n    - [Scripting with Python](https:\/\/www.schrodinger.com\/\/AcrobatFile.php?type=supportdocs&type2=&ident=404)\n    - [**Can I use Python as a bash replacement?**](http:\/\/stackoverflow.com\/questions\/209470\/can-i-use-python-as-a-bash-replacement)\n\n## Useful Online Courses\n- [Learn Python (Codecademy)](https:\/\/www.codecademy.com\/learn\/python#)\n- [Free Interactive Course: Intro to Python for Data Science (DataCamp)](https:\/\/www.datacamp.com\/courses\/intro-to-python-for-data-science)\n- [Introduction to Computer Science and Programming Using Python (MIT)](https:\/\/www.edx.org\/course\/introduction-computer-science-mitx-6-00-1x-11)\n- [Python for Everybody](https:\/\/www.coursera.org\/learn\/python)\n- [Python Programming Essentials](https:\/\/www.coursera.org\/learn\/python-programming)\n\n## Data Science with Python\n- [**Data Science IPython Notebooks**](https:\/\/github.com\/donnemartin\/data-science-ipython-notebooks)\n- [Awesome Python - Data Analysis](https:\/\/github.com\/vinta\/awesome-python#science-and-data-analysis)\n- Statistics\n  - [Statistics and Data Science](https:\/\/github.com\/svaksha\/pythonidae\/blob\/master\/Statistics.md)\n- [**An Introduction to Scientific Python (and a Bit of the Maths Behind It) \u2013 NumPy**](http:\/\/www.kdnuggets.com\/2016\/06\/intro-scientific-python-numpy.html)\n- [Data Analysis and IPython Notebooks](https:\/\/github.com\/kirang89\/pycrumbs#data-analysis)\n- [Python for Data Science: Basic Concepts](https:\/\/github.com\/gumption\/Python_for_Data_Science\/blob\/master\/2_Data_Science_Basic_Concepts.ipynb)\n- [Pycon India 2015 Notes](http:\/\/www.analyticsvidhya.com\/blog\/2015\/10\/notes-impressions-experience-excitement-pycon-india-2015\/)\n- [**5 important Python Data Science advancements of 2015**](https:\/\/medium.com\/@elgehelge\/the-5-most-important-python-data-science-advancements-of-2015-a136482da89b#.sp2c1la9z)\n- [Data Exploration with Numpy cheat sheet](http:\/\/www.analyticsvidhya.com\/blog\/2015\/07\/11-steps-perform-data-analysis-pandas-python)\n- [Querying Craiglist with Python](http:\/\/chrisholdgraf.com\/querying-craigslist-with-python\/?imm_mid=0d8940&cmp=em-data-na-na-newsltr_20150916)\n- [**An introduction to Numpy and Scipy**](http:\/\/www.engr.ucsb.edu\/~shell\/che210d\/numpy.pdf)\n- [Create NBA Shot Charts](http:\/\/savvastjortjoglou.com\/nba-shot-sharts.html)\n- [PythoR- Python meets R](http:\/\/nipunbatra.github.io\/2016\/01\/pythor\/)\n- [**How do I learn data analysis with Python?**](https:\/\/www.quora.com\/How-do-I-learn-data-analysis-with-Python?redirected_qid=2464720)\n- [What are some interesting things to do with Python?](https:\/\/www.quora.com\/Python-programming-language-What-are-some-interesting-things-to-do-with-Python?redirected_qid=2324227)\n- [**Which is better for data analysis: R or Python?**](https:\/\/www.quora.com\/Which-is-better-for-data-analysis-R-or-Python)\n- [**Web scraping in Python**](https:\/\/github.com\/ujjwalkarn\/Web-Scraping)\n- [The Guide to Learning Python for Data Science](http:\/\/www.datasciencecentral.com\/profiles\/blogs\/the-guide-to-learning-python-for-data-science-2)\n- [Python For Data Science - A Cheat Sheet For Beginners](https:\/\/www.datacamp.com\/community\/tutorials\/python-data-science-cheat-sheet-basics)\n- [Top voted Python data science questions](http:\/\/datascience.stackexchange.com\/questions\/tagged\/python)\n- [Awesome Python - Data Visualization](https:\/\/github.com\/vinta\/awesome-python#data-visualization)\n- [Awesome Python - Map Reduce](https:\/\/github.com\/vinta\/awesome-python#mapreduce)\n\n## Pandas Library in Python\n- [Intro to pandas data structures](http:\/\/www.gregreda.com\/2013\/10\/26\/intro-to-pandas-data-structures\/)\n- [Useful Pandas Cheatsheet](https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/doc\/cheatsheet\/Pandas_Cheat_Sheet.pdf)\n- [An Introduction to Scientific Python \u2013 Pandas](http:\/\/www.datadependence.com\/2016\/05\/scientific-python-pandas\/)\n- [10 minutes to Pandas](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/10min.html)\n- [Useful Pandas Snippets](http:\/\/www.swegler.com\/becky\/blog\/2014\/08\/06\/useful-pandas-snippets\/)\n- [Timeseries analysis using Pandas](http:\/\/nbviewer.jupyter.org\/github\/twiecki\/financial-analysis-python-tutorial\/blob\/master\/1.%20Pandas%20Basics.ipynb)\n- [Pandas Exercises - Practice your Pandas skills](https:\/\/github.com\/guipsamora\/pandas_exercises)\n- [Grouping in Pandas](http:\/\/blog.yhat.com\/posts\/grouping-pandas.html)\n- [**\u201cLarge data\u201d work flows using pandas**](http:\/\/stackoverflow.com\/questions\/14262433\/large-data-work-flows-using-pandas)\n- [Easier data analysis with pandas (video series)](http:\/\/www.dataschool.io\/easier-data-analysis-with-pandas\/)\n- [Pandas Basics Cheat Sheet](https:\/\/www.datacamp.com\/community\/blog\/python-pandas-cheat-sheet)\n- Quick Operations on a Pandas DataFrame\n    - [Renaming Columns in Pandas](http:\/\/stackoverflow.com\/questions\/11346283\/renaming-columns-in-pandas) ([video](https:\/\/www.youtube.com\/watch?v=0uBirYFhizE&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=5))\n    - [Deleting Columns from pandas DataFrame](http:\/\/stackoverflow.com\/questions\/13411544\/delete-column-from-pandas-dataframe) ([video](https:\/\/www.youtube.com\/watch?v=gnUKkS964WQ&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=6))\n    - [Adding new Column to existing DataFrame](http:\/\/stackoverflow.com\/questions\/12555323\/adding-new-column-to-existing-dataframe-in-python-pandas)\n    - [Add one Row in a pandas.DataFrame](http:\/\/stackoverflow.com\/questions\/10715965\/add-one-row-in-a-pandas-dataframe)\n    - [Changing the order of DataFrame Columns](http:\/\/stackoverflow.com\/questions\/13148429\/how-to-change-the-order-of-dataframe-columns)\n    - [Changing data type of Columns](http:\/\/stackoverflow.com\/questions\/15891038\/pandas-change-data-type-of-columns) ([video](https:\/\/www.youtube.com\/watch?v=V0AWyzVMf54&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=13))\n    - [Getting a list of the column headers from a DataFrame](http:\/\/stackoverflow.com\/questions\/19482970\/get-list-from-pandas-dataframe-column-headers)\n    - [Converting list of dictionaries to Dataframe](http:\/\/stackoverflow.com\/questions\/20638006\/convert-list-of-dictionaries-to-dataframe)\n    - [Getting row count of pandas DataFrame](http:\/\/stackoverflow.com\/questions\/15943769\/how-to-get-row-count-of-pandas-dataframe)\n    - [Most efficient way to loop through DataFrames](http:\/\/stackoverflow.com\/questions\/7837722\/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas)\n    - [Deleting DataFrame row based on column value](http:\/\/stackoverflow.com\/questions\/18172851\/deleting-dataframe-row-in-pandas-based-on-column-value)\n    - [Dropping a list of rows from Pandas DataFrame](http:\/\/stackoverflow.com\/questions\/14661701\/how-to-drop-a-list-of-rows-from-pandas-dataframe)\n    - [Sorting a DataFrame or a single column](https:\/\/www.youtube.com\/watch?v=zY4doF6xSxY&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=7)\n    - [Filtering DataFrame rows by column value](https:\/\/www.youtube.com\/watch?v=2AFGPdNn4FM&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=8)\n    - [Filtering DataFrame rows using multiple criteria](https:\/\/www.youtube.com\/watch?v=YPItfQ87qjM&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=9)\n    - [Dropping all non-numeric columns from a DataFrame](https:\/\/youtu.be\/B-r9VuK80dk?t=4m31s)\n    - [Counting and removing missing values](https:\/\/www.youtube.com\/watch?v=fCMrO_VzeL8&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=16)\n    - [Selecting multiple rows and columns from a DataFrame](https:\/\/www.youtube.com\/watch?v=xvpNA7bC8cs&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=19)\n    - [Reducing the size of a DataFrame](https:\/\/www.youtube.com\/watch?v=wDYDYGyN_cw&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=21)\n\n## Machine Learning with Python\n- [AI, ML Related List](https:\/\/github.com\/svaksha\/pythonidae\/blob\/master\/AI.md)\n- [Data Normalization in Python](http:\/\/blog.yhat.com\/posts\/data-normalization-in-python.html)\n- [**Python Machine Learning Book**](https:\/\/github.com\/rasbt\/python-machine-learning-book)\n- [Table of Contents and Code Notebooks](https:\/\/github.com\/rasbt\/python-machine-learning-book\/blob\/master\/README.md#table-of-contents-and-code-notebooks)\n- [Machine Learning with scikit learn](http:\/\/www.dataschool.io\/machine-learning-with-scikit-learn\/)\n- [Machine Learning Algorithms Cheatsheet](http:\/\/www.analyticsvidhya.com\/blog\/2015\/09\/full-cheatsheet-machine-learning-algorithms\/)\n- [**How to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?**](http:\/\/stackoverflow.com\/questions\/31421413\/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case)\n- [One Hot Encoding for Machine learning in Python](http:\/\/stackoverflow.com\/questions\/17469835\/one-hot-encoding-for-machine-learning)\n- [**Building a (semi) Autonomous Drone with Python**](http:\/\/blog.yhat.com\/posts\/autonomous-droning-with-python.html)\n- [Awesome Python - Machine Learning](https:\/\/github.com\/vinta\/awesome-python#machine-learning)\n- Computer Vision\n  - [Awesome Python - Computer Vision](https:\/\/github.com\/vinta\/awesome-python#computer-vision)\n\n## Scikit Learn\n- [scikit learn on Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Scikit-learn)\n- [**Introduction to machine learning with scikit-learn**](https:\/\/github.com\/justmarkham\/scikit-learn-videos), [**Videos!**](http:\/\/blog.kaggle.com\/author\/kevin-markham\/)\n- [**A Gentle Introduction to Scikit-Learn: A Python Machine Learning Library**](http:\/\/machinelearningmastery.com\/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library\/)\n- [**PyData Seattle 2015 Scikit-learn Tutorial**](https:\/\/github.com\/jakevdp\/sklearn_pydata2015), [sklearn_scipy2013](https:\/\/github.com\/jakevdp\/sklearn_scipy2013)\n- [SKLEARN BENCHMARKS: A centralized repository to report scikit-learn model performance across a variety of parameter settings and data sets](https:\/\/github.com\/rhiever\/sklearn-benchmarks), [Report results of sklearn benchmarks at openml.org](http:\/\/www.openml.org\/)\n- [How to get most informative features for scikit-learn classifiers?](http:\/\/stackoverflow.com\/questions\/11116697\/how-to-get-most-informative-features-for-scikit-learn-classifiers)\n- [**Code example to predict prices of Airbnb vacation rentals, using scikit-learn on Spark**](https:\/\/github.com\/mapr-demos\/spark-sklearn-airbnb-predict)\n- [**Machine Learning with scikit learn tutorial**](http:\/\/amueller.github.io\/sklearn_tutorial\/)\n- [Parallel and Large Scale Machine Learning with scikit-learn](https:\/\/speakerdeck.com\/ogrisel\/parallel-and-large-scale-machine-learning-with-scikit-learn), [Meetup](http:\/\/datasciencelondon.org\/machine-learning-python-scikit-learn-ipython-dsldn-data-science-london-kaggle\/)\n- [Saving classifier to disk in scikit-learn](http:\/\/stackoverflow.com\/questions\/10592605\/save-classifier-to-disk-in-scikit-learn)\n\n\n## Linear Regression in Python\n- [Linear Regression in Python](http:\/\/nbviewer.ipython.org\/github\/justmarkham\/DAT4\/blob\/master\/notebooks\/08_linear_regression.ipynb), [Blog Post](http:\/\/www.dataschool.io\/linear-regression-in-python\/)\n- [Linear Regression using Scikit Learn](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html)\n- [A friendly introduction to linear regression (using Python)](http:\/\/www.dataschool.io\/linear-regression-in-python\/)\n- [Linear Regression Example in Python](http:\/\/scipy-cookbook.readthedocs.io\/items\/LinearRegression.html)\n- [Regression analysis using Python StatsModels package](http:\/\/www.turingfinance.com\/regression-analysis-using-python-statsmodels-and-quandl\/)\n- [Run an OLS regression with Pandas Data Frame](http:\/\/stackoverflow.com\/questions\/19991445\/run-an-ols-regression-with-pandas-data-frame)\n\n## Logistic Regression in Python\n- [Logistic Regression with scikit learn](http:\/\/www.dataschool.io\/logistic-regression-in-python-using-scikit-learn\/)\n- [Logistic Regression in Python](http:\/\/blog.yhat.com\/posts\/logistic-regression-and-python.html)\n- [Implementing the softmax function in Python](http:\/\/stackoverflow.com\/questions\/34968722\/softmax-function-python)\n- [**What is the inverse of regularization strength in Logistic Regression? How should it affect my code?**](http:\/\/stackoverflow.com\/questions\/22851316\/what-is-the-inverse-of-regularization-strength-in-logistic-regression-how-shoul)\n- [The Yhat Blog: Logistic Regression in Python](http:\/\/blog.yhat.com\/posts\/logistic-regression-and-python.html)\n- [Example of logistic regression in Python using scikit-learn](http:\/\/www.dataschool.io\/logistic-regression-in-python-using-scikit-learn\/)\n- [TUTORIAL ON LOGISTIC REGRESSION AND OPTIMIZATION IN PYTHON](https:\/\/learningwithdata.wordpress.com\/2015\/04\/30\/tutorial-on-logistic-regression-and-optimization-in-python\/)\n- [Using Logistic Regression in Python for Data Science](http:\/\/www.dummies.com\/how-to\/content\/using-logistic-regression-in-python-for-data-scien.html)\n\n## k Nearest Neighbours in Python\n- [A good tutorial on implementing K Nearest Neighbors using scikit learn](http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html)\n- [**Is it possible to specify your own distance function using scikit-learn K-Means Clustering?**](http:\/\/stackoverflow.com\/questions\/5529625\/is-it-possible-to-specify-your-own-distance-function-using-scikit-learn-k-means)\n- [Tutorial To Implement k-Nearest Neighbors in Python From Scratch](http:\/\/machinelearningmastery.com\/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch\/)\n- [Implementing your own k-nearest neighbour algorithm using Python](https:\/\/blog.cambridgecoding.com\/2016\/01\/16\/machine-learning-under-the-hood-writing-your-own-k-nearest-neighbour-algorithm\/)\n- [knn Python implementation on StackOverflow](http:\/\/stackoverflow.com\/questions\/5565935\/k-nearest-neighbour-in-python)\n- [kNN with big sparse matrices in Python](http:\/\/stackoverflow.com\/questions\/20333092\/knn-with-big-sparse-matrices-in-python)\n- [Sklearn kNN usage with a user defined metric](http:\/\/stackoverflow.com\/questions\/21052509\/sklearn-knn-usage-with-a-user-defined-metric)\n\n\n## Neural Networks in Python\n- [Implementing a Neural Network from scratch in Python](http:\/\/www.wildml.com\/2015\/09\/implementing-a-neural-network-from-scratch\/), [Code](https:\/\/github.com\/dennybritz\/nn-from-scratch)\n- [A Neural Network in 11 lines of Python](http:\/\/iamtrask.github.io\/2015\/07\/12\/basic-python-network\/)\n- [Speeding up your Neural Network with Theano and the gpu](http:\/\/www.wildml.com\/2015\/09\/speeding-up-your-neural-network-with-theano-and-the-gpu\/), [Code](https:\/\/github.com\/dennybritz\/nn-theano)\n- [What is the best neural network library for Python?](https:\/\/www.quora.com\/What-is-the-best-neural-network-library-for-Python)\n- [Recurrent Neural Net Tutorial in Python Part 1](http:\/\/www.wildml.com\/2015\/09\/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns\/), [Part 2](http:\/\/www.wildml.com\/2015\/09\/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano\/), [Code](https:\/\/github.com\/dennybritz\/rnn-tutorial-rnnlm\/)\n- [PyBrain: modular Machine Learning Library for Python](http:\/\/pybrain.org\/)\n- [Neural Networks Tutorial \u2013 a Pathway to Deep Learning](http:\/\/www.adventuresinmachinelearning.com\/neural-networks-tutorial\/)\n\n\n## Decision Trees in Python\n- [How to extract the decision rules from scikit-learn decision-tree?](http:\/\/stackoverflow.com\/questions\/20224526\/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree)\n- [**How do I find which attributes my tree splits on, when using scikit-learn?**](http:\/\/stackoverflow.com\/questions\/20156951\/how-do-i-find-which-attributes-my-tree-splits-on-when-using-scikit-learn)\n- [Quora: What is a good Python library for decision trees?](https:\/\/www.quora.com\/What-is-a-good-Python-library-for-decision-trees), [StackOverflow](http:\/\/stackoverflow.com\/questions\/3127922\/what-is-a-good-python-library-for-decision-trees) \n- [Building Decision Trees in Python](http:\/\/www.onlamp.com\/pub\/a\/python\/2006\/02\/09\/ai_decision_trees.html?page=1)\n- [Pure Python Decision Trees](http:\/\/kldavenport.com\/pure-python-decision-trees\/)\n- [Building a decision tree from scratch in Python - a beginner's tutorial](http:\/\/www.patricklamle.com\/Tutorials\/Decision%20tree%20python\/tuto_decision%20tree.html)\n- [Using Python to Build and Use a Simple Decision Tree Classifier](https:\/\/github.com\/gumption\/Python_for_Data_Science\/blob\/master\/4_Python_Simple_Decision_Tree.ipynb)\n- [Decision trees in python with scikit-learn and pandas](http:\/\/chrisstrelioff.ws\/sandbox\/2015\/06\/08\/decision_trees_in_python_with_scikit_learn_and_pandas.html)\n- [Code for simple decision tree in Python](https:\/\/github.com\/gumption\/Python_for_Data_Science\/blob\/master\/simple_decision_tree.py)\n- [Lesson notebook: Regression and Classification Trees](http:\/\/nbviewer.jupyter.org\/github\/justmarkham\/DAT8\/blob\/master\/notebooks\/17_decision_trees.ipynb)\n- [Discover structure behind data with decision trees](http:\/\/vooban.com\/en\/tips-articles-geek-stuff\/discover-structure-behind-data-with-decision-trees\/)\n\n## Random Forest with Python\n- [Getting Started with Random Forests: Titanic Competition on Kaggle](https:\/\/www.kaggle.com\/c\/titanic\/details\/getting-started-with-random-forests), [Python sample code](https:\/\/www.kaggle.com\/c\/digit-recognizer\/forums\/t\/2299\/getting-started-python-sample-code-random-forest)\n- [RandomForestClassifier vs ExtraTreesClassifier in scikit learn](http:\/\/stackoverflow.com\/questions\/22409855\/randomforestclassifier-vs-extratreesclassifier-in-scikit-learn)\n- [Powerful Guide to learn Random Forest](http:\/\/www.analyticsvidhya.com\/blog\/2015\/09\/random-forest-algorithm-multiple-challenges\/)\n- [How are Feature Importances in RandomForestClassifier determined?](http:\/\/stackoverflow.com\/questions\/15810339\/how-are-feature-importances-in-randomforestclassifier-determined)\n- [Random forest interpretation with scikit-learn](http:\/\/blog.datadive.net\/random-forest-interpretation-with-scikit-learn\/)\n- [Random Forests in Python Tutorial](http:\/\/blog.yhat.com\/posts\/random-forests-in-python.html)\n- [Unbalanced classification using RandomForestClassifier in sklearn](http:\/\/stackoverflow.com\/questions\/20082674\/unbalanced-classification-using-randomforestclassifier-in-sklearn)\n- [Random Forest with categorical features in sklearn](http:\/\/stackoverflow.com\/questions\/24715230\/random-forest-with-categorical-features-in-sklearn)\n- [How to output RandomForest Classifier from python?](http:\/\/stackoverflow.com\/questions\/23000693\/how-to-output-randomforest-classifier-from-python)\n- [Lesson notebook: Ensembling, Bagging, and Random Forests](http:\/\/nbviewer.jupyter.org\/github\/justmarkham\/DAT8\/blob\/master\/notebooks\/18_ensembling.ipynb)\n\n## Support Vector Machine in Python\n- [Fastest SVM implementation usable in Python](http:\/\/stackoverflow.com\/questions\/9299346\/fastest-svm-implementation-usable-in-python)\n- [An example using python bindings for SVM library, LIBSVM](http:\/\/stackoverflow.com\/questions\/4214868\/an-example-using-python-bindings-for-svm-library-libsvm)\n- [What is the best SVM library usable from Python?](https:\/\/www.quora.com\/What-is-the-best-SVM-library-usable-from-Python)\n- [How does sklearn.svm.svc's function predict_proba() work internally?](http:\/\/stackoverflow.com\/questions\/15111408\/how-does-sklearn-svm-svcs-function-predict-proba-work-internally)\n- [Support vector machine in Python using libsvm example of features](http:\/\/stackoverflow.com\/questions\/30991592\/support-vector-machine-in-python-using-libsvm-example-of-features)\n- [Linear SVC Machine learning SVM example with Python](https:\/\/pythonprogramming.net\/linear-svc-example-scikit-learn-svm-python\/)\n- [Understanding Support Vector Machine algorithm from examples (along with code)](http:\/\/www.analyticsvidhya.com\/blog\/2015\/10\/understaing-support-vector-machine-example-code\/)\n\n## NLP \/ Text Mining in Python\n- [**NLP with Python ORiley Book**](http:\/\/www.nltk.org\/book_1ed\/), [Python 3](http:\/\/www.nltk.org\/book\/)\n- [Awesome Python - NLP](https:\/\/github.com\/vinta\/awesome-python#natural-language-processing)\n- [Awesome Python - Text Processing](https:\/\/github.com\/vinta\/awesome-python#text-processing)\n- [Text Analytics : Intro and Tokenization](http:\/\/a4analytics.blogspot.sg\/2015\/03\/text-mining-post-1.html)\n- [NLTK BOOK](http:\/\/www.nltk.org\/book\/ch01.html)\n- [Elegant N-gram Generation in Python](http:\/\/locallyoptimal.com\/blog\/2013\/01\/20\/elegant-n-gram-generation-in-python\/)\n- [**Computing N Grams using Python**](http:\/\/stackoverflow.com\/questions\/13423919\/computing-n-grams-using-python)\n- [N-grams: Explanation + 2 applications](http:\/\/stackoverflow.com\/questions\/1032288\/n-grams-explanation-2-applications)\n- [NLP Tutorial with Python](http:\/\/www.datasciencecentral.com\/profiles\/blogs\/python-nlp-tools)\n\n## Sentiment Analysis with Python\n- [A Comprehensive Guide to Sentiment Analysis](https:\/\/monkeylearn.com\/sentiment-analysis\/)\n- [Twitter-Sentiment-Analysis](https:\/\/github.com\/ujjwalkarn\/Twitter-Sentiment-Analysis)\n- [Basic Sentiment Analysis with Python](http:\/\/fjavieralba.com\/basic-sentiment-analysis-with-python.html)\n- [What is the best way to do Sentiment Analysis with Python?](https:\/\/www.quora.com\/What-is-the-best-way-to-do-Sentiment-Analysis-with-Python-1)\n- [How to Calculate Twitter Sentiment Using AlchemyAPI with Python](http:\/\/www.alchemyapi.com\/developers\/getting-started-guide\/twitter-sentiment-analysis)\n- [Second Try: Sentiment Analysis in Python](http:\/\/andybromberg.com\/sentiment-analysis-python\/)\n- [Sentiment Analysis with Python NLTK Text Classification](http:\/\/text-processing.com\/demo\/sentiment\/)\n- Codes and Explanation\n    - [**Sentiment Analysis with bag-of-words**](http:\/\/ataspinar.com\/2016\/01\/21\/sentiment-analysis-with-bag-of-words\/)\n    - [**Sentiment Analysis with Naive Bayes**](http:\/\/ataspinar.com\/2016\/02\/15\/sentiment-analysis-with-the-naive-bayes-classifier\/)\n\n## Pickle: convert a python object into a character stream\n- [Python serialization - Why pickle?](http:\/\/stackoverflow.com\/questions\/8968884\/python-serialization-why-pickle)\n- [**Serializing Python Objects**](http:\/\/www.diveinto.org\/python3\/serializing.html), [**Binary Files**](http:\/\/www.diveinto.org\/python3\/files.html#binary)\n- [What is Pickle in python ?](https:\/\/pythontips.com\/2013\/08\/02\/what-is-pickle-in-python\/)\n- [How to cPickle dump and load separate dictionaries to the same file?](http:\/\/stackoverflow.com\/questions\/11641493\/how-to-cpickle-dump-and-load-separate-dictionaries-to-the-same-file)\n- [**Understanding Pickling in Python**](http:\/\/stackoverflow.com\/questions\/7501947\/understanding-pickling-in-python)\n\n## AutoML\n- [TPOT: A Python tool for automating data science](http:\/\/www.randalolson.com\/2016\/05\/08\/tpot-a-python-tool-for-automating-data-science\/), [GitHub repo](https:\/\/github.com\/rhiever\/tpot)\n\n## Regex Related\n- [RegExr](http:\/\/regexr.com\/)\n- [Regex101](https:\/\/regex101.com\/)\n- [Pythex](http:\/\/pythex.org\/)\n- [How to use Regular Expressions (Regex) in Microsoft Excel both in-cell and loops](http:\/\/stackoverflow.com\/questions\/22542834\/how-to-use-regular-expressions-regex-in-microsoft-excel-both-in-cell-and-loops)\n- [Advanced Filters: Excel\u2019s Amazing Alternative To Regex](http:\/\/searchengineland.com\/advanced-filters-excels-amazing-alternative-to-regex-143680)\n\n## Shell Scripting\n- [**Calling an external command in Python**](http:\/\/stackoverflow.com\/questions\/89228\/calling-an-external-command-in-python)\n- [**Running shell command from Python and capturing the output**](http:\/\/stackoverflow.com\/questions\/4760215\/running-shell-command-from-python-and-capturing-the-output)\n- [**Can I use Python as a bash replacement?**](http:\/\/stackoverflow.com\/questions\/209470\/can-i-use-python-as-a-bash-replacement)\n- [Python Scripts as a Replacement for Bash Utility Scripts](http:\/\/www.linuxjournal.com\/content\/python-scripts-replacement-bash-utility-scripts)\n- [How to Write a Shell Script using Bash Shell in Ubuntu](https:\/\/www.youtube.com\/watch?v=He-5BpUGSag)\n- Red Hat Magazine | Python for Bash scripters: A well-kept secret\n- [Embed bash in python](http:\/\/stackoverflow.com\/questions\/2651874\/embed-bash-in-python)\n- [Bash2py: A Bash to Python Translator](https:\/\/cs.uwaterloo.ca\/~ijdavis\/bash2py-final.pdf)\n- [Beginners\/BashScripting](https:\/\/help.ubuntu.com\/community\/Beginners\/BashScripting)\n- [The Beginner\u2019s Guide to Shell Scripting: The Basics](http:\/\/www.howtogeek.com\/67469\/the-beginners-guide-to-shell-scripting-the-basics\/)\n- [Linux Shell Scripting Tutorial v1.05r3 A Beginner's handbook](http:\/\/www.freeos.com\/guides\/lsst\/)\n\n## Other good lists\n- [pycrumbs - Bits and bytes of Python from the Internet](https:\/\/github.com\/kirang89\/pycrumbs)\n- [python github projects - Collect and classify python projects on Github](https:\/\/github.com\/checkcheckzz\/python-github-projects)\n- [python reference - Useful functions, tutorials, and other Python-related things](https:\/\/github.com\/rasbt\/python_reference)\n- [pythonidae - Curated decibans of scientific programming resources in Python](https:\/\/github.com\/svaksha\/pythonidae)\n","132":"MachineLearning\n====================\n\n\n\n\u4e00\u4e9b\u5e38\u89c1\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u5b9e\u73b0\u4ee3\u7801\uff0c\u672c\u4eba\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u505a\u7684\u603b\u7ed3\uff0c\u8d44\u5386\u5c1a\u6d45\uff0c\u5982\u6709\u9519\u8bef\u8bf7\u4e0d\u541d\u6307\u51fa\u3002\n\n\n## \u76ee\u5f55\u4ecb\u7ecd\n\n- **DeepLearning Tutorials**\n\n   \u8fd9\u4e2a\u6587\u4ef6\u5939\u4e0b\u5305\u542b\u4e00\u4e9b\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u5b9e\u73b0\u4ee3\u7801\uff0c\u4ee5\u53ca\u5177\u4f53\u7684\u5e94\u7528\u5b9e\u4f8b\uff0c\u5305\u542b\uff1a\n\n   [dive_into _keras](https:\/\/github.com\/wepe\/MachineLearning\/tree\/master\/DeepLearning%20Tutorials\/dive_into_keras) Keras\u4f7f\u7528\u8fdb\u9636\u3002\u4ecb\u7ecd\u4e86\u600e\u4e48\u4fdd\u5b58\u8bad\u7ec3\u597d\u7684CNN\u6a21\u578b\uff0c\u600e\u4e48\u5c06CNN\u7528\u4f5c\u7279\u5f81\u63d0\u53d6\uff0c\u600e\u4e48\u53ef\u89c6\u5316\u5377\u79ef\u56fe\u3002[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/45581421)\uff0c \u66f4\u591a\u8fdb\u9636\u4f7f\u7528\u65b9\u6cd5\uff1a[gist](https:\/\/gist.github.com\/wepe\/a05ad572dca002046de443061909ff7a)\n      \n   [keras_usage](https:\/\/github.com\/wepe\/MachineLearning\/tree\/master\/DeepLearning%20Tutorials\/keras_usage) \u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7b80\u5355\u6613\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6keras\uff0c\u7528\u7ecf\u5178\u7684Mnist\u5206\u7c7b\u95ee\u9898\u5bf9\u8be5\u6846\u67b6\u7684\u4f7f\u7528\u8fdb\u884c\u8bf4\u660e\uff0c\u8bad\u7ec3\u4e00\u4e2aCNN\uff0c\u603b\u5171\u4e0d\u8d85\u8fc730\u884c\u4ee3\u7801\u3002[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/45397033)\n\n   [FaceRecognition_CNN(olivettifaces)](https:\/\/github.com\/wepe\/MachineLearning-Demo\/tree\/master\/DeepLearning%20Tutorials\/FaceRecognition_CNN(olivettifaces))\n      \u5c06\u5377\u79ef\u795e\u7ecf\u7f51\u7edcCNN\u5e94\u7528\u4e8e\u4eba\u8138\u8bc6\u522b\u7684\u4e00\u4e2ademo\uff0c\u4eba\u8138\u6570\u636e\u5e93\u91c7\u7528olivettifaces\uff0cCNN\u6a21\u578b\u53c2\u8003LeNet5\uff0c\u57fa\u4e8epython+theano+numpy+PIL\u5b9e\u73b0\u3002\u8be6\u7ec6\u4ecb\u7ecd\u8fd9\u4e2ademo\u7684\u6587\u7ae0\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/43277187)\n\n\n   [cnn_LeNet](https:\/\/github.com\/wepe\/MachineLearning-Demo\/tree\/master\/DeepLearning%20Tutorials\/cnn_LeNet)  CNN\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u7684\u5b9e\u73b0\uff0c\u6a21\u578b\u4e3a\u7b80\u5316\u7248\u7684LeNet\uff0c\u5e94\u7528\u4e8eMNIST\u6570\u636e\u96c6\uff08\u624b\u5199\u6570\u5b57\uff09\uff0c\u6765\u81ea\u4e8eDeepLearning.net\u4e0a\u7684\u4e00\u4e2a\u6559\u7a0b\uff0c\u57fa\u4e8epython+theano\uff0c\u6211\u7528\u4e86\u4e2d\u6587\u5c06\u539f\u59cb\u7684\u4ee3\u7801\u8fdb\u884c\u8be6\u7ec6\u7684\u89e3\u8bfb\uff0c\u5e76\u7b80\u5355\u603b\u7ed3\u4e86CNN\u7b97\u6cd5\uff0c\u76f8\u5e94\u7684\u6587\u7ae0\u53d1\u5728\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/43225445)\n\n   [mlp](https:\/\/github.com\/wepe\/MachineLearning-Demo\/tree\/master\/DeepLearning%20Tutorials\/mlp)  \u591a\u5c42\u611f\u77e5\u673a\u7b97\u6cd5\u7684\u5b9e\u73b0\uff0c\u4ee3\u7801\u5b9e\u73b0\u4e86\u6700\u7b80\u5355\u7684\u4e09\u5c42\u611f\u77e5\u673a\uff0c\u5e76\u5e94\u7528\u4e8eMNIST\u6570\u636e\u96c6\uff0c\u6765\u81eaDeepLearning.net\u4e0a\u7684\u4e00\u4e2a\u6559\u7a0b\uff0c\u57fa\u4e8epython+theano\uff0c\u6211\u5199\u4e86\u4e00\u7bc7\u6587\u7ae0\u603b\u7ed3\u4ecb\u7ecd\u4e86MLP\u7b97\u6cd5\uff0c\u540c\u65f6\u7528\u4e2d\u6587\u8be6\u7ec6\u89e3\u8bfb\u4e86\u539f\u59cb\u7684\u4ee3\u7801\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/43221829)\n\n   [Softmax_sgd(or logistic_sgd)](https:\/\/github.com\/wepe\/MachineLearning-Demo\/tree\/master\/DeepLearning%20Tutorials\/Softmax_sgd(or%20logistic_sgd)) Softmax\u56de\u5f52\u7b97\u6cd5\u7684\u5b9e\u73b0\uff0c\u5e94\u7528\u4e8eMNIST\u6570\u636e\u96c6\uff0c\u57fa\u4e8ePython+theano\uff0c\u6765\u81eaDeepLearning.net\u4e0a\u7684\u4e00\u4e2a\u6559\u7a0b\uff0c\u57fa\u4e8epython+theano\uff0c\u6211\u5199\u4e86\u4e00\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86Softmax\u56de\u5f52\u7b97\u6cd5\uff0c\u540c\u65f6\u7528\u4e2d\u6587\u8be6\u7ec6\u89e3\u8bfb\u4e86\u539f\u59cb\u7684\u4ee3\u7801\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/43157801)\n\n- **PCA**\n\n   \u57fa\u4e8epython+numpy\u5b9e\u73b0\u4e86\u4e3b\u6210\u4efd\u5206\u6790PCA\u7b97\u6cd5\uff0c\u8fd9\u91cc\u8be6\u7ec6\u5730\u4ecb\u7ecd\u4e86PCA\u7b97\u6cd5\uff0c\u4ee5\u53ca\u4ee3\u7801\u5f00\u53d1\u6d41\u7a0b\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/42177327)\n\n- **kNN**\n      \n   \u57fa\u4e8epython+numpy\u5b9e\u73b0\u4e86K\u8fd1\u90bb\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u5728MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u8be6\u7ec6\u7684\u4ecb\u7ecd\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/41768407)\n\n- **logistic regression**\n\n   - \u57fa\u4e8eC++\u4ee5\u53ca\u7ebf\u6027\u4ee3\u6570\u5e93Eigen\u5b9e\u73b0\u7684logistic\u56de\u5f52\uff0c[\u4ee3\u7801](https:\/\/github.com\/wepe\/MachineLearning\/tree\/master\/logistic%20regression\/use_cpp_and_eigen)\n\n   - \u57fa\u4e8epython+numpy\u5b9e\u73b0\u4e86logistic\u56de\u5f52\uff08\u4e8c\u7c7b\u522b\uff09\uff0c\u8be6\u7ec6\u7684\u4ecb\u7ecd\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/41844495)\n\n- **ManifoldLearning**\n\n\t[DimensionalityReduction_DataVisualizing](https:\/\/github.com\/wepe\/MachineLearning\/tree\/master\/ManifoldLearning\/DimensionalityReduction_DataVisualizing) \u8fd0\u7528\u591a\u79cd\u6d41\u5f62\u5b66\u4e60\u65b9\u6cd5\u5c06\u9ad8\u7ef4\u6570\u636e\u964d\u7ef4\uff0c\u5e76\u7528matplotlib\u5c06\u6570\u636e\u53ef\u89c6\u5316(2\u7ef4\u548c3\u7ef4)\n     \n- **SVM**    \n\n\t[libsvm liblinear-usage](https:\/\/github.com\/wepe\/MachineLearning\/tree\/master\/SVM\/libsvm%20liblinear-usage) \u5bf9\u4f7f\u7528\u5e7f\u6cdb\u7684libsvm\u3001liblinear\u7684\u4f7f\u7528\u65b9\u6cd5\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u8be6\u7ec6\u4ecb\u7ecd\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/45206813)\n\n    [SVM by SMO](.\/SVM\/SVM_by_SMO) - \u7528SMO\u5b9e\u73b0\u4e86SVM\n\n    [SVM by QP](.\/SVM\/SVM_by_QP) - \u7528\u4e8c\u6b21\u7f16\u7a0b\uff08QP\uff09\u5b9e\u73b0\u4e86SVM\n\n\n- **GMM**\n\n\tGMM\u548ck-means\u4f5c\u4e3aEM\u7b97\u6cd5\u7684\u5e94\u7528\uff0c\u5728\u67d0\u79cd\u7a0b\u5ea6\u6709\u4e9b\u76f8\u4f3c\u4e4b\u5904\uff0c\u4e0d\u8fc7GMM\u660e\u663e\u5b66\u4e60\u51fa\u4e00\u4e9b\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u6765\uff0c\u7ed3\u5408\u76f8\u5173\u7406\u89e3\u5199\u6210python\u7248\u672c\uff0c\u8be6\u7ec6\u4ecb\u7ecd\uff1a[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/gugugujiawei\/article\/details\/45583051)\n\n- **DecisionTree**\n\n\tPython\u3001Numpy\u3001Matplotlib\u5b9e\u73b0\u7684ID3\u3001C4.5\uff0c\u5176\u4e2dC4.5\u6709\u5f85\u5b8c\u5584\uff0c\u540e\u7eed\u52a0\u5165CART\u3002\u6587\u7ae0\u5f85\u603b\u7ed3\u3002[\u4ee3\u7801](https:\/\/github.com\/wepe\/MachineLearning\/tree\/master\/DecisionTree)\n\n- **KMeans**\n\n\t\u4ecb\u7ecd\u4e86\u805a\u7c7b\u5206\u6790\u4e2d\u6700\u5e38\u7528\u7684KMeans\u7b97\u6cd5\uff08\u53ca\u4e8c\u5206KMeans\u7b97\u6cd5\uff09\uff0c\u57fa\u4e8eNumPy\u7684\u7b97\u6cd5\u5b9e\u73b0\uff0c\u4ee5\u53ca\u57fa\u4e8eMatplotlib\u7684\u805a\u7c7b\u8fc7\u7a0b\u53ef\u89c6\u5316\u3002[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/47811235)\n\n- **NaiveBayes**\n\n\t\u6734\u7d20\u8d1d\u53f6\u65af\u7b97\u6cd5\u7684\u7406\u8bba\u63a8\u5bfc\uff0c\u4ee5\u53ca\u4e09\u79cd\u5e38\u89c1\u6a21\u578b\uff08\u591a\u9879\u5f0f\u6a21\u578b\uff0c\u9ad8\u65af\u6a21\u578b\uff0c\u4f2f\u52aa\u5229\u6a21\u578b\uff09\u7684\u4ecb\u7ecd\u4e0e\u7f16\u7a0b\u5b9e\u73b0\uff08\u57fa\u4e8ePython\uff0cNumpy\uff09\u3002[\u6587\u7ae0\u94fe\u63a5](http:\/\/blog.csdn.net\/u012162613\/article\/details\/48323777)\n\n- **Ridge and Kernel Ridge**\n\n    \u4ecb\u7ecd\u4e86Ridge\u56de\u5f52\u548c\u5b83\u7684Kernel\u7248\u672c\u3002[\u4ee3\u7801](.\/Ridge\/kernel_ridge\/kernel_ridge.py)\n\n## Contributor\n\n- [wepon](https:\/\/github.com\/wepe)\n- [Gogary](https:\/\/github.com\/enjoyhot)\n- [Locky](https:\/\/github.com\/junlulocky)\n","133":"[![DOI](https:\/\/joss.theoj.org\/papers\/10.21105\/joss.00638\/status.svg)](https:\/\/doi.org\/10.21105\/joss.00638)\n[![PyPI version](https:\/\/badge.fury.io\/py\/mlxtend.svg)](http:\/\/badge.fury.io\/py\/mlxtend)\n[![Anaconda-Server Badge](https:\/\/anaconda.org\/conda-forge\/mlxtend\/badges\/version.svg)](https:\/\/anaconda.org\/conda-forge\/mlxtend)\n[![Build statu  s](https:\/\/ci.appveyor.com\/api\/projects\/status\/7vx20e0h5dxcyla2\/branch\/master?svg=true)](https:\/\/ci.appveyor.com\/project\/rasbt\/mlxtend\/branch\/master)\n[![Coverage Status](https:\/\/coveralls.io\/repos\/rasbt\/mlxtend\/badge.svg?branch=master&service=github)](https:\/\/coveralls.io\/github\/rasbt\/mlxtend?branch=master)\n![Python 3](https:\/\/img.shields.io\/badge\/python-3-blue.svg)\n![License](https:\/\/img.shields.io\/badge\/license-BSD-blue.svg)\n[![Discuss](https:\/\/img.shields.io\/badge\/discuss-github-blue.svg)](https:\/\/github.com\/rasbt\/mlxtend\/discussions)\n\n![](.\/docs\/sources\/img\/logo.png)\n\n**Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks.**\n\n<br>\n\nSebastian Raschka 2014-2022\n\n<br>\n\n## Links\n\n- **Documentation:** [http:\/\/rasbt.github.io\/mlxtend](http:\/\/rasbt.github.io\/mlxtend)\n- PyPI: [https:\/\/pypi.python.org\/pypi\/mlxtend](https:\/\/pypi.python.org\/pypi\/mlxtend)\n- Changelog: [http:\/\/rasbt.github.io\/mlxtend\/CHANGELOG](http:\/\/rasbt.github.io\/mlxtend\/CHANGELOG)\n- Contributing: [http:\/\/rasbt.github.io\/mlxtend\/CONTRIBUTING](http:\/\/rasbt.github.io\/mlxtend\/CONTRIBUTING)\n- Questions? Check out the [GitHub Discussions board](https:\/\/github.com\/rasbt\/mlxtend\/discussions)\n\n<br>\n<br>\n\n## Installing mlxtend\n\n#### PyPI\n\nTo install mlxtend, just execute  \n\n```bash\npip install mlxtend  \n```\n\nAlternatively, you could download the package manually from the Python Package Index [https:\/\/pypi.python.org\/pypi\/mlxtend](https:\/\/pypi.python.org\/pypi\/mlxtend), unzip it, navigate into the package, and use the command:\n\n```bash\npython setup.py install\n```\n\n#### Conda\nIf you use conda, to install mlxtend just execute\n\n```bash\nconda install -c conda-forge mlxtend \n```\n\n#### Dev Version\n\nThe mlxtend version on PyPI may always be one step behind; you can install the latest development version from the GitHub repository by executing\n\n```bash\npip install git+git:\/\/github.com\/rasbt\/mlxtend.git#egg=mlxtend\n```\n\nOr, you can fork the GitHub repository from https:\/\/github.com\/rasbt\/mlxtend and install mlxtend from your local drive via\n\n```bash\npython setup.py install\n```\n\n<br>\n<br>\n\n## Examples\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.plotting import plot_decision_regions\n\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=0)\nclf2 = RandomForestClassifier(random_state=0)\nclf3 = SVC(random_state=0, probability=True)\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft')\n\n# Loading some example data\nX, y = iris_data()\nX = X[:,[0, 2]]\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         ['Logistic Regression', 'Random Forest', 'RBF kernel SVM', 'Ensemble'],\n                         itertools.product([0, 1], repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n    plt.title(lab)\nplt.show()\n```\n\n![](.\/docs\/sources\/img\/ensemble_decision_regions_2d.png)\n\n---\n\nIf you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI:\n\n\n```\n@article{raschkas_2018_mlxtend,\n  author       = {Sebastian Raschka},\n  title        = {MLxtend: Providing machine learning and data science \n                  utilities and extensions to Python\u2019s  \n                  scientific computing stack},\n  journal      = {The Journal of Open Source Software},\n  volume       = {3},\n  number       = {24},\n  month        = apr,\n  year         = 2018,\n  publisher    = {The Open Journal},\n  doi          = {10.21105\/joss.00638},\n  url          = {http:\/\/joss.theoj.org\/papers\/10.21105\/joss.00638}\n}\n```\n\n- Raschka, Sebastian (2018) MLxtend: Providing machine learning and data science utilities and extensions to Python's scientific computing stack.\nJ Open Source Softw 3(24).\n\n---\n\n## License\n\n- This project is released under a permissive new BSD open source license ([LICENSE-BSD3.txt](https:\/\/github.com\/rasbt\/mlxtend\/blob\/master\/LICENSE-BSD3.txt)) and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose.\n- In addition, you may use, copy, modify and redistribute all artistic creative works (figures and images) included in this distribution under the directory\naccording to the terms and conditions of the Creative Commons Attribution 4.0 International License.  See the file [LICENSE-CC-BY.txt](https:\/\/github.com\/rasbt\/mlxtend\/blob\/master\/LICENSE-CC-BY.txt) for details. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above).\n\n## Contact\n\nThe best way to ask questions is via the [GitHub Discussions channel](https:\/\/github.com\/rasbt\/mlxtend\/discussions). In case you encounter usage bugs, please don't hesitate to use the [GitHub's issue tracker](https:\/\/github.com\/rasbt\/mlxtend\/issues) directly. \n","134":"<p align=\"center\">\n  <img src=\".github\/wb-logo-lightbg.png#gh-light-mode-only\" width=\"600\" alt=\"Weights & Biases\"\/>\n  <img src=\".github\/wb-logo-darkbg.png#gh-dark-mode-only\" width=\"600\" alt=\"Weights & Biases\"\/>\n<\/p>\n\n# Weights and Biases [![ci](https:\/\/circleci.com\/gh\/wandb\/client.svg?style=svg)](https:\/\/circleci.com\/gh\/wandb\/client) [![pypi](https:\/\/img.shields.io\/pypi\/v\/wandb.svg)](https:\/\/pypi.python.org\/pypi\/wandb) [![codecov](https:\/\/codecov.io\/gh\/wandb\/client\/branch\/master\/graph\/badge.svg?token=41Iw2WzViQ)](https:\/\/codecov.io\/gh\/wandb\/client)\n\nUse W&B to build better models faster. Track and visualize all the pieces of your machine learning pipeline, from datasets to production models.\n\n- Quickly identify model regressions. Use W&B to visualize results in real time, all in a central dashboard.\n- Focus on the interesting ML. Spend less time manually tracking results in spreadsheets and text files.\n- Capture dataset versions with W&B Artifacts to identify how changing data affects your resulting models.\n- Reproduce any model, with saved code, hyperparameters, launch commands, input data, and resulting model weights.\n\n[Sign up for a free account \u2192](https:\/\/wandb.com)\n\n## Features\n\n-   Store hyper-parameters used in a training run\n-   Search, compare, and visualize training runs\n-   Analyze system usage metrics alongside runs\n-   Collaborate with team members\n-   Replicate historic results\n-   Run parameter sweeps\n-   Keep records of experiments available forever\n\n[Documentation \u2192](https:\/\/docs.wandb.com)\n\nIf you have any questions, please don't hesitate to ask in our [user forum](http:\/\/wandb.me\/forum).\n\n# \ud83e\udd1d Simple integration with any framework\nInstall `wandb` library and login:\n```\npip install wandb\nwandb login\n```\nFlexible integration for any Python script:\n```python\nimport wandb\n\n# 1. Start a W&B run\nwandb.init(project='gpt3')\n\n# 2. Save model inputs and hyperparameters\nconfig = wandb.config\nconfig.learning_rate = 0.01\n\n# Model training code here ...\n\n# 3. Log metrics over time to visualize performance\nfor i in range (10):\n    wandb.log({\"loss\": loss})\n```\n\n### [Try in a colab \u2192](http:\/\/wandb.me\/intro)\n\nIf you have any questions, please don't hesitate to ask in our [user forum](http:\/\/wandb.me\/forum).\n\n![](https:\/\/i.imgur.com\/TU34QFZ.png)\n\n**[Explore a W&B dashboard](https:\/\/www.youtube.com\/watch?v=gnD8BFuyVUA)**\n\n\n# Academic Researchers\n\nIf you'd like a free academic account for your research group, [reach out to us \u2192](https:\/\/www.wandb.com\/academic)\n\nWe make it easy to cite W&B in your published paper. [Learn more \u2192](https:\/\/www.wandb.com\/academic)\n[![](https:\/\/i.imgur.com\/loKLiez.png)](https:\/\/www.wandb.com\/academic)\n\n\n# \ud83d\udcc8 Track model and data pipeline hyperparameters\nSet `wandb.config` once at the beginning of your script to save your hyperparameters, input settings (like dataset name or model type), and any other independent variables for your experiments. This is useful for analyzing your experiments and reproducing your work in the future. Setting configs also allows you to [visualize](https:\/\/docs.wandb.com\/sweeps\/visualize-sweep-results) the relationships between features of your model architecture or data pipeline and the model performance (as seen in the screenshot above).\n\n```python\nwandb.init()\nwandb.config.epochs = 4\nwandb.config.batch_size = 32\nwandb.config.learning_rate = 0.001\nwandb.config.architecture = \"resnet\"\n```\n\n- **[See how to set configs in a colab \u2192](http:\/\/wandb.me\/config-colab)**\n- [Docs](https:\/\/docs.wandb.com\/library\/config)\n\n# \ud83c\udfd7 Use your favorite framework\n\n## \ud83e\udd55 Keras\nIn Keras, you can use our callback to automatically save all the metrics tracked in `model.fit`. To get you started here's a minimal example:\n```python\n# Import W&B\nimport wandb\nfrom wandb.keras import WandbCallback\n\n# Step1: Initialize W&B run\nwandb.init(project='project_name')\n\n# 2. Save model inputs and hyperparameters\nconfig = wandb.config\nconfig.learning_rate = 0.01\n\n# Model training code here ...\n\n# Step 3: Add WandbCallback \nmodel.fit(X_train, y_train,  validation_data=(X_test, y_test),\n          callbacks=[WandbCallback()])\n```\n\n- **[Try in a colab \u2192](http:\/\/wandb.me\/keras-colab)**\n- [Learn More](https:\/\/app.wandb.ai\/wandb\/getting-started\/reports\/Keras--VmlldzoyMTEwNjQ)\n- [Docs](https:\/\/docs.wandb.com\/library\/integrations\/keras)\n\n## \ud83d\udd25 PyTorch\nW&B provides first class support for PyTorch. To automatically log gradients and store the network topology, you can call `.watch` and pass in your PyTorch model.\nThen use `.log` for anything else you want to track, like so:\n```python\nimport wandb\n\n# 1. Start a new run\nwandb.init(project=\"gpt-3\")\n\n# 2. Save model inputs and hyperparameters\nconfig = wandb.config\nconfig.dropout = 0.01\n\n# 3. Log gradients and model parameters\nwandb.watch(model)\nfor batch_idx, (data, target) in enumerate(train_loader):\n  ...  \n  if batch_idx % args.log_interval == 0:      \n    # 4. Log metrics to visualize performance\n    wandb.log({\"loss\": loss})\n```\n\n- **[Try in a colab \u2192](http:\/\/wandb.me\/pytorch-colab)**\n- [Learn More](https:\/\/app.wandb.ai\/wandb\/getting-started\/reports\/Pytorch--VmlldzoyMTEwNzM)\n- [Docs](https:\/\/docs.wandb.com\/library\/integrations\/pytorch)\n\n\n## \ud83c\udf0a TensorFlow\nThe simplest way to log metrics in TensorFlow is by logging `tf.summary` with our TensorFlow logger:\n```python\nimport wandb\n\n# 1. Start a W&B run\nwandb.init(project='gpt3')\n\n# 2. Save model inputs and hyperparameters\nconfig = wandb.config\nconfig.learning_rate = 0.01\n\n# Model training here\n\n# 3. Log metrics over time to visualize performance\nwith tf.Session() as sess:\n  # ...\n  wandb.tensorflow.log(tf.summary.merge_all())\n```\n\n- **[Try in a colab \u2192](http:\/\/wandb.me\/tf-colab)**\n- [Docs](https:\/\/docs.wandb.com\/library\/integrations\/tensorflow)\n\n\n## \ud83d\udca8 fastai\nVisualize, compare, and iterate on fastai models using Weights & Biases with the `WandbCallback`.\n```python\nimport wandb\nfrom fastai.callback.wandb import WandbCallback\n\n# 1. Start a new run\nwandb.init(project=\"gpt-3\")\n\n# 2. Automatically log model metrics\nlearn.fit(..., cbs=WandbCallback())\n```\n\n- **[Try in a colab \u2192](http:\/\/wandb.me\/fastai-colab)**\n- [Docs](https:\/\/docs.wandb.com\/library\/integrations\/fastai)\n\n\n## \u26a1\ufe0f PyTorch Lightning\nBuild scalable, structured, high-performance PyTorch models with Lightning and log them with W&B.\n```python\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning import Trainer\n\nwandb_logger = WandbLogger(project=\"gpt-3\")\ntrainer = Trainer(logger=wandb_logger)\n```\n\n- **[Try in a colab \u2192](http:\/\/wandb.me\/lit-colab)**\n- [Docs](https:\/\/docs.wandb.ai\/guides\/integrations\/lightning)\n\n\n## \ud83e\udd17 HuggingFace\nJust run a script using HuggingFace's Trainer in an environment where `wandb` is installed\nand we'll automatically log losses, evaluation metrics, model topology and gradients:\n```python\n# 1. Install the wandb library\npip install wandb\n\n# 2. Run a script that has the Trainer to automatically logs metrics, model topology and gradients\npython run_glue.py \\\n --model_name_or_path bert-base-uncased \\\n --task_name MRPC \\\n --data_dir $GLUE_DIR\/$TASK_NAME \\\n --do_train \\\n --evaluate_during_training \\\n --max_seq_length 128 \\\n --per_gpu_train_batch_size 32 \\\n --learning_rate 2e-5 \\\n --num_train_epochs 3 \\\n --output_dir \/tmp\/$TASK_NAME\/ \\\n --overwrite_output_dir \\\n --logging_steps 50\n```\n\n- **[Try in a colab \u2192](http:\/\/wandb.me\/hf)**\n- [Docs](https:\/\/docs.wandb.com\/library\/integrations\/huggingface)\n\n# \ud83e\uddf9 Optimize hyperparameters with Sweeps\nUse Weights & Biases Sweeps to automate hyperparameter optimization and explore the space of possible models.\n\n### [Get started in 5 mins \u2192](https:\/\/docs.wandb.com\/sweeps\/quickstart)\n### [Try Sweeps in PyTorch in a Colab \u2192](http:\/\/wandb.me\/sweeps-colab)\n\n### Benefits of using W&B Sweeps \n- **Quick to setup:** With just a few lines of code you can run W&B sweeps.\n- **Transparent:** We cite all the algorithms we're using, and our code is [open source](https:\/\/github.com\/wandb\/client\/tree\/master\/wandb\/sweeps).\n- **Powerful:** Our sweeps are completely customizable and configurable. You can launch a sweep across dozens of machines, and it's just as easy as starting a sweep on your laptop.\n\n<img src=\"https:\/\/gblobscdn.gitbook.com\/assets%2F-Lqya5RvLedGEWPhtkjU%2F-LyfPCyvV8By5YBltxfh%2F-LyfQsxswLC-6WKGgfGj%2Fcentral%20sweep%20server%203.png?alt=media&token=c81e4fe7-7ee4-48ea-a4cd-7b28113c6088\" width=\"400\" alt=\"Weights & Biases\" \/>\n\n### Common use cases\n- **Explore:** Efficiently sample the space of hyperparameter combinations to discover promising regions and build an intuition about your model.\n- **Optimize:**  Use sweeps to find a set of hyperparameters with optimal performance.\n- **K-fold cross validation:** [Here's a brief code example](https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation) of _k_-fold cross validation with W&B Sweeps.\n\n### Visualize Sweeps results\nThe hyperparameter importance plot surfaces which hyperparameters were the best predictors of, and highly correlated to desirable values for your metrics.\n\n<img src=\"https:\/\/paper-attachments.dropbox.com\/s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695757573_image.png\" width=\"720\" alt=\"Weights & Biases\" \/>\n\nParallel coordinates plots map hyperparameter values to model metrics. They're useful for honing in on combinations of hyperparameters that led to the best model performance.\n\n<img src=\"https:\/\/i.imgur.com\/THYXBN0.png\" width=\"720\" alt=\"Weights & Biases\" \/>\n\n# \ud83d\udcdc Share insights with Reports\nReports let you [organize visualizations, describe your findings, and share updates with collaborators](https:\/\/www.youtube.com\/watch?v=o2dOSIDDr1w&&ab_channel=Weights%26Biases).\n\n### Common use cases\n- **Notes:** Add a graph with a quick note to yourself.\n- **Collaboration:** Share findings with your colleagues.\n- **Work log:** Track what you've tried and plan next steps.\n\n**Explore reports in [The Gallery \u2192](https:\/\/app.wandb.ai\/gallery) | Read the [Docs](https:\/\/docs.wandb.com\/reports)**\n\nOnce you have experiments in W&B, you can visualize and document results in Reports with just a few clicks. Here's a quick [demo video](https:\/\/www.youtube.com\/watch?v=jWBGKGAjt6w&t=2s&ab_channel=Weights%26Biases).\n\n![](https:\/\/i.imgur.com\/dn0Dyd8.png)\n\n# \ud83c\udffa Version control datasets and models with Artifacts\nGit and GitHub make code version control easy,\nbut they're not optimized for tracking the other parts of the ML pipeline:\ndatasets, models, and other large binary files.\n\nW&B's Artifacts are.\nWith just a few extra lines of code,\nyou can start tracking you and your team's outputs,\nall directly linked to run.\n\n### [Try Artifacts in a Colab \u2192](http:\/\/wandb.me\/artifacts-colab)\n\n![](https:\/\/i.imgur.com\/zvBWhGx.png)\n\n### Common use cases\n- **Pipeline Management:** Track and visualize the inputs and outputs of your runs as a graph\n- **Don't Repeat Yourself\u2122:** Prevent the duplication of compute effort\n- **Sharing Data in Teams:** Collaborate on models and datasets without all the headaches\n\n![](https:\/\/i.imgur.com\/w92cYQm.png)\n\n**Learn about Artifacts [here \u2192](https:\/\/www.wandb.com\/articles\/announcing-artifacts) | Read the [Docs](https:\/\/docs.wandb.com\/artifacts)**\n\n# \ud83d\udcbb  Run W&B Server Locally\nW&amp;B Local is a privately hosted Weights &amp; Biases server.  Securely and quickly deploy a W&amp;B production server in Docker, Kubernettes, or in a privately-managed cloud.  Learn more about setting up a [production W&amp;B deployment \u2192](https:\/\/docs.wandb.ai\/guides\/self-hosted\/setup).\n\n## Quickstart\n1. On a machine with [Docker](https:\/\/docker.com) and [Python](https:\/\/www.python.org\/) installed, run:\n    ```\n    1 pip install wandb --upgrade\n    2 wandb local\n    ```\n2. Generate a free license from the [Deployer](https:\/\/deploy.wandb.ai\/).\n3. Add it to your local settings.\n\n  **Paste the license in the \/system-admin page on your localhost**\n\n  ![2022-02-24 22 13 59](https:\/\/user-images.githubusercontent.com\/25806817\/166265834-6a9d1be8-2af5-4c63-872e-8e5b3e4082aa.gif)\n\n## Docker\nRunning `wandb local` will start our server and forward port 8080 on the host.  To have other machines report metrics to this server run: `wandb login --host=http:\/\/X.X.X.X:8080`. \n\nUse Docker to manually run W&amp;B Local:\n```\ndocker run --rm -d -v wandb:\/vol -p 8080:8080 --name wandb-local wandb\/local\n```\n\n# Testing\n\nTo run basic test use `make test`.  More detailed information can be found at CONTRIBUTING.md.\n\nWe use [circleci](https:\/\/circleci.com) for CI.\n","135":"# Yellowbrick\n\n\n[![Build Status](https:\/\/github.com\/DistrictDataLabs\/yellowbrick\/actions\/workflows\/ci.yml\/badge.svg?branch=develop)](https:\/\/github.com\/DistrictDataLabs\/yellowbrick\/actions\/workflows\/ci.yml)\n[![Coverage Status](https:\/\/codecov.io\/gh\/DistrictDataLabs\/yellowbrick\/branch\/develop\/graph\/badge.svg?token=BnaSECZz2r)](https:\/\/codecov.io\/gh\/DistrictDataLabs\/yellowbrick)\n[![Total Alerts](https:\/\/img.shields.io\/lgtm\/alerts\/g\/DistrictDataLabs\/yellowbrick.svg?logo=lgtm&logoWidth=18)](https:\/\/lgtm.com\/projects\/g\/DistrictDataLabs\/yellowbrick\/alerts\/)\n[![Language Grade: Python](https:\/\/img.shields.io\/lgtm\/grade\/python\/g\/DistrictDataLabs\/yellowbrick.svg?logo=lgtm&logoWidth=18)](https:\/\/lgtm.com\/projects\/g\/DistrictDataLabs\/yellowbrick\/context:python)\n[![PyPI version](https:\/\/badge.fury.io\/py\/yellowbrick.svg)](https:\/\/badge.fury.io\/py\/yellowbrick)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/yellowbrick\/badge\/?version=latest)](http:\/\/yellowbrick.readthedocs.io\/en\/latest\/?badge=latest)\n[![DOI](https:\/\/zenodo.org\/badge\/DOI\/10.5281\/zenodo.1206239.svg)](https:\/\/doi.org\/10.5281\/zenodo.1206239)\n[![JOSS](http:\/\/joss.theoj.org\/papers\/10.21105\/joss.01075\/status.svg)](https:\/\/doi.org\/10.21105\/joss.01075)\n[![Binder](https:\/\/mybinder.org\/badge.svg)](https:\/\/mybinder.org\/v2\/gh\/DistrictDataLabs\/yellowbrick\/develop?filepath=examples%2Fexamples.ipynb)\n\n\n**Visual analysis and diagnostic tools to facilitate machine learning model selection.**\n\n[![Banner](docs\/images\/readme\/banner.png)](https:\/\/www.scikit-yb.org\/en\/latest\/gallery.html)\n\n## What is Yellowbrick?\n\nYellowbrick is a suite of visual diagnostic tools called \"Visualizers\" that extend the scikit-learn API to allow human steering of the model selection process. In a nutshell, Yellowbrick combines scikit-learn with matplotlib in the best tradition of the scikit-learn documentation, but to produce visualizations for _your_ machine learning workflow!\n\nFor complete documentation on the Yellowbrick API, a gallery of available visualizers, the contributor's guide, tutorials and teaching resources, frequently asked questions, and more, please visit our documentation at [www.scikit-yb.org](https:\/\/www.scikit-yb.org\/).\n\n## Installing Yellowbrick\n\nYellowbrick is compatible with Python 3.4 or later and also depends on scikit-learn and matplotlib. The simplest way to install Yellowbrick and its dependencies is from PyPI with pip, Python's preferred package installer.\n\n    $ pip install yellowbrick\n\nNote that Yellowbrick is an active project and routinely publishes new releases with more visualizers and updates. In order to upgrade Yellowbrick to the latest version, use pip as follows.\n\n    $ pip install -U yellowbrick\n\nYou can also use the `-U` flag to update scikit-learn, matplotlib, or any other third party utilities that work well with Yellowbrick to their latest versions.\n\nIf you're using Anaconda (recommended for Windows users), you can take advantage of the conda utility to install Yellowbrick:\n\n    conda install -c districtdatalabs yellowbrick\n\n## Using Yellowbrick\n\nThe Yellowbrick API is specifically designed to play nicely with scikit-learn. Here is an example of a typical workflow sequence with scikit-learn and Yellowbrick:\n\n### Feature Visualization\n\nIn this example, we see how Rank2D performs pairwise comparisons of each feature in the data set with a specific metric or algorithm and then returns them ranked as a lower left triangle diagram.\n\n```python\nfrom yellowbrick.features import Rank2D\n\nvisualizer = Rank2D(\n    features=features, algorithm='covariance'\n)\nvisualizer.fit(X, y)                # Fit the data to the visualizer\nvisualizer.transform(X)             # Transform the data\nvisualizer.show()                   # Finalize and render the figure\n```\n\n### Model Visualization\n\nIn this example, we instantiate a scikit-learn classifier and then use Yellowbrick's ROCAUC class to visualize the tradeoff between the classifier's sensitivity and specificity.\n\n```python\nfrom sklearn.svm import LinearSVC\nfrom yellowbrick.classifier import ROCAUC\n\nmodel = LinearSVC()\nvisualizer = ROCAUC(model)\nvisualizer.fit(X,y)\nvisualizer.score(X,y)\nvisualizer.show()\n```\n\nFor additional information on getting started with Yellowbrick, view the [Quick Start Guide](https:\/\/www.scikit-yb.org\/en\/latest\/quickstart.html) in the [documentation](https:\/\/www.scikit-yb.org\/en\/latest\/) and check out our [examples notebook](https:\/\/github.com\/DistrictDataLabs\/yellowbrick\/blob\/develop\/examples\/examples.ipynb).\n\n## Contributing to Yellowbrick\n\nYellowbrick is an open source project that is supported by a community who will gratefully and humbly accept any contributions you might make to the project. Large or small, any contribution makes a big difference; and if you've never contributed to an open source project before, we hope you will start with Yellowbrick!\n\nIf you are interested in contributing, check out our [contributor's guide](https:\/\/www.scikit-yb.org\/en\/latest\/contributing\/index.html). Beyond creating visualizers, there are many ways to contribute:\n\n- Submit a bug report or feature request on [GitHub Issues](https:\/\/github.com\/DistrictDataLabs\/yellowbrick\/issues).\n- Contribute a Jupyter notebook to our examples [gallery](https:\/\/github.com\/DistrictDataLabs\/yellowbrick\/tree\/develop\/examples).\n- Assist us with [user testing](https:\/\/www.scikit-yb.org\/en\/latest\/evaluation.html).\n- Add to the documentation or help with our website, [scikit-yb.org](https:\/\/www.scikit-yb.org).\n- Write [unit or integration tests](https:\/\/www.scikit-yb.org\/en\/latest\/contributing\/developing_visualizers.html#integration-tests) for our project.\n- Answer questions on our issues, mailing list, Stack Overflow, and elsewhere.\n- Translate our documentation into another language.\n- Write a blog post, tweet, or share our project with others.\n- [Teach](https:\/\/www.scikit-yb.org\/en\/latest\/teaching.html) someone how to use Yellowbrick.\n\nAs you can see, there are lots of ways to get involved and we would be very happy for you to join us! The only thing we ask is that you abide by the principles of openness, respect, and consideration of others as described in the [Python Software Foundation Code of Conduct](https:\/\/www.python.org\/psf\/codeofconduct\/).\n\nFor more information, checkout the `CONTRIBUTING.md` file in the root of the repository or the detailed documentation at [Contributing to Yellowbrick](https:\/\/www.scikit-yb.org\/en\/latest\/contributing\/index.html)\n\n## Yellowbrick Datasets\n\nYellowbrick gives easy access to several datasets that are used for the examples in the documentation and testing. These datasets are hosted in our CDN and must be downloaded for use. Typically, when a user calls one of the data loader functions, e.g. `load_bikeshare()` the data is automatically downloaded if it's not already on the user's computer. However, for development and testing, or if you know you will be working without internet access, it might be easier to simply download all the data at once.\n\nThe data downloader script can be run as follows:\n\n    $ python -m yellowbrick.download\n\nThis will download the data to the fixtures directory inside of the Yellowbrick site packages. You can specify the location of the download either as an argument to the downloader script (use `--help` for more details) or by setting the `$YELLOWBRICK_DATA` environment variable. This is the preferred mechanism because this will also influence how data is loaded in Yellowbrick.\n\n_Note: Developers who have downloaded data from Yellowbrick versions earlier than v1.0 may experience some problems with the older data format. If this occurs, you can clear out your data cache as follows:_\n\n    $ python -m yellowbrick.download --cleanup\n\n_This will remove old datasets and download the new ones. You can also use the `--no-download` flag to simply clear the cache without re-downloading data. Users who are having difficulty with datasets can also use this or they can uninstall and reinstall Yellowbrick using `pip`._\n\n## Citing Yellowbrick\n\nWe would be glad if you used Yellowbrick in your scientific publications! If you do, please cite us using the [citation guidelines](https:\/\/www.scikit-yb.org\/en\/latest\/about.html#citing-yellowbrick).\n\n## Affiliations\n\n[![District Data Labs](docs\/images\/readme\/affiliates_ddl.png)](https:\/\/districtdatalabs.com\/) [![NumFOCUS Affiliated Project](docs\/images\/readme\/affiliates_numfocus.png)](https:\/\/numfocus.org)\n","136":"<p align=\"center\"><img width=\"100%\" src=\"ML\/others\/logo\/torch_and_tf.svg\" \/><\/p>\n\n--------------------------------------------------------------------------------\n\n\n[![Build Status](https:\/\/travis-ci.com\/aladdinpersson\/Machine-Learning-Collection.svg?branch=master)](https:\/\/travis-ci.com\/aladdinpersson\/Machine-Learning-Collection) [![License: MIT](https:\/\/img.shields.io\/badge\/License-MIT-yellow.svg)](https:\/\/opensource.org\/licenses\/MIT)\n\n[logo]: https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/others\/logo\/youtube_logo.png\n\n# Machine Learning Collection\nIn this repository you will find tutorials and projects related to Machine Learning. I try to make the code as clear as possible, and the goal is be to used as a learning resource and a way to lookup problems to solve specific problems. For most I have also done video explanations on YouTube if you want a walkthrough for the code. If you got any questions or suggestions for future videos I prefer if you ask it on [YouTube](https:\/\/www.youtube.com\/c\/AladdinPersson). This repository is contribution friendly, so if you feel you want to add something then I'd happily merge a PR :smiley:\n\n## Table Of Contents\n- [Machine Learning Algorithms](#machine-learning)\n- [PyTorch Tutorials](#pytorch-tutorials)\n\t- [Basics](#basics)\n\t- [More Advanced](#more-advanced)\n    - [Object Detection](#Object-Detection)\n\t- [Generative Adversarial Networks](#Generative-Adversarial-Networks)\n\t- [Architectures](#architectures)\n- [TensorFlow Tutorials](#tensorflow-tutorials)\n\t- [Beginner Tutorials](#beginner-tutorials)\n\t- [Architectures](#CNN-Architectures)\n\n## Machine Learning\n* [![Youtube Link][logo]](https:\/\/youtu.be\/pCCUnoes1Po) &nbsp; [Linear Regression](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/linearregression\/linear_regression_gradient_descent.py) **- With Gradient Descent** :white_check_mark: \n* [![Youtube Link][logo]](https:\/\/youtu.be\/DQ6xfe75CDk) &nbsp; [Linear Regression](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/linearregression\/linear_regression_normal_equation.py) **- With Normal Equation** :white_check_mark:\n* [![Youtube Link][logo]](https:\/\/youtu.be\/x1ez9vi611I) &nbsp; [Logistic Regression](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/logisticregression\/logistic_regression.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/3trW5Lig7BU) &nbsp; [Naive Bayes](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/naivebayes\/naivebayes.py) **- Gaussian Naive Bayes**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/QzAaRuDskyc) &nbsp; [K-nearest neighbors](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/knn\/knn.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/W4fSRHeafMo) &nbsp; [K-means clustering](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/kmeans\/kmeansclustering.py) \n* [![Youtube Link][logo]](https:\/\/youtu.be\/gBTtR0bs-1k) &nbsp; [Support Vector Machine](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/svm\/svm.py) **- Using CVXOPT**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/NJvojeoTnNM) &nbsp; [Neural Network](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/neuralnetwork\/NN.py)\n* [Decision Tree](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/algorithms\/decisiontree\/decision_tree.py)\n\n## PyTorch Tutorials\nIf you have any specific video suggestion please make a comment on YouTube :)\n\n### Basics\n* [![Youtube Link][logo]](https:\/\/youtu.be\/x9JiIFvlUwk) &nbsp; [Tensor Basics](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_tensorbasics.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/Jy4wM2X21u0) &nbsp; [Feedforward Neural Network](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_simple_fullynet.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/wnK3uWv_WkU) &nbsp; [Convolutional Neural Network](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_simple_CNN.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/Gl2WXLIMvKA) &nbsp; [Recurrent Neural Network](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_rnn_gru_lstm.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/jGst43P-TJA) &nbsp; [Bidirectional Recurrent Neural Network](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_bidirectional_lstm.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/g6kQl_EFn84) &nbsp; [Loading and saving model](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_loadsave.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/ZoZHd0Zm3RY) &nbsp; [Custom Dataset (Images)](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/Basics\/custom_dataset)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/9sHcLvVXsns) &nbsp; [Custom Dataset (Text)](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/Basics\/custom_dataset_txt)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/qaDe0qQZ5AQ) &nbsp; [Transfer Learning and finetuning](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_pretrain_finetune.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/Zvd276j9sZ8) &nbsp; [Data augmentation using Torchvision](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_transforms.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/rAdLwKJBvPM) &nbsp; [Data augmentation using Albumentations](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/Basics\/albumentations_tutorial)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/RLqsxWaQdHE) &nbsp; [TensorBoard Example](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_tensorboard_.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/y6IEcEBRZks) &nbsp; [Calculate Mean and STD of Images](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_std_mean.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/RKHopFfbPao) &nbsp; [Simple Progress bar](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_progress_bar.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/1SZocGaCAr8) &nbsp; [Deterministic Behavior](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/set_deterministic_behavior\/pytorch_set_seeds.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/P31hB37g4Ak) &nbsp; [Learning Rate Scheduler](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_lr_ratescheduler.py) \n* [![Youtube Link][logo]](https:\/\/youtu.be\/xWQ-p_o0Uik) &nbsp; [Initialization of weights](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/Basics\/pytorch_init_weights.py)\n\n\n### More Advanced\n* [![Youtube Link][logo]](https:\/\/youtu.be\/WujVlF_6h5A) &nbsp; [Text Generating LSTM](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Projects\/text_generation_babynames\/generating_names.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/IHq1t7NxS8k) &nbsp; [Semantic Segmentation w. U-NET](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/image_segmentation\/semantic_segmentation_unet)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/y2BaTt1fxJU) &nbsp; [Image Captioning](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/more_advanced\/image_captioning)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/imX4kSKDY7s) &nbsp; [Neural Style Transfer](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/neuralstyle\/nst.py)\n* [![Youtube Link][logo]](https:\/\/www.youtube.com\/playlist?list=PLhhyoLH6IjfzxdlsLrclcCTsS8kIcfWJb) &nbsp; [Torchtext [1]](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/torchtext\/torchtext_tutorial1.py) [Torchtext [2]](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/torchtext\/torchtext_tutorial2.py) [Torchtext [3]](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/torchtext\/torchtext_tutorial3.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/EoGUlvhRYpk) &nbsp; [Seq2Seq](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/Seq2Seq\/seq2seq.py) **- Sequence to Sequence (LSTM)**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/sQUqQddQtB4) &nbsp; [Seq2Seq + Attention](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/Seq2Seq_attention\/seq2seq_attention.py) **- Sequence to Sequence with Attention (LSTM)**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/M6adRGJe5cQ) &nbsp; [Seq2Seq Transformers](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/seq2seq_transformer\/seq2seq_transformer.py) **- Sequence to Sequence with Transformers**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/U0s0f995w14) &nbsp; [Transformers from scratch](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/more_advanced\/transformer_from_scratch\/transformer_from_scratch.py) **- Attention Is All You Need**\n\n### Object Detection\n[Object Detection Playlist](https:\/\/youtube.com\/playlist?list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/XXYG5ZWtjj0) &nbsp; [Intersection over Union](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/object_detection\/metrics\/iou.py) \n* [![Youtube Link][logo]](https:\/\/youtu.be\/YDkjWEN8jNA) &nbsp; [Non-Max Suppression](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/object_detection\/metrics\/nms.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/FppOzcDvaDI) &nbsp; [Mean Average Precision](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/object_detection\/metrics\/mean_avg_precision.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/n9_XyCGr-MI) &nbsp; [YOLOv1 from scratch](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/object_detection\/YOLO)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/Grir6TZbc1M) &nbsp; [YOLOv3 from scratch](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/object_detection\/YOLOv3)\n\n### Generative Adversarial Networks\n[GAN Playlist](https:\/\/youtube.com\/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va)\n\n* [![Youtube Link][logo]](https:\/\/youtu.be\/OljTVUVzPpM) &nbsp; [Simple FC GAN](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/GANs\/1.%20SimpleGAN\/fc_gan.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/IZtv9s_Wx9I) &nbsp; [DCGAN](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/2.%20DCGAN)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/pG0QZ7OddX4) &nbsp; [WGAN](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/3.%20WGAN)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/pG0QZ7OddX4) &nbsp; [WGAN-GP](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/4.%20WGAN-GP)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/SuddDSqGRzg) &nbsp; [Pix2Pix](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/Pix2Pix)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/4LktBHGCNfw) &nbsp; [CycleGAN](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/CycleGAN)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/nkQHASviYac) &nbsp; [ProGAN](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/ProGAN)\n* [SRGAN](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/SRGAN)\n* [ESRGAN](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/ESRGAN)\n* [StyleGAN](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/Pytorch\/GANs\/StyleGAN) - NOTE: NOT DONE\n\n\n\n### Architectures\n* [![Youtube Link][logo]](https:\/\/youtu.be\/fcOW-Zyb5Bo) &nbsp; [LeNet5](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/79f2e1928906f3cccbae6c024f3f79fd05262cd1\/ML\/Pytorch\/CNN_architectures\/lenet5_pytorch.py#L15-L35) **- CNN architecture**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/ACmuBbuXn20) &nbsp; [VGG](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/79f2e1928906f3cccbae6c024f3f79fd05262cd1\/ML\/Pytorch\/CNN_architectures\/pytorch_vgg_implementation.py#L16-L62) **- CNN architecture**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/uQc4Fs7yx5I) &nbsp; [Inception v1](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/CNN_architectures\/pytorch_inceptionet.py) **- CNN architecture**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/DkNIBBBvcPs) &nbsp; [ResNet](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/CNN_architectures\/pytorch_resnet.py) **- CNN architecture**\n* [![Youtube Link][logo]](https:\/\/youtu.be\/fR_0o25kigM) &nbsp; [EfficientNet](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/CNN_architectures\/pytorch_efficientnet.py) **- CNN architecture**\n \n\n## TensorFlow Tutorials\nIf you have any specific video suggestion please make a comment on YouTube :)\n\n### Beginner Tutorials\n* [![Youtube Link][logo]](https:\/\/youtu.be\/5Ym-dOS9ssA) &nbsp; Tutorial 1 - Installation, Video Only\n* [![Youtube Link][logo]](https:\/\/youtu.be\/HPjBY1H-U4U) &nbsp; [Tutorial 2 - Tensor Basics](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial2-tensorbasics.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/pAhPiF3yiXI) &nbsp; [Tutorial 3 - Neural Network](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial3-neuralnetwork.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/WAciKiDP2bo) &nbsp; [Tutorial 4 - Convolutional Neural Network](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial4-convnet.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/kJSUq1PLmWg) &nbsp; [Tutorial 5 - Regularization](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial5-regularization.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/WAciKiDP2bo) &nbsp; [Tutorial 6 - RNN, GRU, LSTM](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial6-rnn-gru-lstm.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/kJSUq1PLmWg) &nbsp; [Tutorial 7 - Functional API](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial7-indepth-functional.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/WcZ_1IAH_nM) &nbsp; [Tutorial 8 - Keras Subclassing](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial8_keras_subclassing.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/cKMJDkWSDnY) &nbsp; [Tutorial 9 - Custom Layers](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial9-custom-layers.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/idus3KO6Wic) &nbsp; [Tutorial 10 - Saving and Loading Models](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial10-save-model.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/WJZoywOG1cs) &nbsp; [Tutorial 11 - Transfer Learning](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial11-transfer-learning.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/YrMy-BAqk8k) &nbsp; [Tutorial 12 - TensorFlow Datasets](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial12-tensorflowdatasets.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/8wwfVV7ixyY) &nbsp; [Tutorial 13 - Data Augmentation](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial13-data-augmentation.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/WUzLJZCKNu4) &nbsp; [Tutorial 14 - Callbacks](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial14-callbacks.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/S6tLSI8bjGs) &nbsp; [Tutorial 15 - Custom model.fit](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial15-customizing-modelfit.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/_u7AVsxANes) &nbsp; [Tutorial 16 - Custom Loops](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/blob\/master\/ML\/TensorFlow\/Basics\/tutorial16-customloops.py)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/k7KfYXXrOj0) &nbsp; [Tutorial 17 - TensorBoard](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/Basics\/tutorial17-tensorboard)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/q7ZuZ8ZOErE) &nbsp; [Tutorial 18 - Custom Dataset Images](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/Basics\/tutorial18-customdata-images)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/NoKvCREx36Q) &nbsp; [Tutorial 19 - Custom Dataset Text](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/Basics\/tutorial19-customdata-text)\n* [![Youtube Link][logo]](https:\/\/youtu.be\/ea5Z1smiR3U) &nbsp; [Tutorial 20 - Classifying Skin Cancer](https:\/\/github.com\/AladdinPerzon\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/Basics\/tutorial20-classify-cancer-beginner-project-example) **- Beginner Project Example**\n\n### CNN Architectures\n* [LeNet](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/CNN_architectures\/LeNet5)\n* [AlexNet](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/CNN_architectures\/AlexNet)\n* [VGG](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/CNN_architectures\/VGGNet)\n* [GoogLeNet](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/CNN_architectures\/GoogLeNet)\n* [ResNet](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/tree\/master\/ML\/TensorFlow\/CNN_architectures\/ResNet)\n ","137":"<\/br>\n\n<p align=\"center\">\n  <img height=\"80px\" src=\"docs\/img\/logo.svg\" alt=\"river_logo\">\n<\/p>\n\n<\/br>\n\n<p align=\"center\">\n  <!-- Tests -->\n  <a href=\"https:\/\/github.com\/online-ml\/river\/actions\/workflows\/unit-tests.yml\">\n    <img src=\"https:\/\/github.com\/online-ml\/river\/actions\/workflows\/unit-tests.yml\/badge.svg\" alt=\"tests\">\n  <\/a>\n  <!-- Code coverage -->\n  <a href=\"https:\/\/codecov.io\/gh\/online-ml\/river\">\n    <img src=\"https:\/\/codecov.io\/gh\/online-ml\/river\/branch\/main\/graph\/badge.svg?token=luK6eFoMa9\"\/>\n  <\/a>\n  <!-- Documentation -->\n  <a href=\"https:\/\/riverml.xyz\">\n    <img src=\"https:\/\/img.shields.io\/website?label=docs&style=flat-square&url=https%3A%2F%2Friverml.xyz%2F\" alt=\"documentation\">\n  <\/a>\n  <!-- Roadmap -->\n  <a href=\"https:\/\/www.notion.so\/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1\">\n    <img src=\"https:\/\/img.shields.io\/website?label=roadmap&style=flat-square&url=https:\/\/www.notion.so\/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1\" alt=\"roadmap\">\n  <\/a>\n  <!-- PyPI -->\n  <a href=\"https:\/\/pypi.org\/project\/river\">\n    <img src=\"https:\/\/img.shields.io\/pypi\/v\/river.svg?label=release&color=blue&style=flat-square\" alt=\"pypi\">\n  <\/a>\n  <!-- PePy -->\n  <a href=\"https:\/\/pepy.tech\/project\/river\">\n    <img src=\"https:\/\/static.pepy.tech\/badge\/river?style=flat-square\" alt=\"pepy\">\n  <\/a>\n  <!-- License -->\n  <a href=\"https:\/\/opensource.org\/licenses\/BSD-3-Clause\">\n    <img src=\"https:\/\/img.shields.io\/badge\/License-BSD%203--Clause-blue.svg?style=flat-square\" alt=\"bsd_3_license\">\n  <\/a>\n<\/p>\n\n<\/br>\n\n<p align=\"center\">\n  River is a Python library for <a href=\"https:\/\/www.wikiwand.com\/en\/Online_machine_learning\">online machine learning<\/a>. It is the result of a merger between <a href=\"https:\/\/github.com\/MaxHalford\/creme\">creme<\/a> and <a href=\"https:\/\/github.com\/scikit-multiflow\/scikit-multiflow\">scikit-multiflow<\/a>. River's ambition is to be the go-to library for doing machine learning on streaming data.\n<\/p>\n\n## \u26a1\ufe0f Quickstart\n\nAs a quick example, we'll train a logistic regression to classify the [website phishing dataset](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Website+Phishing). Here's a look at the first observation in the dataset.\n\n```python\n>>> from pprint import pprint\n>>> from river import datasets\n\n>>> dataset = datasets.Phishing()\n\n>>> for x, y in dataset:\n...     pprint(x)\n...     print(y)\n...     break\n{'age_of_domain': 1,\n 'anchor_from_other_domain': 0.0,\n 'empty_server_form_handler': 0.0,\n 'https': 0.0,\n 'ip_in_url': 1,\n 'is_popular': 0.5,\n 'long_url': 1.0,\n 'popup_window': 0.0,\n 'request_from_other_domain': 0.0}\nTrue\n\n```\n\nNow let's run the model on the dataset in a streaming fashion. We sequentially interleave predictions and model updates. Meanwhile, we update a performance metric to see how well the model is doing.\n\n```python\n>>> from river import compose\n>>> from river import linear_model\n>>> from river import metrics\n>>> from river import preprocessing\n\n>>> model = compose.Pipeline(\n...     preprocessing.StandardScaler(),\n...     linear_model.LogisticRegression()\n... )\n\n>>> metric = metrics.Accuracy()\n\n>>> for x, y in dataset:\n...     y_pred = model.predict_one(x)      # make a prediction\n...     metric = metric.update(y, y_pred)  # update the metric\n...     model = model.learn_one(x, y)      # make the model learn\n\n>>> metric\nAccuracy: 89.20%\n\n```\n\n## \ud83d\udee0 Installation\n\nRiver is intended to work with **Python 3.6 or above**. Installation can be done with `pip`:\n\n```sh\npip install river\n```\n\nThere are [wheels available](https:\/\/pypi.org\/project\/river\/#files) for Linux, MacOS, and Windows, which means that you most probably won't have to build River from source.\n\nYou can install the latest development version from GitHub as so:\n\n```sh\npip install git+https:\/\/github.com\/online-ml\/river --upgrade\n```\n\nOr, through SSH:\n\n```sh\npip install git+ssh:\/\/git@github.com\/online-ml\/river.git --upgrade\n```\n\n## \ud83e\udde0 Philosophy\n\nMachine learning is often done in a batch setting, whereby a model is fitted to a dataset in one go. This results in a static model which has to be retrained in order to learn from new data. In many cases, this isn't elegant nor efficient, and usually incurs [a fair amount of technical debt](https:\/\/research.google\/pubs\/pub43146\/). Indeed, if you're using a batch model, then you need to think about maintaining a training set, monitoring real-time performance, model retraining, etc.\n\nWith River, we encourage a different approach, which is to continuously learn a stream of data. This means that the model process one observation at a time, and can therefore be updated on the fly. This allows to learn from massive datasets that don't fit in main memory. Online machine learning also integrates nicely in cases where new data is constantly arriving. It shines in many use cases, such as time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT applications. If you're bored with retraining models and want to instead build dynamic models, then online machine learning (and therefore River!) might be what you're looking for.\n\nHere are some benefits of using River (and online machine learning in general):\n\n- **Incremental**: models can update themselves in real-time.\n- **Adaptive**: models can adapt to [concept drift](https:\/\/www.wikiwand.com\/en\/Concept_drift).\n- **Production-ready**: working with data streams makes it simple to replicate production scenarios during model development.\n- **Efficient**: models don't have to be retrained and require little compute power, which [lowers their carbon footprint](https:\/\/arxiv.org\/abs\/1907.10597)\n- **Fast**: when the goal is to learn and predict with a single instance at a time, then River is an order of magnitude faster than PyTorch, Tensorflow, and scikit-learn.\n\n## \ud83d\udd25 Features\n\n- Linear models with a wide array of optimizers\n- Nearest neighbors, decision trees, na\u00efve Bayes\n- [Progressive model validation](https:\/\/hunch.net\/~jl\/projects\/prediction_bounds\/progressive_validation\/coltfinal.pdf)\n- Model pipelines as a first-class citizen\n- Anomaly detection\n- Recommender systems\n- Time series forecasting\n- Imbalanced learning\n- Clustering\n- Feature extraction and selection\n- Online statistics and metrics\n- Built-in datasets\n- And [much more](https:\/\/riverml.xyz\/latest\/api\/overview\/)\n\n## \ud83d\udd17 Useful links\n\n- [Documentation](https:\/\/riverml.xyz)\n- [Benchmarks](https:\/\/github.com\/online-ml\/river\/tree\/main\/benchmarks)\n- [Issue tracker](https:\/\/github.com\/online-ml\/river\/issues)\n- [Package releases](https:\/\/pypi.org\/project\/river\/#history)\n\n## \ud83d\udc41\ufe0f Media\n\n- PyData Amsterdam 2019 presentation ([slides](https:\/\/maxhalford.github.io\/slides\/creme-pydata), [video](https:\/\/www.youtube.com\/watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11))\n- [Toulouse Data Science Meetup presentation](https:\/\/maxhalford.github.io\/slides\/creme-tds)\n- [Machine learning for streaming data with creme](https:\/\/towardsdatascience.com\/machine-learning-for-streaming-data-with-creme-dacf5fb469df)\n- [Hong Kong Data Science Meetup presentation](https:\/\/maxhalford.github.io\/slides\/hkml2020.pdf)\n\n## \ud83d\udc4d Contributing\n\nFeel free to contribute in any way you like, we're always open to new ideas and approaches.\n\nThere are three ways for users to get involved:\n\n- [Issue tracker](https:\/\/github.com\/online-ml\/river\/issues): this place is meant to report bugs, request for minor features, or small improvements. Issues should be short-lived and solved as fast as possible.\n- [Discussions](https:\/\/github.com\/online-ml\/river\/discussions): you can ask for new features, submit your questions and get help, propose new ideas, or even show the community what you are achieving with River! If you have a new technique or want to port a new functionality to River, this is the place to discuss.\n- [Roadmap](https:\/\/www.notion.so\/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can check what we are doing, what are the next planned milestones for River, and look for cool ideas that still need someone to make them become a reality!\n\nPlease check out the [contribution guidelines](https:\/\/github.com\/online-ml\/river\/blob\/main\/CONTRIBUTING.md) if you want to bring modifications to the code base. You can view the list of people who have contributed [here](https:\/\/github.com\/online-ml\/river\/graphs\/contributors).\n\n## \u2764\ufe0f They've used us\n\nThese are companies that we know have been using River, be it in production or for prototyping.\n\n<p align=\"center\">\n  <img width=\"70%\" src=\"https:\/\/docs.google.com\/drawings\/d\/e\/2PACX-1vQbCUQkTU74dBf411r4nDl4udmqOEbLqzRtokUC-N7JDJUA7BGTfnMGmiMNqbcSuOaWAmazp1rFGwDC\/pub?w=1194&h=567\" alt=\"companies\">\n<\/p>\n\nFeel welcome to get in touch if you want us to add your company logo!\n\n## \ud83e\udd1d Affiliations\n\n**Sponsors**\n\n<p align=\"center\">\n  <img width=\"55%\" src=\"https:\/\/docs.google.com\/drawings\/d\/e\/2PACX-1vSagEhWAjDsb0c24En_fhWAf9DJZbyh5YjU7lK0sNowD2m9uv9TuFm-U77k6ObqTyN2mP05Avf6TCJc\/pub?w=2073&h=1127\" alt=\"sponsors\">\n<\/p>\n\n**Collaborating institutions and groups**\n\n<p align=\"center\">\n  <img width=\"55%\" src=\"https:\/\/docs.google.com\/drawings\/d\/e\/2PACX-1vQB0C8YgnkCt_3C3cp-Csaw8NLZUwishdbJFB3iSbBPUD0AxEVS9AlF-Rs5PJq8UVRzRtFwZIOucuXj\/pub?w=1442&h=489\" alt=\"collaborations\">\n<\/p>\n\n## \ud83d\udcac Citation\n\nIf `river` has been useful for your research and you would like to cite it in an scientific publication, please refer to this [paper](https:\/\/arxiv.org\/abs\/2012.04740):\n\n```bibtex\n@misc{2020river,\n      title={River: machine learning for streaming data in Python},\n      author={Jacob Montiel and Max Halford and Saulo Martiello Mastelini\n              and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse\n              and Adil Zouitine and Heitor Murilo Gomes and Jesse Read\n              and Talel Abdessalem and Albert Bifet},\n      year={2020},\n      eprint={2012.04740},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n## \ud83d\udcdd License\n\nRiver is free and open-source software licensed under the [3-clause BSD license](https:\/\/github.com\/online-ml\/river\/blob\/main\/LICENSE).\n","138":"![](https:\/\/raw.githubusercontent.com\/cleanlab\/assets\/master\/cleanlab\/cleanlab_logo_open_source_transparent_optimized_size.png)\n\ncleanlab automatically finds and fixes errors in any ML dataset. This data-centric AI package facilitates **machine learning with messy, real-world data** by providing **clean lab**els during training.\n\n```python\n\n# Cleanlab works with **any classifier**. Yup, you can use sklearn\/PyTorch\/TensorFlow\/XGBoost\/etc.\ncl = cleanlab.classification.CleanLearning(sklearn.YourFavoriteClassifier())\n\n# cleanlab finds data and label issues in **any dataset**... in ONE line of code!\nlabel_issues = cl.find_label_issues(data, labels)\n\n# cleanlab trains a robust version of your model that works more reliably with noisy data.\ncl.fit(data, labels)\n\n# cleanlab estimates the predictions you would have gotten if you had trained with *no* label issues.\ncl.predict(test_data)\n\n# A true data-centric AI package, cleanlab quantifies class-level issues and overall data quality, for any dataset.\ncleanlab.dataset.health_summary(labels, confident_joint=cl.confident_joint)\n```\n\nGet started with: [documentation](https:\/\/docs.cleanlab.ai\/), [tutorials](https:\/\/docs.cleanlab.ai\/v2.0.0\/tutorials\/image.html), [examples](https:\/\/github.com\/cleanlab\/examples), and [blogs](https:\/\/cleanlab.ai\/blog\/).\n\n - [Learn how to](https:\/\/docs.cleanlab.ai\/v2.0.0\/tutorials\/index) run cleanlab on your own data in just 5 minutes!\n - Quickstart with 5-minute tutorials for classification with: [image](https:\/\/docs.cleanlab.ai\/v2.0.0\/tutorials\/image.html), [text](https:\/\/docs.cleanlab.ai\/v2.0.0\/tutorials\/text.html), [audio](https:\/\/docs.cleanlab.ai\/v2.0.0\/tutorials\/audio.html), and [tabular](https:\/\/docs.cleanlab.ai\/v2.0.0\/tutorials\/tabular.html) data.\n\n\n[![pypi](https:\/\/img.shields.io\/pypi\/v\/cleanlab.svg)](https:\/\/pypi.org\/pypi\/cleanlab\/)\n[![os](https:\/\/img.shields.io\/badge\/platform-noarch-lightgrey)](https:\/\/pypi.org\/pypi\/cleanlab\/)\n[![py\\_versions](https:\/\/img.shields.io\/badge\/python-3.6%2B-blue)](https:\/\/pypi.org\/pypi\/cleanlab\/)\n[![build\\_status](https:\/\/github.com\/cleanlab\/cleanlab\/workflows\/CI\/badge.svg)](https:\/\/github.com\/cleanlab\/cleanlab\/actions?query=workflow%3ACI)\n[![coverage](https:\/\/codecov.io\/gh\/cleanlab\/cleanlab\/branch\/master\/graph\/badge.svg)](https:\/\/app.codecov.io\/gh\/cleanlab\/cleanlab)\n[![docs](https:\/\/img.shields.io\/static\/v1?logo=github&style=flat&color=pink&label=docs&message=cleanlab)](https:\/\/docs.cleanlab.ai\/)\n[![Slack Community](https:\/\/img.shields.io\/static\/v1?logo=slack&style=flat&color=white&label=slack&message=community)](https:\/\/join.slack.com\/t\/cleanlab-community\/shared_invite\/zt-wpi8ocuf-O87JiSAvuvguytAUBiEwNQ)\n[![Twitter](https:\/\/img.shields.io\/twitter\/follow\/CleanlabAI?style=social)](https:\/\/twitter.com\/CleanlabAI)\n\n-----\n\n<details><summary><b>News! (2022) <\/b> -- cleanlab made accessible for everybody, not just ML researchers (<b>click to learn more<\/b>) <\/summary>\n<p>\n<ul>\n<li> <b>April 2022 \ud83d\udcd6<\/b> cleanlab 2.0.0 released! Lays foundations for this library to grow into a general-purpose data-centric AI toolkit. <\/li>\n<li> <b>March 2022 \ud83d\udcd6<\/b>  Documentation migrated to new website: <a href=\"https:\/\/docs.cleanlab.ai\/\">docs.cleanlab.ai<\/a> with quickstart tutorials for image\/text\/audio\/tabular data.<\/li>\n<li> <b>Feb 2022 \ud83d\udcbb<\/b> APIs <a href=\"https:\/\/docs.cleanlab.ai\/master\/migrating\/migrate_v2.html\">simplified<\/a> to make cleanlab accessible for everybody, not just ML researchers <\/li>\n<\/ul>\n<\/p>\n<\/details>\n\n<details><summary><b>News! (2021) <\/b> -- cleanlab finds pervasive label errors in the most common ML datasets (<b>click to learn more<\/b>) <\/summary>\n<p>\n<ul>\n<li> <b>Dec 2021 \ud83c\udf89<\/b>  NeurIPS published the <a href=\"https:\/\/arxiv.org\/abs\/2103.14749\">label errors paper (Northcutt, Athalye, & Mueller, 2021)<\/a>.<\/li>\n<li> <b>Apr 2021 \ud83c\udf89<\/b>  Journal of AI Research published the <a href=\"https:\/\/jair.org\/index.php\/jair\/article\/view\/12125\">confident learning paper (Northcutt, Jiang, & Chuang, 2021)<\/a>.<\/li>\n<li> <b>Mar 2021 \ud83d\ude32<\/b>  cleanlab used to find and fix label issues in 10 of the most common ML benchmark datasets, published in: <a href=\"https:\/\/datasets-benchmarks-proceedings.neurips.cc\/paper\/2021\/hash\/f2217062e9a397a1dca429e7d70bc6ca-Abstract-round1.html\">NeurIPS 2021<\/a>. Along with <a href=\"https:\/\/arxiv.org\/abs\/2103.14749\">the paper (Northcutt, Athalye, & Mueller, 2021)<\/a>, the authors launched <a href=\"https:\/\/labelerrors.com\">labelerrors.com<\/a> where you can view the label issues in these datasets.<\/li>\n<\/ul>\n<\/p>\n<\/details>\n\n<details><summary><b>News! (2020) <\/b> -- cleanlab adds support for all OS, achieves state-of-the-art, supports co-teaching and more (<b>click to learn more<\/b>) <\/summary>\n<p>\n<ul>\n<li> <b>Dec 2020 \ud83c\udf89<\/b>  cleanlab supports NeurIPS workshop paper <a href=\"https:\/\/securedata.lol\/camera_ready\/28.pdf\">(Northcutt, Athalye, & Lin, 2020)<\/a>.<\/li>\n<li> <b>Dec 2020 \ud83e\udd16<\/b>  cleanlab supports <a href=\"https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/classification.py#L214\">PU learning<\/a>.<\/li>\n<li> <b>Feb 2020 \ud83e\udd16<\/b>  cleanlab now natively supports Mac, Linux, and Windows.<\/li>\n<li> <b>Feb 2020 \ud83e\udd16<\/b>  cleanlab now supports <a href=\"https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/experimental\/coteaching.py\">Co-Teaching<\/a> <a href=\"https:\/\/arxiv.org\/abs\/1804.06872\">(Han et al., 2018)<\/a>.<\/li>\n<li> <b>Jan 2020 \ud83c\udf89<\/b> cleanlab achieves state-of-the-art on CIFAR-10 with noisy labels. Code to reproduce:  <a href=\"https:\/\/github.com\/cleanlab\/examples\/tree\/master\/contrib\/v1\/cifar10\">examples\/cifar10<\/a>. This is a great place to see how to use cleanlab on real datasets (with predicted probabilities from trained model already precomputed for you).<\/li>\n<\/ul>\n<\/p>\n<\/details>\n\nRelease notes for past versions are available [here](https:\/\/github.com\/cleanlab\/cleanlab\/releases). Details behind certain updates are explained in our [blog](https:\/\/cleanlab.ai\/blog\/).\n\n**Long-time cleanlab user?**\n\n* Here's a [guide](https:\/\/docs.cleanlab.ai\/v2.0.0\/migrating\/migrate_v2.html) on how to migrate to cleanlab 2.0.0.\n\n## So fresh, so cleanlab\n\ncleanlab **clean**s your data's **lab**els via state-of-the-art *confident learning* algorithms, published in this [paper](https:\/\/jair.org\/index.php\/jair\/article\/view\/12125) and [blog](https:\/\/l7.curtisnorthcutt.com\/confident-learning). See datasets cleaned with cleanlab at [labelerrors.com](https:\/\/labelerrors.com). This package helps you find all the label issues lurking in your data and train more reliable ML models.\n\ncleanlab is:\n\n1. **backed by theory**\n   - with [provable guarantees](https:\/\/arxiv.org\/abs\/1911.00068) of exact noise estimation and label error finding in realistic cases with imperfect models.\n2. **fast**\n   - Code is optimized and parallel-threaded (< 1 second to find label issues in ImageNet with pre-computed probabilities).\n4. **easy-to-use**\n   - Find label issues or train noise-robust models in one line of code. By default, cleanlab requires no hyper-parameters.\n6. **general**\n   -  Works with **[any dataset](https:\/\/labelerrors.com\/)** and **any model**, e.g., TensorFlow, PyTorch, sklearn, xgboost, etc.\n<br\/>\n\n![](https:\/\/raw.githubusercontent.com\/cleanlab\/assets\/master\/cleanlab\/label-errors-examples.png)\n<p align=\"center\">\nExamples of incorrect given labels in various image datasets <a href=\"https:\/\/l7.curtisnorthcutt.com\/label-errors\">found and corrected<\/a> using cleanlab.\n<\/p>\n\n\n## Run cleanlab\n\n\ncleanlab supports Linux, macOS, and Windows and runs on Python 3.6+.\n\n- Get started [here](https:\/\/docs.cleanlab.ai\/)! Install via `pip` or `conda` as described [here](https:\/\/docs.cleanlab.ai\/).\n- Developers who install the bleeding-edge master branch from source should refer to [this master version of documentation](https:\/\/docs.cleanlab.ai\/master\/index.html).\n\n<details><summary>\ncleanlab core package components\n(<b>click to learn more<\/b>)\n<\/summary>\n<br\/>\n\nMany methods have default parameters not covered here. Check out the [documentation for the master branch version](https:\/\/docs.cleanlab.ai\/master\/)\n\n### cleanlab Core Package Components\n\n1.  **cleanlab\/classification.py** - [CleanLearning()](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/classification.py#L106) class for learning with noisy labels.\n2.  **cleanlab\/count.py** - Estimates and fully characterizes all variants of label noise.\n3.  **cleanlab\/filter.py** - Finds the examples with label issues in a dataset.\n4.  **cleanlab\/rank.py** - Rank every example in a dataset with various label quality scores.\n5.  **cleanlab.dataset.py** - Provides dataset-level and class-level overviews of issues in your dataset.\n6.  **cleanlab\/benchmarking\/noise\\_generation.py** - Generate noisy labels for benchmarking, reproduction, and ML research.\n\n<br\/>\n<\/details>\n\n\n## Use cleanlab with any model (TensorFlow, PyTorch, sklearn, xgboost, etc.)\n\nAll features of cleanlab work with **any dataset** and **any model**. Yes, any model: scikit-learn, PyTorch, Tensorflow, Keras, JAX, HuggingFace, MXNet, XGBoost, etc.\nIf you use a sklearn-compatible classifier, cleanlab methods work out-of-the-box.\n\n<details><summary>\nIt\u2019s also easy to use your favorite non-sklearn-compatible model (<b>click to learn more<\/b>)\n<\/summary>\n<br\/>\n\nThere's nothing you need to do if your model already has `.fit()`, `.predict()`, and `.predict_proba()` methods.\nOtherwise, just wrap your custom model into a Python class that inherits the `sklearn.base.BaseEstimator`:\n\n``` python\nfrom sklearn.base import BaseEstimator\nclass YourFavoriteModel(BaseEstimator): # Inherits sklearn base classifier\n    def __init__(self, ):\n        pass  # ensure this re-initializes parameters for neural net models\n    def fit(self, X, y, sample_weight=None):\n        pass\n    def predict(self, X):\n        pass\n    def predict_proba(self, X):\n        pass\n    def score(self, X, y, sample_weight=None):\n        pass\n```\n\nThis inheritance allows to apply a wide range of sklearn functionality like hyperparameter-optimization to your custom model.\nNow you can use your model with every method in cleanlab. Here's one example:\n\n``` python\nfrom cleanlab.classification import CleanLearning\ncl = CleanLearning(clf=YourFavoriteModel())  # has all the same methods of YourFavoriteModel\ncl.fit(train_data, train_labels_with_errors)\ncl.predict(test_data)\n```\n\n#### Want to see a working example? [Here\u2019s a compliant PyTorch MNIST CNN class](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/experimental\/mnist_pytorch.py)\n\nMore details are provided in documentation of [cleanlab.classification.CleanLearning](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/classification.py#L106).\n\nNote, some libraries exist to give you sklearn-compatibility for free. For PyTorch, check out the [skorch](https:\/\/skorch.readthedocs.io\/) Python library which will wrap your PyTorch model into a sklearn-compatible model ([example](https:\/\/docs.cleanlab.ai\/master\/tutorials\/image.html)). For TensorFlow\/Keras, check out [SciKeras](https:\/\/www.adriangb.com\/scikeras\/) ([example](https:\/\/docs.cleanlab.ai\/master\/tutorials\/text.html)). Many libraries also already offer a special scikit-learn API, for example: [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/stable\/python\/python_api.html#module-xgboost.sklearn) or [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html).\n\n<br\/>\n<\/details>\n\n\n## Cool cleanlab applications\n\n<details><summary>\nReproducing results in <a href=\"https:\/\/arxiv.org\/abs\/1911.00068\">Confident Learning paper<\/a>\n(<b>click to learn more<\/b>)\n<\/summary>\n<br\/>\n\nFor additional details, check out the: [confidentlearning-reproduce repository](https:\/\/github.com\/cgnorthcutt\/confidentlearning-reproduce).\n\n### State of the Art Learning with Noisy Labels in CIFAR\n\nA step-by-step guide to reproduce these results is available [here](https:\/\/github.com\/cleanlab\/examples\/tree\/master\/contrib\/v1\/cifar10). This guide is also a good tutorial for using cleanlab on any large dataset. You'll need to `git clone`\n[confidentlearning-reproduce](https:\/\/github.com\/cgnorthcutt\/confidentlearning-reproduce) which contains the data and files needed to reproduce the CIFAR-10 results.\n\n![](https:\/\/raw.githubusercontent.com\/cleanlab\/assets\/master\/cleanlab\/cifar10_benchmarks.png)\n\nComparison of confident learning (CL), as implemented in cleanlab, versus seven recent methods for learning with noisy labels in CIFAR-10. Highlighted cells show CL robustness to sparsity. The five CL methods estimate label issues, remove them, then train on the cleaned data using [Co-Teaching](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/experimental\/coteaching.py).\n\nObserve how cleanlab (i.e. the CL method) is robust to large sparsity in label noise whereas prior art tends to reduce in performance for increased sparsity, as shown by the red highlighted regions. This is important because real-world label noise is often sparse, e.g. a tiger is likely to be mislabeled as a lion, but not as most other classes like airplane, bathtub, and microwave.\n\n### Find label issues in ImageNet\n\nUse cleanlab to identify \\~100,000 label errors in the 2012 ILSVRC ImageNet training dataset: [examples\/imagenet](https:\/\/github.com\/cleanlab\/examples\/tree\/master\/contrib\/v1\/imagenet).\n\n![](https:\/\/raw.githubusercontent.com\/cleanlab\/assets\/master\/cleanlab\/imagenet_train_label_errors_32.jpg)\n\nLabel issues in ImageNet train set found via cleanlab. Label Errors are boxed in red. Ontological issues in green. Multi-label images in blue.\n\n### Find Label Errors in MNIST\n\nUse cleanlab to identify \\~50 label errors in the MNIST dataset: [examples\/mnist](https:\/\/github.com\/cleanlab\/examples\/tree\/master\/contrib\/v1\/mnist).\n\n![](https:\/\/raw.githubusercontent.com\/cleanlab\/assets\/master\/cleanlab\/mnist_training_label_errors24_prune_by_noise_rate.png)\n\nTop 24 least-confident labels in the original MNIST **train** dataset, algorithmically identified via cleanlab. Examples are ordered left-right, top-down by increasing self-confidence (predicted probability that the **given** label is correct), denoted **conf** in teal. The most-likely correct label (with largest predicted probability) is in green. Overt label errors highlighted in red.\n\n<br\/>\n<\/details>\n\n\n\n<details><summary>\ncleanlab performance across 4 data distributions and 9 classifiers\n(<b>click to learn more<\/b>)\n<\/summary>\n<br\/>\n\ncleanlab is a general tool that can learn with noisy labels regardless of dataset distribution or classifier type: [examples\/classifier\\_comparison](https:\/\/github.com\/cleanlab\/examples\/blob\/master\/classifier_comparison.ipynb).\n\n![](https:\/\/raw.githubusercontent.com\/cleanlab\/assets\/master\/cleanlab\/demo_cleanlab_across_datasets_and_classifiers.png)\n\nEach sub-figure above depicts the decision boundary learned using [cleanlab.classification.CleanLearning](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/classification.py#L106) in the presence of extreme (\\~35%) label errors (circled in green). Label noise is class-conditional (not uniformly random). Columns are organized by the classifier used, except the left-most column which depicts the ground-truth data distribution. Rows are organized by dataset.\n\nEach sub-figure depicts accuracy scores on a test set (with correct non-noisy labels) as decimal values:\n\n*  LEFT (in black): The classifier test accuracy trained with perfect labels (no label errors).\n*  MIDDLE (in blue): The classifier test accuracy trained with noisy labels using cleanlab.\n*  RIGHT (in white): The baseline classifier test accuracy trained with noisy labels.\n\nAs an example, the table below is the noise matrix (noisy channel) *P(s | y)\ncharacterizing the label noise for the first dataset row in the figure. *s* represents the observed noisy labels and *y* represents the latent, true labels. The trace of this matrix is 2.6. A trace of 4 implies no label noise. A cell in this matrix is read like: \"Around 38% of true underlying '3' labels were randomly flipped to '2' labels in the\nobserved dataset.\"\n\n| `p(label\ufe31y)` | y=0  | y=1  | y=2  | y=3  |\n|--------------|------|------|------|------|\n| label=0      | 0.55 | 0.01 | 0.07 | 0.06 |\n| label=1      | 0.22 | 0.87 | 0.24 | 0.02 |\n| label=2      | 0.12 | 0.04 | 0.64 | 0.38 |\n| label=3      | 0.11 | 0.08 | 0.05 | 0.54 |\n\n<br\/>\n<\/details>\n\n<details><summary>\nML research using cleanlab\n(<b>click to learn more<\/b>)\n<\/summary>\n<br\/>\n\nResearchers may find some components of this package useful for evaluating algorithms for ML with noisy labels. For additional details\/notation, refer to [the Confident Learning paper](https:\/\/jair.org\/index.php\/jair\/article\/view\/12125).\n\n### Methods to Standardize Research with Noisy Labels\n\ncleanlab supports a number of functions to generate noise for benchmarking and standardization in research. This next example shows how to generate valid, class-conditional, uniformly random noisy channel matrices:\n\n``` python\n# Generate a valid (necessary conditions for learnability are met) noise matrix for any trace > 1\nfrom cleanlab.benchmarking.noise_generation import generate_noise_matrix_from_trace\nnoise_matrix=generate_noise_matrix_from_trace(\n    K=number_of_classes,\n    trace=float_value_greater_than_1_and_leq_K,\n    py=prior_of_y_actual_labels_which_is_just_an_array_of_length_K,\n    frac_zero_noise_rates=float_from_0_to_1_controlling_sparsity,\n)\n\n# Check if a noise matrix is valid (necessary conditions for learnability are met)\nfrom cleanlab.benchmarking.noise_generation import noise_matrix_is_valid\nis_valid=noise_matrix_is_valid(\n    noise_matrix,\n    prior_of_y_which_is_just_an_array_of_length_K,\n)\n```\n\nFor a given noise matrix, this example shows how to generate noisy labels. Methods can be seeded for reproducibility.\n\n``` python\n# Generate noisy labels using the noise_marix. Guarantees exact amount of noise in labels.\nfrom cleanlab.benchmarking.noise_generation import generate_noisy_labels\ns_noisy_labels = generate_noisy_labels(y_hidden_actual_labels, noise_matrix)\n\n# This package is a full of other useful methods for learning with noisy labels.\n# The tutorial stops here, but you don't have to. Inspect method docstrings for full docs.\n```\n\n<br\/>\n<\/details>\n\n\n<details><summary>\ncleanlab for advanced users\n(<b>click to learn more<\/b>)\n<\/summary>\n<br\/>\n\nMany methods and their default parameters are not covered here. Check out the [documentation for the master branch version](https:\/\/docs.cleanlab.ai\/master\/) for the full suite of features supported by the cleanlab API.\n\n## Use any custom model's predicted probabilities to find label errors in 1 line of code\n\npred_probs (num_examples x num_classes matrix of predicted probabilities) should already be computed on your own, with any classifier. pred_probs must be obtained in a holdout\/out-of-sample manner (e.g. via cross-validation).\n* cleanlab can do this for you via [`cleanlab.count.estimate_cv_predicted_probabilities`](https:\/\/docs.cleanlab.ai\/master\/cleanlab\/count.html)]\n* Tutorial with more info: [[here](https:\/\/docs.cleanlab.ai\/master\/tutorials\/pred_probs_cross_val.html)]\n* Examples how to compute pred_probs with: [[CNN image classifier (PyTorch)](https:\/\/docs.cleanlab.ai\/v2.0.0\/tutorials\/image.html)], [[NN text classifier (TensorFlow)](https:\/\/docs.cleanlab.ai\/v2.0.0\/tutorials\/text.html)]\n\n```python\n# label issues are ordered by likelihood of being an error. First index is most likely error.\nfrom cleanlab.filter import find_label_issues\n\nordered_label_issues = find_label_issues(  # One line of code!\n    labels=numpy_array_of_noisy_labels,\n    pred_probs=numpy_array_of_predicted_probabilities,\n    return_indices_ranked_by='normalized_margin', # Orders label issues\n )\n```\n\nPre-computed **out-of-sample** predicted probabilities for CIFAR-10 train set are available: [here](https:\/\/github.com\/cleanlab\/examples\/tree\/master\/contrib\/v1\/cifar10#pre-computed-psx-for-every-noise--sparsity-condition).\n\n## Fully characterize label noise and uncertainty in your dataset.\n\n*s* denotes a random variable that represents the observed, noisy label and *y* denotes a random variable representing the hidden, actual labels. Both *s* and *y* take any of the m classes as values. The cleanlab package supports different levels of granularity for computation depending on the needs of the user. Because of this, we support multiple alternatives, all no more than a few lines, to estimate these latent distribution arrays, enabling the user to reduce computation time by only computing what they need to compute, as seen in the examples below.\n\nThroughout these examples, you\u2019ll see a variable called *confident\\_joint*. The confident joint is an m x m matrix (m is the number of classes) that counts, for every observed, noisy class, the number of examples that confidently belong to every latent, hidden class. It counts the number of examples that we are confident are labeled correctly or incorrectly for every pair of observed and unobserved classes. The confident joint is an unnormalized estimate of the complete-information latent joint distribution, *Ps,y*.\n\nThe label flipping rates are denoted *P(s | y)*, the inverse rates are *P(y | s)*, and the latent prior of the unobserved, true labels, *p(y)*.\n\nMost of the methods in the **cleanlab** package start by first estimating the *confident\\_joint*. You can learn more about this in the [confident learning paper](https:\/\/arxiv.org\/abs\/1911.00068).\n\n### Option 1: Compute the confident joint and predicted probs first. Stop if that\u2019s all you need.\n\n``` python\nfrom cleanlab.count import estimate_latent\nfrom cleanlab.count import estimate_confident_joint_and_cv_pred_proba\n\n# Compute the confident joint and the n x m predicted probabilities matrix (pred_probs),\n# for n examples, m classes. Stop here if all you need is the confident joint.\nconfident_joint, pred_probs = estimate_confident_joint_and_cv_pred_proba(\n    X=X_train,\n    labels=train_labels_with_errors,\n    clf=logreg(), # default, you can use any classifier\n)\n\n# Estimate latent distributions: p(y) as est_py, P(s|y) as est_nm, and P(y|s) as est_inv\nest_py, est_nm, est_inv = estimate_latent(\n    confident_joint,\n    labels=train_labels_with_errors,\n)\n```\n\n### Option 2: Estimate the latent distribution matrices in a single line of code.\n\n``` python\nfrom cleanlab.count import estimate_py_noise_matrices_and_cv_pred_proba\nest_py, est_nm, est_inv, confident_joint, pred_probs = estimate_py_noise_matrices_and_cv_pred_proba(\n    X=X_train,\n    labels=train_labels_with_errors,\n)\n```\n\n### Option 3: Skip computing the predicted probabilities if you already have them.\n\n``` python\n# Already have pred_probs? (n x m matrix of predicted probabilities)\n# For example, you might get them from a pre-trained model (like resnet on ImageNet)\n# With the cleanlab package, you estimate directly with pred_probs.\nfrom cleanlab.count import estimate_py_and_noise_matrices_from_probabilities\nest_py, est_nm, est_inv, confident_joint = estimate_py_and_noise_matrices_from_probabilities(\n    labels=train_labels_with_errors,\n    pred_probs=pred_probs,\n)\n```\n\n## Completely characterize label noise in a dataset:\n\nThe joint probability distribution of noisy and true labels, *P(s,y)*, completely characterizes label noise with a class-conditional *m x m* matrix.\n\n``` python\nfrom cleanlab.count import estimate_joint\njoint = estimate_joint(\n    labels=noisy_labels,\n    pred_probs=probabilities,\n    confident_joint=None,  # Provide if you have it already\n)\n```\n\n<br\/>\n<\/details>\n\n<details><summary>\nPositive-Unlabeled learning with cleanlab\n(<b>click to learn more<\/b>)\n<\/summary>\n<br\/>\n\nPositive-Unlabeled (PU) learning (in which your data only contains a few positively labeled examples with the rest unlabeled) is just a special case of [CleanLearning](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/classification.py#L106) when one of the classes has no error. `P` stands for the positive class and **is assumed to have zero label errors** and `U` stands for unlabeled data, but in practice, we just assume the `U` class is a noisy negative class that actually contains some positive examples. Thus, the goal of PU learning is to (1) estimate the proportion of negatively labeled examples that actually belong to the positive class (see`fraction\\_noise\\_in\\_unlabeled\\_class` in the last example), (2) find the errors (see last example), and (3) train on clean data (see first example below). cleanlab does all three, taking into account that there are no label errors in whichever class you specify as positive.\n\nThere are two ways to use cleanlab for PU learning. We'll look at each here.\n\nMethod 1. If you are using the cleanlab classifier [CleanLearning()](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/classification.py#L106), and your dataset has exactly two classes (positive = 1, and negative = 0), PU\nlearning is supported directly in cleanlab. You can perform PU learning like this:\n\n``` python\nfrom cleanlab.classification import CleanLearning\nfrom sklearn.linear_model import LogisticRegression\n# Wrap around any classifier. Yup, you can use sklearn\/pyTorch\/TensorFlow\/FastText\/etc.\npu_class = 0 # Should be 0 or 1. Label of class with NO ERRORS. (e.g., P class in PU)\ncl = CleanLearning(clf=LogisticRegression(), pulearning=pu_class)\ncl.fit(X=X_train_data, labels=train_noisy_labels)\n# Estimate the predictions you would have gotten by training with *no* label errors.\npredicted_test_labels = cl.predict(X_test)\n```\n\nMethod 2. However, you might be using a more complicated classifier that doesn't work well with [CleanLearning](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/cleanlab\/classification.py#L106) (see this example for CIFAR-10). Or you might have 3 or more classes. Here's how to use cleanlab for PU learning in this situation. To let cleanlab know which class has no error (in standard PU learning, this is the P class), you need to set the threshold for that class to 1 (1 means the probability that the labels of that class are correct is 1, i.e. that class has no\nerror). Here's the code:\n\n``` python\nimport numpy as np\n# K is the number of classes in your dataset\n# pred_probs are the cross-validated predicted probabilities.\n# s is the array\/list\/iterable of noisy labels\n# pu_class is a 0-based integer for the class that has no label errors.\nthresholds = np.asarray([np.mean(pred_probs[:, k][s == k]) for k in range(K)])\nthresholds[pu_class] = 1.0\n```\n\nNow you can use cleanlab however you were before. Just be sure to pass in `thresholds` as a parameter wherever it applies. For example:\n\n``` python\n# Uncertainty quantification (characterize the label noise\n# by estimating the joint distribution of noisy and true labels)\ncj = compute_confident_joint(s, pred_probs, thresholds=thresholds, )\n# Now the noise (cj) has been estimated taking into account that some class(es) have no error.\n# We can use cj to find label errors like this:\nindices_of_label_issues = find_label_issues(s, pred_probs, confident_joint=cj, )\n\n# In addition to label issues, cleanlab can find the fraction of noise in the unlabeled class.\n# First we need the inv_noise_matrix which contains P(y|s) (proportion of mislabeling).\n_, _, inv_noise_matrix = estimate_latent(confident_joint=cj, labels=s, )\n# Because inv_noise_matrix contains P(y|s), p (y = anything | labels = pu_class) should be 0\n# because the prob(true label is something else | example is in pu_class) is 0.\n# What's more interesting is p(y = anything | s is not put_class), or in the binary case\n# this translates to p(y = pu_class | s = 1 - pu_class) because pu_class is 0 or 1.\n# So, to find the fraction_noise_in_unlabeled_class, for binary, you just compute:\nfraction_noise_in_unlabeled_class = inv_noise_matrix[pu_class][1 - pu_class]\n```\n\nNow that you have `indices_of_label_errors`, you can remove those label issues and train on clean data (or only remove some of the label issues and iteratively use confident learning \/ cleanlab to improve results).\n\n<br\/>\n<\/details>\n\n## Citation and related publications\n\ncleanlab is based on peer-reviewed research. Here are the relevant papers to cite if you use this package:\n\n<details><summary><a href=\"https:\/\/arxiv.org\/abs\/1911.00068\">Confident Learning (JAIR '21)<\/a> (<b>click to show bibtex<\/b>) <\/summary>\n\n    @article{northcutt2021confidentlearning,\n        title={Confident Learning: Estimating Uncertainty in Dataset Labels},\n        author={Curtis G. Northcutt and Lu Jiang and Isaac L. Chuang},\n        journal={Journal of Artificial Intelligence Research (JAIR)},\n        volume={70},\n        pages={1373--1411},\n        year={2021}\n    }\n\n<\/details>\n\n<details><summary><a href=\"https:\/\/arxiv.org\/abs\/1705.01936\">Rank Pruning (UAI '17)<\/a> (<b>click to show bibtex<\/b>) <\/summary>\n\n    @inproceedings{northcutt2017rankpruning,\n        author={Northcutt, Curtis G. and Wu, Tailin and Chuang, Isaac L.},\n        title={Learning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels},\n        booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence},\n        series = {UAI'17},\n        year = {2017},\n        location = {Sydney, Australia},\n        numpages = {10},\n        url = {http:\/\/auai.org\/uai2017\/proceedings\/papers\/35.pdf},\n        publisher = {AUAI Press},\n    }\n\n<\/details>\n\n## Other resources\n\n- [Blog post: Introduction to Confident Learning](https:\/\/l7.curtisnorthcutt.com\/confident-learning)\n\n- [NeurIPS 2021 paper: Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks](https:\/\/arxiv.org\/abs\/2103.14749)\n\n- [Cleanlab Blog](https:\/\/cleanlab.ai\/blog\/)\n\n## Join our community\n\n* The best place to learn is [our Slack community](https:\/\/join.slack.com\/t\/cleanlab-community\/shared_invite\/zt-wpi8ocuf-O87JiSAvuvguytAUBiEwNQ).\n\n* Have ideas for the future of cleanlab? How are you using cleanlab? [Join the discussion](https:\/\/github.com\/cleanlab\/cleanlab\/discussions).\n\n* Have code improvements for cleanlab? See the [development guide](DEVELOPMENT.md) and [submit a pull request](CONTRIBUTING.md).\n\n* Have an issue with cleanlab? [Search existing issues](https:\/\/github.com\/cleanlab\/cleanlab\/issues?q=is%3Aissue) or [submit a new issue](https:\/\/github.com\/cleanlab\/cleanlab\/issues\/new).\n\n\n## License\n\nCopyright (c) 2017-2022 Cleanlab Inc.\n\ncleanlab is free software: you can redistribute it and\/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n\ncleanlab is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nSee [GNU Affero General Public LICENSE](https:\/\/github.com\/cleanlab\/cleanlab\/blob\/master\/LICENSE) for details.\n","139":"Discovering ketosis: _how to effectively lose weight_\n=====================================================\n\n### _Here is a chart of my weight vs. time in the past 16 months or so:_\n\n ![weight vs time in the past 16 months or so](weight.2015.png  \"weight loss progress\")\n\n\nThe chart was generated from a data-set [`weight.2015.csv`](weight.2015.csv) by the script [`date-weight.r`](date-weight.r) in this git repository.  It requires [`R`](http:\/\/r-project.org) and [`ggplot2`](http:\/\/ggplot2.org\/).\n\n\nIn the following I'll describe the thought process, some other people ideas, and the code I used to separate signal from noise. This separation was critical to help lead me in the right direction.\n\nThis github repository includes my code, [a Q&A section](QandA.md), and links\nfor further reading.\n\n\n#### Disclaimers:\n\nThe below is what worked for me. Your situation may be different. Listen to your own body. The code here is designed to be used on your own data, not on mine.\n\nAlso: this was *not* a scientific experiment, or a \"study\"; rather, it was a personal journey of experimentation and discovery.\n\nWith these behind us, I'd like to channel [Galileo in the face of the inquisition](https:\/\/en.wikipedia.org\/wiki\/Galileo_affair): evolution has been hard at work for 2 billion years shaping the chemistry of all eukaryotes, multi-cellular life and eventually mammals. The Krebs cycle, glucose metabolism, insulin spikes, glycogen in the liver, carnitine, lipase, are as real for you as they are for me. We may be very different in our genes and traits, some are more insulin resistant, for example, but we cannot be too different in our most fundamental metabolic chemistry. The chemistry which drives fat synthesis and break-up.\n\n\n## Salient facts & initial observations\n\n- I used to be a pretty thin person. My 1st DMV card below, says 143 lb.\n- Unfortunately, since moving to the US, I've been gaining more and more weight. I peaked in 2015, over 50 lbs higher.\n- The US is a country where obesity is an epidemic.\n- Poorer demographics in the US have higher levels of obesity.\n\n![First DMV photo and weight (with full clothing)](1992-ariel-dmv.png \"143 pounds, sometime in the 90's\")\n\n\nDoes a US typical lifestyle has anything to do with this epidemic? After reading on the subject, I could point at a few of the main suspects:\n\n - Fast food is highly available, and is very cheap compared to most alternatives\n - Most food we buy and eat is heavily processed -- watch [Food, Inc. (documentary)](http:\/\/www.takepart.com\/foodinc\/film)\n - \"No Fat\" and \"Low Fat\" labels are everywhere on supermarket shelves\n - Many foods are enriched and sweetened with high-fructose corn-syrup -- watch [Sugar Coated (documentary)](http:\/\/sugarcoateddoc.com\/)\n\nAs in many other instances, I realized I need to think for myself. Ignore all \"expert\" advice. Question widely accepted ideas like the FDA \"food pyramid\". Start listening to my own body, my own logic & data I can collect myself and trust.\n\nOnce I did, the results followed.\n\n## What didn't work\n\nIn the past, I tried several times to change my diet. After reading one of Atkins' books, I realized, checked, and accepted the fact that excess carbs are a major factor in gaining weight. But that realization alone has not led to success.\n\nMy will power, apparently, was insufficient. I had too much love of pizza and bread.  I would reduce my carb consumption, lose a few pounds (typically ~5 pounds), and then break-down, go back to consuming excess carbs, and gain all these pounds back, and then some. My longest diet stretch lasted just a few months.\n\nIt was obvious that something was missing in my method. I just had to find it.  I could increase my physical activity, say start training for a mini-marathon, but that's not something I felt comfortable with.\n\nI realized early on that I need to adopt a lifestyle that not just reduces carbs, or add exercise, but is also sustainable and even enjoyable so it can turn into a painless routine. Something that:\n\n> - I could do for years\n> - Never feel the urge to break habits\n> - Is not hard, or unpleasant for me to do\n\n\n## Early insights & eureka moments\n\nEarly in the process I figured I could use [machine learning](https:\/\/en.wikipedia.org\/wiki\/Machine_learning) to identify the factors that made me gain or lose weight. I used a simple method: every morning I would weigh myself, and record both the new weights and whatever I did in the past ~24 hours, not just the food I ate, but also whether I exercised, slept too little or too much, etc.\n\nThe file I kept was fairly simple. A CSV with 3 columns:\n\n> *Date*, *MorningWeight*, *Yesterday's lifestyle\/food\/actions*\n\nThe last column is a arbitrary-length list of *`word[:weight]`* items.\n\nThe (optional) numerical-weight following `:`, expresses higher\/lower quantities. The default weight, when missing is 1:\n\n    #\n    # -- Comment lines (ignored)\n    #\n    Date,MorningWeight,YesterdayFactors\n    2012-06-10,185.0,\n    2012-06-11,182.6,salad sleep bacon cheese tea halfnhalf icecream\n    2012-06-12,181.0,sleep egg\n    2012-06-13,183.6,mottsfruitsnack:2 pizza:0.5 bread:0.5 date:3 dietsnapple splenda milk nosleep\n    2012-06-14,183.6,coffeecandy:2 egg mayo cheese:2 rice meat bread:0.5 peanut:0.4\n    2012-06-15,183.4,meat sugarlesscandy salad cherry:4 bread:0 dietsnapple:0.5 egg mayo oliveoil\n    2012-06-16,183.6,caprise bread grape:0.2 pasadena sugaryogurt dietsnapple:0.5 peanut:0.4 hotdog\n    2012-06-17,182.6,grape meat pistachio:5 peanut:5 cheese sorbet:5 orangejuice:2\n    # and so on ...\n\n\nThen I wrote [a script](lifestyle-csv2vw) to convert this file to [vowpal-wabbit](https:\/\/github.com\/JohnLangford\/vowpal_wabbit\/wiki) training-set regression format. In the converted train-set the label (target feature) is the change in weight (delta) in the past 24 hours, and the input features are what I've done or ate in the ~24 hours leading to this delta -- a straight copy of the 3rd column.\n\nI was not dieting at that time. Just collecting data.\n\nThe machine learning process error-convergence after partly sorting the lines descending, by `abs(delta)` to smooth it out and try to amplify very weak signals from the data, and 4-passes over the data, looks like this:\n\n![error convergence (after partial descending sort by delta)](vw-convergence.png  \"loss convergence in 4 data passes\")\n\nYou can reproduce my work by compiling your own data-file, installing all prerequisites, and running `make` in this directory.  I wrote a [HOWTO file with more detailed instructions](HOWTO.md). Please open an issue, if anything doesn't work for you.\n\nWhen you type `make` in this directory -- some magic happens.\n\nHere's how a typical result looks like.\n\n    $ make\n\n    ... (output trimmed for brevity) ...\n\n    FeatureName       HashVal   ...   Weight RelScore\n    nosleep            143407   ...  +0.6654 90.29%\n    melon              234655   ...  +0.4636 62.91%\n    sugarlemonade      203375   ...  +0.3975 53.94%\n    trailmix           174671   ...  +0.3362 45.63%\n    bread              135055   ...  +0.3345 45.40%\n    caramelizedwalnut  148079   ...  +0.3316 44.99%\n    bun                  1791   ...  +0.3094 41.98%\n\n    ... (trimmed for brevity. Caveat: data is too noisy anyway) ...\n\n    stayhome           148879   ...  -0.2690 -36.50%\n    bacon               64431   ...  -0.2998 -40.69%\n    egg                197743   ...  -0.3221 -43.70%\n    parmesan             3119   ...  -0.3385 -45.94%\n    oliveoil           156831   ...  -0.3754 -50.95%\n    halfnhalf          171855   ...  -0.4673 -63.41%\n    sleep              127071   ...  -0.7369 -100.00%\n\nThe positive (top) relative-score values are life-style choices that make you ***gain weight***, while the negative ones (bottom) make you ***lose weight***.\n\n\n##### And here's a variable-importance chart made from a similar data-set:\n\n<a href=\"scores.png\" target=\"_blank\"><img src=\"scores.png\" width=\"900\"><\/a>\n\nDisclaimer: please don't read too much into the particulars of this data. Working with this particular data set, was pretty challenging, since:\n\n- The number of original data-points (a bit over 100 days) may be too small to establish enough significance.\n- Typical daily changes in body weight are very small, often ~0.1 lb.\n- My scales are not accurate: you may note that my data has 0.2 pound resolution. This is not ideal. Getting scales with 0.1 pound resolution is highly recommended.\n- You may also note that the loss-convergence chart hits a hard floor at ~0.2 even when you do multiple-passes over the data (overfit the training-set) for a similar reason.\n- Items that make you lose and gain weight, often appear together on the same line so they cancel each other. This throws the automatic learning process off-course.\n- There were some misspellings in the original data (I hope I fixed all of these by now)\n\nSo I focused mostly on the extremes (start and end) of the list as presented above, and just used the hints as general guidance for further study, experimentation, and action.\n\nDespite the noisy & insufficient data, and the inaccuracies in weighting, the machine-learning experiments made 4 facts pretty clear, pretty early:\n\n- Sleeping longer consistently appeared as *the* #1 factor in losing weight.\n- Lack of sleep did the opposite: too little sleep lead to weight gains.\n- Carbs made me gain weight. The worst were high-starch and sugary foods.\n- Fatty and oily foods tended to do the opposite: they were positively correlated with weight-loss.\n\nThe 'stayhome' lifestlye, which fell mostly on weekends, may have been a red-herring: I slept longer when I didn't have to commute to work, OTOH: my diet on stay-home days may have been different.\n\nIt took me a while to figure out the sleep part. *When we sleep we don't eat*. It is that simple.\n\nMoreover: we tend to binge and snack while not particularly hungry, but we never do it during sleep.\n\nOur sleeping time is our longest daily fasting time.\n\nPlease note that my explanations of the effects may not in fact be accurate or deeply scientific.\nThe goal of all this was incremental discovery: experiment, check effect, rinse, repeat.\n\n## Further progress\n\nYou may note that in the top (date vs. weight) chart there's a notable acceleration in the rate of weight-loss.  The cause was deeper insights and better ability to sustain the diet the more I understood the problem.\n\n***Extending the fasting time*** was one major accelerator of weight-loss rate. I did that by:\n\n> - Skipping breakfast and\n> - Stop eating earlier in the evening before going to bed.\n\nThis gave me 14-16 hours of fasting each day. Rather than the more typical 10-12 hours\/day of fasting.\n\nThe 2nd accelerator was ***consuming fatty stuff*** (instead of carbs) in order to feel full.\n\nThe 3rd accelerator was understanding the concepts of [Glycemic index](https:\/\/en.wikipedia.org\/wiki\/Glycemic_index) and [***Glycemic Load***](https:\/\/en.wikipedia.org\/wiki\/Glycemic_load), and shifting whatever I chose to eat towards ***lower Glycemic loads***.\n\nI now believe and hope that I can go all the way back to my original weight when I first landed on US soil.\n\nIf I can keep the present rate, it should take 1-2 years to completely reverse the damage of the past ~20 years.\n\nIt is important to stress that I also *feel much better the more weight I lose*. As a welcome side-effect, the few borderline\/high levels in my blood tests, have moved significantly towards normal averages, during the period I lost weight.\n\n### What was my data and clear improvement in health saying?\n\nLooking at my data, and reading more, convinced me that I should beware of doctors [who push statins](https:\/\/www.google.com\/search?q=the+truth+about+statins) instead of suggesting a better diet. I started doubting anyone who told me I need to *reduce* fat. I run away if anyone now tells me \"high cholesterol\" in the diet is dangerous.\n\nCholesterol, by the way, is an essential building block for many essential body by-products. The liver produces as much cholesterol as we need.\n\nOur body is an amazing machine. Billions of years of evolution have made it extremely *adaptive*.\n\nIt is not our ***high fat consumption***, it is the ***storage of fat process*** that makes us acummulate fat in the tissues and become unhealthy.\n\nAn enzyme called *Lipase* breaks-up fat. Raise the levels of Lipase and our body fat gets consumed faster. To get there, we need to give the body fat as an *alternative* to carbohydrates.  When the body has depleted both the blood sugar, and the glycogen (hydrated sugar) buffer in the liver, it has no other choice but to *adapt and compensate*.  Our source of energy -- [ATP synthesis](https:\/\/en.wikipedia.org\/wiki\/Adenosine_triphosphate) -- switches from carbs to fats by producing more fat-breaking agents.  The body is a \"Flex Fuel\" kind of machine, that has simply replaced one fuel (carbs) with another (fat).\n\nWhen Lipase, and all other agents in the fat-to-ATP chemical path, aka [Beta oxidation](https:\/\/en.wikipedia.org\/wiki\/Beta_oxidation) mobilize, and their levels are elevated, we burn more fat and lose weight over time.\n\nIn a low-carb\/high-fat (LCHF) regime, our night sleep (fasting time) becomes our friend.  The fat-breaking agents keep working while we sleep, breaking-up the stored fat.  This leads to weight-loss, and a healthier state.\n\nAnd when we push even further, and cut carbs to *really* low levels, we may reach a new steady state, called ketosis, in which practically all our energy comes from fat, and that's when we really win big in the weight-loss battle.\n\nThe above is a very simplified, and hopefuly easy to digest, version of what some diet books try to explain in hundreds of pages.\n\n## My bottom-line recipe:\n\n- The hardest part (especially at the beginning) is reducing carbs. The worst are starch rich foods (pizza, pasta, bread etc.), then processed foods with high sugar content (sweet sodas, no-pulp juices, etc). This doesn't mean ***no*** carbs. You may afford yourself carbs from time to time (say a pizza once a week). As it turns out, an occasional lapse isn't enough to completely reverse any steady-state.  However, you need to make sure you consume ***much less carbs*** and ***less frequently*** than before. In particular, you must avoid binging on snacks like chips, pizza, doughnuts, pasta, and bread, or drinking sugar-rich drinks.\n\n- [Look-up Glycemic index](https:\/\/en.wikipedia.org\/wiki\/Glycemic_index) and [Glycemic Load](https:\/\/en.wikipedia.org\/wiki\/Glycemic_load) on wikipedia. ***Avoid foods with high glycemic load***. This prevents the blood sugar spikes which lead to insulin spikes and tell the body chemical cycles to revert back from ketosis, or near ketosis, to fat-accumulation.  Have a sweet tooth? Eat an orange instead of drinking orange juice. The two have vastly different glycemic loads and this makes a huge difference. If you must add sweetness to your cup of tea or coffee, use a [Splenda (sucralose+dextrose) tablet](https:\/\/en.wikipedia.org\/wiki\/Splenda), or [a Stevia drop\/tablet](https:\/\/en.wikipedia.org\/wiki\/Stevia) which typically weight just ~0.1 gram, rather than a tea-spoon of sugar (~4.2g, about 40x more). Result: similar sweetness effect, but much lower Glycemic load and resulting levels of blood-glucose.\n\n- High fat: I switched from milk to half-and-half and am considering heavy (and unsweetened) whipped cream. It has less carbs (lactose) and more fat; plus, it tastes better.  Eat avocados, olive oil, mayo, coconut oil, nuts.  I never worry about *natural* fat, I eat as much fat as I want. This is what makes it much easier to avoid carbs. When I stuff myself with fat I feel much less hungry and miss the carbs less. The body is very good at figuring this out: \"I have too much fat in the blood, so let's increase the amount of enzymes which break-up fat\" and this makes me lose weight in the long run.  Most importantly, I always ***avoid any products labeled \"low-fat\" or \"fat-free\"***. The food industry usually replaces fat with sugar, so it tastes better - otherwise it tastes awful. You'll often hear about \"bad\" vs \"good\" fat. My take: as long as it is natural, it is ok. The worst trans-fat is fat that's artificially hydrogenated, to increase shelf-life, by the food industry. The less saturated fat is, the better. Mono-saturated (plant) liquid oil is the best, then come the poly-unsaturated fats, and finally near saturated (but not fully saturated) fats that come from animals. My buttery-spread spectrum is:  *Margarine: no; Butter: ok; Earth Balance: no problem*. At any rate, even the most saturated fat, gets broken and depleted by the natural processes in the body.\n\n- A bit of exercise.  Of course, more is better, but for many this may prove difficult. I don't excercise too much. I just bike to work and back about 20 min each way, meaning 40 min\/day, 5 out of 7 days\/week. You can try walking the dog (but walk faster), or Zumba dance to music. The trick is to find something that you don't find hard to do. Or find company to do it together. Then, do a little bit of it every day.\n\n- ***Longer fasting periods:*** This is the #1 contributor to weight-loss. sleep longer, stop eating as early as possible before going to sleep and start eating as late as possible after sleeping. *Skip breakfast*, after some time you won't feel hungry in the morning anymore.  After long periods of fasting, the body chemistry adjusts. It needs ATP, but there's a too low level of glucose in the blood. The glycogen in the liver is fully consumed (this takes about 1-2 days of low or no carbs) so there's no other option, but to start looking for other sources, like stored fat. This elevates the enzymes that help with breaking up fat and the Krebs cycle reverses direction in the critical paths. Instead of transforming excess-carbs into stored fat, we break-up stored fat for energy.\n\n- Eat eggs.  They are a wonderful combo of fat and protein with no carbs at all.  I read an interview with a [Japanese woman who reached 114 years](Longevity.md) and one of her secrets was to eat eggs daily.  My favorite food is a scrambled egg with grilled onions (onions are a bit high on carbs, but too tasty to give up) and olives.\n\n- Eat slower, and chew longer... don't swallow just yet! Humans, just like dogs, tend to swallow too soon. Stop eating when you feel full. There's about 20 min delay before your brain registers that you are full so don't over-eat.\n\n***\n\n## Further reading:\n\n- [The Krebs (aka Citric acid) cycle](https:\/\/en.wikipedia.org\/wiki\/Citric_acid_cycle)\n- [Spikes of Insulin and their effects](https:\/\/en.wikipedia.org\/wiki\/Sugar_crash) -- what the body does when it has excess of sugar vs excess of fat.\n- [Glycemic Index](https:\/\/en.wikipedia.org\/wiki\/Glycemic_index)\n- [Glycemic Load](https:\/\/en.wikipedia.org\/wiki\/Glycemic_load) -- a better metric for weight-loss than Glycemic Index.\n- [Glycogen and its storage in the liver](https:\/\/en.wikipedia.org\/wiki\/Glycogen)\n- [Ketone bodies](https:\/\/en.wikipedia.org\/wiki\/Ketone_bodies)\n- [Ketosis -- not to be confused with keto-acidosis](https:\/\/en.wikipedia.org\/wiki\/Ketosis)\n- [Ketogenic diet](https:\/\/en.wikipedia.org\/wiki\/Ketogenic_diet)\n\n\n<!--\n- [The Eating Academy \/ Peter Attia, M.D.](http:\/\/eatingacademy.com\/)\n-->\n\n- [Why We Get Fat: And What to Do About It \/ Gary Taubes](http:\/\/www.amazon.com\/gp\/product\/0307272702)\n- [Summary of Good Calories, Bad Calories \/ Gary Taub by Lower Thought](https:\/\/lowerthought.wordpress.com\/complete-notes-to-good-calories-bad-calories\/)\n- [The Obesity Code: Unlocking the Secrets of Weight Loss \/ Jason Fung](https:\/\/www.amazon.com\/Obesity-Code-Unlocking-Secrets-Weight-ebook\/dp\/B01C6D0LCK\/)\n- [The best summary about statins I've seen](http:\/\/www.newswithviews.com\/Howenstine\/james23.htm)\n- [High cholesterol doesn't cause heart disease](http:\/\/www.telegraph.co.uk\/science\/2016\/06\/12\/high-cholesterol-does-not-cause-heart-disease-new-research-finds\/)\n- [Dr. Mark Hyman take on a good diet (a bit different than mine)](http:\/\/drhyman.com\/blog\/2014\/08\/18\/one-test-doctor-isnt-save-life\/)\n\n#### Documentaries:\n\n- [Food, Inc. (2008)](https:\/\/www.netflix.com\/title\/70108783)\n-  [Sugar Coated (2015)](https:\/\/www.netflix.com\/title\/80100595)\n\n#### More videos\n\n- [Reversing Type 2 diabetes starts with ignoring the guidelines | Sarah Hallberg | TEDxPurdueU](https:\/\/www.youtube.com\/watch?v=da1vvigy5tQ)\n\nA nice 7:41 minute video of James McCarter in Quantified Self (an eye opener for me):\n\n- [James McCarter: The Effects of a Year in Ketosis](https:\/\/vimeo.com\/147795263)\n\n#### Questions, Answers, Comments\n\n[Some questions and comments I got and tried to answer](QandA.md)\n\n<!--\n#### More friendly interface\n\n[Shyal Beardsley](http:\/\/shyal.com) has built a starter front-end for this: ***[weightbrains.com](http:\/\/weightbrains.com)***\n(Note and fair warning: this is a prototype, experimental, work in progress)\n-->\n\n## Acknowledgements\n\nBig thanks to the following people for contributing to this project in myriad ways,\ncomments, references, corrections, etc.\n\n_Anat Faigon, Ingrid Kane, Hans Lee, Steve Malmskog, Eyal Friedman, Shiri Shoham, Gabi Harel, Shingi, Noa_\n\n_Update: 2016-08-12: this project made [Hacker News](https:\/\/news.ycombinator.com\/item?id=12279415) and reached the top place for a while. Thanks for some great comments by benkuhn, aab0, zzleeper, and others which helped me make it better._\n![image of this project on Hacker News 2016-08-12](hackernews-2016-08-12.png)\n\nSpecial thanks to John Langford and the many other contributors to [vowpal wabbit](https:\/\/en.wikipedia.org\/wiki\/Vowpal_Wabbit).\n\n\n#### License:\n\nThis code and additional material are released under a permissive and simple [2-clause BSD licence](Licence.md).  The one sentence summary of this is \"as long as you don't sue me and not claim it as your own, you should be ok.\"\n\n","140":"<!--Do not modify this file. It is auto-generated from a template (infra\/templates\/README.md.jinja2)-->\n\n<p align=\"center\">\n    <a href=\"https:\/\/feast.dev\/\">\n      <img src=\"docs\/assets\/feast_logo.png\" width=\"550\">\n    <\/a>\n<\/p>\n<br \/>\n\n[![unit-tests](https:\/\/github.com\/feast-dev\/feast\/actions\/workflows\/unit_tests.yml\/badge.svg?branch=master&event=push)](https:\/\/github.com\/feast-dev\/feast\/actions\/workflows\/unit_tests.yml)\n[![integration-tests-and-build](https:\/\/github.com\/feast-dev\/feast\/actions\/workflows\/master_only.yml\/badge.svg?branch=master&event=push)](https:\/\/github.com\/feast-dev\/feast\/actions\/workflows\/master_only.yml)\n[![java-integration-tests](https:\/\/github.com\/feast-dev\/feast\/actions\/workflows\/java_master_only.yml\/badge.svg?branch=master&event=push)](https:\/\/github.com\/feast-dev\/feast\/actions\/workflows\/java_master_only.yml)\n[![linter](https:\/\/github.com\/feast-dev\/feast\/actions\/workflows\/linter.yml\/badge.svg?branch=master&event=push)](https:\/\/github.com\/feast-dev\/feast\/actions\/workflows\/linter.yml)\n[![Docs Latest](https:\/\/img.shields.io\/badge\/docs-latest-blue.svg)](https:\/\/docs.feast.dev\/)\n[![Python API](https:\/\/img.shields.io\/readthedocs\/feast\/master?label=Python%20API)](http:\/\/rtd.feast.dev\/)\n[![License](https:\/\/img.shields.io\/badge\/License-Apache%202.0-blue)](https:\/\/github.com\/feast-dev\/feast\/blob\/master\/LICENSE)\n[![GitHub Release](https:\/\/img.shields.io\/github\/v\/release\/feast-dev\/feast.svg?style=flat&sort=semver&color=blue)](https:\/\/github.com\/feast-dev\/feast\/releases)\n\n## Overview\n\nFeast is an open source feature store for machine learning. Feast is the fastest path to productionizing analytic data for model training and online inference.\n\nPlease see our [documentation](https:\/\/docs.feast.dev\/) for more information about the project.\n\n## \ud83d\udcd0 Architecture\n![](docs\/assets\/feast-marchitecture.png)\n\nThe above architecture is the minimal Feast deployment. Want to run the full Feast on Snowflake\/GCP\/AWS? Click [here](https:\/\/docs.feast.dev\/how-to-guides\/feast-snowflake-gcp-aws).\n\n## \ud83d\udc23 Getting Started\n\n### 1. Install Feast\n```commandline\npip install feast\n```\n\n### 2. Create a feature repository\n```commandline\nfeast init my_feature_repo\ncd my_feature_repo\n```\n\n### 3. Register your feature definitions and set up your feature store\n```commandline\nfeast apply\n```\n\n### 4. Explore your data in the web UI (experimental)\n\n![Web UI](ui\/sample.png)\n```commandline\nfeast ui\n```\n\n### 5. Build a training dataset\n```python\nfrom feast import FeatureStore\nimport pandas as pd\nfrom datetime import datetime\n\nentity_df = pd.DataFrame.from_dict({\n    \"driver_id\": [1001, 1002, 1003, 1004],\n    \"event_timestamp\": [\n        datetime(2021, 4, 12, 10, 59, 42),\n        datetime(2021, 4, 12, 8,  12, 10),\n        datetime(2021, 4, 12, 16, 40, 26),\n        datetime(2021, 4, 12, 15, 1 , 12)\n    ]\n})\n\nstore = FeatureStore(repo_path=\".\")\n\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    features = [\n        'driver_hourly_stats:conv_rate',\n        'driver_hourly_stats:acc_rate',\n        'driver_hourly_stats:avg_daily_trips'\n    ],\n).to_df()\n\nprint(training_df.head())\n\n# Train model\n# model = ml.fit(training_df)\n```\n```commandline\n            event_timestamp  driver_id  conv_rate  acc_rate  avg_daily_trips\n0 2021-04-12 08:12:10+00:00       1002   0.713465  0.597095              531\n1 2021-04-12 10:59:42+00:00       1001   0.072752  0.044344               11\n2 2021-04-12 15:01:12+00:00       1004   0.658182  0.079150              220\n3 2021-04-12 16:40:26+00:00       1003   0.162092  0.309035              959\n\n```\n\n### 6. Load feature values into your online store\n```commandline\nCURRENT_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%S\")\nfeast materialize-incremental $CURRENT_TIME\n```\n\n```commandline\nMaterializing feature view driver_hourly_stats from 2021-04-14 to 2021-04-15 done!\n```\n\n### 7. Read online features at low latency\n```python\nfrom pprint import pprint\nfrom feast import FeatureStore\n\nstore = FeatureStore(repo_path=\".\")\n\nfeature_vector = store.get_online_features(\n    features=[\n        'driver_hourly_stats:conv_rate',\n        'driver_hourly_stats:acc_rate',\n        'driver_hourly_stats:avg_daily_trips'\n    ],\n    entity_rows=[{\"driver_id\": 1001}]\n).to_dict()\n\npprint(feature_vector)\n\n# Make prediction\n# model.predict(feature_vector)\n```\n```json\n{\n    \"driver_id\": [1001],\n    \"driver_hourly_stats__conv_rate\": [0.49274],\n    \"driver_hourly_stats__acc_rate\": [0.92743],\n    \"driver_hourly_stats__avg_daily_trips\": [72]\n}\n```\n\n## \ud83d\udce6 Functionality and Roadmap\n\nThe list below contains the functionality that contributors are planning to develop for Feast\n\n* Items below that are in development (or planned for development) will be indicated in parentheses.\n* We welcome contribution to all items in the roadmap!\n* Want to influence our roadmap and prioritization? Submit your feedback to [this form](https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSfa1nRQ0sKz-JEFnMMCi4Jseag\\_yDssO\\_3nV9qMfxfrkil-wA\/viewform).\n* Want to speak to a Feast contributor? We are more than happy to jump on a call. Please schedule a time using [Calendly](https:\/\/calendly.com\/d\/x2ry-g5bb\/meet-with-feast-team).\n\n* **Data Sources**\n  * [x] [Snowflake source](https:\/\/docs.feast.dev\/reference\/data-sources\/snowflake)\n  * [x] [Redshift source](https:\/\/docs.feast.dev\/reference\/data-sources\/redshift)\n  * [x] [BigQuery source](https:\/\/docs.feast.dev\/reference\/data-sources\/bigquery)\n  * [x] [Parquet file source](https:\/\/docs.feast.dev\/reference\/data-sources\/file)\n  * [x] [Synapse source (community plugin)](https:\/\/github.com\/Azure\/feast-azure)\n  * [x] [Hive (community plugin)](https:\/\/github.com\/baineng\/feast-hive)\n  * [x] [Postgres (contrib plugin)](https:\/\/docs.feast.dev\/reference\/data-sources\/postgres)\n  * [x] [Spark (contrib plugin)](https:\/\/docs.feast.dev\/reference\/data-sources\/spark)\n  * [x] Kafka \/ Kinesis sources (via [push support into the online store](https:\/\/docs.feast.dev\/reference\/data-sources\/push))\n  * [ ] HTTP source\n* **Offline Stores**\n  * [x] [Snowflake](https:\/\/docs.feast.dev\/reference\/offline-stores\/snowflake)\n  * [x] [Redshift](https:\/\/docs.feast.dev\/reference\/offline-stores\/redshift)\n  * [x] [BigQuery](https:\/\/docs.feast.dev\/reference\/offline-stores\/bigquery)\n  * [x] [Synapse (community plugin)](https:\/\/github.com\/Azure\/feast-azure)\n  * [x] [Hive (community plugin)](https:\/\/github.com\/baineng\/feast-hive)\n  * [x] [Postgres (contrib plugin)](https:\/\/docs.feast.dev\/reference\/offline-stores\/postgres)\n  * [x] [Trino (contrib plugin)](https:\/\/github.com\/Shopify\/feast-trino)\n  * [x] [Spark (contrib plugin)](https:\/\/docs.feast.dev\/reference\/offline-stores\/spark)\n  * [x] [In-memory \/ Pandas](https:\/\/docs.feast.dev\/reference\/offline-stores\/file)\n  * [x] [Custom offline store support](https:\/\/docs.feast.dev\/how-to-guides\/adding-a-new-offline-store)\n* **Online Stores**\n  * [x] [DynamoDB](https:\/\/docs.feast.dev\/reference\/online-stores\/dynamodb)\n  * [x] [Redis](https:\/\/docs.feast.dev\/reference\/online-stores\/redis)\n  * [x] [Datastore](https:\/\/docs.feast.dev\/reference\/online-stores\/datastore)\n  * [x] [SQLite](https:\/\/docs.feast.dev\/reference\/online-stores\/sqlite)\n  * [x] [Azure Cache for Redis (community plugin)](https:\/\/github.com\/Azure\/feast-azure)\n  * [x] [Postgres (contrib plugin)](https:\/\/docs.feast.dev\/reference\/online-stores\/postgres)\n  * [x] [Custom online store support](https:\/\/docs.feast.dev\/how-to-guides\/adding-support-for-a-new-online-store)\n  * [ ] Bigtable (in progress)\n  * [ ] Cassandra\n* **Streaming**\n  * [x] [Custom streaming ingestion job support](https:\/\/docs.feast.dev\/how-to-guides\/creating-a-custom-provider)\n  * [x] [Push based streaming data ingestion](https:\/\/docs.feast.dev\/reference\/data-sources\/push.md)\n  * [ ] Streaming ingestion on AWS\n  * [ ] Streaming ingestion on GCP\n* **Feature Engineering**\n  * [x] On-demand Transformations (Alpha release. See [RFC](https:\/\/docs.google.com\/document\/d\/1lgfIw0Drc65LpaxbUu49RCeJgMew547meSJttnUqz7c\/edit#))\n  * [ ] Batch transformation (In progress. See [RFC](https:\/\/docs.google.com\/document\/d\/1964OkzuBljifDvkV-0fakp2uaijnVzdwWNGdz7Vz50A\/edit))\n  * [ ] Streaming transformation\n* **Deployments**\n  * [x] AWS Lambda (Alpha release. See [RFC](https:\/\/docs.google.com\/document\/d\/1eZWKWzfBif66LDN32IajpaG-j82LSHCCOzY6R7Ax7MI\/edit))\n  * [x] Kubernetes (See [guide](https:\/\/docs.feast.dev\/how-to-guides\/running-feast-in-production#4.3.-java-based-feature-server-deployed-on-kubernetes))\n  * [ ] Cloud Run\n  * [ ] KNative\n* **Feature Serving**\n  * [x] Python Client\n  * [x] REST Feature Server (Python) (Alpha release. See [RFC](https:\/\/docs.google.com\/document\/d\/1iXvFhAsJ5jgAhPOpTdB3j-Wj1S9x3Ev\\_Wr6ZpnLzER4\/edit))\n  * [x] gRPC Feature Server (Java) (See [#1497](https:\/\/github.com\/feast-dev\/feast\/issues\/1497))\n  * [x] Push API\n  * [ ] Java Client\n  * [ ] Go Client\n  * [ ] Delete API\n  * [ ] Feature Logging (for training)\n* **Data Quality Management (See [RFC](https:\/\/docs.google.com\/document\/d\/110F72d4NTv80p35wDSONxhhPBqWRwbZXG4f9mNEMd98\/edit))**\n  * [x] Data profiling and validation (Great Expectations)\n  * [ ] Training-serving skew detection (in progress)\n  * [ ] Metric production\n  * [ ] Drift detection\n* **Feature Discovery and Governance**\n  * [x] Python SDK for browsing feature registry\n  * [x] CLI for browsing feature registry\n  * [x] Model-centric feature tracking (feature services)\n  * [x] Amundsen integration (see [Feast extractor](https:\/\/github.com\/amundsen-io\/amundsen\/blob\/main\/databuilder\/databuilder\/extractor\/feast_extractor.py))\n  * [x] Feast Web UI (Alpha release. See [documentation](https:\/\/docs.feast.dev\/reference\/alpha-web-ui.md))\n  * [ ] REST API for browsing feature registry\n  * [ ] Feature versioning\n\n\n## \ud83c\udf93 Important Resources\n\nPlease refer to the official documentation at [Documentation](https:\/\/docs.feast.dev\/)\n * [Quickstart](https:\/\/docs.feast.dev\/getting-started\/quickstart)\n * [Tutorials](https:\/\/docs.feast.dev\/tutorials\/tutorials-overview)\n * [Running Feast with Snowflake\/GCP\/AWS](https:\/\/docs.feast.dev\/how-to-guides\/feast-snowflake-gcp-aws)\n * [Change Log](https:\/\/github.com\/feast-dev\/feast\/blob\/master\/CHANGELOG.md)\n * [Slack (#Feast)](https:\/\/slack.feast.dev\/)\n\n## \ud83d\udc4b Contributing\nFeast is a community project and is still under active development. Please have a look at our contributing and development guides if you want to contribute to the project:\n- [Contribution Process for Feast](https:\/\/docs.feast.dev\/project\/contributing)\n- [Development Guide for Feast](https:\/\/docs.feast.dev\/project\/development-guide)\n- [Development Guide for the Main Feast Repository](.\/CONTRIBUTING.md)\n\n## \u2728 Contributors\n\nThanks goes to these incredible people:\n\n<a href=\"https:\/\/github.com\/feast-dev\/feast\/graphs\/contributors\">\n  <img src=\"https:\/\/contrib.rocks\/image?repo=feast-dev\/feast\" \/>\n<\/a>","141":"# \u673a\u5668\u5b66\u4e60100\u5929\n\n\u82f1\u6587\u539f\u7248\u8bf7\u79fb\u6b65[Avik-Jain](https:\/\/github.com\/Avik-Jain\/100-Days-Of-ML-Code)\u3002\n\n[\u4e2d\u6587\u6700\u65b0\u7248](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code)\u3002\u5e38\u89c1\u95ee\u9898\u89e3\u7b54\u89c1[FAQ](https:\/\/github.com\/MLEveryday\/100-Days-Of-ML-Code\/blob\/master\/FAQ.MD)\u3002\n\n# \u76ee\u5f55\n- \u6709\u76d1\u7763\u5b66\u4e60\n  - [\u6570\u636e\u9884\u5904\u7406](#\u6570\u636e\u9884\u5904\u7406--\u7b2c1\u5929)\n  - [\u7b80\u5355\u7ebf\u6027\u56de\u5f52](#\u7b80\u5355\u7ebf\u6027\u56de\u5f52--\u7b2c2\u5929)\n  - [\u591a\u5143\u7ebf\u6027\u56de\u5f52](#\u591a\u5143\u7ebf\u6027\u56de\u5f52--\u7b2c3\u5929)\n  - [\u903b\u8f91\u56de\u5f52](#\u903b\u8f91\u56de\u5f52--\u7b2c4\u5929)\n  - [k\u8fd1\u90bb\u6cd5(k-NN)](#k\u8fd1\u90bb\u6cd5k-nn--\u7b2c7\u5929)\n  - [\u652f\u6301\u5411\u91cf\u673a(SVM)](#\u652f\u6301\u5411\u91cf\u673asvm--\u7b2c12\u5929)\n  - [\u51b3\u7b56\u6811](#\u51b3\u7b56\u6811--\u7b2c23\u5929)\n  - [\u968f\u673a\u68ee\u6797](#\u968f\u673a\u68ee\u6797--\u7b2c33\u5929)\n- \u65e0\u76d1\u7763\u5b66\u4e60\n  - [K-\u5747\u503c\u805a\u7c7b](#k-\u5747\u503c\u805a\u7c7b--\u7b2c43\u5929)\n  - [\u5c42\u6b21\u805a\u7c7b](#\u5c42\u6b21\u805a\u7c7b--\u7b2c54\u5929)\n\n## \u6570\u636e\u9884\u5904\u7406 | \u7b2c1\u5929\n[\u6570\u636e\u9884\u5904\u7406\u5b9e\u73b0](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%201_Data_Preprocessing.md)\n\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%201.jpg\">\n<\/p>\n\n## \u7b80\u5355\u7ebf\u6027\u56de\u5f52 | \u7b2c2\u5929\n[\u7b80\u5355\u7ebf\u6027\u56de\u5f52\u5b9e\u73b0](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%202_Simple_Linear_Regression.md)\n\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%202.jpg\">\n<\/p>\n\n## \u591a\u5143\u7ebf\u6027\u56de\u5f52 | \u7b2c3\u5929\n[\u591a\u5143\u7ebf\u6027\u56de\u5f52\u5b9e\u73b0](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%203_Multiple_Linear_Regression.md)\n\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%203.png\">\n<\/p>\n\n## \u903b\u8f91\u56de\u5f52 | \u7b2c4\u5929\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%204.jpg\">\n<\/p>\n\n## \u903b\u8f91\u56de\u5f52 | \u7b2c5\u5929\n\u4eca\u5929\u6211\u6df1\u5165\u7814\u7a76\u4e86\u903b\u8f91\u56de\u5f52\u5230\u5e95\u662f\u4ec0\u4e48\uff0c\u4ee5\u53ca\u5b83\u80cc\u540e\u7684\u6570\u5b66\u662f\u4ec0\u4e48\u3002\u5b66\u4e60\u4e86\u5982\u4f55\u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570\uff0c\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u6765\u5c06\u4ee3\u4ef7\u51fd\u6570\u964d\u4f4e\u5230\u6700\u5c0f\u3002<br>\n\u7531\u4e8e\u65f6\u95f4\u5173\u7cfb\uff0c\u6211\u5c06\u9694\u5929\u53d1\u5e03\u4fe1\u606f\u56fe\u3002\u5982\u679c\u6709\u4eba\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u6709\u4e00\u5b9a\u7ecf\u9a8c\uff0c\u5e76\u613f\u610f\u5e2e\u6211\u7f16\u5199\u4ee3\u7801\u6587\u6863\uff0c\u4e5f\u4e86\u89e3github\u7684Markdown\u8bed\u6cd5\uff0c\u8bf7\u5728\u9886\u82f1\u8054\u7cfb\u6211\u3002\n\n## \u903b\u8f91\u56de\u5f52 | \u7b2c6\u5929\n[\u903b\u8f91\u56de\u5f52\u5b9e\u73b0](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%206_Logistic_Regression.md)\n\n## K\u8fd1\u90bb\u6cd5(k-NN) | \u7b2c7\u5929\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%207.jpg\">\n<\/p>\n\n## \u903b\u8f91\u56de\u5f52\u80cc\u540e\u7684\u6570\u5b66 | \u7b2c8\u5929\n\u4e3a\u4e86\u4f7f\u6211\u5bf9\u903b\u8f91\u56de\u5f52\u7684\u89c1\u89e3\u66f4\u52a0\u6e05\u6670\uff0c\u6211\u5728\u7f51\u4e0a\u641c\u7d22\u4e86\u4e00\u4e9b\u8d44\u6e90\u6216\u6587\u7ae0\uff0c\u7136\u540e\u6211\u5c31\u53d1\u73b0\u4e86Saishruthi Swaminathan\u7684<a href = \"https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc\">\u8fd9\u7bc7\u6587\u7ae0<\/a><br>\n\n\u5b83\u7ed9\u51fa\u4e86\u903b\u8f91\u56de\u5f52\u7684\u8be6\u7ec6\u63cf\u8ff0\u3002\u8bf7\u52a1\u5fc5\u770b\u4e00\u770b\u3002\n\n## \u652f\u6301\u5411\u91cf\u673a(SVM) | \u7b2c9\u5929\n\u76f4\u89c2\u4e86\u89e3SVM\u662f\u4ec0\u4e48\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u5b83\u6765\u89e3\u51b3\u5206\u7c7b\u95ee\u9898\u3002\n\n## \u652f\u6301\u5411\u91cf\u673a\u548cK\u8fd1\u90bb\u6cd5 | \u7b2c10\u5929\n\u4e86\u89e3\u66f4\u591a\u5173\u4e8eSVM\u5982\u4f55\u5de5\u4f5c\u548c\u5b9e\u73b0knn\u7b97\u6cd5\u7684\u77e5\u8bc6\u3002\n\n## K\u8fd1\u90bb\u6cd5(k-NN) | \u7b2c11\u5929\n[K\u8fd1\u90bb\u6cd5(k-NN)\u5b9e\u73b0](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2011_K-NN.md)\n\n## \u652f\u6301\u5411\u91cf\u673a(SVM) | \u7b2c12\u5929\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%2012.jpg\">\n<\/p>\n\n## \u652f\u6301\u5411\u91cf\u673a(SVM) | \u7b2c13\u5929\n[SVM\u5b9e\u73b0](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2013_SVM.md)\n\n## \u652f\u6301\u5411\u91cf\u673a(SVM)\u7684\u5b9e\u73b0 | \u7b2c14\u5929\n\u4eca\u5929\u6211\u5728\u7ebf\u6027\u76f8\u5173\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86SVM\u3002\u4f7f\u7528Scikit-Learn\u5e93\u3002\u5728scikit-learn\u4e2d\u6211\u4eec\u6709SVC\u5206\u7c7b\u5668\uff0c\u6211\u4eec\u7528\u5b83\u6765\u5b8c\u6210\u8fd9\u4e2a\u4efb\u52a1\u3002\u5c06\u5728\u4e0b\u4e00\u6b21\u5b9e\u73b0\u65f6\u4f7f\u7528kernel-trick\u3002Python\u4ee3\u7801\u89c1[\u6b64\u5904](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2013_SVM.py),Jupyter notebook\u89c1[\u6b64\u5904](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2013_SVM.ipynb)\u3002\n\n## \u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668(Naive Bayes Classifier)\u548c\u9ed1\u76d2\u673a\u5668\u5b66\u4e60(Black Box Machine Learning) | \u7b2c15\u5929\n\u5b66\u4e60\u4e0d\u540c\u7c7b\u578b\u7684\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u540c\u65f6\u5f00\u59cb<a href=\"https:\/\/bloomberg.github.io\/foml\/#home\">Bloomberg<\/a>\u7684\u8bfe\u7a0b\u3002\u8bfe\u7a0b\u5217\u8868\u4e2d\u7684\u7b2c\u4e00\u4e2a\u662f\u9ed1\u6cb3\u673a\u5668\u5b66\u4e60\u3002\u5b83\u7ed9\u51fa\u4e86\u9884\u6d4b\u51fd\u6570\uff0c\u7279\u5f81\u63d0\u53d6\uff0c\u5b66\u4e60\u7b97\u6cd5\uff0c\u6027\u80fd\u8bc4\u4f30\uff0c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u6837\u672c\u504f\u5dee\uff0c\u975e\u5e73\u7a33\u6027\uff0c\u8fc7\u5ea6\u62df\u5408\u548c\u8d85\u53c2\u6570\u8c03\u6574\u7684\u6574\u4f53\u89c2\u70b9\u3002\n\n## \u901a\u8fc7\u5185\u6838\u6280\u5de7\u5b9e\u73b0\u652f\u6301\u5411\u91cf\u673a | \u7b2c16\u5929\n\u4f7f\u7528Scikit-Learn\u5e93\u5b9e\u73b0\u4e86SVM\u7b97\u6cd5\u4ee5\u53ca\u5185\u6838\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5c06\u6211\u4eec\u7684\u6570\u636e\u70b9\u6620\u5c04\u5230\u66f4\u9ad8\u7ef4\u5ea6\u4ee5\u627e\u5230\u6700\u4f73\u8d85\u5e73\u9762\u3002\n\n## \u5728Coursera\u5f00\u59cb\u6df1\u5ea6\u5b66\u4e60\u7684\u4e13\u4e1a\u8bfe\u7a0b | \u7b2c17\u5929\n\u57281\u5929\u5185\u5b8c\u6210\u7b2c1\u5468\u548c\u7b2c2\u5468\u5185\u5bb9\u4ee5\u53ca\u5b66\u4e60\u8bfe\u7a0b\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u795e\u7ecf\u7f51\u7edc\u3002\n\n## \u7ee7\u7eedCoursera\u4e0a\u7684\u6df1\u5ea6\u5b66\u4e60\u4e13\u4e1a\u8bfe\u7a0b | \u7b2c18\u5929\n\u5b8c\u6210\u8bfe\u7a0b1\u3002\u7528Python\u81ea\u5df1\u5b9e\u73b0\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u3002\n\n## \u5b66\u4e60\u95ee\u9898\u548cYaser Abu-Mostafa\u6559\u6388 | \u7b2c19\u5929\n\u5f00\u59cbYaser Abu-Mostafa\u6559\u6388\u7684Caltech\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b-CS156\u4e2d\u7684\u8bfe\u7a0b1\u3002\u8fd9\u57fa\u672c\u4e0a\u662f\u5bf9\u5373\u5c06\u5230\u6765\u7684\u8bfe\u7a0b\u7684\u4e00\u79cd\u4ecb\u7ecd\u3002\u4ed6\u4e5f\u4ecb\u7ecd\u4e86\u611f\u77e5\u7b97\u6cd5\u3002\n\n## \u6df1\u5ea6\u5b66\u4e60\u4e13\u4e1a\u8bfe\u7a0b2 | \u7b2c20\u5929\n\u5b8c\u6210\u6539\u8fdb\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b2c1\u5468\u5185\u5bb9\uff1a\u53c2\u6570\u8c03\u6574\uff0c\u6b63\u5219\u5316\u548c\u4f18\u5316\u3002\n\n## \u7f51\u9875\u641c\u7f57 | \u7b2c21\u5929\n\u89c2\u770b\u4e86\u4e00\u4e9b\u5173\u4e8e\u5982\u4f55\u4f7f\u7528Beautiful Soup\u8fdb\u884c\u7f51\u7edc\u722c\u866b\u7684\u6559\u7a0b\uff0c\u4ee5\u4fbf\u6536\u96c6\u7528\u4e8e\u6784\u5efa\u6a21\u578b\u7684\u6570\u636e\u3002\n\n## \u5b66\u4e60\u8fd8\u53ef\u884c\u5417? | \u7b2c22\u5929\n\u5b8c\u6210Yaser Abu-Mostafa\u6559\u6388\u7684Caltech\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b-CS156\u4e2d\u7684\u8bfe\u7a0b2\u3002\u5b66\u4e60Hoeffding\u4e0d\u7b49\u5f0f\u3002\n\n## \u51b3\u7b56\u6811 | \u7b2c23\u5929\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%2023%20-%20Chinese.jpg\">\n<\/p>\n\n## \u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u7684\u4ecb\u7ecd | \u7b2c24\u5929\nBloomberg ML\u8bfe\u7a0b\u7684\u7b2c3\u8bfe\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6838\u5fc3\u6982\u5ff5\uff0c\u5982\u8f93\u5165\u7a7a\u95f4\uff0c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7ed3\u679c\u7a7a\u95f4\uff0c\u9884\u6d4b\u51fd\u6570\uff0c\u635f\u5931\u51fd\u6570\u548c\u5047\u8bbe\u7a7a\u95f4\u3002\n\n## \u51b3\u7b56\u6811 | \u7b2c25\u5929\n[\u51b3\u7b56\u6811\u5b9e\u73b0](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2025_Decision_Tree.md)\n\n## \u8df3\u5230\u590d\u4e60\u7ebf\u6027\u4ee3\u6570 | \u7b2c26\u5929\n\u53d1\u73b0YouTube\u4e00\u4e2a\u795e\u5947\u7684\u9891\u9053[3Blue1Brown](https:\/\/www.youtube.com\/channel\/UCYO_jab_esuFRV4b17AJtAw)\uff0c\u5b83\u6709\u4e00\u4e2a\u64ad\u653e\u5217\u8868\u300a\u7ebf\u6027\u4ee3\u6570\u7684\u672c\u8d28\u300b\u3002\u770b\u5b8c\u4e864\u4e2a\u89c6\u9891\uff0c\u5305\u62ec\u4e86\u5411\u91cf\uff0c\u7ebf\u6027\u7ec4\u5408\uff0c\u8de8\u5ea6\uff0c\u57fa\u5411\u91cf\uff0c\u7ebf\u6027\u53d8\u6362\u548c\u77e9\u9635\u4e58\u6cd5\u3002\n\nB\u7ad9\u64ad\u653e\u5217\u8868\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=9450)\u3002\n\n## \u8df3\u5230\u590d\u4e60\u7ebf\u6027\u4ee3\u6570 | \u7b2c27\u5929\n\u7ee7\u7eed\u89c2\u770b\u4e864\u4e2a\u89c6\u9891\uff0c\u5185\u5bb9\u5305\u62ec\u4e09\u7ef4\u53d8\u6362\u3001\u884c\u5217\u5f0f\u3001\u9006\u77e9\u9635\u3001\u5217\u7a7a\u95f4\u3001\u96f6\u7a7a\u95f4\u548c\u975e\u65b9\u77e9\u9635\u3002\n\nB\u7ad9\u64ad\u653e\u5217\u8868\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=9450)\u3002\n\n## \u8df3\u5230\u590d\u4e60\u7ebf\u6027\u4ee3\u6570 | \u7b2c28\u5929\n\u7ee7\u7eed\u89c2\u770b\u4e863\u4e2a\u89c6\u9891\uff0c\u5185\u5bb9\u5305\u62ec\u70b9\u79ef\u548c\u53c9\u79ef\u3002\n\nB\u7ad9\u64ad\u653e\u5217\u8868\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=9450)\u3002\n\n## \u8df3\u5230\u590d\u4e60\u7ebf\u6027\u4ee3\u6570 | \u7b2c29\u5929\n\u89c2\u770b\u4e86\u5269\u4f59\u7684\u89c6\u989112\u523014\uff0c\u5185\u5bb9\u5305\u62ec\u7279\u5f81\u5411\u91cf\u548c\u7279\u5f81\u503c\uff0c\u4ee5\u53ca\u62bd\u8c61\u5411\u91cf\u7a7a\u95f4\u3002\n\nB\u7ad9\u64ad\u653e\u5217\u8868\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=9450)\u3002\n\n## \u5fae\u79ef\u5206\u7684\u672c\u8d28 | \u7b2c30\u5929\n\u5b8c\u6210\u4e0a\u4e00\u64ad\u653e\u5217\u8868\u540e\uff0cYouTube\u63a8\u8350\u4e86\u65b0\u5185\u5bb9\u300a\u5fae\u79ef\u5206\u7684\u672c\u8d28\u300b\uff0c\u4eca\u5929\u770b\u5b8c\u4e86\u5176\u4e2d\u76843\u4e2a\u89c6\u9891\uff0c\u5305\u62ec\u5bfc\u6570\u3001\u94fe\u5f0f\u6cd5\u5219\u3001\u4e58\u79ef\u6cd5\u5219\u548c\u6307\u6570\u5bfc\u6570\u3002\n\nB\u7ad9\u64ad\u653e\u5217\u8868\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=13407)\u3002\n\n## \u5fae\u79ef\u5206\u7684\u672c\u8d28 | \u7b2c31\u5929\n\u89c2\u770b\u4e862\u4e2a\u89c6\u9891\uff0c\u5185\u5bb9\u5305\u62ec\u9690\u5206\u5316\u4e0e\u6781\u9650\u3002\n\nB\u7ad9\u64ad\u653e\u5217\u8868\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=13407)\u3002\n\n## \u5fae\u79ef\u5206\u7684\u672c\u8d28 | \u7b2c32\u5929\n\u89c2\u770b\u4e86\u5269\u4f59\u76844\u4e2a\u89c6\u9891\uff0c\u5185\u5bb9\u5305\u62ec\u79ef\u5206\u4e0e\u9ad8\u9636\u5bfc\u6570\u3002\n\nB\u7ad9\u64ad\u653e\u5217\u8868\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=13407)\u3002\n\n## \u968f\u673a\u68ee\u6797 | \u7b2c33\u5929\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%2033.png\">\n<\/p>\n\n## \u968f\u673a\u68ee\u6797 | \u7b2c34\u5929\n[\u968f\u673a\u68ee\u6797\u5b9e\u73b0](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2034_Random_Forests.md)\n\n## \u4ec0\u4e48\u662f\u795e\u7ecf\u7f51\u7edc\uff1f | \u6df1\u5ea6\u5b66\u4e60\uff0c\u7b2c1\u7ae0 | \u7b2c 35\u5929\nYoutube\u9891\u90533Blue1Brown\u4e2d\u6709\u7cbe\u5f69\u7684\u89c6\u9891\u4ecb\u7ecd\u795e\u7ecf\u7f51\u7edc\u3002\u8fd9\u4e2a\u89c6\u9891\u63d0\u4f9b\u4e86\u5f88\u597d\u7684\u89e3\u91ca\uff0c\u5e76\u4f7f\u7528\u624b\u5199\u6570\u5b57\u6570\u636e\u96c6\u6f14\u793a\u57fa\u672c\u6982\u5ff5\u3002\n\nB\u7ad9\u89c6\u9891\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=26587)\u3002\n\n## \u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5b66\u4e60 | \u6df1\u5ea6\u5b66\u4e60\uff0c\u7b2c2\u7ae0 | \u7b2c36\u5929\nYoutube\u9891\u90533Blue1Brown\u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7b2c2\u90e8\u5206\uff0c\u8fd9\u4e2a\u89c6\u9891\u7528\u6709\u8da3\u7684\u65b9\u5f0f\u89e3\u91ca\u4e86\u68af\u5ea6\u4e0b\u964d\u6cd5\u3002\u63a8\u8350\u5fc5\u987b\u89c2\u770b169.\n\nB\u7ad9\u89c6\u9891\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=26587)\u3002\n\n## \u53cd\u5411\u4f20\u64ad\u6cd5\u7a76\u7adf\u505a\u4ec0\u4e48\uff1f | \u6df1\u5ea6\u5b66\u4e60\uff0c\u7b2c3\u7ae0 | \u7b2c37\u5929\nYoutube\u9891\u90533Blue1Brown\u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7b2c3\u90e8\u5206\uff0c\u8fd9\u4e2a\u89c6\u9891\u4e3b\u8981\u4ecb\u7ecd\u4e86\u504f\u5bfc\u6570\u548c\u53cd\u5411\u4f20\u64ad\u6cd5\u3002\n\nB\u7ad9\u89c6\u9891\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=26587)\u3002\n\n## \u53cd\u5411\u4f20\u64ad\u6cd5\u6f14\u7b97 | \u6df1\u5ea6\u5b66\u4e60\uff0c\u7b2c4\u7ae0 | \u7b2c38\u5929\nYoutube\u9891\u90533Blue1Brown\u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7b2c3\u90e8\u5206\uff0c\u8fd9\u4e2a\u89c6\u9891\u4e3b\u8981\u4ecb\u7ecd\u4e86\u504f\u5bfc\u6570\u548c\u53cd\u5411\u4f20\u64ad\u6cd5\u3002\n\nB\u7ad9\u89c6\u9891\u5728[\u8fd9\u91cc](https:\/\/space.bilibili.com\/88461692\/#\/channel\/detail?cid=26587)\u3002\n\n## \u7b2c1\u90e8\u5206 | \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840Python\uff0cTensorFlow\u548cKeras | \u7b2c39\u5929\n\u89c6\u9891\u5730\u5740\u5728[\u8fd9\u91cc](https:\/\/www.youtube.com\/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)\u3002\n<br>\u4e2d\u6587\u6587\u5b57\u7248[notebook](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2039.ipynb)\u3002\n\n## \u7b2c2\u90e8\u5206 | \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840Python\uff0cTensorFlow\u548cKeras | \u7b2c40\u5929\n\u89c6\u9891\u5730\u5740\u5728[\u8fd9\u91cc](https:\/\/www.youtube.com\/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)\u3002\n<br>\u4e2d\u6587\u6587\u5b57\u7248[notebook](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2040.ipynb)\u3002\n\n## \u7b2c3\u90e8\u5206 | \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840Python\uff0cTensorFlow\u548cKeras | \u7b2c41\u5929\n\u89c6\u9891\u5730\u5740\u5728[\u8fd9\u91cc](https:\/\/www.youtube.com\/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)\u3002\n<br>\u4e2d\u6587\u6587\u5b57\u7248[notebook](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2041.ipynb)\u3002\n\n## \u7b2c4\u90e8\u5206 | \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840Python\uff0cTensorFlow\u548cKeras | \u7b2c42\u5929\n\u89c6\u9891\u5730\u5740\u5728[\u8fd9\u91cc](https:\/\/www.youtube.com\/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)\u3002\n<br>\u4e2d\u6587\u6587\u5b57\u7248[notebook](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Code\/Day%2042.ipynb)\u3002\n\n## K-\u5747\u503c\u805a\u7c7b | \u7b2c43\u5929\n\u8f6c\u5230\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u5e76\u7814\u7a76\u4e86\u805a\u7c7b\u3002\u53ef\u5728[\u4f5c\u8005\u7f51\u7ad9](http:\/\/www.avikjain.me\/)\u67e5\u8be2\u3002\u53d1\u73b0\u4e00\u4e2a\u5947\u5999\u7684[\u52a8\u753b](http:\/\/shabal.in\/visuals\/kmeans\/6.html)\u6709\u52a9\u4e8e\u7406\u89e3K-\u5747\u503c\u805a\u7c7b\u3002\n\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%2043.jpg\">\n<\/p>\n\n## K-\u5747\u503c\u805a\u7c7b | \u7b2c44\u5929\n\u5b9e\u73b0\uff08\u5f85\u6dfb\u52a0\u4ee3\u7801\uff09\n\n## \u6df1\u5165\u7814\u7a76 | NUMPY | \u7b2c45\u5929\n\u5f97\u5230JK VanderPlas\u5199\u7684\u4e66\u300aPython\u6570\u636e\u79d1\u5b66\u624b\u518c\uff08Python Data Science HandBook\uff09\u300b\uff0cJupyter notebooks\u5728[\u8fd9\u91cc](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook)\u3002\n<br>**[\u9ad8\u6e05\u4e2d\u6587\u7248pdf](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Other%20Docs\/Python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E6%89%8B%E5%86%8C.zip)**\n<br>\u7b2c2\u7ae0\uff1aNumPy\u4ecb\u7ecd\uff0c\u5305\u62ec\u6570\u636e\u7c7b\u578b\u3001\u6570\u7ec4\u548c\u6570\u7ec4\u8ba1\u7b97\u3002\n<br>\u4ee3\u7801\u5982\u4e0b\uff1a\n<br>[2 NumPy\u5165\u95e8](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.00-Introduction-to-NumPy.ipynb)\n<br>[2.1 \u7406\u89e3Python\u4e2d\u7684\u6570\u636e\u7c7b\u578b](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.01-Understanding-Data-Types.ipynb)\n<br>[2.2 NumPy\u6570\u7ec4\u57fa\u7840](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.02-The-Basics-Of-NumPy-Arrays.ipynb)\n<br>[2.3 NumPy\u6570\u7ec4\u7684\u8ba1\u7b97\uff1a\u901a\u7528\u51fd\u6570](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.03-Computation-on-arrays-ufuncs.ipynb)\n\n## \u6df1\u5165\u7814\u7a76 | NUMPY | \u7b2c46\u5929\n\u7b2c2\u7ae0\uff1a \u805a\u5408, \u6bd4\u8f83\u8fd0\u7b97\u7b26\u548c\u5e7f\u64ad\u3002\n<br>\u4ee3\u7801\u5982\u4e0b\uff1a\n<br>[2.4 \u805a\u5408\uff1a\u6700\u5c0f\u503c\u3001\u6700\u5927\u503c\u548c\u5176\u4ed6\u503c](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.04-Computation-on-arrays-aggregates.ipynb)\n<br>[2.5 \u6570\u7ec4\u7684\u8ba1\u7b97\uff1a\u5e7f\u64ad](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.05-Computation-on-arrays-broadcasting.ipynb)\n<br>[2.6 \u6bd4\u8f83\u3001\u63a9\u7801\u548c\u5e03\u5c14\u8fd0\u7b97](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.06-Boolean-Arrays-and-Masks.ipynb)\n\n## \u6df1\u5165\u7814\u7a76 | NUMPY | \u7b2c47\u5929\n\u7b2c2\u7ae0\uff1a \u82b1\u54e8\u7684\u7d22\u5f15\uff0c\u6570\u7ec4\u6392\u5e8f\uff0c\u7ed3\u6784\u5316\u6570\u636e\u3002\n<br>\u4ee3\u7801\u5982\u4e0b\uff1a\n<br>[2.7 \u82b1\u54e8\u7684\u7d22\u5f15](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.07-Fancy-Indexing.ipynb)\n<br>[2.8 \u6570\u7ec4\u7684\u6392\u5e8f](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.08-Sorting.ipynb)\n<br>[2.9 \u7ed3\u6784\u5316\u6570\u636e\uff1aNumPy\u7684\u7ed3\u6784\u5316\u6570\u7ec4](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/02.09-<br>Structured-Data-NumPy.ipynb)\n\n## \u6df1\u5165\u7814\u7a76 | PANDAS | \u7b2c48\u5929\n\u7b2c3\u7ae0\uff1aPandas\u6570\u636e\u5904\u7406\n<br>\u5305\u542bPandas\u5bf9\u8c61\uff0c\u6570\u636e\u53d6\u503c\u4e0e\u9009\u62e9\uff0c\u6570\u503c\u8fd0\u7b97\u65b9\u6cd5\uff0c\u5904\u7406\u7f3a\u5931\u503c\uff0c\u5c42\u7ea7\u7d22\u5f15\uff0c\u5408\u5e76\u6570\u636e\u96c6\u3002\n<br>\u4ee3\u7801\u5982\u4e0b\uff1a\n<br>[3 Pandas\u6570\u636e\u5904\u7406](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.00-Introduction-to-Pandas.ipynb)\n<br>[3.1 Pandas\u5bf9\u8c61\u7b80\u4ecb](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.01-Introducing-Pandas-Objects.ipynb)\n<br>[3.2 \u6570\u636e\u53d6\u503c\u4e0e\u9009\u62e9](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.02-Data-Indexing-and-Selection.ipynb)\n<br>[3.3 Pandas\u6570\u503c\u8fd0\u7b97\u65b9\u6cd5](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.03-Operations-in-Pandas.ipynb)\n<br>[3.4 \u5904\u7406\u7f3a\u5931\u503c](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.04-Missing-Values.ipynb)\n<br>[3.5 \u5c42\u7ea7\u7d22\u5f15](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.05-Hierarchical-Indexing.ipynb)\n<br>[3.6 \u5408\u5e76\u6570\u636e\u96c6\uff1aConCat\u548cAppend\u65b9\u6cd5](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.06-Concat-And-Append.ipynb)\n\n## \u6df1\u5165\u7814\u7a76 | PANDAS | \u7b2c49\u5929\n\u7b2c3\u7ae0\uff1a\u5b8c\u6210\u5269\u4f59\u5185\u5bb9-\u5408\u5e76\u4e0e\u8fde\u63a5\uff0c\u7d2f\u8ba1\u4e0e\u5206\u7ec4\uff0c\u6570\u636e\u900f\u89c6\u8868\u3002\n<br>\u4ee3\u7801\u5982\u4e0b\uff1a\n<br>[3.7 \u5408\u5e76\u6570\u636e\u96c6\uff1a\u5408\u5e76\u4e0e\u8fde\u63a5](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.07-Merge-and-Join.ipynb)\n<br>[3.8 \u7d2f\u8ba1\u4e0e\u5206\u7ec4](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.08-Aggregation-and-Grouping.ipynb)\n<br>[3.9 \u6570\u636e\u900f\u89c6\u8868](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.09-Pivot-Tables.ipynb)\n\n## \u6df1\u5165\u7814\u7a76 | PANDAS | \u7b2c50\u5929\n\u7b2c3\u7ae0\uff1a\u5411\u91cf\u5316\u5b57\u7b26\u4e32\u64cd\u4f5c\uff0c\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u3002\n<br>\u4ee3\u7801\u5982\u4e0b\uff1a\n<br>[3.10 \u5411\u91cf\u5316\u5b57\u7b26\u4e32\u64cd\u4f5c](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.10-Working-With-Strings.ipynb)\n<br>[3.11 \u5904\u7406\u65f6\u95f4\u5e8f\u5217](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.11-Working-with-Time-Series.ipynb)\n<br>[3.12 \u9ad8\u6027\u80fdPandas\uff1aeval()\u4e0equery()](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/03.12-Performance-Eval-and-Query.ipynb)\n\n## \u6df1\u5165\u7814\u7a76 | MATPLOTLIB | \u7b2c51\u5929\n\u7b2c4\u7ae0\uff1aMatplotlib\u6570\u636e\u53ef\u89c6\u5316\n<br>\u5b66\u4e60\u7b80\u6613\u7ebf\u5f62\u56fe, \u7b80\u6613\u6563\u70b9\u56fe\uff0c\u5bc6\u5ea6\u56fe\u4e0e\u7b49\u9ad8\u7ebf\u56fe.\n<br>\u4ee3\u7801\u5982\u4e0b\uff1a\n<br>[4 Matplotlib\u6570\u636e\u53ef\u89c6\u5316](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.00-Introduction-To-Matplotlib.ipynb)\n<br>[4.1 \u7b80\u6613\u7ebf\u5f62\u56fe](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.01-Simple-Line-Plots.ipynb)\n<br>[4.2 \u7b80\u6613\u6563\u70b9\u56fe](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.02-Simple-Scatter-Plots.ipynb)\n<br>[4.3 \u53ef\u89c6\u5316\u5f02\u5e38\u5904\u7406](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.03-Errorbars.ipynb)\n<br>[4.4 \u5bc6\u5ea6\u56fe\u4e0e\u7b49\u9ad8\u7ebf\u56fe](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.04-Density-and-Contour-Plots.ipynb)\n\n## \u6df1\u5165\u7814\u7a76 | MATPLOTLIB | \u7b2c52\u5929\n\u7b2c4\u7ae0\uff1aMatplotlib\u6570\u636e\u53ef\u89c6\u5316\n<br>\u5b66\u4e60\u76f4\u65b9\u56fe\uff0c\u914d\u7f6e\u56fe\u4f8b\uff0c\u914d\u7f6e\u989c\u8272\u6761\uff0c\u591a\u5b50\u56fe\u3002\n<br>\u4ee3\u7801\u5982\u4e0b\uff1a \n<br>[4.5 \u76f4\u65b9\u56fe](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.05-Histograms-and-Binnings.ipynb)\n<br>[4.6 \u914d\u7f6e\u56fe\u4f8b](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.06-Customizing-Legends.ipynb)\n<br>[4.7 \u914d\u7f6e\u989c\u8272\u6761](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.07-Customizing-Colorbars.ipynb)\n<br>[4.8 \u591a\u5b50\u56fe](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.08-Multiple-Subplots.ipynb)\n<br>[4.9 \u6587\u5b57\u4e0e\u6ce8\u91ca](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.09-Text-and-Annotation.ipynb)\n\n## \u6df1\u5165\u7814\u7a76 | MATPLOTLIB | \u7b2c53\u5929\n\u7b2c4\u7ae0\uff1aMatplotlib\u6570\u636e\u53ef\u89c6\u5316\n<br>\u5b66\u4e60\u4e09\u7ef4\u7ed8\u56fe\u3002\n<br>[4.12 \u753b\u4e09\u7ef4\u56fe](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook\/blob\/master\/notebooks\/04.12-Three-Dimensional-Plotting.ipynb)\n\n## \u5c42\u6b21\u805a\u7c7b | \u7b2c54\u5929\n[\u52a8\u753b\u6f14\u793a](https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Other%20Docs\/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB.gif)\n\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/MachineLearning100\/100-Days-Of-ML-Code\/blob\/master\/Info-graphs\/Day%2054.jpg\">\n<\/p>\n","142":"# NEW LIST 2022 - 2024: Machine-Learning \/ Deep-Learning \/ AI + Web3 -Tutorials\n\nHi - Thanks for dropping by!<br>\n<br>\nI will be updating this tutorials site on a <b>daily basis<\/b> adding all relevant topcis for 2022 - 2024 especially pertaining to **GPU programming, Data Centric AI, Emerging topics like Sustainable AI with Web3AI.js (DeFI, DAO, NFT) and much more**.<br>\n<br>\nMore importantly the applications of ML\/DL\/AI into industry areas such as Transportation, Medicine\/Healthcare etc. will be something I'll watch with keen interest and would love to share the same with you.\n<br>\nFinally, it is **YOUR** help I will seek to make it more useful and less boring, so please do suggest\/comment\/contribute!\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\/blob\/master\/images\/DK.png\">\n<\/p>\n\n## Index\n\n* [deep-learning](#deep-learning)\n   * [UBER | Pyro](#uber-pyro-probabalistic-tutorials)\n   * [Netflix | VectorFlow](#netflix-vectorflow-tutorials)\n   * [PyTorch](#pytorch-tutorials)\n   * [tensorflow](#tensor-flow-tutorials)\n   * [theano](#theano-tutorials)\n   * [keras](#keras-tutorials)\n   * [caffe](#deep-learning-misc)\n   * [Torch\/Lua]()\n   * [MXNET]()\n   \n* [scikit-learn](#scikit-learn)\n* [statistical-inference-scipy](#statistical-inference-scipy)\n* [pandas](#pandas)\n* [matplotlib](#matplotlib)\n* [numpy](#numpy)\n* [python-data](#python-data)\n* [kaggle-and-business-analyses](#kaggle-and-business-analyses)\n* [spark](#spark)\n* [mapreduce-python](#mapreduce-python)\n* [amazon web services](#aws)\n* [command lines](#commands)\n* [misc](#misc)\n* [notebook-installation](#notebook-installation)\n* [Curated list of Deep Learning \/ AI blogs](#curated-list-of-deeplearning-blogs)\n* [credits](#credits)\n* [contributing](#contributing)\n* [contact-info](#contact-info)\n* [license](#license)\n\n## deep-learning\n\nIPython Notebook(s) and other programming tools such as Torch\/Lua\/D lang in demonstrating deep learning functionality.\n\n### uber-pyro-probabalistic-tutorials\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\/blob\/master\/images\/pyro.png\">\n<\/p>\n\nAdditional PyRo tutorials:\n\n* [pyro-examples\/full examples](http:\/\/pyro.ai\/examples\/)\n* [pyro-examples\/Variational Autoencoders](http:\/\/pyro.ai\/examples\/vae.html)\n* [pyro-examples\/Bayesian Regression](http:\/\/pyro.ai\/examples\/bayesian_regression.html)\n* [pyro-examples\/Deep Markov Model](http:\/\/pyro.ai\/examples\/dmm.html)\n* [pyro-examples\/AIR(Attend Infer Repeat)](http:\/\/pyro.ai\/examples\/air.html)\n* [pyro-examples\/Semi-Supervised VE](http:\/\/pyro.ai\/examples\/ss-vae.html)\n* [pyro-examples\/GMM](http:\/\/pyro.ai\/examples\/gmm.html)\n* [pyro-examples\/Gaussian Process](http:\/\/pyro.ai\/examples\/gp.html)\n* [pyro-examples\/Bayesian Optimization](http:\/\/pyro.ai\/examples\/bo.html)\n* [Full Pyro Code](https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\/tree\/master\/deep-learning\/UBER-pyro)\n\n\n\n### netflix-vectorflow-tutorials\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\/blob\/master\/images\/VectorFlow.png\">\n<\/p>\n\n* [MNIST Example, running with Dlang](https:\/\/github.com\/Netflix\/vectorflow\/tree\/master\/examples)\n\n### pytorch-tutorials\n<p align=\"center\">\n  <img src=\"https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\/blob\/master\/images\/PyTorch.png\">\n<\/p>\n\n| Level | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Beginners\/Zakizhou](https:\/\/github.com\/pytorch\/tutorials\/tree\/master\/beginner_source) | Learning the basics of PyTorch from Facebook. |\n| [Intermedia\/Quanvuong](https:\/\/github.com\/pytorch\/tutorials\/tree\/master\/intermediate_source) | Learning the intermediate stuff about PyTorch of from Facebook. |\n| [Advanced\/Chsasank](https:\/\/github.com\/pytorch\/tutorials\/tree\/master\/advanced_source) | Learning the advanced stuff about PyTorch of from Facebook. |\n| [Learning PyTorch by Examples - Numpy, Tensors and Autograd](https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\/tree\/master\/pytorch) | At its core, PyTorch provides two main features an n-dimensional Tensor, similar to numpy but can run on GPUs AND automatic differentiation for building and training neural networks. |\n| [PyTorch - Getting to know autograd.Variable, Gradient, Neural Network](https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\/blob\/master\/pytorch\/PyTorch%20NN%20Basics%20-%20Autograd%20Gradient%20Neural%20Network%20Loss%20Backprop.ipynb) | Here we start with ultimate basics of Tensors, wrap a Tensor with Variable module, play with nn.Module and implement forward and backward function. |\n\n\n### tensor-flow-tutorials\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/avatars0.githubusercontent.com\/u\/15658638?v=3&s=100\">\n<\/p>\nAdditional TensorFlow tutorials:\n\n* [pkmital\/tensorflow_tutorials](https:\/\/github.com\/pkmital\/tensorflow_tutorials)\n* [nlintz\/TensorFlow-Tutorials](https:\/\/github.com\/nlintz\/TensorFlow-Tutorials)\n* [alrojo\/tensorflow-tutorial](https:\/\/github.com\/alrojo\/tensorflow-tutorial)\n* [BinRoot\/TensorFlow-Book](https:\/\/github.com\/BinRoot\/TensorFlow-Book)\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-basics](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/1_intro\/basic_operations.ipynb) | Learn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google. |\n| [tsf-linear](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/2_basic_classifiers\/linear_regression.ipynb) | Implement linear regression in TensorFlow. |\n| [tsf-logistic](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/2_basic_classifiers\/logistic_regression.ipynb) | Implement logistic regression in TensorFlow. |\n| [tsf-nn](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/2_basic_classifiers\/nearest_neighbor.ipynb) | Implement nearest neighboars in TensorFlow. |\n| [tsf-alex](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/3_neural_networks\/alexnet.ipynb) | Implement AlexNet in TensorFlow. |\n| [tsf-cnn](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/3_neural_networks\/convolutional_network.ipynb) | Implement convolutional neural networks in TensorFlow. |\n| [tsf-mlp](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/3_neural_networks\/multilayer_perceptron.ipynb) | Implement multilayer perceptrons in TensorFlow. |\n| [tsf-rnn](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/3_neural_networks\/recurrent_network.ipynb) | Implement recurrent neural networks in TensorFlow. |\n| [tsf-gpu](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/4_multi_gpu\/multigpu_basics.ipynb) | Learn about basic multi-GPU computation in TensorFlow. |\n| [tsf-gviz](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/5_ui\/graph_visualization.ipynb) | Learn about graph visualization in TensorFlow. |\n| [tsf-lviz](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-examples\/notebooks\/5_ui\/loss_visualization.ipynb) | Learn about loss visualization in TensorFlow. |\n\n### tensor-flow-exercises\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-not-mnist](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-exercises\/1_notmnist.ipynb) | Learn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow. |\n| [tsf-fully-connected](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-exercises\/2_fullyconnected.ipynb) | Progressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow. |\n| [tsf-regularization](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-exercises\/3_regularization.ipynb) | Explore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow. |\n| [tsf-convolutions](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-exercises\/4_convolutions.ipynb) | Create convolutional neural networks in TensorFlow. |\n| [tsf-word2vec](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-exercises\/5_word2vec.ipynb) | Train a skip-gram model over Text8 data in TensorFlow. |\n| [tsf-lstm](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/tensor-flow-exercises\/6_lstm.ipynb) | Train a LSTM character model over Text8 data in TensorFlow. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"http:\/\/www.deeplearning.net\/software\/theano\/_static\/theano_logo_allblue_200x46.png\">\n<\/p>\n\n### theano-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [theano-intro](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/theano-tutorial\/intro_theano\/intro_theano.ipynb) | Intro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation. |\n| [theano-scan](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/theano-tutorial\/scan_tutorial\/scan_tutorial.ipynb) | Learn scans, a mechanism to perform loops in a Theano graph. |\n| [theano-logistic](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/theano-tutorial\/intro_theano\/logistic_regression.ipynb) | Implement logistic regression in Theano. |\n| [theano-rnn](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/theano-tutorial\/rnn_tutorial\/simple_rnn.ipynb) | Implement recurrent neural networks in Theano. |\n| [theano-mlp](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/theano-tutorial\/theano_mlp\/theano_mlp.ipynb) | Implement multilayer perceptrons in Theano. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"http:\/\/i.imgur.com\/L45Q8c2.jpg\">\n<\/p>\n\n### keras-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| keras | Keras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano. |\n| [setup](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/README.md) | Learn about the tutorial goals and how to set up your Keras environment. |\n| [intro-deep-learning-ann](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/1.%20ANN\/1.1%20Introduction%20-%20Deep%20Learning%20and%20ANN.ipynb) | Get an intro to deep learning with Keras and Artificial Neural Networks (ANN). |\n| [Perceptrons and Adaline](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/1.%20ANN\/1.1.1%20Perceptron%20and%20Adaline.ipynb) | Implement Peceptron and adaptive linear neurons. |\n| [MLP and MNIST Data](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/1.%20ANN\/1.1.2%20MLP%20and%20MNIST.ipynb) | Classifying handwritten digits,implement MLP, train and debug ANN |\n| [theano](http:\/\/nbviewer.ipython.org\/github\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/1.2%20Introduction%20-%20Theano.ipynb) | Learn about Theano by working with weights matrices and gradients. |\n| [keras-otto](http:\/\/nbviewer.ipython.org\/github\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/1.3%20Introduction%20-%20Keras.ipynb) | Learn about Keras by looking at the Kaggle Otto challenge. |\n| [ann-mnist](http:\/\/nbviewer.ipython.org\/github\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/1.4%20(Extra)%20A%20Simple%20Implementation%20of%20ANN%20for%20MNIST.ipynb) | Review a simple implementation of ANN for MNIST using Keras. |\n| [conv-nets](http:\/\/nbviewer.ipython.org\/github\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/2.1%20Supervised%20Learning%20-%20ConvNets.ipynb) | Learn about Convolutional Neural Networks (CNNs) with Keras. |\n| [conv-net-1](http:\/\/nbviewer.ipython.org\/github\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/2.2.1%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20I.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 1. |\n| [conv-net-2](http:\/\/nbviewer.ipython.org\/github\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/2.2.2%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20II.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 2. |\n| [keras-models](http:\/\/nbviewer.ipython.org\/github\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/2.3%20Supervised%20Learning%20-%20Famous%20Models%20with%20Keras.ipynb) | Use pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras. |\n| [auto-encoders](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/6.%20AutoEncoders%20and%20Embeddings\/6.1.%20AutoEncoders%20and%20Embeddings.ipynb) | Learn about Autoencoders with Keras. |\n| [rnn-lstm](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/7.%20Recurrent%20Neural%20Networks\/7.1%20RNN%20and%20LSTM.ipynb) | Learn about Recurrent Neural Networks (RNNs) with Keras. |\n| [lstm-sentence-gen](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/7.%20Recurrent%20Neural%20Networks\/7.2%20LSTM%20for%20Sentence%20Generation.ipynb) |  Learn about RNNs using Long Short Term Memory (LSTM) networks with Keras. |\n| [nlp-deep-learning](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/6.%20AutoEncoders%20and%20Embeddings\/6.2%20NLP%20and%20Deep%20Learning.ipynb) | Learn about NLP using ANN (Artificial Neural Networks. |\n| [hyperparamter-tuning](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow\/blob\/master\/5.%20HyperParameter%20Tuning%20and%20Transfer%20Learning\/5.1%20HyperParameter%20Tuning.ipynb) | Hyperparamters tuning using keras-wrapper.scikit-learn |\n\n### deep-learning-misc\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [deep-dream](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/deep-learning\/deep-dream\/dream.ipynb) | Caffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/scikitlearn.png\">\n<\/p>\n\n## scikit-learn\n\nIPython Notebook(s) demonstrating scikit-learn functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [intro](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-intro.ipynb) | Intro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [knn](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-intro.ipynb#K-Nearest-Neighbors-Classifier) | Implement k-nearest neighbors in scikit-learn. |\n| [linear-reg](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-linear-reg.ipynb) | Implement linear regression in scikit-learn. |\n| [svm](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-svm.ipynb) | Implement support vector machine classifiers with and without kernels in scikit-learn. |\n| [random-forest](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-random-forest.ipynb) | Implement random forest classifiers and regressors in scikit-learn. |\n| [k-means](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-k-means.ipynb) | Implement k-means clustering in scikit-learn. |\n| [pca](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-pca.ipynb) | Implement principal component analysis in scikit-learn. |\n| [gmm](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-gmm.ipynb) | Implement Gaussian mixture models in scikit-learn. |\n| [validation](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scikit-learn\/scikit-learn-validation.ipynb) | Implement validation and model selection in scikit-learn. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/scipy.png\">\n<\/p>\n\n## statistical-inference-scipy\n\nIPython Notebook(s) demonstrating statistical inference with SciPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| scipy | SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data. |\n| [effect-size](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scipy\/effect_size.ipynb) | Explore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States. |\n| [sampling](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scipy\/sampling.ipynb) | Explore random sampling by analyzing the average weight of men and women in the United States using BRFSS data. |\n| [hypothesis](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/scipy\/hypothesis.ipynb) | Explore hypothesis testing by analyzing the difference of first-born babies compared with others. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/pandas.png\">\n<\/p>\n\n## pandas\n\nIPython Notebook(s) demonstrating pandas functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [pandas](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/pandas.ipynb) | Software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series. |\n| [github-data-wrangling](https:\/\/github.com\/donnemartin\/viz\/blob\/master\/githubstats\/data_wrangling.ipynb) | Learn how to load, clean, merge, and feature engineer by analyzing GitHub data from the [`Viz`](https:\/\/github.com\/donnemartin\/viz) repo. |\n| [Introduction-to-Pandas](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.00-Introduction-to-Pandas.ipynb) | Introduction to Pandas. |\n| [Introducing-Pandas-Objects](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.01-Introducing-Pandas-Objects.ipynb) | Learn about Pandas objects. |\n| [Data Indexing and Selection](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.02-Data-Indexing-and-Selection.ipynb) | Learn about data indexing and selection in Pandas. |\n| [Operations-in-Pandas](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.03-Operations-in-Pandas.ipynb) | Learn about operating on data in Pandas. |\n| [Missing-Values](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.04-Missing-Values.ipynb) | Learn about handling missing data in Pandas. |\n| [Hierarchical-Indexing](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.05-Hierarchical-Indexing.ipynb) | Learn about hierarchical indexing in Pandas. |\n| [Concat-And-Append](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.06-Concat-And-Append.ipynb) | Learn about combining datasets: concat and append in Pandas. |\n| [Merge-and-Join](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.07-Merge-and-Join.ipynb) | Learn about combining datasets: merge and join in Pandas. |\n| [Aggregation-and-Grouping](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.08-Aggregation-and-Grouping.ipynb) | Learn about aggregation and grouping in Pandas. |\n| [Pivot-Tables](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.09-Pivot-Tables.ipynb) | Learn about pivot tables in Pandas. |\n| [Working-With-Strings](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.10-Working-With-Strings.ipynb) | Learn about vectorized string operations in Pandas. |\n| [Working-with-Time-Series](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.11-Working-with-Time-Series.ipynb) | Learn about working with time series in pandas. |\n| [Performance-Eval-and-Query](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/pandas\/03.12-Performance-Eval-and-Query.ipynb) | Learn about high-performance Pandas: eval() and query() in Pandas. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/matplotlib.png\">\n<\/p>\n\n## matplotlib\n\nIPython Notebook(s) demonstrating matplotlib functionality.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [matplotlib](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/matplotlib.ipynb) | Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. |\n| [matplotlib-applied](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/matplotlib-applied.ipynb) | Apply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots. |\n| [Introduction-To-Matplotlib](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.00-Introduction-To-Matplotlib.ipynb) | Introduction to Matplotlib. |\n| [Simple-Line-Plots](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.01-Simple-Line-Plots.ipynb) | Learn about simple line plots in Matplotlib. |\n| [Simple-Scatter-Plots](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.02-Simple-Scatter-Plots.ipynb) | Learn about simple scatter plots in Matplotlib. |\n| [Errorbars.ipynb](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.03-Errorbars.ipynb) | Learn about visualizing errors in Matplotlib. |\n| [Density-and-Contour-Plots](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.04-Density-and-Contour-Plots.ipynb) | Learn about density and contour plots in Matplotlib. |\n| [Histograms-and-Binnings](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.05-Histograms-and-Binnings.ipynb) | Learn about histograms, binnings, and density in Matplotlib. |\n| [Customizing-Legends](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.06-Customizing-Legends.ipynb) | Learn about customizing plot legends in Matplotlib. |\n| [Customizing-Colorbars](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.07-Customizing-Colorbars.ipynb) | Learn about customizing colorbars in Matplotlib. |\n| [Multiple-Subplots](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.08-Multiple-Subplots.ipynb) | Learn about multiple subplots in Matplotlib. |\n| [Text-and-Annotation](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.09-Text-and-Annotation.ipynb) | Learn about text and annotation in Matplotlib. |\n| [Customizing-Ticks](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.10-Customizing-Ticks.ipynb) | Learn about customizing ticks in Matplotlib. |\n| [Settings-and-Stylesheets](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.11-Settings-and-Stylesheets.ipynb) | Learn about customizing Matplotlib: configurations and stylesheets. |\n| [Three-Dimensional-Plotting](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.12-Three-Dimensional-Plotting.ipynb) | Learn about three-dimensional plotting in Matplotlib. |\n| [Geographic-Data-With-Basemap](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.13-Geographic-Data-With-Basemap.ipynb) | Learn about geographic data with basemap in Matplotlib. |\n| [Visualization-With-Seaborn](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/matplotlib\/04.14-Visualization-With-Seaborn.ipynb) | Learn about visualization with Seaborn. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/numpy.png\">\n<\/p>\n\n## numpy\n\nIPython Notebook(s) demonstrating NumPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [numpy](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/numpy.ipynb) | Adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [Introduction-to-NumPy](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.00-Introduction-to-NumPy.ipynb) | Introduction to NumPy. |\n| [Understanding-Data-Types](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.01-Understanding-Data-Types.ipynb) | Learn about data types in Python. |\n| [The-Basics-Of-NumPy-Arrays](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.02-The-Basics-Of-NumPy-Arrays.ipynb) | Learn about the basics of NumPy arrays. |\n| [Computation-on-arrays-ufuncs](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.03-Computation-on-arrays-ufuncs.ipynb) | Learn about computations on NumPy arrays: universal functions. |\n| [Computation-on-arrays-aggregates](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.04-Computation-on-arrays-aggregates.ipynb) | Learn about aggregations: min, max, and everything in between in NumPy. |\n| [Computation-on-arrays-broadcasting](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.05-Computation-on-arrays-broadcasting.ipynb) | Learn about computation on arrays: broadcasting in NumPy. |\n| [Boolean-Arrays-and-Masks](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.06-Boolean-Arrays-and-Masks.ipynb) | Learn about comparisons, masks, and boolean logic in NumPy. |\n| [Fancy-Indexing](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.07-Fancy-Indexing.ipynb) | Learn about fancy indexing in NumPy. |\n| [Sorting](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.08-Sorting.ipynb) | Learn about sorting arrays in NumPy. |\n| [Structured-Data-NumPy](http:\/\/nbviewer.jupyter.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/numpy\/02.09-Structured-Data-NumPy.ipynb) | Learn about structured data: NumPy's structured arrays. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/python.png\">\n<\/p>\n\n## python-data\n\nIPython Notebook(s) demonstrating Python functionality geared towards data analysis.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|\n| [data structures](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/python-data\/structs.ipynb) | Learn Python basics with tuples, lists, dicts, sets. |\n| [data structure utilities](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/python-data\/structs_utils.ipynb) | Learn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions. |\n| [functions](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/python-data\/functions.ipynb) | Learn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools. |\n| [datetime](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/python-data\/datetime.ipynb) | Learn how to work with Python dates and times: datetime, strftime, strptime, timedelta. |\n| [logging](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/python-data\/logs.ipynb) | Learn about Python logging with RotatingFileHandler and TimedRotatingFileHandler. |\n| [pdb](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/python-data\/pdb.ipynb) | Learn how to debug in Python with the interactive source code debugger. |\n| [unit tests](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/python-data\/unit_tests.ipynb) | Learn how to test in Python with Nose unit tests. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/kaggle.png\">\n<\/p>\n\n## kaggle-and-business-analyses\n\nIPython Notebook(s) used in [kaggle](https:\/\/www.kaggle.com\/) competitions and business analyses.\n\n| Notebook | Description |\n|-------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n| [titanic](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/kaggle\/titanic.ipynb) | Predict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning. |\n| [churn-analysis](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/analyses\/churn.ipynb) | Predict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration\/descrimination.|\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/spark.png\">\n<\/p>\n\n## spark\n\nIPython Notebook(s) demonstrating spark and HDFS functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [spark](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/spark\/spark.ipynb) | In-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms. |\n| [hdfs](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/spark\/hdfs.ipynb) | Reliably stores very large files across machines in a large cluster. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/mrjob.png\">\n<\/p>\n\n## mapreduce-python\n\nIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [mapreduce-python](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/mapreduce\/mapreduce-python.ipynb) | Runs MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and [mrjob](https:\/\/github.com\/Yelp\/mrjob) config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  [Disco](https:\/\/github.com\/discoproject\/disco\/) is another python-based alternative.|\n\n<br\/>\n\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/aws.png\">\n<\/p>\n\n## aws\n\nIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.\n\n\nAlso check out:\n\n* [SAWS](https:\/\/github.com\/donnemartin\/saws): A Supercharged AWS command line interface (CLI).\n* [Awesome AWS](https:\/\/github.com\/donnemartin\/awesome-aws): A curated list of libraries, open source repos, guides, blogs, and other resources.\n\n| Notebook | Description |\n|------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [boto](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/aws\/aws.ipynb#Boto) | Official AWS SDK for Python. |\n| [s3cmd](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/aws\/aws.ipynb#s3cmd) | Interacts with S3 through the command line. |\n| [s3distcp](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/aws\/aws.ipynb#s3distcp) | Combines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster. |\n| [s3-parallel-put](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/aws\/aws.ipynb#s3-parallel-put) | Uploads multiple files to S3 in parallel. |\n| [redshift](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/aws\/aws.ipynb#redshift) | Acts as a fast data warehouse built on top of technology from massive parallel processing (MPP). |\n| [kinesis](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/aws\/aws.ipynb#kinesis) | Streams data in real time with the ability to process thousands of data streams per second. |\n| [lambda](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/aws\/aws.ipynb#lambda) | Runs code in response to events, automatically managing compute resources. |\n\n<br\/>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/donnemartin\/data-science-ipython-notebooks\/master\/images\/commands.png\">\n<\/p>\n\n## commands\n\nIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [linux](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/commands\/linux.ipynb) | Unix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.|\n| [anaconda](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/commands\/misc.ipynb#anaconda) | Distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. |\n| [ipython notebook](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/commands\/misc.ipynb#ipython-notebook) | Web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. |\n| [git](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/commands\/misc.ipynb#git) | Distributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows. |\n| [ruby](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/commands\/misc.ipynb#ruby) | Used to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages. |\n| [jekyll](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/commands\/misc.ipynb#jekyll) | Simple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server. |\n| [pelican](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/commands\/misc.ipynb#pelican) | Python-based alternative to Jekyll. |\n| [django](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/commands\/misc.ipynb#django) | High-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports\/analyses and for blogging. Lighter-weight alternatives include [Pyramid](https:\/\/github.com\/Pylons\/pyramid), [Flask](https:\/\/github.com\/pallets\/flask), [Tornado](https:\/\/github.com\/tornadoweb\/tornado), and [Bottle](https:\/\/github.com\/bottlepy\/bottle).\n\n## misc\n\nIPython Notebook(s) demonstrating miscellaneous functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [regex](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/misc\/regex.ipynb) | Regular expression cheat sheet useful in data wrangling.|\n[algorithmia](http:\/\/nbviewer.ipython.org\/github\/TarrySingh\/Machine-Learning-Tutorials\/blob\/master\/misc\/Algorithmia.ipynb) | Algorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.|\n\n## notebook-installation\n\n### anaconda\n\nAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.\n\nFollow instructions to install [Anaconda](https:\/\/docs.continuum.io\/anaconda\/install) or the more lightweight [miniconda](http:\/\/conda.pydata.org\/miniconda.html).\n\n### dev-setup\n\nFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the [dev-setup](https:\/\/github.com\/donnemartin\/dev-setup) repo.\n\n### running-notebooks\n\nNote: If you intend to learn the hard way (preferred method)then I'd strongly advice to write as much code as you can yourself and not just run pre-written code. If you still want to test it, then do the following: \n\nTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found [here.](http:\/\/ipython.org\/notebook.html)\n\n    $ git clone https:\/\/github.com\/TarrySingh\/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials.git\n    $ cd Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\n    $ jupyter notebook\n    \n\nNotebooks tested with Python 3.7+\n\n## curated-list-of-deeplearning-blogs\n\n* A Blog From a Human-engineer-being http:\/\/www.erogol.com\/ [(RSS)](http:\/\/www.erogol.com\/feed\/)\n* Aakash Japi http:\/\/aakashjapi.com\/ [(RSS)](http:\/\/logicx24.github.io\/feed.xml)\n* Adit Deshpande https:\/\/adeshpande3.github.io\/ [(RSS)](https:\/\/adeshpande3.github.io\/adeshpande3.github.io\/feed.xml)\n* Advanced Analytics & R http:\/\/advanceddataanalytics.net\/ [(RSS)](http:\/\/advanceddataanalytics.net\/feed\/)\n* Adventures in Data Land http:\/\/blog.smola.org [(RSS)](http:\/\/blog.smola.org\/rss)\n* Agile Data Science http:\/\/blog.sense.io\/ [(RSS)](http:\/\/blog.sense.io\/rss\/)\n* Ahmed El Deeb https:\/\/medium.com\/@D33B [(RSS)](https:\/\/medium.com\/feed\/@D33B)\n* Airbnb Data blog http:\/\/nerds.airbnb.com\/data\/ [(RSS)](http:\/\/nerds.airbnb.com\/feed\/)\n* Alex Castrounis | InnoArchiTech http:\/\/www.innoarchitech.com\/ [(RSS)](http:\/\/www.innoarchitech.com\/feed.xml)\n* Alex Perrier http:\/\/alexperrier.github.io\/ [(RSS)](http:\/\/alexperrier.github.io\/feed.xml)\n* Algobeans | Data Analytics Tutorials & Experiments for the Layman https:\/\/algobeans.com [(RSS)](https:\/\/algobeans.com\/feed\/)\n* Amazon AWS AI Blog https:\/\/aws.amazon.com\/blogs\/ai\/ [(RSS)](https:\/\/aws.amazon.com\/blogs\/amazon-ai\/feed\/)\n* Analytics Vidhya http:\/\/www.analyticsvidhya.com\/blog\/ [(RSS)](http:\/\/feeds.feedburner.com\/AnalyticsVidhya)\n* Analytics and Visualization in Big Data @ Sicara https:\/\/blog.sicara.com [(RSS)](https:\/\/blog.sicara.com\/feed)\n* Andreas M\u00fcller http:\/\/peekaboo-vision.blogspot.com\/ [(RSS)](http:\/\/peekaboo-vision.blogspot.com\/atom.xml)\n* Andrej Karpathy blog http:\/\/karpathy.github.io\/ [(RSS)](http:\/\/karpathy.github.io\/feed.xml)\n* Andrew Brooks http:\/\/brooksandrew.github.io\/simpleblog\/ [(RSS)](http:\/\/brooksandrew.github.io\/simpleblog\/feed.xml)\n* Andrey Kurenkov http:\/\/www.andreykurenkov.com\/writing\/ [(RSS)](http:\/\/www.andreykurenkov.com\/writing\/feed.xml\/)\n* Anton Lebedevich's Blog http:\/\/mabrek.github.io\/ [(RSS)](http:\/\/mabrek.github.io\/feed.xml)\n* Arthur Juliani https:\/\/medium.com\/@awjuliani [(RSS)](https:\/\/medium.com\/feed\/@awjuliani)\n* Audun M. \u00d8ygard http:\/\/www.auduno.com\/ [(RSS)](http:\/\/auduno.tumblr.com\/rss)\n* Avi Singh https:\/\/avisingh599.github.io\/ [(RSS)](http:\/\/avisingh599.github.io\/feed.xml)\n* Beautiful Data http:\/\/beautifuldata.net\/ [(RSS)](http:\/\/beautifuldata.net\/feed\/)\n* Beckerfuffle http:\/\/mdbecker.github.io\/ [(RSS)](http:\/\/mdbecker.github.io\/atom.xml)\n* Becoming A Data Scientist http:\/\/www.becomingadatascientist.com\/ [(RSS)](http:\/\/www.becomingadatascientist.com\/feed\/)\n* Ben Bolte's Blog http:\/\/benjaminbolte.com\/ml\/ [(RSS)](http:\/\/benjaminbolte.com\/ml\/)\n* Ben Frederickson http:\/\/www.benfrederickson.com\/blog\/ [(RSS)](http:\/\/www.benfrederickson.com\/atom.xml)\n* Berkeley AI Research http:\/\/bair.berkeley.edu\/blog\/ [(RSS)](http:\/\/bair.berkeley.edu\/blog\/feed.xml)\n* Big-Ish Data http:\/\/bigishdata.com\/ [(RSS)](http:\/\/bigishdata.com\/feed\/)\n* Blog on neural networks http:\/\/yerevann.github.io\/ [(RSS)](http:\/\/yerevann.github.io\/atom.xml)\n* Blogistic RegressionAbout Projects http:\/\/d10genes.github.io\/blog\/ [(RSS)](http:\/\/d10genes.github.io\/blog\/feed.xml)\n* blogR | R tips and tricks from a scientist https:\/\/drsimonj.svbtle.com\/ [(RSS)](https:\/\/drsimonj.svbtle.com\/)\n* Brain of mat kelcey http:\/\/matpalm.com\/blog\/ [(RSS)](http:\/\/matpalm.com\/blog\/feed)\n* Brilliantly wrong thoughts on science and programming https:\/\/arogozhnikov.github.io\/ [(RSS)](http:\/\/arogozhnikov.github.io\/feed.xml)\n* Bugra Akyildiz http:\/\/bugra.github.io\/ [(RSS)](http:\/\/bugra.github.io\/feeds\/all.atom.xml)\n* Building Babylon https:\/\/building-babylon.net\/ [(RSS)](http:\/\/building-babylon.net\/feed\/)\n* Carl Shan http:\/\/carlshan.com\/ [(RSS)](http:\/\/feeds.feedburner.com\/carlshan)\n* Chris Stucchio https:\/\/www.chrisstucchio.com\/blog\/index.html [(RSS)](http:\/\/www.chrisstucchio.com\/blog\/atom.xml)\n* Christophe Bourguignat https:\/\/medium.com\/@chris_bour [(RSS)](https:\/\/medium.com\/feed\/@chris_bour)\n* Christopher Nguyen https:\/\/medium.com\/@ctn [(RSS)](https:\/\/medium.com\/feed\/@ctn)\n* Cloudera Data Science Posts http:\/\/blog.cloudera.com\/blog\/category\/data-science\/ [(RSS)](http:\/\/blog.cloudera.com\/blog\/category\/data-science\/feed\/)\n* colah's blog http:\/\/colah.github.io\/archive.html [(RSS)](http:\/\/colah.github.io\/rss.xml)\n* Cortana Intelligence and Machine Learning Blog https:\/\/blogs.technet.microsoft.com\/machinelearning\/ [(RSS)](http:\/\/blogs.technet.com\/b\/machinelearning\/rss.aspx)\n* Daniel Forsyth http:\/\/www.danielforsyth.me\/ [(RSS)](http:\/\/www.danielforsyth.me\/rss\/)\n* Daniel Homola http:\/\/danielhomola.com\/category\/blog\/ [(RSS)](http:\/\/danielhomola.com\/feed\/)\n* Daniel Nee http:\/\/danielnee.com [(RSS)](http:\/\/danielnee.com\/?feed=rss2)\n* Data Based Inventions http:\/\/datalab.lu\/ [(RSS)](http:\/\/datalab.lu\/atom.xml)\n* Data Blogger https:\/\/www.data-blogger.com\/ [(RSS)](https:\/\/www.data-blogger.com\/feed\/)\n* Data Labs http:\/\/blog.insightdatalabs.com\/ [(RSS)](http:\/\/blog.insightdatalabs.com\/rss\/)\n* Data Meets Media http:\/\/datameetsmedia.com\/ [(RSS)](http:\/\/datameetsmedia.com\/feed\/)\n* Data Miners Blog http:\/\/blog.data-miners.com\/ [(RSS)](http:\/\/blog.data-miners.com\/feeds\/posts\/default?alt=rss)\n* Data Mining Research http:\/\/www.dataminingblog.com\/ [(RSS)](http:\/\/feeds.feedburner.com\/dataminingblog)\n* Data Mining: Text Mining, Visualization and Social Media http:\/\/datamining.typepad.com\/data_mining\/ [(RSS)](http:\/\/datamining.typepad.com\/data_mining\/atom.xml)\n* Data Piques http:\/\/blog.ethanrosenthal.com\/ [(RSS)](http:\/\/blog.ethanrosenthal.com\/feeds\/all.atom.xml)\n* Data School http:\/\/www.dataschool.io\/ [(RSS)](http:\/\/www.dataschool.io\/rss\/)\n* Data Science 101 http:\/\/101.datascience.community\/ [(RSS)](http:\/\/101.datascience.community\/feed\/)\n* Data Science @ Facebook https:\/\/research.facebook.com\/blog\/datascience\/ [(RSS)](https:\/\/research.facebook.com\/blog\/datascience\/)\n* Data Science Insights http:\/\/www.datasciencebowl.com\/data-science-insights\/ [(RSS)](http:\/\/www.datasciencebowl.com\/feed\/)\n* Data Science Tutorials https:\/\/codementor.io\/data-science\/tutorial [(RSS)](https:\/\/www.codementor.io\/data-science\/tutorial\/feed)\n* Data Science Vademecum http:\/\/datasciencevademecum.wordpress.com\/ [(RSS)](http:\/\/datasciencevademecum.wordpress.com\/feed\/)\n* Dataaspirant http:\/\/dataaspirant.com\/ [(RSS)](http:\/\/dataaspirant.wordpress.com\/feed\/)\n* Dataclysm http:\/\/blog.okcupid.com\/ [(RSS)](http:\/\/blog.okcupid.com\/index.php\/feed\/)\n* DataGenetics http:\/\/datagenetics.com\/blog.html [(RSS)](http:\/\/datagenetics.com\/feed\/rss.xml)\n* Dataiku https:\/\/www.dataiku.com\/blog\/ [(RSS)](http:\/\/www.dataiku.com\/feed.xml)\n* DataKind http:\/\/www.datakind.org\/blog [(RSS)](http:\/\/feeds.feedburner.com\/DataKin)\n* DataLook http:\/\/blog.datalook.io\/ [(RSS)](http:\/\/blog.datalook.io\/feed\/)\n* Datanice https:\/\/datanice.wordpress.com\/ [(RSS)](https:\/\/datanice.wordpress.com\/feed\/)\n* Dataquest Blog https:\/\/www.dataquest.io\/blog\/ [(RSS)](https:\/\/www.dataquest.io\/blog\/atom.xml)\n* DataRobot http:\/\/www.datarobot.com\/blog\/ [(RSS)](http:\/\/www.datarobot.com\/feed\/)\n* Datascope http:\/\/datascopeanalytics.com\/blog [(RSS)](http:\/\/datascopeanalytics.com\/rss)\n* DatasFrame http:\/\/tomaugspurger.github.io\/ [(RSS)](http:\/\/tomaugspurger.github.io\/feeds\/all.rss.xml)\n* David Mimno http:\/\/www.mimno.org\/ [(RSS)](http:\/\/mimno.infosci.cornell.edu\/b\/feed.xml)\n* Dayne Batten http:\/\/daynebatten.com [(RSS)](http:\/\/daynebatten.com\/feed\/)\n* Deep Learning http:\/\/deeplearning.net\/blog\/ [(RSS)](http:\/\/deeplearning.net\/feed\/)\n* Deepdish http:\/\/deepdish.io\/ [(RSS)](http:\/\/deepdish.io\/atom.xml)\n* Delip Rao http:\/\/deliprao.com\/ [(RSS)](http:\/\/deliprao.com\/feed)\n* DENNY'S BLOG http:\/\/blog.dennybritz.com\/ [(RSS)](http:\/\/blog.dennybritz.com\/feed\/)\n* Dimensionless https:\/\/dimensionless.in\/blog\/ [(RSS)](https:\/\/dimensionless.in\/feed)\n* Distill http:\/\/distill.pub\/ [(RSS)](http:\/\/distill.pub\/rss.xml)\n* District Data Labs http:\/\/districtdatalabs.silvrback.com\/ [(RSS)](https:\/\/districtdatalabs.silvrback.com\/feed)\n* Diving into data https:\/\/blog.datadive.net\/ [(RSS)](http:\/\/blog.datadive.net\/feed\/)\n* Domino Data Lab's blog http:\/\/blog.dominodatalab.com\/ [(RSS)](http:\/\/blog.dominodatalab.com\/rss\/)\n* Dr. Randal S. Olson http:\/\/www.randalolson.com\/blog\/ [(RSS)](http:\/\/www.randalolson.com\/feed\/)\n* Drew Conway https:\/\/medium.com\/@drewconway [(RSS)](https:\/\/medium.com\/feed\/@drewconway)\n* Dustin Tran http:\/\/dustintran.com\/blog\/ [(RSS)](http:\/\/dustintran.com\/blog\/rss\/)\n* Eder Santana https:\/\/edersantana.github.io\/blog.html [(RSS)](http:\/\/edersantana.github.io\/feed.xml)\n* Edwin Chen http:\/\/blog.echen.me [(RSS)](http:\/\/blog.echen.me\/feeds\/all.rss.xml)\n* EFavDB http:\/\/efavdb.com\/ [(RSS)](http:\/\/efavdb.com\/feed\/)\n* Emilio Ferrara, Ph.D.  http:\/\/www.emilio.ferrara.name\/ [(RSS)](http:\/\/www.emilio.ferrara.name\/feed\/)\n* Entrepreneurial Geekiness http:\/\/ianozsvald.com\/ [(RSS)](http:\/\/ianozsvald.com\/feed\/)\n* Eric Jonas http:\/\/ericjonas.com\/archives.html [(RSS)](http:\/\/ericjonas.com\/archives.html)\n* Eric Siegel http:\/\/www.predictiveanalyticsworld.com\/blog [(RSS)](http:\/\/feeds.feedburner.com\/predictiveanalyticsworld\/GXRy)\n* Erik Bern http:\/\/erikbern.com [(RSS)](http:\/\/erikbern.com\/feed\/)\n* ERIN SHELLMAN http:\/\/www.erinshellman.com\/ [(RSS)](http:\/\/www.erinshellman.com\/feed\/)\n* Eugenio Culurciello http:\/\/culurciello.github.io\/ [(RSS)](http:\/\/culurciello.github.io\/feed.xml)\n* Fabian Pedregosa http:\/\/fa.bianp.net\/ [(RSS)](http:\/\/fa.bianp.net\/blog\/feed\/)\n* Fast Forward Labs http:\/\/blog.fastforwardlabs.com\/ [(RSS)](http:\/\/blog.fastforwardlabs.com\/rss)\n* FastML http:\/\/fastml.com\/ [(RSS)](http:\/\/fastml.com\/atom.xml)\n* Florian Hartl http:\/\/florianhartl.com\/ [(RSS)](http:\/\/florianhartl.com\/feed\/)\n* FlowingData http:\/\/flowingdata.com\/ [(RSS)](http:\/\/flowingdata.com\/feed\/)\n* Full Stack ML http:\/\/fullstackml.com\/ [(RSS)](http:\/\/fullstackml.com\/feed\/)\n* GAB41 http:\/\/www.lab41.org\/gab41\/ [(RSS)](http:\/\/www.lab41.org\/feed\/)\n* Garbled Notes http:\/\/www.chioka.in\/ [(RSS)](http:\/\/www.chioka.in\/feed.xml)\n* Greg Reda http:\/\/www.gregreda.com\/blog\/ [(RSS)](http:\/\/www.gregreda.com\/feeds\/all.atom.xml)\n* Hyon S Chu https:\/\/medium.com\/@adailyventure [(RSS)](https:\/\/medium.com\/feed\/@adailyventure)\n* i am trask http:\/\/iamtrask.github.io\/ [(RSS)](http:\/\/iamtrask.github.io\/feed.xml)\n* I Quant NY http:\/\/iquantny.tumblr.com\/ [(RSS)](http:\/\/iquantny.tumblr.com\/rss)\n* inFERENCe http:\/\/www.inference.vc\/ [(RSS)](http:\/\/www.inference.vc\/rss\/)\n* Insight Data Science https:\/\/blog.insightdatascience.com\/ [(RSS)](https:\/\/blog.insightdatascience.com\/feed)\n* INSPIRATION INFORMATION http:\/\/myinspirationinformation.com\/ [(RSS)](http:\/\/myinspirationinformation.com\/feed\/)\n* Ira Korshunova http:\/\/irakorshunova.github.io\/ [(RSS)](http:\/\/irakorshunova.github.io\/feed.xml)\n* I\u2019m a bandit https:\/\/blogs.princeton.edu\/imabandit\/ [(RSS)](https:\/\/blogs.princeton.edu\/imabandit\/feed\/)\n* Jason Toy http:\/\/www.jtoy.net\/ [(RSS)](http:\/\/jtoy.net\/atom.xml)\n* Jeremy D. Jackson, PhD http:\/\/www.jeremydjacksonphd.com\/ [(RSS)](http:\/\/www.jeremydjacksonphd.com\/?feed=rss2)\n* Jesse Steinweg-Woods https:\/\/jessesw.com\/ [(RSS)](https:\/\/jessesw.com\/feed.xml)\n* Joe Cauteruccio http:\/\/www.joecjr.com\/ [(RSS)](http:\/\/www.joecjr.com\/feed\/)\n* John Myles White http:\/\/www.johnmyleswhite.com\/ [(RSS)](http:\/\/www.johnmyleswhite.com\/feed\/)\n* John's Soapbox http:\/\/joschu.github.io\/ [(RSS)](http:\/\/joschu.github.io\/feed.xml)\n* Jonas Degrave http:\/\/317070.github.io\/ [(RSS)](http:\/\/317070.github.io\/feed.xml)\n* Joy Of Data http:\/\/www.joyofdata.de\/blog\/ [(RSS)](http:\/\/www.joyofdata.de\/blog\/feed\/)\n* Julia Evans http:\/\/jvns.ca\/ [(RSS)](http:\/\/jvns.ca\/atom.xml)\n* KDnuggets http:\/\/www.kdnuggets.com\/ [(RSS)](http:\/\/feeds.feedburner.com\/kdnuggets-data-mining-analytics)\n* Keeping Up With The Latest Techniques http:\/\/colinpriest.com\/ [(RSS)](http:\/\/colinpriest.com\/feed\/)\n* Kenny Bastani http:\/\/www.kennybastani.com\/ [(RSS)](http:\/\/www.kennybastani.com\/feeds\/posts\/default?alt=rss)\n* Kevin Davenport http:\/\/kldavenport.com\/ [(RSS)](http:\/\/kldavenport.com\/feed\/)\n* kevin frans http:\/\/kvfrans.com\/ [(RSS)](http:\/\/kvfrans.com\/rss\/)\n* korbonits | Math \u2229 Data http:\/\/korbonits.github.io\/ [(RSS)](http:\/\/korbonits.github.io\/feed.xml)\n* Large Scale Machine Learning  http:\/\/bickson.blogspot.com\/ [(RSS)](http:\/\/bickson.blogspot.com\/feeds\/posts\/default)\n* LATERAL BLOG https:\/\/blog.lateral.io\/ [(RSS)](https:\/\/blog.lateral.io\/feed\/)\n* Lazy Programmer http:\/\/lazyprogrammer.me\/ [(RSS)](http:\/\/lazyprogrammer.me\/feed\/)\n* Learn Analytics Here https:\/\/learnanalyticshere.wordpress.com\/ [(RSS)](https:\/\/learnanalyticshere.wordpress.com\/feed\/)\n* LearnDataSci http:\/\/www.learndatasci.com\/ [(RSS)](http:\/\/www.learndatasci.com\/feed\/)\n* Learning With Data http:\/\/learningwithdata.com\/ [(RSS)](http:\/\/learningwithdata.com\/rss_feed.xml)\n* Life, Language, Learning http:\/\/daoudclarke.github.io\/ [(RSS)](http:\/\/daoudclarke.github.io\/atom.xml)\n* Locke Data https:\/\/itsalocke.com\/blog\/ [(RSS)](https:\/\/itsalocke.com\/feed)\n* Louis Dorard http:\/\/www.louisdorard.com\/blog\/ [(RSS)](http:\/\/www.louisdorard.com\/blog?format=rss)\n* M.E.Driscoll http:\/\/medriscoll.com\/ [(RSS)](http:\/\/medriscoll.com\/rss)\n* Machinalis http:\/\/www.machinalis.com\/blog [(RSS)](http:\/\/www.machinalis.com\/blog\/feeds\/rss\/)\n* Machine Learning (Theory) http:\/\/hunch.net\/ [(RSS)](http:\/\/hunch.net\/?feed=rss2)\n* Machine Learning and Data Science http:\/\/alexhwoods.com\/blog\/ [(RSS)](http:\/\/alexhwoods.com\/feed\/)\n* Machine Learning https:\/\/charlesmartin14.wordpress.com\/ [(RSS)](http:\/\/charlesmartin14.wordpress.com\/feed\/)\n* Machine Learning Mastery http:\/\/machinelearningmastery.com\/blog\/ [(RSS)](http:\/\/machinelearningmastery.com\/feed\/)\n* Machine Learning Blogs https:\/\/machinelearningblogs.com\/ [(RSS)](https:\/\/machinelearningblogs.com\/feed\/)\n* Machine Learning, etc http:\/\/yaroslavvb.blogspot.com [(RSS)](http:\/\/yaroslavvb.blogspot.com\/feeds\/posts\/default)\n* Machine Learning, Maths and Physics https:\/\/mlopezm.wordpress.com\/ [(RSS)](https:\/\/mlopezm.wordpress.com\/feed\/)\n* Machine Learning Flashcards https:\/\/machinelearningflashcards.com\/ $10, but a nicely illustrated set of 300 flash cards\n* Machined Learnings http:\/\/www.machinedlearnings.com\/ [(RSS)](http:\/\/www.machinedlearnings.com\/feeds\/posts\/default)\n* MAPPING BABEL https:\/\/jack-clark.net\/ [(RSS)](https:\/\/jack-clark.net\/feed\/)\n* MAPR Blog https:\/\/www.mapr.com\/blog [(RSS)](https:\/\/www.mapr.com\/bigdata.xml)\n* MAREK REI http:\/\/www.marekrei.com\/blog\/ [(RSS)](http:\/\/www.marekrei.com\/blog\/feed\/)\n* MARGINALLY INTERESTING http:\/\/blog.mikiobraun.de\/ [(RSS)](http:\/\/feeds.feedburner.com\/MarginallyInteresting)\n* Math \u2229 Programming http:\/\/jeremykun.com\/ [(RSS)](http:\/\/jeremykun.wordpress.com\/feed\/)\n* Matthew Rocklin http:\/\/matthewrocklin.com\/blog\/ [(RSS)](http:\/\/matthewrocklin.com\/blog\/atom.xml)\n* Melody Wolk http:\/\/melodywolk.com\/projects\/ [(RSS)](http:\/\/melodywolk.com\/feed\/)\n* Mic Farris http:\/\/www.micfarris.com\/ [(RSS)](http:\/\/www.micfarris.com\/feed\/)\n* Mike Tyka http:\/\/mtyka.github.io\/ [(RSS)](http:\/\/mtyka.github.io\/\/feed.xml)\n* minimaxir | Max Woolf's Blog http:\/\/minimaxir.com\/ [(RSS)](http:\/\/minimaxir.com\/rss.xml)\n* Mirror Image https:\/\/mirror2image.wordpress.com\/ [(RSS)](http:\/\/mirror2image.wordpress.com\/feed\/)\n* Mitch Crowe http:\/\/www.dataphoric.com\/ [(RSS)](http:\/\/www.dataphoric.com\/feed.xml)\n* MLWave http:\/\/mlwave.com\/ [(RSS)](http:\/\/mlwave.com\/feed\/)\n* MLWhiz http:\/\/mlwhiz.com\/ [(RSS)](http:\/\/mlwhiz.com\/atom.xml)\n* Models are illuminating and wrong https:\/\/peadarcoyle.wordpress.com\/ [(RSS)](http:\/\/peadarcoyle.wordpress.com\/feed\/)\n* Moody Rd http:\/\/blog.mrtz.org\/ [(RSS)](http:\/\/blog.mrtz.org\/feed.xml)\n* Moonshots http:\/\/jxieeducation.com\/ [(RSS)](http:\/\/jxieeducation.com\/feed.xml)\n* Mourad Mourafiq http:\/\/mourafiq.com\/ [(RSS)](http:\/\/mourafiq.com\/atom.xml)\n* My thoughts on Data science, predictive analytics, Python http:\/\/shahramabyari.com\/ [(RSS)](http:\/\/shahramabyari.com\/feed\/)\n* Natural language processing blog http:\/\/nlpers.blogspot.fr\/ [(RSS)](http:\/\/nlpers.blogspot.com\/feeds\/posts\/default)\n* Neil Lawrence http:\/\/inverseprobability.com\/blog.html [(RSS)](http:\/\/inverseprobability.com\/rss.xml)\n* NLP and Deep Learning enthusiast http:\/\/camron.xyz\/ [(RSS)](http:\/\/camron.xyz\/index.php\/feed\/)\n* no free hunch http:\/\/blog.kaggle.com\/ [(RSS)](http:\/\/blog.kaggle.com\/feed\/)\n* Nuit Blanche http:\/\/nuit-blanche.blogspot.com\/ [(RSS)](http:\/\/nuit-blanche.blogspot.com\/feeds\/posts\/default)\n* Number 2147483647 https:\/\/no2147483647.wordpress.com\/ [(RSS)](http:\/\/no2147483647.wordpress.com\/feed\/)\n* On Machine Intelligence https:\/\/aimatters.wordpress.com\/ [(RSS)](https:\/\/aimatters.wordpress.com\/feed\/)\n* Opiate for the masses Data is our religion. http:\/\/opiateforthemass.es\/ [(RSS)](http:\/\/opiateforthemass.es\/feed.xml)\n* p-value.info http:\/\/www.p-value.info\/ [(RSS)](http:\/\/www.p-value.info\/feeds\/posts\/default)\n* Pete Warden's blog http:\/\/petewarden.com\/ [(RSS)](http:\/\/feeds.feedburner.com\/typepad\/petewarden)\n* Plotly Blog http:\/\/blog.plot.ly\/ [(RSS)](http:\/\/blog.plot.ly\/rss)\n* Probably Overthinking It http:\/\/allendowney.blogspot.ca\/ [(RSS)](http:\/\/allendowney.blogspot.com\/feeds\/posts\/default)\n* Prooffreader.com http:\/\/www.prooffreader.com [(RSS)](http:\/\/www.prooffreader.com\/feeds\/posts\/default)\n* ProoffreaderPlus http:\/\/prooffreaderplus.blogspot.ca\/ [(RSS)](http:\/\/prooffreaderplus.blogspot.ca\/feeds\/posts\/default)\n* Publishable Stuff http:\/\/www.sumsar.net\/ [(RSS)](http:\/\/www.sumsar.net\/atom.xml)\n* PyImageSearch http:\/\/www.pyimagesearch.com\/ [(RSS)](http:\/\/feeds.feedburner.com\/Pyimagesearch)\n* Pythonic Perambulations https:\/\/jakevdp.github.io\/ [(RSS)](http:\/\/jakevdp.github.com\/atom.xml)\n* quintuitive http:\/\/quintuitive.com\/ [(RSS)](http:\/\/quintuitive.com\/feed\/)\n* R and Data Mining https:\/\/rdatamining.wordpress.com\/ [(RSS)](http:\/\/rdatamining.wordpress.com\/feed\/)\n* R-bloggers http:\/\/www.r-bloggers.com\/ [(RSS)](http:\/\/feeds.feedburner.com\/RBloggers)\n* R2RT http:\/\/r2rt.com\/ [(RSS)](http:\/\/r2rt.com\/feeds\/all.atom.xml)\n* Ramiro G\u00f3mez http:\/\/ramiro.org\/notebooks\/ [(RSS)](http:\/\/ramiro.org\/notebook\/rss.xml)\n* Random notes on Computer Science, Mathematics and Software Engineering http:\/\/barmaley-exe.github.io\/ [(RSS)](http:\/\/feeds.feedburner.com\/barmaley-exe-blog-feed)\n* Randy Zwitch http:\/\/randyzwitch.com\/ [(RSS)](http:\/\/randyzwitch.com\/feed.xml)\n* RaRe Technologies http:\/\/rare-technologies.com\/blog\/ [(RSS)](http:\/\/rare-technologies.com\/feed\/)\n* Rayli.Net http:\/\/rayli.net\/blog\/ [(RSS)](http:\/\/rayli.net\/blog\/feed\/)\n* Revolutions http:\/\/blog.revolutionanalytics.com\/ [(RSS)](http:\/\/blog.revolutionanalytics.com\/atom.xml)\n* Rinu Boney http:\/\/rinuboney.github.io\/ [(RSS)](http:\/\/rinuboney.github.io\/feed.xml)\n* RNDuja Blog http:\/\/rnduja.github.io\/ [(RSS)](http:\/\/rnduja.github.io\/feed.xml)\n* Robert Chang https:\/\/medium.com\/@rchang [(RSS)](https:\/\/medium.com\/feed\/@rchang)\n* Rocket-Powered Data Science http:\/\/rocketdatascience.org [(RSS)](http:\/\/rocketdatascience.org\/?feed=rss2)\n* Sachin Joglekar's blog https:\/\/codesachin.wordpress.com\/ [(RSS)](https:\/\/codesachin.wordpress.com\/feed\/)\n* samim https:\/\/medium.com\/@samim [(RSS)](https:\/\/medium.com\/feed\/@samim)\n* Sean J. Taylor http:\/\/seanjtaylor.com\/ [(RSS)](http:\/\/seanjtaylor.com\/rss)\n* Sebastian Raschka http:\/\/sebastianraschka.com\/blog\/index.html [(RSS)](http:\/\/sebastianraschka.com\/rss_feed.xml)\n* Sebastian Ruder http:\/\/sebastianruder.com\/ [(RSS)](http:\/\/sebastianruder.com\/rss\/)\n* Sebastian's slow blog http:\/\/www.nowozin.net\/sebastian\/blog\/ [(RSS)](http:\/\/www.nowozin.net\/sebastian\/blog\/feeds\/all.atom.xml)\n* SFL Scientific Blog https:\/\/sflscientific.com\/blog\/ [(RSS)](http:\/\/sflscientific.com\/blog\/?format=rss)\n* Shakir's Machine Learning Blog http:\/\/blog.shakirm.com\/ [(RSS)](http:\/\/blog.shakirm.com\/feed\/)\n* Simply Statistics http:\/\/simplystatistics.org [(RSS)](http:\/\/simplystatistics.org\/feed\/)\n* Springboard Blog http:\/\/springboard.com\/blog\n* Startup.ML Blog http:\/\/startup.ml\/blog [(RSS)](http:\/\/www.startup.ml\/blog?format=RSS)\n* Statistical Modeling, Causal Inference, and Social Science http:\/\/andrewgelman.com\/ [(RSS)](http:\/\/andrewgelman.com\/feed\/)\n* Stigler Diet http:\/\/stiglerdiet.com\/ [(RSS)](http:\/\/stiglerdiet.com\/feeds\/all.atom.xml)\n* Stitch Fix Tech Blog http:\/\/multithreaded.stitchfix.com\/blog\/ [(RSS)](http:\/\/multithreaded.stitchfix.com\/feed.xml)\n* Stochastic R&D Notes http:\/\/arseny.info\/ [(RSS)](http:\/\/arseny.info\/feeds\/all.rss.xml)\n* Storytelling with Statistics on Quora http:\/\/datastories.quora.com\/ [(RSS)](http:\/\/datastories.quora.com\/rss)\n* StreamHacker http:\/\/streamhacker.com\/ [(RSS)](http:\/\/feeds.feedburner.com\/StreamHacker)\n* Subconscious Musings http:\/\/blogs.sas.com\/content\/subconsciousmusings\/ [(RSS)](http:\/\/feeds.feedburner.com\/advanalytics)\n* Swan Intelligence http:\/\/swanintelligence.com\/ [(RSS)](http:\/\/swanintelligence.com\/feeds\/all.rss.xml)\n* TechnoCalifornia http:\/\/technocalifornia.blogspot.se\/ [(RSS)](http:\/\/technocalifornia.blogspot.com\/feeds\/posts\/default)\n* TEXT ANALYSIS BLOG | AYLIEN http:\/\/blog.aylien.com\/ [(RSS)](http:\/\/blog.aylien.com\/rss)\n* The Angry Statistician http:\/\/angrystatistician.blogspot.com\/ [(RSS)](http:\/\/angrystatistician.blogspot.com\/feeds\/posts\/default)\n* The Clever Machine https:\/\/theclevermachine.wordpress.com\/ [(RSS)](http:\/\/theclevermachine.wordpress.com\/feed\/)\n* The Data Camp Blog https:\/\/www.datacamp.com\/community\/blog [(RSS)](http:\/\/blog.datacamp.com\/feed\/)\n* The Data Incubator http:\/\/blog.thedataincubator.com\/ [(RSS)](http:\/\/blog.thedataincubator.com\/feed\/)\n* The Data Science Lab https:\/\/datasciencelab.wordpress.com\/ [(RSS)](http:\/\/datasciencelab.wordpress.com\/feed\/)\n* THE ETZ-FILES http:\/\/alexanderetz.com\/ [(RSS)](http:\/\/nicebrain.wordpress.com\/feed\/)\n* The Science of Data http:\/\/www.martingoodson.com [(RSS)](http:\/\/www.martingoodson.com\/rss\/)\n* The Shape of Data https:\/\/shapeofdata.wordpress.com [(RSS)](https:\/\/shapeofdata.wordpress.com\/feed\/)\n* The unofficial Google data science Blog http:\/\/www.unofficialgoogledatascience.com\/ [(RSS)](http:\/\/www.unofficialgoogledatascience.com\/feeds\/posts\/default)\n* Tim Dettmers http:\/\/timdettmers.com\/ [(RSS)](http:\/\/timdettmers.com\/feed\/)\n* Tombone's Computer Vision Blog http:\/\/www.computervisionblog.com\/ [(RSS)](http:\/\/www.computervisionblog.com\/feeds\/posts\/default)\n* Tommy Blanchard http:\/\/tommyblanchard.com\/category\/projects [(RSS)](http:\/\/tommyblanchard.com\/feeds\/all.atom.xml)\n* Trevor Stephens http:\/\/trevorstephens.com\/ [(RSS)](http:\/\/trevorstephens.com\/feed.xml)\n* Trey Causey http:\/\/treycausey.com\/ [(RSS)](http:\/\/treycausey.com\/feeds\/all.atom.xml)\n* UW Data Science Blog http:\/\/datasciencedegree.wisconsin.edu\/blog\/ [(RSS)](http:\/\/datasciencedegree.wisconsin.edu\/feed\/)\n* Wellecks http:\/\/wellecks.wordpress.com\/ [(RSS)](http:\/\/wellecks.wordpress.com\/feed\/)\n* Wes McKinney http:\/\/wesmckinney.com\/archives.html [(RSS)](http:\/\/wesmckinney.com\/feeds\/all.atom.xml)\n* While My MCMC Gently Samples http:\/\/twiecki.github.io\/ [(RSS)](http:\/\/twiecki.github.io\/atom.xml)\n* WildML http:\/\/www.wildml.com\/ [(RSS)](http:\/\/www.wildml.com\/feed\/)\n* Will do stuff for stuff http:\/\/rinzewind.org\/blog-en [(RSS)](http:\/\/rinzewind.org\/feed-en)\n* Will wolf http:\/\/willwolf.io\/ [(RSS)](http:\/\/willwolf.io\/feed\/)\n* WILL'S NOISE http:\/\/www.willmcginnis.com\/ [(RSS)](http:\/\/www.willmcginnis.com\/feed\/)\n* William Lyon http:\/\/www.lyonwj.com\/ [(RSS)](http:\/\/www.lyonwj.com\/atom.xml)\n* Win-Vector Blog http:\/\/www.win-vector.com\/blog\/ [(RSS)](http:\/\/www.win-vector.com\/blog\/feed\/)\n* Yanir Seroussi http:\/\/yanirseroussi.com\/ [(RSS)](http:\/\/yanirseroussi.com\/feed\/)\n* Zac Stewart http:\/\/zacstewart.com\/ [(RSS)](http:\/\/zacstewart.com\/feed.xml)\n* \u0177hat http:\/\/blog.yhat.com\/ [(RSS)](http:\/\/blog.yhat.com\/rss.xml)\n* \u211auantitative \u221aourney http:\/\/outlace.com\/ [(RSS)](http:\/\/outlace.com\/feed.xml)\n* \u5927\u30c8\u30ed http:\/\/blog.otoro.net\/ [(RSS)](http:\/\/blog.otoro.net\/feed.xml)\n\n\n## credits\n\n* [Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython](http:\/\/www.amazon.com\/Python-Data-Analysis-Wrangling-IPython\/dp\/1449319793) by Wes McKinney\n* [PyCon 2015 Scikit-learn Tutorial](https:\/\/github.com\/jakevdp\/sklearn_pycon2015) by Jake VanderPlas\n* [Python Data Science Handbook](https:\/\/github.com\/jakevdp\/PythonDataScienceHandbook) by Jake VanderPlas\n* [Parallel Machine Learning with scikit-learn and IPython](https:\/\/github.com\/ogrisel\/parallel_ml_tutorial) by Olivier Grisel\n* [Statistical Interference Using Computational Methods in Python](https:\/\/github.com\/AllenDowney\/CompStats) by Allen Downey\n* [TensorFlow Examples](https:\/\/github.com\/aymericdamien\/TensorFlow-Examples) by Aymeric Damien\n* [TensorFlow Tutorials](https:\/\/github.com\/pkmital\/tensorflow_tutorials) by Parag K Mital\n* [TensorFlow Tutorials](https:\/\/github.com\/nlintz\/TensorFlow-Tutorials) by Nathan Lintz\n* [TensorFlow Tutorials](https:\/\/github.com\/alrojo\/tensorflow-tutorial) by Alexander R Johansen\n* [TensorFlow Book](https:\/\/github.com\/BinRoot\/TensorFlow-Book) by Nishant Shukla\n* [Summer School 2015](https:\/\/github.com\/mila-udem\/summerschool2015) by mila-udem\n* [Keras tutorials](https:\/\/github.com\/leriomaggio\/deep-learning-keras-tensorflow) by Valerio Maggio\n* [Kaggle](https:\/\/www.kaggle.com\/)\n* [Yhat Blog](http:\/\/blog.yhat.com\/)\n\n## contributing\n\nContributions are welcome!  For bug reports or requests please [submit an issue](https:\/\/github.com\/tarrysingh\/Machine-Learning-Tutorials\/\/issues).\n\n## contact-info\n\nFeel free to contact me to discuss any issues, questions, or comments.\n\n* Email: [tarry.singh@gmail.com](mailto:tarry.singh@gmail.com)\n* Twitter: [@tarrysingh](https:\/\/twitter.com\/tarrysingh)\n* GitHub: [tarrysingh](https:\/\/github.com\/tarrysingh.com)\n* LinkedIn: [Tarry Singh](https:\/\/www.linkedin.com\/in\/tarrysingh)\n* Website: [tarrysingh.com](https:\/\/tarrysingh.com)\n* Medium: [tarry@Medium](https:\/\/medium.com\/@tarrysingh)\n* Quora : [Answers from Tarry on Quora](https:\/\/www.quora.com\/profile\/Tarry-Singh)\n\n## license\n\nThis repository contains a variety of content; some developed by Tarry Singh and some from third-parties and a lot will be maintained by me. The third-party content is distributed under the license provided by those parties.\n\nThe content was originally developed by Donne Martin is distributed under the following license. I will be maintaining and revamping it by adding PyTorch, Torch\/Lua, MXNET and much more:\n\n*I am providing code and resources in this repository to you under an open source license.*\n\n    Copyright 2017 Tarry Singh\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n","143":"[![License: Apache 2](https:\/\/img.shields.io\/badge\/License-apache2-blue.svg?style=flat&longCache=true)](LICENSE)\n[![Polyaxon API](https:\/\/img.shields.io\/docker\/pulls\/polyaxon\/polyaxon-api)](https:\/\/hub.docker.com\/r\/polyaxon\/polyaxon-api)\n[![Slack](https:\/\/img.shields.io\/badge\/Slack-1.4k%20members-blue.svg?style=flat&logo=slack&longCache=true)](https:\/\/polyaxon.com\/slack\/)\n\n[![Docs](https:\/\/img.shields.io\/badge\/docs-stable-brightgreen.svg?style=flat&longCache=true)](https:\/\/polyaxon.com\/docs\/)\n[![Release](https:\/\/img.shields.io\/badge\/release-v1.17.2-brightgreen.svg?longCache=true)](https:\/\/polyaxon.com\/docs\/releases\/1-17\/)\n[![GitHub](https:\/\/img.shields.io\/badge\/issue_tracker-github-blue?style=flat&logo=github&longCache=true)](https:\/\/github.com\/polyaxon\/polyaxon\/issues)\n[![GitHub](https:\/\/img.shields.io\/badge\/roadmap-github-blue?style=flat&logo=github&longCache=true)](https:\/\/github.com\/orgs\/polyaxon\/projects\/5)\n\n[![CLI](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/core.yml\/badge.svg)](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/core.yml)\n[![Deploy](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/datatile.yml\/badge.svg)](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/deploy.yml)\n[![Traceml](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/traceml.yml\/badge.svg)](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/traceml.yml)\n[![Datatile](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/datatile.yml\/badge.svg)](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/datatile.yml)\n[![Platform](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/platform.yml\/badge.svg)](https:\/\/github.com\/polyaxon\/polyaxon\/actions\/workflows\/platform.yml)\n[![Codacy Badge](https:\/\/api.codacy.com\/project\/badge\/Grade\/90c05b6b112548c1a88b950beceacb69)](https:\/\/www.codacy.com\/app\/polyaxon\/polyaxon?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=polyaxon\/polyaxon&amp;utm_campaign=Badge_Grade)\n\n<br>\n<p align=\"center\">\n  <p align=\"center\">\n    <a href=\"https:\/\/polyaxon.com\/?utm_source=github&utm_medium=logo\" target=\"_blank\">\n      <img src=\"https:\/\/raw.githubusercontent.com\/polyaxon\/polyaxon\/master\/artifacts\/logo\/vector\/primary-white-default-monochrome.svg\" alt=\"polyaxon\" height=\"100\">\n    <\/a>\n  <\/p>\n  <p align=\"center\">\n    Reproduce, Automate, Scale your data science.\n  <\/p>\n<\/p>\n<br>\n\n\nWelcome to Polyaxon, a platform for building, training, and monitoring large scale deep learning applications.\nWe are making a system to solve reproducibility, automation, and scalability for machine learning applications.\n\nPolyaxon deploys into any data center, cloud provider, or can be hosted and managed by Polyaxon, and it supports all the major deep learning frameworks such as Tensorflow, MXNet, Caffe, Torch, etc.\n\nPolyaxon makes it faster, easier, and more efficient to develop deep learning applications by managing workloads with smart container and node management. And it turns GPU servers into shared, self-service resources for your team or organization.\n\n<br>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/polyaxon\/polyaxon\/master\/artifacts\/demo.gif\" alt=\"demo\" width=\"80%\">\n<\/p>\n<br>\n\n# Install\n\n#### TL;DR;\n\n* Install CLI\n\n    ```bash\n    # Install Polyaxon CLI\n    $ pip install -U polyaxon\n    ```\n\n * Create a deployment\n\n    ```bash\n    # Create a namespace\n    $ kubectl create namespace polyaxon\n\n    # Add Polyaxon charts repo\n    $ helm repo add polyaxon https:\/\/charts.polyaxon.com\n\n    # Deploy Polyaxon\n    $ polyaxon admin deploy -f config.yaml\n\n    # Access API\n    $ polyaxon port-forward\n    ```\n\nPlease check [polyaxon installation guide](https:\/\/polyaxon.com\/docs\/setup\/)\n\n# Quick start\n\n#### TL;DR;\n\n * Start a project\n\n    ```bash\n    # Create a project\n    $ polyaxon project create --name=quick-start --description='Polyaxon quick start.'\n    ```\n\n * Train and track logs & resources\n\n    ```bash\n    # Upload code and start experiments\n    $ polyaxon run -f experiment.yaml -u -l\n    ```\n\n * Dashboard\n\n    ```bash\n    # Start Polyaxon dashboard\n    $ polyaxon dashboard\n\n    Dashboard page will now open in your browser. Continue? [Y\/n]: y\n    ```\n\n<br>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/polyaxon\/polyaxon\/master\/artifacts\/compare.png\" alt=\"compare\" width=\"400\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/polyaxon\/polyaxon\/master\/artifacts\/dashboards.png\" alt=\"dashboards\" width=\"400\">\n<\/p>\n<br>\n\n * Notebook\n    ```bash\n    # Start Jupyter notebook for your project\n    $ polyaxon run --hub notebook\n    ```\n\n<br>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/polyaxon\/polyaxon\/master\/artifacts\/notebook.png\" alt=\"compare\" width=\"400\">\n<\/p>\n<br>\n\n * Tensorboard\n    ```bash\n    # Start TensorBoard for a run's output\n    $ polyaxon run --hub tensorboard -P uuid=UUID\n    ```\n\n<br>\n<p align=\"center\">\n  <img src=\"https:\/\/raw.githubusercontent.com\/polyaxon\/polyaxon\/master\/artifacts\/tensorboard.png\" alt=\"tensorboard\" width=\"400\">\n<\/p>\n<br>\n\nPlease check our [quick start guide](https:\/\/polyaxon.com\/docs\/intro\/quick-start\/) to start training your first experiment.\n\n# Distributed job\n\nPolyaxon supports and simplifies distributed jobs.\nDepending on the framework you are using, you need to deploy the corresponding operator, adapt your code to enable the distributed training,\nand update your polyaxonfile.\n\nHere are some examples of using distributed training: \n\n * [Distributed Tensorflow](https:\/\/polyaxon.com\/docs\/experimentation\/distributed\/tf-jobs\/)\n * [Distributed Pytorch](https:\/\/polyaxon.com\/docs\/experimentation\/distributed\/pytorch-jobs\/)\n * [Distributed MPI](https:\/\/polyaxon.com\/docs\/experimentation\/distributed\/mpi-jobs\/)\n * [Horovod](https:\/\/polyaxon.com\/integrations\/horovod\/)\n * [Spark](https:\/\/polyaxon.com\/docs\/experimentation\/distributed\/spark-jobs\/)\n * [Dask](https:\/\/polyaxon.com\/docs\/experimentation\/distributed\/dask-jobs\/)\n\n# Hyperparameters tuning\n\nPolyaxon has a concept for suggesting hyperparameters and managing their results very similar to Google Vizier called experiment groups.\nAn experiment group in Polyaxon defines a search algorithm, a search space, and a model to train.\n\n * [Grid search](https:\/\/polyaxon.com\/docs\/automation\/optimization-engine\/grid-search\/)\n * [Random search](https:\/\/polyaxon.com\/docs\/automation\/optimization-engine\/random-search\/)\n * [Hyperband](https:\/\/polyaxon.com\/docs\/automation\/optimization-engine\/hyperband\/)\n * [Bayesian Optimization](https:\/\/polyaxon.com\/docs\/automation\/optimization-engine\/bayesian-optimization\/)\n * [Hyperopt](https:\/\/polyaxon.com\/docs\/automation\/optimization-engine\/hyperopt\/)\n * [Custom Iterative Optimization](https:\/\/polyaxon.com\/docs\/automation\/optimization-engine\/iterative\/)\n\n# Parallel executions\n\nYou can run your processing or model training jobs in parallel, Polyaxon provides a [mapping](https:\/\/polyaxon.com\/docs\/automation\/mapping\/) abstraction to manage concurrent jobs.\n\n# DAGs and workflows\n\n[Polyaxon DAGs](https:\/\/polyaxon.com\/docs\/automation\/flow-engine\/) is a tool that provides container-native engine for running machine learning pipelines. \nA DAG manages multiple operations with dependencies. Each operation is defined by a component runtime. \nThis means that operations in a DAG can be jobs, services, distributed jobs, parallel executions, or nested DAGs.\n \n\n# Architecture\n\n![Polyaxon architecture](artifacts\/polyaxon_architecture.png)\n\n# Documentation\n\nCheck out our [documentation](https:\/\/polyaxon.com\/docs\/) to learn more about Polyaxon.\n\n# Dashboard\n\nPolyaxon comes with a dashboard that shows the projects and experiments created by you and your team members.\n\nTo start the dashboard, just run the following command in your terminal\n\n```bash\n$ polyaxon dashboard -y\n```\n\n# Project status\n\nPolyaxon is stable and it's running in production mode at many startups and Fortune 500 companies. \n\n# Contributions\n\nPlease follow the contribution guide line: *[Contribute to Polyaxon](CONTRIBUTING.md)*.\n\n\n# Research\n\nIf you use Polyaxon in your academic research, we would be grateful if you could cite it.\n\nFeel free to [contact us](mailto:contact@polyaxon.com), we would love to learn about your project and see how we can support your custom need.\n","144":"<div align=\"center\">\n  <a href=\"https:\/\/github.com\/uber\/causalml\"><img width=\"380px\" height=\"140px\" src=\"https:\/\/raw.githubusercontent.com\/uber\/causalml\/master\/docs\/_static\/img\/logo\/causalml_logo.png\"><\/a>\n<\/div>\n\n------------------------------------------------------\n\n[![PyPI Version](https:\/\/badge.fury.io\/py\/causalml.svg)](https:\/\/pypi.org\/project\/causalml\/)\n[![Build Status](https:\/\/github.com\/uber\/causalml\/actions\/workflows\/python-test.yaml\/badge.svg)](https:\/\/github.com\/uber\/causalml\/actions\/workflows\/python-test.yaml)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/causalml\/badge\/?version=latest)](http:\/\/causalml.readthedocs.io\/en\/latest\/?badge=latest)\n[![Downloads](https:\/\/pepy.tech\/badge\/causalml)](https:\/\/pepy.tech\/project\/causalml)\n[![CII Best Practices](https:\/\/bestpractices.coreinfrastructure.org\/projects\/3015\/badge)](https:\/\/bestpractices.coreinfrastructure.org\/projects\/3015)\n\n\n# Disclaimer\nThis project is stable and being incubated for long-term support. It may contain new experimental code, for which APIs are subject to change.\n\n# Causal ML: A Python Package for Uplift Modeling and Causal Inference with ML\n\n**Causal ML** is a Python package that provides a suite of uplift modeling and causal inference methods using machine learning algorithms based on recent\nresearch [[1]](#Literature). It provides a standard interface that allows user to estimate the Conditional Average Treatment Effect (CATE) or Individual Treatment\n Effect (ITE) from experimental or observational data. Essentially, it estimates the causal impact of intervention `T` on outcome `Y` for users\n with observed features `X`, without strong assumptions on the model form. Typical use cases include\n\n* **Campaign targeting optimization**: An important lever to increase ROI in an advertising campaign is to target the ad to the set of customers who will have a favorable response in a given KPI such as engagement or sales. CATE identifies these customers by estimating the effect of the KPI from ad exposure at the individual level from A\/B experiment or historical observational data.\n\n* **Personalized engagement**: A company has multiple options to interact with its customers such as different product choices in up-sell or messaging channels for communications. One can use CATE to estimate the heterogeneous treatment effect for each customer and treatment option combination for an optimal personalized recommendation system.\n\nThe package currently supports the following methods\n\n* **Tree-based algorithms**\n    * Uplift tree\/random forests on KL divergence, Euclidean Distance, and Chi-Square [[2]](#Literature)\n    * Uplift tree\/random forests on Contextual Treatment Selection [[3]](#Literature)\n    * Causal Tree [[4]](#Literature) - Work-in-progress\n* **Meta-learner algorithms**\n    * S-learner [[5]](#Literature)\n    * T-learner [[5]](#Literature)\n    * X-learner [[5]](#Literature)\n    * R-learner [[6]](#Literature)\n    * Doubly Robust (DR) learner [[7]](#Literature)\n    * TMLE learner [[8]](#Literature)\n* **Instrumental variables algorithms**\n    * 2-Stage Least Squares (2SLS)\n    * Doubly Robust (DR) IV [[9]](#Literature)\n* **Neural-network-based algorithms**\n    * CEVAE [[10]](#Literature)\n    * DragonNet [[11]](#Literature) - with `causalml[tf]` installation (see [Installation](#installation))\n\n\n# Installation\n\nInstallation with `conda` is recommended. `conda` environment files for Python 3.6, 3.7, 3.8 and 3.9 are available in the repository. To use models under the `inference.tf` module (e.g. `DragonNet`), additional dependency of `tensorflow` is required. For detailed instructions, see below.\n\n## Install using `conda`:\n### Install from `conda-forge`\nDirectly install from the conda-forge channel using conda.\n\n```sh\n$ conda install -c conda-forge causalml\n```\n\n### Install with the `conda` virtual environment\nThis will create a new `conda` virtual environment named `causalml-[tf-]py3x`, where `x` is in `[6, 7, 8, 9]`. e.g. `causalml-py37` or `causalml-tf-py38`. If you want to change the name of the environment, update the relevant YAML file in `envs\/`\n\n```\n$ git clone https:\/\/github.com\/uber\/causalml.git\n$ cd causalml\/envs\/\n$ conda env create -f environment-py38.yml\t# for the virtual environment with Python 3.8 and CausalML\n$ conda activate causalml-py38\n(causalml-py38)\n```\n\n### Install `causalml` with `tensorflow`\n```\n$ git clone https:\/\/github.com\/uber\/causalml.git\n$ cd causalml\/envs\/\n$ conda env create -f environment-tf-py38.yml\t# for the virtual environment with Python 3.8 and CausalML\n$ conda activate causalml-tf-py38\n(causalml-tf-py38) pip install -U numpy\t\t\t# this step is necessary to fix [#338](https:\/\/github.com\/uber\/causalml\/issues\/338)\n```\n\n## Install using `pip`:\n\n```\n$ git clone https:\/\/github.com\/uber\/causalml.git\n$ cd causalml\n$ pip install -r requirements.txt\n$ pip install causalml\n```\n\n### Install `causalml` with `tensorflow`\n```\n$ git clone https:\/\/github.com\/uber\/causalml.git\n$ cd causalml\n$ pip install -r requirements-tf.txt\n$ pip install causalml[tf]\n$ pip install -U numpy\t\t\t\t\t\t\t# this step is necessary to fix [#338](https:\/\/github.com\/uber\/causalml\/issues\/338)\n```\n\n## Install from source:\n\n```\n$ git clone https:\/\/github.com\/uber\/causalml.git\n$ cd causalml\n$ pip install -r requirements.txt\n$ python setup.py build_ext --inplace\n$ python setup.py install\n```\n\n\n# Quick Start\n\n## Average Treatment Effect Estimation with S, T, X, and R Learners\n\n```python\nfrom causalml.inference.meta import LRSRegressor\nfrom causalml.inference.meta import XGBTRegressor, MLPTRegressor\nfrom causalml.inference.meta import BaseXRegressor\nfrom causalml.inference.meta import BaseRRegressor\nfrom xgboost import XGBRegressor\nfrom causalml.dataset import synthetic_data\n\ny, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)\n\nlr = LRSRegressor()\nte, lb, ub = lr.estimate_ate(X, treatment, y)\nprint('Average Treatment Effect (Linear Regression): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n\nxg = XGBTRegressor(random_state=42)\nte, lb, ub = xg.estimate_ate(X, treatment, y)\nprint('Average Treatment Effect (XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n\nnn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n                 learning_rate_init=.1,\n                 early_stopping=True,\n                 random_state=42)\nte, lb, ub = nn.estimate_ate(X, treatment, y)\nprint('Average Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n\nxl = BaseXRegressor(learner=XGBRegressor(random_state=42))\nte, lb, ub = xl.estimate_ate(X, treatment, y, e)\nprint('Average Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n\nrl = BaseRRegressor(learner=XGBRegressor(random_state=42))\nte, lb, ub =  rl.estimate_ate(X=X, p=e, treatment=treatment, y=y)\nprint('Average Treatment Effect (BaseRRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n```\n\nSee the [Meta-learner example notebook](https:\/\/github.com\/uber\/causalml\/blob\/master\/examples\/meta_learners_with_synthetic_data.ipynb) for details.\n\n\n## Interpretable Causal ML\n\nCausal ML provides methods to interpret the treatment effect models trained as follows:\n\n### Meta Learner Feature Importances\n\n```python\nfrom causalml.inference.meta import BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor\nfrom causalml.dataset.regression import synthetic_data\n\n# Load synthetic data\ny, X, treatment, tau, b, e = synthetic_data(mode=1, n=10000, p=25, sigma=0.5)\nw_multi = np.array(['treatment_A' if x==1 else 'control' for x in treatment]) # customize treatment\/control names\n\nslearner = BaseSRegressor(LGBMRegressor(), control_name='control')\nslearner.estimate_ate(X, w_multi, y)\nslearner_tau = slearner.fit_predict(X, w_multi, y)\n\nmodel_tau_feature = RandomForestRegressor()  # specify model for model_tau_feature\n\nslearner.get_importance(X=X, tau=slearner_tau, model_tau_feature=model_tau_feature,\n                        normalize=True, method='auto', features=feature_names)\n\n# Using the feature_importances_ method in the base learner (LGBMRegressor() in this example)\nslearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='auto')\n\n# Using eli5's PermutationImportance\nslearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='permutation')\n\n# Using SHAP\nshap_slearner = slearner.get_shap_values(X=X, tau=slearner_tau)\n\n# Plot shap values without specifying shap_dict\nslearner.plot_shap_values(X=X, tau=slearner_tau)\n\n# Plot shap values WITH specifying shap_dict\nslearner.plot_shap_values(X=X, shap_dict=shap_slearner)\n\n# interaction_idx set to 'auto' (searches for feature with greatest approximate interaction)\nslearner.plot_shap_dependence(treatment_group='treatment_A',\n                              feature_idx=1,\n                              X=X,\n                              tau=slearner_tau,\n                              interaction_idx='auto')\n```\n<div align=\"center\">\n  <img width=\"629px\" height=\"618px\" src=\"https:\/\/raw.githubusercontent.com\/uber\/causalml\/master\/docs\/_static\/img\/shap_vis.png\">\n<\/div>\n\nSee the [feature interpretations example notebook](https:\/\/github.com\/uber\/causalml\/blob\/master\/examples\/feature_interpretations_example.ipynb) for details.\n\n### Uplift Tree Visualization\n\n```python\nfrom IPython.display import Image\nfrom causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier\nfrom causalml.inference.tree import uplift_tree_string, uplift_tree_plot\n\nuplift_model = UpliftTreeClassifier(max_depth=5, min_samples_leaf=200, min_samples_treatment=50,\n                                    n_reg=100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df[features].values,\n                 treatment=df['treatment_group_key'].values,\n                 y=df['conversion'].values)\n\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree, features)\nImage(graph.create_png())\n```\n<div align=\"center\">\n  <img width=\"800px\" height=\"479px\" src=\"https:\/\/raw.githubusercontent.com\/uber\/causalml\/master\/docs\/_static\/img\/uplift_tree_vis.png\">\n<\/div>\n\nSee the [Uplift Tree visualization example notebook](https:\/\/github.com\/uber\/causalml\/blob\/master\/examples\/uplift_tree_visualization.ipynb) for details.\n\n# Contributing\n\nWe welcome community contributors to the project. Before you start, please read our [code of conduct](https:\/\/github.com\/uber\/causalml\/blob\/master\/CODE_OF_CONDUCT.md) and check out [contributing guidelines](.\/CONTRIBUTING.md) first.\n\n\n# Versioning\n\nWe document versions and changes in our [changelog](https:\/\/github.com\/uber\/causalml\/blob\/master\/docs\/changelog.rst).\n\n\n# License\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](https:\/\/github.com\/uber\/causalml\/blob\/master\/LICENSE) file for details.\n\n\n# References\n\n## Documentation\n* [Causal ML API documentation](https:\/\/causalml.readthedocs.io\/en\/latest\/about.html)\n\n## Conference Talks and Publications by CausalML Team\n* (Talk) Introduction to CausalML at [Causal Data Science Meeting 2021](https:\/\/www.causalscience.org\/meeting\/program\/day-2\/)\n* (Talk) Introduction to CausalML at [2021 Conference on Digital Experimentation @ MIT (CODE@MIT)](https:\/\/ide.mit.edu\/events\/2021-conference-on-digital-experimentation-mit-codemit\/)\n* (Talk) Causal Inference and Machine Learning in Practice with EconML and CausalML: Industrial Use Cases at Microsoft, TripAdvisor, Uber at [KDD 2021 Tutorials](https:\/\/kdd.org\/kdd2021\/tutorials) ([website and slide links](https:\/\/causal-machine-learning.github.io\/kdd2021-tutorial\/))\n* (Publication) CausalML White Paper [Causalml: Python package for causal machine learning](https:\/\/arxiv.org\/abs\/2002.11631)\n* (Publication) [Uplift Modeling for Multiple Treatments with Cost Optimization](https:\/\/ieeexplore.ieee.org\/document\/8964199) at [2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA)](http:\/\/203.170.84.89\/~idawis33\/dsaa2019\/preliminary-program\/)\n* (Publication) [Feature Selection Methods for Uplift Modeling](https:\/\/arxiv.org\/abs\/2005.03447)\n\n## Citation\nTo cite CausalML in publications, you can refer to the following sources:\n\nWhitepaper:\n[CausalML: Python Package for Causal Machine Learning](https:\/\/arxiv.org\/abs\/2002.11631)\n\nBibtex:\n> @misc{chen2020causalml,\n>    title={CausalML: Python Package for Causal Machine Learning},\n>    author={Huigang Chen and Totte Harinen and Jeong-Yoon Lee and Mike Yung and Zhenyu Zhao},\n>    year={2020},\n>    eprint={2002.11631},\n>    archivePrefix={arXiv},\n>    primaryClass={cs.CY}\n>}\n\n\n## Literature\n\n1. Chen, Huigang, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu Zhao. \"Causalml: Python package for causal machine learning.\" arXiv preprint arXiv:2002.11631 (2020).\n2. Radcliffe, Nicholas J., and Patrick D. Surry. \"Real-world uplift modelling with significance-based uplift trees.\" White Paper TR-2011-1, Stochastic Solutions (2011): 1-33.\n3. Zhao, Yan, Xiao Fang, and David Simchi-Levi. \"Uplift modeling with multiple treatments and general response types.\" Proceedings of the 2017 SIAM International Conference on Data Mining. Society for Industrial and Applied Mathematics, 2017.\n4. Athey, Susan, and Guido Imbens. \"Recursive partitioning for heterogeneous causal effects.\" Proceedings of the National Academy of Sciences 113.27 (2016): 7353-7360.\n5. K\u00fcnzel, S\u00f6ren R., et al. \"Metalearners for estimating heterogeneous treatment effects using machine learning.\" Proceedings of the national academy of sciences 116.10 (2019): 4156-4165.\n6. Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment effects.\" arXiv preprint arXiv:1712.04912 (2017).\n7. Bang, Heejung, and James M. Robins. \"Doubly robust estimation in missing data and causal inference models.\" Biometrics 61.4 (2005): 962-973.\n8. Van Der Laan, Mark J., and Daniel Rubin. \"Targeted maximum likelihood learning.\" The international journal of biostatistics 2.1 (2006).\n9. Kennedy, Edward H. \"Optimal doubly robust estimation of heterogeneous causal effects.\" arXiv preprint arXiv:2004.14497 (2020).\n10. Louizos, Christos, et al. \"Causal effect inference with deep latent-variable models.\" arXiv preprint arXiv:1705.08821 (2017).\n11. Shi, Claudia, David M. Blei, and Victor Veitch. \"Adapting neural networks for the estimation of treatment effects.\" 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), 2019.\n12. Zhao, Zhenyu, Yumin Zhang, Totte Harinen, and Mike Yung. \"Feature Selection Methods for Uplift Modeling.\" arXiv preprint arXiv:2005.03447 (2020).\n13. Zhao, Zhenyu, and Totte Harinen. \"Uplift modeling for multiple treatments with cost optimization.\" In 2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA), pp. 422-431. IEEE, 2019.\n \n\n## Related projects\n\n* [uplift](https:\/\/cran.r-project.org\/web\/packages\/uplift\/index.html): uplift models in R\n* [grf](https:\/\/cran.r-project.org\/web\/packages\/grf\/index.html): generalized random forests that include heterogeneous treatment effect estimation in R\n* [rlearner](https:\/\/github.com\/xnie\/rlearner): A R package that implements R-Learner\n* [DoWhy](https:\/\/github.com\/Microsoft\/dowhy):  Causal inference in Python based on Judea Pearl's do-calculus\n* [EconML](https:\/\/github.com\/microsoft\/EconML): A Python package that implements heterogeneous treatment effect estimators from econometrics and machine learning methods\n","145":"# Adversarial Robustness Toolbox (ART) v1.10\n<p align=\"center\">\n  <img src=\"docs\/images\/art_lfai.png?raw=true\" width=\"467\" title=\"ART logo\">\n<\/p>\n<br \/>\n\n![Continuous Integration](https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/workflows\/Continuous%20Integration\/badge.svg)\n![CodeQL](https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/workflows\/CodeQL\/badge.svg)\n[![Documentation Status](https:\/\/readthedocs.org\/projects\/adversarial-robustness-toolbox\/badge\/?version=latest)](http:\/\/adversarial-robustness-toolbox.readthedocs.io\/en\/latest\/?badge=latest)\n[![PyPI](https:\/\/badge.fury.io\/py\/adversarial-robustness-toolbox.svg)](https:\/\/badge.fury.io\/py\/adversarial-robustness-toolbox)\n[![Language grade: Python](https:\/\/img.shields.io\/lgtm\/grade\/python\/g\/Trusted-AI\/adversarial-robustness-toolbox.svg?logo=lgtm&logoWidth=18)](https:\/\/lgtm.com\/projects\/g\/Trusted-AI\/adversarial-robustness-toolbox\/context:python)\n[![Total alerts](https:\/\/img.shields.io\/lgtm\/alerts\/g\/Trusted-AI\/adversarial-robustness-toolbox.svg?logo=lgtm&logoWidth=18)](https:\/\/lgtm.com\/projects\/g\/Trusted-AI\/adversarial-robustness-toolbox\/alerts\/)\n[![codecov](https:\/\/codecov.io\/gh\/Trusted-AI\/adversarial-robustness-toolbox\/branch\/main\/graph\/badge.svg)](https:\/\/codecov.io\/gh\/Trusted-AI\/adversarial-robustness-toolbox)\n[![Code style: black](https:\/\/img.shields.io\/badge\/code%20style-black-000000.svg)](https:\/\/github.com\/psf\/black)\n[![License: MIT](https:\/\/img.shields.io\/badge\/License-MIT-yellow.svg)](https:\/\/opensource.org\/licenses\/MIT)\n[![PyPI - Python Version](https:\/\/img.shields.io\/pypi\/pyversions\/adversarial-robustness-toolbox)](https:\/\/pypi.org\/project\/adversarial-robustness-toolbox\/)\n[![slack-img](https:\/\/img.shields.io\/badge\/chat-on%20slack-yellow.svg)](https:\/\/ibm-art.slack.com\/)\n[![Downloads](https:\/\/pepy.tech\/badge\/adversarial-robustness-toolbox)](https:\/\/pepy.tech\/project\/adversarial-robustness-toolbox)\n[![Downloads](https:\/\/pepy.tech\/badge\/adversarial-robustness-toolbox\/month)](https:\/\/pepy.tech\/project\/adversarial-robustness-toolbox)\n[![CII Best Practices](https:\/\/bestpractices.coreinfrastructure.org\/projects\/5090\/badge)](https:\/\/bestpractices.coreinfrastructure.org\/projects\/5090)\n\n\n\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u5de5\u5177\u96c6\uff08ART\uff09\u662f\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u5b89\u5168\u6027\u7684Python\u5e93\u3002ART\u63d0\u4f9b\u7684\u5de5\u5177\u53ef\n\u5e2e\u52a9\u5f00\u53d1\u4eba\u5458\u548c\u7814\u7a76\u4eba\u5458\u9488\u5bf9\u4ee5\u4e0b\u65b9\u9762\u634d\u536b\u548c\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5e94\u7528\u7a0b\u5e8f\uff1a\n\u9003\u9038\uff0c\u6570\u636e\u6c61\u67d3\uff0c\u6a21\u578b\u63d0\u53d6\u548c\u63a8\u65ad\u7684\u5bf9\u6297\u6027\u5a01\u80c1\u3002ART\u652f\u6301\u6240\u6709\u6d41\u884c\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\n\uff08TensorFlow\uff0cKeras\uff0cPyTorch\uff0cMXNet\uff0cscikit-learn\uff0cXGBoost\uff0cLightGBM\uff0cCatBoost\uff0cGPy\u7b49\uff09\uff0c\u6240\u6709\u6570\u636e\u7c7b\u578b\n\uff08\u56fe\u50cf\uff0c\u8868\u683c\uff0c\u97f3\u9891\uff0c\u89c6\u9891\u7b49\uff09\u548c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff08\u5206\u7c7b\uff0c\u7269\u4f53\u68c0\u6d4b\uff0c\u8bed\u97f3\u8bc6\u522b\uff0c\n\u751f\u6210\u6a21\u578b\uff0c\u8ba4\u8bc1\u7b49\uff09\u3002\n\n## Adversarial Threats\n\n<p align=\"center\">\n  <img src=\"docs\/images\/adversarial_threats_attacker.png?raw=true\" width=\"400\" title=\"ART logo\">\n  <img src=\"docs\/images\/adversarial_threats_art.png?raw=true\" width=\"400\" title=\"ART logo\">\n<\/p>\n<br \/>\n\n## ART for Red and Blue Teams (selection)\n\n<p align=\"center\">\n  <img src=\"docs\/images\/white_hat_blue_red.png?raw=true\" width=\"800\" title=\"ART Red and Blue Teams\">\n<\/p>\n<br \/>\n\n## \u5b66\u5230\u66f4\u591a\n\n| **[\u5f00\u59cb\u4f7f\u7528][get-started]**     | **[\u6587\u732e\u8d44\u6599][documentation]**     | **[\u8d21\u732e][contributing]**           |\n|-------------------------------------|-------------------------------|-----------------------------------|\n| - [\u5b89\u88c5][installation]<br>- [\u4f8b\u5b50](examples\/README.md)<br>- [Notebooks](notebooks\/README.md) | - [\u653b\u51fb][attacks]<br>- [\u9632\u5fa1][defences]<br>- [\u8bc4\u4f30\u5668][estimators]<br>- [\u6307\u6807][metrics]<br>- [\u6280\u672f\u6587\u6863](https:\/\/adversarial-robustness-toolbox.readthedocs.io) | - [Slack](https:\/\/ibm-art.slack.com), [\u9080\u8bf7\u51fd](https:\/\/join.slack.com\/t\/ibm-art\/shared_invite\/enQtMzkyOTkyODE4NzM4LTA4NGQ1OTMxMzFmY2Q1MzE1NWI2MmEzN2FjNGNjOGVlODVkZDE0MjA1NTA4OGVkMjVkNmQ4MTY1NmMyOGM5YTg)<br>- [\u8d21\u732e](CONTRIBUTING.md)<br>- [\u8def\u7ebf\u56fe][roadmap]<br>- [\u5f15\u7528][citing] |\n\n[get-started]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/Get-Started\n[attacks]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/ART-Attacks\n[defences]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/ART-Defences\n[estimators]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/ART-Estimators\n[metrics]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/ART-Metrics\n[contributing]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/Contributing\n[documentation]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/Documentation\n[installation]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/Get-Started#setup\n[roadmap]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/Roadmap\n[citing]: https:\/\/github.com\/Trusted-AI\/adversarial-robustness-toolbox\/wiki\/Contributing#citing-art\n\n\u8be5\u5e93\u6b63\u5728\u4e0d\u65ad\u5f00\u53d1\u4e2d\u3002\u6b22\u8fce\u53cd\u9988\uff0c\u9519\u8bef\u62a5\u544a\u548c\u8d21\u732e\uff01\n\n# \u81f4\u8c22\n\n\u672c\u6750\u6599\u90e8\u5206\u57fa\u4e8e\u56fd\u9632\u9ad8\u7ea7\u7814\u7a76\u8ba1\u5212\u5c40\uff08DARPA\uff09\u652f\u6301\u7684\u5de5\u4f5c\uff0c\u5408\u540c\u7f16\u53f7HR001120C0013\u3002\n\u672c\u6750\u6599\u4e2d\u8868\u8fbe\u7684\u4efb\u4f55\u610f\u89c1\uff0c\u53d1\u73b0\u548c\u7ed3\u8bba\u6216\u5efa\u8bae\u5747\u4e3a\u4f5c\u8005\u7684\u89c2\u70b9\uff0c\u5e76\u4e0d\u4e00\u5b9a\u53cd\u6620\u56fd\u9632\u9ad8\u7ea7\u7814\u7a76\u8ba1\u5212\u5c40\uff08DARPA\uff09\u7684\u89c2\u70b9\u3002\n","146":"","147":"<div align=\"center\">\n   <a href=\"https:\/\/hudsonthames.org\/mlfinlab\">\n   <img src=\"https:\/\/hudsonthames.org\/wp-content\/uploads\/2021\/11\/mlfinlab_github_header_v2.jpg\" width=\"100%\" \n   style=\"margin-left: auto; margin-right: auto; display:block;\">\n   \n   <\/a>\n  <\/br>\n<\/div>\n\n\n# Welcome to Machine Learning Financial Laboratory! \n\n<div align=\"center\">\n    <br>\n<\/div>\n\n>This repo is public facing and exists for the sole purpose of providing users with an easy way to raise bugs, feature requests, and other issues.\n\n<div align=\"center\">\n    <br>\n<\/div>\n\n## What is MlFinLab?\nMlFinlab python library is a perfect toolbox that every financial machine learning researcher needs. \n\nIt covers every step of the ML strategy creation, starting from data structures generation and finishing with backtest statistics.\nWe pride ourselves in the robustness of our codebase - every line of code existing in the modules is extensively tested and \ndocumented.\n\n\n## Documentation, Example Notebooks and Lecture Videos\nFor every technique present in the library we not only provide extensive documentation, with both theoretical explanations\nand detailed descriptions of available functions, but also supplement the modules with ever-growing array of lecture videos and slides \non the implemented methods.\n \nWe want you to be able to use the tools right away. To achieve that, every module comes with a number of example notebooks \nwhich include detailed examples of the usage of the algorithms. Our goal is to show you the whole pipeline, starting from \nimporting the libraries and ending with strategy performance metrics so you can get the added value from the get-go.\n\n<div align=\"left\">\n   <a href=\"https:\/\/portal.hudsonthames.org\/sign-in\">\n   <img src=\"https:\/\/hudsonthames.org\/wp-content\/uploads\/2021\/11\/purchase_mlfinlab_v2.png\" height=\"100px\" \n   style=\"margin-left: auto; margin-right: auto; display:inline-block;\">\n   <\/a>\n   <a href=\"https:\/\/hudsonthames.org\/\">\n   <img src=\"https:\/\/hudsonthames.org\/wp-content\/uploads\/2021\/11\/website_link_m.png\" height=\"100px\">\n   <\/a>\n   <a href=\"https:\/\/www.youtube.com\/channel\/UC8hI87gt0dmTAIEupEcsckA\">\n   <img src=\"https:\/\/hudsonthames.org\/wp-content\/uploads\/2021\/11\/youtube_mlfinlab.png\" height=\"100px\">\n   <\/a>\n<\/div>\n\n\n### Included modules:\n\n- Backtest Overfitting Tools\n- Data Structures\n- Labeling\n- Sampling\n- Feature Engineering\n- Models\n- Clustering\n- Cross-Validation\n- Hyper-Parameter Tuning\n- Feature Importance\n- Bet Sizing\n- Synthetic Data Generation\n- Networks\n- Measures of Codependence\n- Useful Financial Features\n\n\n## Licensing options\nThis project is licensed under an all rights reserved [licence](https:\/\/github.com\/hudson-and-thames\/mlfinlab\/blob\/master\/LICENSE.txt).\n\n* Business\n* Enterprise\n\n\n## Community\nWith the purchase of the library, our clients get access to the Hudson & Thames Slack community, where our engineers and other quants \nare always ready to answer your questions.\n\nAlternatively, you can email us at: research@hudsonthames.org.\n\n<div align=\"center\">\n   <a>\n   <img src=\"https:\/\/hudsonthames.org\/wp-content\/uploads\/2021\/11\/header_github_ht.jpg\" width=\"100%\" \n   style=\"margin-left: auto; margin-right: auto; display:block;\">\n   <\/a>\n<\/div>\n\n\n## Who is Hudson & Thames?\nHudson and Thames Quantitative Research is a company with the goal of bridging the gap between the advanced research developed in \nquantitative finance and its practical application. We have created three premium python libraries so you can effortlessly access the\nlatest techniques and focus on what matters most: **creating your own winning strategy**.\n\n\n### What was only possible with the help of huge R&D teams is now at your disposal, anywhere, anytime.\n","148":"[![Coverage Status](https:\/\/coveralls.io\/repos\/github\/kubeflow\/pipelines\/badge.svg?branch=master)](https:\/\/coveralls.io\/github\/kubeflow\/pipelines?branch=master)\n[![SDK Documentation Status](https:\/\/readthedocs.org\/projects\/kubeflow-pipelines\/badge\/?version=latest)](https:\/\/kubeflow-pipelines.readthedocs.io\/en\/stable\/?badge=latest)\n[![SDK Package version](https:\/\/img.shields.io\/pypi\/v\/kfp?color=%2334D058&label=pypi%20package)](https:\/\/pypi.org\/project\/kfp)\n[![SDK Supported Python versions](https:\/\/img.shields.io\/pypi\/pyversions\/kfp.svg?color=%2334D058)](https:\/\/pypi.org\/project\/kfp)\n\n## Overview of the Kubeflow pipelines service\n\n[Kubeflow](https:\/\/www.kubeflow.org\/) is a machine learning (ML) toolkit that is dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable.\n\n**Kubeflow pipelines** are reusable end-to-end ML workflows built using the Kubeflow Pipelines SDK.\n\nThe Kubeflow pipelines service has the following goals:\n\n* End to end orchestration: enabling and simplifying the orchestration of end to end machine learning pipelines\n* Easy experimentation: making it easy for you to try numerous ideas and techniques, and manage your various trials\/experiments.\n* Easy re-use: enabling you to re-use components and pipelines to quickly cobble together end to end solutions, without having to re-build each time.\n\n## Installation\n\n* Install Kubeflow Pipelines from choices described in [Installation Options for Kubeflow Pipelines](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/).\n\n* :star: [Alpha] Starting from Kubeflow Pipelines 1.7, try out [Emissary Executor](https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/installation\/choose-executor\/#emissary-executor). Emissary executor is Container runtime agnostic meaning you are able to run Kubeflow Pipelines on Kubernetes cluster with any [Container runtimes](https:\/\/kubernetes.io\/docs\/setup\/production-environment\/container-runtimes\/). The default Docker executor depends on Docker container runtime, which will be deprecated on Kubernetes 1.20+.\n\n## Documentation\n\nGet started with your first pipeline and read further information in the [Kubeflow Pipelines overview](https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/introduction\/).\n\nSee the various ways you can [use the Kubeflow Pipelines SDK](https:\/\/www.kubeflow.org\/docs\/pipelines\/sdk\/sdk-overview\/).\n\nSee the Kubeflow [Pipelines API doc](https:\/\/www.kubeflow.org\/docs\/pipelines\/reference\/api\/kubeflow-pipeline-api-spec\/) for API specification.\n\nConsult the [Python SDK reference docs](https:\/\/kubeflow-pipelines.readthedocs.io\/en\/stable\/) when writing pipelines using the Python SDK.\n\nRefer to the [versioning policy](.\/docs\/release\/versioning-policy.md) and [feature stages](.\/docs\/release\/feature-stages.md) documentation for more information about how we manage versions and feature stages (such as Alpha, Beta, and Stable).\n\n## Contributing to Kubeflow Pipelines\n\nBefore you start contributing to Kubeflow Pipelines, read the guidelines in [How to Contribute](.\/CONTRIBUTING.md). To learn how to build and deploy Kubeflow Pipelines from source code, read the [developer guide](.\/developer_guide.md).\n\n\n## Kubeflow Pipelines Community Meeting\n\nThe meeting is happening every other Wed 10-11AM (PST)\n[Calendar Invite](https:\/\/calendar.google.com\/event?action=TEMPLATE&tmeid=NTdoNG5uMDBtcnJlYmdlOWt1c2lkY25jdmlfMjAxOTExMTNUMTgwMDAwWiBqZXNzaWV6aHVAZ29vZ2xlLmNvbQ&tmsrc=jessiezhu%40google.com&scp=ALL) or [Join Meeting Directly](https:\/\/meet.google.com\/phd-ixfj-kcr\/)\n\n[Meeting notes](http:\/\/bit.ly\/kfp-meeting-notes)\n\n## Kubeflow Pipelines Slack Channel\n\n[#kubeflow-pipelines](https:\/\/kubeflow.slack.com)\n\n## Blog posts\n\n* [Getting started with Kubeflow Pipelines](https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/getting-started-kubeflow-pipelines) (By Amy Unruh)\n* How to create and deploy a Kubeflow Machine Learning Pipeline (By Lak Lakshmanan)\n  * [Part 1: How to create and deploy a Kubeflow Machine Learning Pipeline](https:\/\/towardsdatascience.com\/how-to-create-and-deploy-a-kubeflow-machine-learning-pipeline-part-1-efea7a4b650f)\n  * [Part 2: How to deploy Jupyter notebooks as components of a Kubeflow ML pipeline](https:\/\/towardsdatascience.com\/how-to-deploy-jupyter-notebooks-as-components-of-a-kubeflow-ml-pipeline-part-2-b1df77f4e5b3)\n  * [Part 3: How to carry out CI\/CD in Machine Learning (\u201cMLOps\u201d) using Kubeflow ML pipelines](https:\/\/medium.com\/google-cloud\/how-to-carry-out-ci-cd-in-machine-learning-mlops-using-kubeflow-ml-pipelines-part-3-bdaf68082112)\n* [Kubeflow Pipelines meets Tekton](https:\/\/developer.ibm.com\/blogs\/kubeflow-pipelines-with-tekton-and-watson\/) (By Animesh Singh)\n## Acknowledgments\n\nKubeflow pipelines uses [Argo Workflows](https:\/\/github.com\/argoproj\/argo-workflows) by default under the hood to orchestrate Kubernetes resources. The Argo community has been very supportive and we are very grateful. Additionally there is Tekton backend available as well. To access it, please refer to [Kubeflow Pipelines with Tekton repository](https:\/\/github.com\/kubeflow\/kfp-tekton).\n","149":"<!-- markdownlint-disable MD033 MD041 -->\n<h1 align=\"center\">\n    Opyrator\n<\/h1>\n\n<p align=\"center\">\n    <strong>Turns your Python functions into microservices with web API, interactive GUI, and more.<\/strong>\n<\/p>\n\n<p align=\"center\">\n    <a href=\"https:\/\/pypi.org\/project\/opyrator\/\" title=\"PyPi Version\"><img src=\"https:\/\/img.shields.io\/pypi\/v\/opyrator?color=green&style=flat\"><\/a>\n    <a href=\"https:\/\/pypi.org\/project\/opyrator\/\" title=\"Python Version\"><img src=\"https:\/\/img.shields.io\/badge\/Python-3.6%2B-blue&style=flat\"><\/a>\n    <a href=\"https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/LICENSE\" title=\"Project License\"><img src=\"https:\/\/img.shields.io\/badge\/License-MIT-green.svg\"><\/a>\n    <a href=\"https:\/\/github.com\/ml-tooling\/opyrator\/actions?query=workflow%3Abuild-pipeline\" title=\"Build status\"><img src=\"https:\/\/img.shields.io\/github\/workflow\/status\/ml-tooling\/opyrator\/build-pipeline?style=flat\"><\/a>\n    <a href=\"ttps:\/\/mltooling.substack.com\/subscribe\" title=\"Subscribe to newsletter\"><img src=\"http:\/\/bit.ly\/2Md9rxM\"><\/a>\n    <a href=\"https:\/\/twitter.com\/mltooling\" title=\"Follow on Twitter\"><img src=\"https:\/\/img.shields.io\/twitter\/follow\/mltooling.svg?style=social&label=Follow\"><\/a>\n<\/p>\n\n<p align=\"center\">\n  <a href=\"#getting-started\">Getting Started<\/a> \u2022\n  <a href=\"#features\">Features<\/a> \u2022\n  <a href=\"#examples\">Examples<\/a> \u2022\n  <a href=\"#support--feedback\">Support<\/a> \u2022\n  <a href=\"https:\/\/github.com\/ml-tooling\/opyrator\/issues\/new?labels=bug&template=01_bug-report.md\">Report a Bug<\/a> \u2022\n  <a href=\"#contribution\">Contribution<\/a> \u2022\n  <a href=\"https:\/\/github.com\/ml-tooling\/opyrator\/releases\">Changelog<\/a>\n<\/p>\n\nInstantly turn your Python functions into production-ready microservices. Deploy and access your services via HTTP API or interactive UI. Seamlessly export your services into portable, shareable, and executable files or Docker images. Opyrator builds on open standards - OpenAPI,  JSON Schema, and Python type hints - and is powered by FastAPI, Streamlit, and Pydantic. It cuts out all the pain for productizing and sharing your Python code - or anything you can wrap into a single Python function.\n\n<sup>Alpha Version: Only suggested for experimental usage.<\/sup>\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/opyrator-header.png\"\/>\n\n---\n\n<p align=\"center\">\n     Try out and explore various examples in our playground <a href=\"https:\/\/opyrator-playground.mltooling.org\">here<\/a>.\n<\/p>\n\n---\n\n## Highlights\n\n- \ud83e\ude84&nbsp; Turn functions into production-ready services within seconds.\n- \ud83d\udd0c&nbsp; Auto-generated HTTP API based on FastAPI.\n- \ud83c\udf05&nbsp; Auto-generated Web UI based on Streamlit.\n- \ud83d\udce6&nbsp; Save and share as self-contained executable file or Docker image.\n- \ud83e\udde9&nbsp; Reuse pre-defined components & combine with existing Opyrators.\n- \ud83d\udcc8&nbsp; Instantly deploy and scale for production usage.\n\n## Getting Started\n\n### Installation\n\n> _Requirements: Python 3.6+._\n\n```bash\npip install opyrator\n```\n\n### Usage\n\n1. A simple Opyrator-compatible function could look like this:\n\n    ```python\n    from pydantic import BaseModel\n\n    class Input(BaseModel):\n        message: str\n\n    class Output(BaseModel):\n        message: str\n\n    def hello_world(input: Input) -> Output:\n        \"\"\"Returns the `message` of the input data.\"\"\"\n        return Output(message=input.message)\n    ```\n\n    _\ud83d\udca1 An Opyrator-compatible function is required to have an `input` parameter and return value based on [Pydantic models](https:\/\/pydantic-docs.helpmanual.io\/). The input and output models are specified via [type hints](https:\/\/docs.python.org\/3\/library\/typing.html)._\n\n2. Copy this code to a file, e.g. `my_opyrator.py`\n3. Run the UI server from command-line:\n\n    ```bash\n    opyrator launch-ui my_opyrator:hello_world\n    ```\n\n    _In the output, there's a line that shows where your web app is being served, on your local machine._\n\n    <img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/opyrator-hello-world-ui.png\"\/>\n\n4. Run the HTTP API server from command-line:\n\n    ```bash\n    opyrator launch-api my_opyrator:hello_world\n    ```\n    _In the output, there's a line that shows where your web service is being served, on your local machine._\n\n    <img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/opyrator-hello-world-api.png\"\/>\n\n5. Find out more usage information in the [Features](#features) section or get inspired by our [examples](#examples).\n\n## Examples\n\n---\n\n<p align=\"center\">\n     \ud83d\udc49&nbsp; Try out and explore these examples in our playground <a href=\"https:\/\/opyrator-playground.mltooling.org\">here<\/a>\n<\/p>\n\n---\n\nThe following collection of examples demonstrate how Opyrator can support a variety of different tasks and use-cases. All these examples are bundled into a demo playground which you can also deploy on your own machine via Docker:\n\n```bash\ndocker run -p 8080:8080 mltooling\/opyrator-playground:latest\n```\n\n### Text Generation\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/text-generation-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/generate_text\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/generate_text_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/generate-text-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/generate_text\/\npip install -r requirements.txt\nopyrator launch-ui app:generate_text --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n### Question Answering\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/question-answering-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/question_answering\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/question_answering_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/question-answering-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/question_answering\/\npip install -r requirements.txt\nopyrator launch-ui app:question_answering --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n### Image Super Resolution\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/image-super-resolution-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/image_super_resolution\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/image_super_resolution_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/image-super-resolution-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/image_super_resolution\/\npip install -r requirements.txt\nopyrator launch-ui app:image_super_resolution --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n### Text Preprocessing\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/text-preprocessing-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/preprocess_text\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/preprocess_text_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/preprocess-text-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/preprocess_text\/\npip install -r requirements.txt\nopyrator launch-ui app:preprocess_text --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n### Language Detection\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/language-detection-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/detect_language\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/detect_language_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/detect-language-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/detect_language\/\npip install -r requirements.txt\nopyrator launch-ui app:detect_language --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n### Audio Separation\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/audio-separation-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/separate_audio\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/seperate_audio_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/separate-audio-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/separate_audio\/\npip install -r requirements.txt\nopyrator launch-ui app:separate_audio --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n### Word Vectors Training\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/train-word-vectors-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/train_word_vectors\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/train_word_vectors_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/train-word-vectors-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/train_word_vectors\/\npip install -r requirements.txt\nopyrator launch-ui app:train_word_vectors --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n### Named Entity Recognition\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/named-entity-recognition-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/named_entity_recognition\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/named_entity_recognition_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/named-entity-recognition-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/named_entity_recognition\/\npip install -r requirements.txt\nopyrator launch-ui app:named_entity_recognition --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n### Components Showcase\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/components-showcase-demo.png\"\/>\n\n- \ud83d\udcc4&nbsp; [Source Code](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/examples\/showcase_components\/app.py)\n- \ud83c\udf05&nbsp; [UI Demo](https:\/\/play.mltooling.com\/opyrator\/demos\/showcase_components_ui\/)\n- \ud83d\udd0c&nbsp; [OpenAPI Spec](https:\/\/editor.swagger.io\/?url=https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/openapi-demo-specs\/showcase-components-openapi-spec.json)\n\n<details>\n<summary>Run this demo on your machine (click to expand...)<\/summary>\n\nTo run the demo on your local machine just execute the following commands:\n\n```bash\ngit clone https:\/\/github.com\/ml-tooling\/opyrator\ncd .\/opyrator\/examples\/showcase_components\/\npip install -r requirements.txt\nopyrator launch-ui app:showcase_components --port 8051\n```\n\nVisit http:\/\/localhost:8051 in your browser to access the UI of the demo. Use `launch-api` instead of `launch-ui` to launch the HTTP API server.\n\n<\/details>\n\n## Support & Feedback\n\nThis project is maintained by [Benjamin R\u00e4thlein](https:\/\/twitter.com\/raethlein), [Lukas Masuch](https:\/\/twitter.com\/LukasMasuch), and [Jan Kalkan](https:\/\/www.linkedin.com\/in\/jan-kalkan-b5390284\/). Please understand that we won't be able to provide individual support via email. We also believe that help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n| Type                     | Channel                                              |\n| ------------------------ | ------------------------------------------------------ |\n| \ud83d\udea8&nbsp; **Bug Reports**       | <a href=\"https:\/\/github.com\/ml-tooling\/opyrator\/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3Abug+sort%3Areactions-%2B1-desc+\" title=\"Open Bug Report\"><img src=\"https:\/\/img.shields.io\/github\/issues\/ml-tooling\/opyrator\/bug.svg?label=bug\"><\/a>                                 |\n| \ud83c\udf81&nbsp; **Feature Requests**  | <a href=\"https:\/\/github.com\/ml-tooling\/opyrator\/issues?q=is%3Aopen+is%3Aissue+label%3Afeature+sort%3Areactions-%2B1-desc\" title=\"Open Feature Request\"><img src=\"https:\/\/img.shields.io\/github\/issues\/ml-tooling\/opyrator\/feature.svg?label=feature%20request\"><\/a>                                 |\n| \ud83d\udc69\u200d\ud83d\udcbb&nbsp; **Usage Questions**   |  <a href=\"https:\/\/github.com\/ml-tooling\/opyrator\/issues?q=is%3Aopen+is%3Aissue+label%3Asupport+sort%3Areactions-%2B1-desc\" title=\"Open Support Request\"> <img src=\"https:\/\/img.shields.io\/github\/issues\/ml-tooling\/opyrator\/support.svg?label=support%20request\"><\/a> <a href=\"https:\/\/gitter.im\/ml-tooling\/community\" title=\"Chat on Gitter\"><img src=\"https:\/\/badges.gitter.im\/ml-tooling\/community.svg\"><\/a> |\n| \ud83d\udce2&nbsp; **Announcements** | <a href=\"https:\/\/gitter.im\/ml-tooling\/community\" title=\"Chat on Gitter\"><img src=\"https:\/\/badges.gitter.im\/mml-tooling\/community.svg\"><\/a>  <a href=\"https:\/\/mltooling.substack.com\/subscribe\" title=\"Subscribe for updates\"><img src=\"http:\/\/bit.ly\/2Md9rxM\"><\/a> <a href=\"https:\/\/twitter.com\/mltooling\" title=\"ML Tooling on Twitter\"><img src=\"https:\/\/img.shields.io\/twitter\/follow\/mltooling.svg?style=social&label=Follow\"> |\n| \u2753&nbsp; **Other Requests** | <a href=\"mailto:team@ml-tooling.org\" title=\"Email ML Tooling Team\"><img src=\"https:\/\/img.shields.io\/badge\/email-ML Tooling-green?logo=mail.ru&logoColor=white\"><\/a> |\n\n## Features\n\n<p align=\"center\">\n  <a href=\"#http-api\">HTTP API<\/a> \u2022\n  <a href=\"#graphical-ui\">Graphical UI<\/a> \u2022\n  <a href=\"#command-line-interface\">CLI<\/a> \u2022\n  <a href=\"#zip-export\">Zip Export<\/a> \u2022\n  <a href=\"#docker-export\">Docker Export<\/a> \u2022\n  <a href=\"#pre-defined-components\">Pre-defined Components<\/a> \u2022\n  <a href=\"#production-deployment\">Production Deployment<\/a>\n<\/p>\n\n### HTTP API\n\nWith Opyrator, you can instantly launch a local HTTP (REST) API server for any [compatible function](#compatible-functions):\n\n```bash\nopyrator launch-api my_opyrator:hello_world\n```\n\nThis will launch a [FastAPI](https:\/\/fastapi.tiangolo.com\/) server based on the [OpenAPI standard](https:\/\/swagger.io\/specification) and with an automatic interactive documentation.\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/opyrator-hello-world-api.png\"\/>\n\n_\ud83d\udca1 Make sure that all requirements of your script are installed in the active Python enviornment._\n\nThe port used by the API server can be provided via CLI arguments:\n\n```bash\nopyrator launch-api my_opyrator:hello_world --port 8080\n```\n\nThe API server can also be started via the exported zip-file format (see [zip export section](#zip-export) below).\n\n```bash\nopyrator launch-api my-opyrator.zip\n```\n\n### Graphical UI\n\nYou can launch a graphical user interface - powered by  [Streamlit](https:\/\/streamlit.io\/) - for your [compatible function](#compatible-functions). The UI is auto-generated from the input- and output-schema of the given function.\n\n```bash\nopyrator launch-ui my_opyrator:hello_world\n```\n\n<img style=\"width: 100%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/opyrator-hello-world-ui.png\"\/>\n\n_\ud83d\udca1 Make sure that all requirements of your script are installed in the active Python environment._\n\nYou can influence most aspects of the UI just by changing and improving the input- and output-schema of your function. Furthermore, it is also possible to define custom UIs for the function's input and output. For more details, refer to the [input- and output-schema](#TODO) section.\n\nThe port used by the UI server can be provided via CLI arguments:\n\n```bash\nopyrator launch-ui my_opyrator:hello_world --port 8080\n```\n\nThe UI server can also be started via the exported zip-file format (see [zip export section](#zip-export) below).\n\n```bash\nopyrator launch-ui my-opyrator.zip\n```\n\nIn addition, the UI server can be started by using an already running Opyrator API endpoint:\n\n```bash\nopyrator launch-ui http:\/\/my-opyrator:8080 \n```\n\nThereby, all Opyrator calls from the UI will be executed via the configured HTTP endpoint instead of the Python function running inside the UI server.\n\n### Command-line Interface\n\nAn Opyrator can also be executed via command-line:\n\n```bash\nopyrator call my_opyrator:hello_world '{\"message\": \"hello\"}'\n```\n\n<img style=\"width: 80%\" src=\"https:\/\/raw.githubusercontent.com\/ml-tooling\/opyrator\/main\/docs\/images\/opyrator-cli.png\"\/>\n\nThe CLI interface also works using the [zip export format](#zip-export):\n\n```bash\nopyrator call my-opyrator.zip '{\"message\": \"hello\"}'\n```\n\nOr, by using an already running Opyrator API endpoint:\n\n```bash\nopyrator call http:\/\/my-opyrator:8080 '{\"message\": \"hello\"}'\n```\n\nThereby, the function call is executed by the Opyrator API server, instead of locally using the Python function.\n\n### Zip Export\n\nOpyrator allows you to package and export a [compatible function](#compatible-functions) into a self-contained zip-file:\n\n```bash\nopyrator export my_opyrator:hello_world my-opyrator.zip\n```\n\nThis exported zip-file packages relevant source code and data artifacts into a single file which can be shared, stored, and used for launching the API or UI as shown above.\n\nExternal requirements are automatically discovered from the working directory based on the following files: `Pipfile` (Pipenv environment), `environment.yml` (Conda environment), `pyproject.toml` (Poetry dependencies), `requirements.txt` (pip-requirements), `setup.py` (Python project requirements), `packages.txt` (apt-get packages), or discovered via [pipreqs](https:\/\/github.com\/bndr\/pipreqs) as fallback. However, external requirements are only included as instructions and are not packaged into the zip-file. If you want to export your Opyrator fully self-contained including all requirements or even the Python interpreter itself, please refer to the [Docker](#docker-export) or [pex](#pex-export) export options.\n\nAs a side note, Opyrators exported as zip-files are (mini) Python libraries that can be pip-installed, imported, and used from other Python code:\n\n```bash\npip install my-opyrator.zip\n```\n\n_WIP: This feature is not finalized yet. You can track the progress and vote for the feature [here](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/3)_\n\n### Docker Export\n\nIn addition to the ZIP export, Opyrator also provides the capability to export to a Docker image:\n\n```bash\nopyrator export my_opyrator:hello_world --format=docker my-opyrator-image:latest\n```\n\n_\ud83d\udca1 The Docker export requires that Docker is installed on your machine._\n\nAfter the successful export, the Docker image can be run as shown below:\n\n```bash\ndocker run -p 8080:8080 my-opyrator-image:latest\n```\n\nRunning your Opyrator within this Docker image has the advantage that only a single port is required to be exposed. The separation between UI and API is done via URL paths: `http:\/\/localhost:8080\/api` (API); `http:\/\/localhost:8080\/ui` (UI). The UI is automatically configured to use the API for all function calls.\n\n_WIP: This feature is not finalized yet. You can track the progress and vote for the feature [here](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/4)._\n\n### Pex Export\n\nOpyrator also provides the capability to export to a pex-file. [Pex](https:\/\/github.com\/pantsbuild\/pex) is a tool to create self-contained executable Python environments that contain all relevant python dependencies.\n\n```bash\nopyrator export my_opyrator:hello_world --format=pex my-opyrator.pex\n```\n\n_WIP: This feature is not finalized yet. You can track the progress and vote for the feature [here](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/5)._\n\n### Python Client\n\nEvery deployed Opyrator provides a Python client library via an endpoint method which can be installed with pip:\n\n```bash\npip install http:\/\/my-opyrator:8080\/client\n```\n\nAnd used in your code, as shown below:\n\n```python\nfrom my_opyrator import Client, Input\nopyrator_client = Client(\"http:\/\/my-opyrator:8080\")\nresult = opyrator_client.call(Input(text=\"hello\", wait=1))\n```\n\n_WIP: This feature is not finalized yet. You can track the progress and vote for the feature [here](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/8)._\n\n### Pre-defined Components\n\nOpyrator provides a growing collection of pre-defined components (input- and output models) for common tasks. Some of these components also provide more advanced UIs and Visualizations. You can reuse these components to speed up your development and, thereby, keep your Opyrators compatible with other functionality improvements or other Opyrators.\n\nYou can find some of the available interfaces in the [examples](#examples) section or in this [source code package](#TODO).\n\n_WIP: This feature is not finalized yet. You can track the progress and vote for the feature [here](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/9)._\n\n### Production Deployment\n\nRolling out your Opyrators for production usage might require additional features such as SSL, authentication, API tokens, unlimited scalability, load balancing, and monitoring. Therefore, we provide capabilities to easily  deploy your Opyrators directly on scalable and secure cloud platforms without any major overhead:\n\n```bash\nopyrator deploy my_opyrator:hello_world <deployment-provider> <deployment-provider-options>\n```\n\n_WIP: This feature is not finalized yet. You can track the progress and vote for the feature [here](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/6)._\n\n## Documentation\n\n### Compatible Functions\n\nA function is compatible with Opyrator if it fulfills the following requirements:\n\n- A single parameter called `input` which MUST be a subclass of the [Pydantic BaseModel](https:\/\/pydantic-docs.helpmanual.io\/usage\/models\/).\n- A single return value that MUST be a subclass of the [Pydantic BaseModel](https:\/\/pydantic-docs.helpmanual.io\/usage\/models\/).\n- The `input` parameter and return value MUST be annotated with Python typing hints.\n\n### Input- and Output-Schema\n\n_WIP_\n\n### Command-line Interface\n\n_WIP_\n\n\n## Contribution\n\n- Pull requests are encouraged and always welcome. Read our [contribution guidelines](https:\/\/github.com\/ml-tooling\/opyrator\/tree\/main\/CONTRIBUTING.md) and check out [help-wanted](https:\/\/github.com\/ml-tooling\/opyrator\/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A\"help+wanted\"+sort%3Areactions-%2B1-desc+) issues.\n- Submit Github issues for any [feature request and enhancement](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/new?assignees=&labels=feature&template=02_feature-request.md&title=), [bugs](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/new?assignees=&labels=bug&template=01_bug-report.md&title=), or [documentation](https:\/\/github.com\/ml-tooling\/opyrator\/issues\/new?assignees=&labels=documentation&template=03_documentation.md&title=) problems.\n- By participating in this project, you agree to abide by its [Code of Conduct](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/.github\/CODE_OF_CONDUCT.md).\n- The [development section](#development) below contains information on how to build and test the project after you have implemented some changes.\n\n## Development\n\nRefer to our [contribution guides](https:\/\/github.com\/ml-tooling\/opyrator\/blob\/main\/CONTRIBUTING.md#development-instructions) for information on our build scripts and development process.\n\n---\n\nLicensed **MIT**. Created and maintained with \u2764\ufe0f&nbsp; by developers from Berlin.\n"}}