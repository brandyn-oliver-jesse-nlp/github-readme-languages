{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afd779f-4dc1-4469-bd66-b7d024fd0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "import prepare\n",
    "import explore\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee365d-7152-45df-b634-d82bfe972ec7",
   "metadata": {},
   "source": [
    "# Github Repository Language Prediction: a Machine Learning Approach Using Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e25cc-07e1-43a2-a784-90ebe5f2c849",
   "metadata": {},
   "source": [
    "## Project Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e44c09-1f5a-48ed-a6d9-7b13616df348",
   "metadata": {},
   "source": [
    "The goal of this project was to predict the programming language used in Github repositories based on the contents of the README file. The ability to classify repositories based on the README is useful for search engine optimization and potentially for data storage strategies. Features from the README texts were built leveraging natural language processing techniques such as bag of words and tf-idf scores. These features were inputted to numerous machine learning classification models to predict the programming language based on the README text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132ba55-cb68-4853-a3b2-44d48dde830e",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc20088-78ae-43b6-8182-0b5b38ee976e",
   "metadata": {},
   "source": [
    "Data was acquired from 150 data science repositories from three different programming languages: Python, C++, and Java. The list of repositories acquired was based on performing a search for \"machine learning\" and specifying the programming language. Acquisition is accomplished using the acquire.py module and results in a dataset with 50 repositories for each language. The acquired data is also saved to a data.json cache to allow for faster subsequent loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da105a79-20d6-4bbc-9cde-e028ae48174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_repo_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b8624-4149-47e9-9f3e-a02ef74ae254",
   "metadata": {},
   "source": [
    "Data consists of the repo name, language, and raw readme_contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d944a9-a111-4b03-85b5-dcd55b6c48fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>janishar/mit-deep-learning-book-pdf</td>\n",
       "      <td>Java</td>\n",
       "      <td>[![Download](https://img.shields.io/badge/down...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angel-ML/angel</td>\n",
       "      <td>Java</td>\n",
       "      <td>![](assets/angel_logo.png)\\n\\n[![license](http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  repo language  \\\n",
       "0  janishar/mit-deep-learning-book-pdf     Java   \n",
       "1                       Angel-ML/angel     Java   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  [![Download](https://img.shields.io/badge/down...  \n",
       "1  ![](assets/angel_logo.png)\\n\\n[![license](http...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e8fd3-86d6-4617-b8e1-1e02f14f8366",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac9c54-c915-4cf7-90b3-ff7ca9851f68",
   "metadata": {},
   "source": [
    "The data was prepared by performing the following:\n",
    "- Cleaning: converting text to lowercase, normalizing the text to remove inconsistencies in unicode character encoding, and encoding strings to ASCII. Also, any text not a through z, a number, a single quote, or whitespace was removed.\n",
    "- Stemming: using the Porter method the stems of each word were extracted\n",
    "- Lemmatizing: using the WordNetLemmatizer in nltk the root word was extracted\n",
    "- Stopword removal: removing words with little or no significance to focus the exploration and modeling on words with lower frequency. Based on exploration extra words were added to the stopword removal process as these words did not add significant value to differentiating the repository's language.\n",
    "- New features were made with the word counts and overall readme length, for the original, stemmed, and lemmatized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c52223-46bc-48b1-8b57-1c09d27a8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words = ['see', 'source', 'example', 'code', 'use', '1', \"'\", ';', '&#9']\n",
    "prepared_data = prepare.prepare_df(df, column =  'readme_contents', extra_words = extra_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4c2282-1d60-451b-933e-c8d4cb17acfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>original_length</th>\n",
       "      <th>stem_length</th>\n",
       "      <th>lem_length</th>\n",
       "      <th>original_word_count</th>\n",
       "      <th>stemmed_word_count</th>\n",
       "      <th>lemmatized_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>janishar/mit-deep-learning-book-pdf</td>\n",
       "      <td>Java</td>\n",
       "      <td>[![Download](https://img.shields.io/badge/down...</td>\n",
       "      <td>downloadhttpsimgshieldsiobadgedownloadbookmark...</td>\n",
       "      <td>downloadhttpsimgshieldsiobadgedownloadbookmark...</td>\n",
       "      <td>downloadhttpsimgshieldsiobadgedownloadbookmark...</td>\n",
       "      <td>2995</td>\n",
       "      <td>1971</td>\n",
       "      <td>2112</td>\n",
       "      <td>369</td>\n",
       "      <td>212</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angel-ML/angel</td>\n",
       "      <td>Java</td>\n",
       "      <td>![](assets/angel_logo.png)\\n\\n[![license](http...</td>\n",
       "      <td>assetsangellogopng licensehttpimgshieldsiobadg...</td>\n",
       "      <td>assetsangellogopng licensehttpimgshieldsiobadg...</td>\n",
       "      <td>assetsangellogopng licensehttpimgshieldsiobadg...</td>\n",
       "      <td>7394</td>\n",
       "      <td>5235</td>\n",
       "      <td>5508</td>\n",
       "      <td>511</td>\n",
       "      <td>349</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  repo language  \\\n",
       "0  janishar/mit-deep-learning-book-pdf     Java   \n",
       "1                       Angel-ML/angel     Java   \n",
       "\n",
       "                                     readme_contents  \\\n",
       "0  [![Download](https://img.shields.io/badge/down...   \n",
       "1  ![](assets/angel_logo.png)\\n\\n[![license](http...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  downloadhttpsimgshieldsiobadgedownloadbookmark...   \n",
       "1  assetsangellogopng licensehttpimgshieldsiobadg...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  downloadhttpsimgshieldsiobadgedownloadbookmark...   \n",
       "1  assetsangellogopng licensehttpimgshieldsiobadg...   \n",
       "\n",
       "                                          lemmatized  original_length  \\\n",
       "0  downloadhttpsimgshieldsiobadgedownloadbookmark...             2995   \n",
       "1  assetsangellogopng licensehttpimgshieldsiobadg...             7394   \n",
       "\n",
       "   stem_length  lem_length  original_word_count  stemmed_word_count  \\\n",
       "0         1971        2112                  369                 212   \n",
       "1         5235        5508                  511                 349   \n",
       "\n",
       "   lemmatized_word_count  \n",
       "0                    214  \n",
       "1                    350  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e707d8-c8fd-4a08-94b6-5b3b18c4c1b5",
   "metadata": {},
   "source": [
    "The data was split into train, validate, and test sets for further exploration and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db55927-67cf-493f-8e51-1e0f51c107f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.train_validate_test_split(prepared_data, target = 'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545e4d9",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea420a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16cb13b3",
   "metadata": {},
   "source": [
    "### Question: What are the most common words in the READMEs?\n",
    "We can create a new dataframe to represent the word counts of our READMEs by using segmenting our data by category and then taking the frequency of the individual words. For the sake of our exploration this was done using our lemmatized words. The frequencies are then combined into a single dataframe, word_counts, where we can analyze the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87530787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the word_counts() function from the explore module with our train df\n",
    "word_counts = explore.word_counts(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c1556a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>java</th>\n",
       "      <th>python</th>\n",
       "      <th>c++</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>install</th>\n",
       "      <td>1123</td>\n",
       "      <td>21</td>\n",
       "      <td>1007</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codea</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detail</th>\n",
       "      <td>712</td>\n",
       "      <td>7</td>\n",
       "      <td>678</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pip</th>\n",
       "      <td>705</td>\n",
       "      <td>11</td>\n",
       "      <td>661</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>704</td>\n",
       "      <td>17</td>\n",
       "      <td>664</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          all  java  python  c++\n",
       "install  1123    21    1007   95\n",
       "codea     894     0     894    0\n",
       "detail    712     7     678   27\n",
       "pip       705    11     661   33\n",
       "open      704    17     664   23"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of seeing what words are the most frequent overall, changing the column for by='{column}' we could\n",
    "# specify which language we want to check the top frequency of words for. \n",
    "word_counts.sort_values(by='all', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cabc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
